{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d467f817-f968-4b2c-84d5-e8f8d4627666",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config (2 I-Nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eeaa4-4c27-41aa-aad8-8fd75ce0a3a7",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c0ab8e-2451-424f-8fca-f26d1a761c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_LR = {\n",
    "    'data': {\n",
    "        'n_datasets': 10_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 20, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 10,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 2,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0.01,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': None,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },    \n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 150,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '4',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576d3d5-a533-46c5-8590-3b1ab0a932c5",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5fe06c-e82f-4e2a-a23f-a717c2ce3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_DT = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'vanilla', #'SDT', 'vanilla'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 15, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        \n",
    "        'use_distribution_list': True,\n",
    "        'random_parameters_distribution': True, ##MAKEPATH DIFFERENT FILES\n",
    "        'max_distributions_per_class': 1, # None; 0; int >= 1  \n",
    "        'exclude_linearly_seperable': True,\n",
    "        'data_generation_filtering': False,\n",
    "        'fixed_class_probability': False,\n",
    "        'balanced_data': True,\n",
    "        'weighted_data_generation': False,\n",
    "        'shift_distrib': False,\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': 3, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'distribution',# 'make_classification_distribution', 'make_classification_distribution_trained', 'distribution', 'distribution_trained', 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'distrib_by_feature': True,\n",
    "        'distribution_list': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'exponential', 'beta', 'binomial', 'poisson'], \n",
    "        'distribution_list_eval': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'beta', 'poisson'],\n",
    "        \n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        'number_of_generated_datasets': 100,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "        \n",
    "        'data_noise': 0, #None or float\n",
    "        \n",
    "        'distrib_param_max': 5,\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-3,\n",
    "        'restore_best_weights': True,\n",
    "        'patience_lambda': 50,\n",
    "        \n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'use_batchnorm_lambda': False,\n",
    "        \n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 100,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        #'dense_layers': [1024, 1024, 256, 2048, 2048],\n",
    "        'dense_layers': [1792, 512, 512],\n",
    "        #'dense_layers': [1792, 512, 512],\n",
    "        \n",
    "        #'dropout': [0, 0, 0, 0, 0.3],#[0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "        'dropout': [0, 0, 0.5],\n",
    "        #'dropout': [0, 0, 0.5],\n",
    "\n",
    "        #'hidden_activation': 'relu',\n",
    "        'hidden_activation': 'sigmoid',\n",
    "        #'hidden_activation': 'swish',\n",
    "\n",
    "        #'optimizer': 'rmsprop', \n",
    "        'optimizer': 'adam', \n",
    "        #'optimizer': 'adam', \n",
    "        \n",
    "        #'learning_rate': 0.001,\n",
    "        'learning_rate': 0.001,\n",
    "        #'learning_rate': 0.001, \n",
    "        \n",
    "        'separate_weight_bias': False,\n",
    "        \n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,        \n",
    "        'additional_hidden': False,\n",
    "        \n",
    "        'loss': 'binary_crossentropy', #mse; binary_crossentropy; 'binary_accuracy'\n",
    "        'metrics': ['binary_accuracy'], #soft_ or _penalized\n",
    "        \n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 5, #Float for fraction, Int for number 0\n",
    "        'evaluate_distribution': True,\n",
    "        'force_evaluate_real_world': False,\n",
    "        \n",
    "        'function_representation_type': 5, # 1=standard representation; 2=sparse representation with classification for variables; 3=softmax to select classes (n top probabilities)\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2,3) #3=autoencoder dimensionality reduction\n",
    "        \n",
    "        'resampling_strategy': None,#'ADASYN', #'SMOTE', None\n",
    "        'resampling_threshold': 0.25,#0.2,\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 60,\n",
    "        'nas_optimizer': 'greedy' #'hyperband',#\"bayesian\",'greedy', 'random'\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        'number_of_random_evaluations_per_distribution': 10,\n",
    "        'random_evaluation_dataset_size_per_distribution': 10_000, \n",
    "        'optimize_sampling': True,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'random_evaluation_dataset_distribution': 'uniform', \n",
    "        \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        #'sklearn_dt_benchmark': False,\n",
    "        #'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': False,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 15,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ba22e7-b06d-4e66-9484-dabfca32d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_config = {\n",
    "        'n_jobs': 15,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558ba97-34f2-4429-bad1-647654431a0f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a20ce18-baf7-46be-a2a2-79238d9a1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "\n",
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c4ee38-f93f-40be-9c54-1a9005c0f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = computation_config['gpu_numbers'] if computation_config['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if computation_config['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if computation_config['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if computation_config['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5239304d-59da-4fb9-93a5-5d8c7c4613e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ca24f1-bd4c-402d-8cb5-9d8bd94ca6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_model = generate_base_model(config_DT)#generate_base_model(config_DT, disable_batchnorm=True)\n",
    "\n",
    "np.random.seed(config_DT['computation']['RANDOM_SEED'])\n",
    "        \n",
    "random_network_parameters = random_model.get_weights()\n",
    "network_parameters_structure = [network_parameter.shape for network_parameter in random_network_parameters]  \n",
    "\n",
    "\n",
    "try:\n",
    "    use_distribution_list = config_DT['data']['use_distribution_list'] if config_DT['data']['max_distributions_per_class'] is not None else False\n",
    "except:\n",
    "    use_distribution_list = False if config_DT['data']['max_distributions_per_class'] is None else True\n",
    "\n",
    "metrics = []\n",
    "loss_function = None\n",
    "\n",
    "if config_DT['i_net']['function_value_loss']:\n",
    "    if config_DT['i_net']['function_representation_type'] == 1:\n",
    "        pass\n",
    "        #metrics.append(tf.keras.losses.get('mae'))\n",
    "    if config_DT['i_net']['optimize_decision_function']:\n",
    "        loss_function = inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list)\n",
    "        #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "            #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "    else:\n",
    "        loss_function = inet_target_function_fv_loss_wrapper(config_DT)\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "else:\n",
    "    if config_DT['i_net']['function_representation_type'] >= 3:\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            \n",
    "            loss_function = inet_decision_function_fv_loss_wrapper_parameters(config_DT)\n",
    "            \n",
    "            metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "            for metric in config_DT['i_net']['metrics']:\n",
    "                metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))    \n",
    "            if False:\n",
    "                metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "                #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "                for metric in config_DT['i_net']['metrics']:\n",
    "                    metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "                    #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))                  \n",
    "    else:\n",
    "        raise SystemExit('Coefficient Loss not implemented for config_DTuration')\n",
    "    \n",
    "    if False:\n",
    "        metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            raise SystemExit('Coefficient Loss not implemented for decision function optimization')            \n",
    "        else:\n",
    "            if config_DT['i_net']['function_representation_type'] == 1:\n",
    "                loss_function = tf.keras.losses.get('mae') #inet_coefficient_loss_wrapper(inet_loss)\n",
    "            else:\n",
    "                raise SystemExit('Coefficient Loss not implemented for selected function representation')\n",
    "\n",
    "                \n",
    "# dill.dumps(loss_function)\n",
    "# dill.dumps(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7b6c-1f54-4d2d-b640-63794af75774",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7cbf86-55d4-4bb4-9d96-7b9461484129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LR_inet():\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = keras.models.load_model(path + '/modelKeras')\n",
    "    print(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb0bfcf-f020-4ddb-8abe-96bced025e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_DT_inet():\n",
    "#    model = load_inet(loss_function=dill.dumps(loss_function), metrics=dill.dumps(metrics), config=config_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9513f469-9f28-4dcc-a9ff-fcebd07df0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inet_custom():\n",
    "    \n",
    "    loss_function_local = dill.dumps(loss_function)\n",
    "    metrics_local = dill.dumps(metrics)\n",
    "    \n",
    "    path = './data/saved_models/lNetSize5000_numLNets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.001_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1792-512-512_drop0-0-0.5e500b256_adam_funcRep5_reshapeNone_depth3_beta1_decisionSpars1_vanilla_reshapeNone'    \n",
    "    model = []\n",
    "    from tensorflow.keras.utils import CustomObjectScope\n",
    "    loss_function_local = dill.loads(loss_function_local)\n",
    "    metrics_local = dill.loads(metrics_local)       \n",
    "\n",
    "    #with CustomObjectScope({'custom_loss': loss_function}):\n",
    "    custom_object_dict = {}\n",
    "    custom_object_dict[loss_function.__name__] = loss_function_local\n",
    "    for metric in  metrics_local:\n",
    "        custom_object_dict[metric.__name__] = metrics_local        \n",
    "        \n",
    "    model = tf.keras.models.load_model(path, custom_objects=custom_object_dict) # #, compile=False\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9270b5-e9bc-4979-b415-76152d44e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = load_inet_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8e007c-ed95-4adb-afe4-8340f8766374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_LR/nda10000_nsa5000_nfe20_nin10_nta2_ncc2_sep1.0_noi0.01_shuTrue_ranNone/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo150_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo1000_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1\n"
     ]
    }
   ],
   "source": [
    "model_LR = load_LR_inet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff70b789-58f8-429a-86b4-e22a4e89fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 2177)]       0           []                               \n",
      "                                                                                                  \n",
      " hidden1_1792 (Dense)           (None, 1792)         3902976     ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " activation1_sigmoid (Activatio  (None, 1792)        0           ['hidden1_1792[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " hidden2_512 (Dense)            (None, 512)          918016      ['activation1_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " activation2_sigmoid (Activatio  (None, 512)         0           ['hidden2_512[0][0]']            \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " hidden3_512 (Dense)            (None, 512)          262656      ['activation2_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " activation3_sigmoid (Activatio  (None, 512)         0           ['hidden3_512[0][0]']            \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dropout3_0.5 (Dropout)         (None, 512)          0           ['activation3_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " output_coeff_105 (Dense)       (None, 105)          53865       ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_1 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_2 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_3 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_4 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_5 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_6 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_7 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_leaf_node_8 (Dense)     (None, 8)            4104        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_combined (Concatenate)  (None, 218)          0           ['output_coeff_105[0][0]',       \n",
      "                                                                  'output_identifier_1[0][0]',    \n",
      "                                                                  'output_identifier_2[0][0]',    \n",
      "                                                                  'output_identifier_3[0][0]',    \n",
      "                                                                  'output_identifier_4[0][0]',    \n",
      "                                                                  'output_identifier_5[0][0]',    \n",
      "                                                                  'output_identifier_6[0][0]',    \n",
      "                                                                  'output_identifier_7[0][0]',    \n",
      "                                                                  'output_leaf_node_8[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,195,482\n",
      "Trainable params: 5,195,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_DT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33922ff4-e0a5-41d6-9dce-23be674d60f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1000)              8363000   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                20020     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,383,020\n",
      "Trainable params: 8,383,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_LR.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8573-c8e5-44aa-8be7-5de1d6c79eeb",
   "metadata": {},
   "source": [
    "# Load Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ed8a8-4c1d-4669-a842-441c3eaca74e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e0bf79-b725-48e6-bb3a-755f90dce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config_LR)\n",
    "\n",
    "# with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "y_coef_truth_test_data_LR = np.load(directory + '/coef_list_targetForInet.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "666b8c92-2b14-4ed6-ab88-165bbf0da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.lambda_path_LR(config_LR)\n",
    "\n",
    "# with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "x_lambda_weights_test_data_LR = np.load(directory + '/lambda_weights_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e540f4d0-d1a0-4f78-a63e-23c2ca1d9be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8362)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lambda_weights_test_data_LR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28443bc8-3a6b-4785-9fea-eceefffd30a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8362)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lambda_weights_test_data_LR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "787f1988-46ec-4bf9-9256-4931282060ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 2:\n",
    "    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], ])\n",
    "    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_targets']])\n",
    "    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], config_LR['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cef52c5-4c16-4390-9e4a-945fb9901ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config_LR)\n",
    "\n",
    "with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    X_datasets_list_LR = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    y_datasets_list_LR = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32a84f54-d361-406d-b0cc-3e7337e80428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8401008 ,  0.92280326,  2.23989907, ..., -0.55136766,\n",
       "        -0.60520431, -0.63277389],\n",
       "       [-1.2511341 ,  0.71312782,  0.32677291, ...,  0.18646706,\n",
       "        -1.57381743, -1.52085394],\n",
       "       [-0.37001888, -1.40234081, -0.18360112, ...,  0.72612712,\n",
       "        -0.85788755, -0.80915735],\n",
       "       ...,\n",
       "       [-0.12935821,  0.83558003,  0.99822303, ..., -1.62740263,\n",
       "        -1.58153914, -0.72611333],\n",
       "       [ 0.72202123,  0.53020562, -1.87553047, ...,  0.81279277,\n",
       "        -0.14161651,  0.50097359],\n",
       "       [ 0.24799997,  0.28745629,  0.86334372, ...,  0.86426244,\n",
       "         0.61354275, -3.11922997]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_datasets_list_LR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f54dd4c4-e685-4411-b8b1-cc764338929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 0., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_datasets_list_LR[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d961df0-5df2-487f-acec-7388358cddd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19b4b0b5-f797-4c58-8838-4eaec1af843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals().update(generate_paths(config_DT, path_type='lambda_net'))\n",
    "\n",
    "# directory = './data/saved_function_lists/functions_' + path_identifier_function_data + '.csv'\n",
    "\n",
    "#directory = './data/saved_function_lists/functions_lNetSize5000_numDatasets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown.csv'\n",
    "#\n",
    "#function_df = pd.read_csv(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8aa6d7cc-8204-43dd-87ba-81a9cbd00a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "893bef4e-f46c-4443-afed-df92c39c085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_data_DT = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906643c-f0be-4c45-ad6c-7798682c5e0d",
   "metadata": {},
   "source": [
    "# Evaluate Inet for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "898ad615-be9a-49e4-9a5c-5946088f5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp, tn, fn):\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3797feda-655a-420e-8ad4-c826a2edacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fp, tn, fn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37144d24-d5b0-4f55-9531-dec28cafbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp, fp, tn, fn):\n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    return 2 * (pre * rec) / (pre + rec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8b9f0d8-f719-4463-8729-6201287d62fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 5ms/step - loss: 0.0757 - mse: 0.0596 - root_mean_squared_error: 0.2441\n"
     ]
    }
   ],
   "source": [
    "score = model_LR.evaluate(x=x_lambda_weights_test_data_LR,\n",
    "    y=y_coef_truth_test_data_LR,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    sample_weight=None,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=10,\n",
    "    use_multiprocessing=True,\n",
    "    return_dict=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec0e857c-9cae-47c4-9132-e26d2cfbd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(X, y):\n",
    "    model = LogisticRegression(penalty='l2',\n",
    "        dual=False,\n",
    "        tol=0.0001,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        intercept_scaling=1,\n",
    "        class_weight=None,\n",
    "        random_state=None,\n",
    "        solver='lbfgs',\n",
    "        max_iter=100,\n",
    "        multi_class='auto',\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        n_jobs=None,\n",
    "        l1_ratio=None\n",
    "                              )\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "592923d0-f576-4549-9a6b-616fae44fd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusionMatrixAggregated_SingleSample(i):\n",
    "    x_lambda_weights = x_lambda_weights_test_data_LR[i, :]\n",
    "    y_coef_truth = y_coef_truth_test_data_LR[i, :]\n",
    "    x_lambda_weights = x_lambda_weights.reshape((1, 8362))\n",
    "    y_coef_pred = model_LR.predict(x=x_lambda_weights,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    model_groundTruth = get_LR(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = model_groundTruth.classes_\n",
    "    \n",
    "    y_coef_pred = y_coef_pred[0]\n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_coef_truth, y_coef_pred\n",
    "    )\n",
    "    \n",
    "    score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_truth_set, y_pred_set).ravel()\n",
    "    \n",
    "    return tn, fp, fn, tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8215d17d-737c-489f-9bec-071026ea10bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#parallel = Parallel(n_jobs=config_LR['n_jobs'], verbose=10, backend='loky') #loky\n",
    "##parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "#\n",
    "#confusion_Matrix_Array = parallel(delayed(confusionMatrixAggregated_SingleSample)(i) for i in range(x_lambda_weights_test_data_LR.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3e5563d-f159-4a79-8a35-3c7535f71b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = np.array(confusion_Matrix_Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1a65db1-c49f-4cfc-a898-2245d32ff2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a55c5da-82d5-424c-841d-e20353688ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "868e8e91-0de1-4f0c-ab67-a123e6d64ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.reshape([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad0728bc-d162-4c79-a614-97869e6b30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disp = ConfusionMatrixDisplay(confusion_Matrix_Array)\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52978a-c03a-4c5f-bd73-fe33945d0f0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate on arbitrary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89b5929c-62cd-4464-81df-fc4bff16ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 3:\n",
    "    y_datasets_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], ])\n",
    "    coef_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    y_datasets_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_targets']])\n",
    "    coef_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], config_LR['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ef84628-c36e-4b01-b837-ffa2d87fedee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1915487452.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [39]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def create_valid_data()\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def create_valid_data()\n",
    "    X_datasets_list_valid, y_datasets_list_valid = sklearn.datasets.make_classification(n_samples=config_LR['data']['n_samples'], \n",
    "                                                                                         n_features=config_LR['data']['n_features'],\n",
    "                                                                                         n_informative=config_LR['data']['n_informative'], \n",
    "                                                                                         n_classes=config_LR['data']['n_targets'], \n",
    "                                                                                         n_clusters_per_class=config_LR['data']['n_clusters_per_class'],\n",
    "                                                                                         weights=None,\n",
    "                                                                                         flip_y=config_LR['data']['noise'],\n",
    "                                                                                         class_sep=config_LR['data']['class_sep'],\n",
    "                                                                                         shuffle=config_LR['data']['shuffle'],\n",
    "                                                                                         random_state=config_LR['data']['random_state'])\n",
    "    model_valid = LogisticRegression()\n",
    "    model_valid.fit(X_datasets_list_valid[i], y_datasets_list_valid[i])\n",
    "    return X_datasets_list_valid, y_datasets_list_valid, model_valid.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2acf3-52fb-47b4-8be1-256f671df495",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "\n",
    "results = parallel(delayed(create_valid_data)() for i in range(config_LR['data']['n_datasets'])\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ff2f2-5e74-4f1f-9d12-cfb1bbdb75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config_LR['data']['n_datasets']):\n",
    "    X_datasets_list_valid[i] = results[i][0]\n",
    "    y_datasets_list_valid[i] = results[i][1]\n",
    "    coef_list_valid[i] = results[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feac40a-ebc2-40fa-87fa-19e1cc5c2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(config_LR['data']['n_datasets']):\n",
    "#    X_datasets_list_valid[i], y_datasets_list_valid[i] = sklearn.datasets.make_classification(n_samples=config_LR['data']['n_samples'], \n",
    "#                                                                                         n_features=config_LR['data']['n_features'],\n",
    "#                                                                                         n_informative=config_LR['data']['n_informative'], \n",
    "#                                                                                         n_classes=config_LR['data']['n_targets'], \n",
    "#                                                                                         n_clusters_per_class=config_LR['data']['n_clusters_per_class'],\n",
    "#                                                                                         weights=None,\n",
    "#                                                                                         flip_y=config_LR['data']['noise'],\n",
    "#                                                                                         class_sep=config_LR['data']['class_sep'],\n",
    "#                                                                                         shuffle=config_LR['data']['shuffle'],\n",
    "#                                                                                         random_state=config_LR['data']['random_state'])\n",
    "#    model_valid = LogisticRegression()\n",
    "#    model_valid.fit(X_datasets_list_valid[i], y_datasets_list_valid[i])\n",
    "#    coef_list_valid[i] = model_valid.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f85b77-50ca-4609-98b6-1e8ea9e48e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f13f94-f13c-4309-9ad9-f4410f0cb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSingleSampleOnValidSet(i):\n",
    "    x_lambda_weights = x_lambda_weights_test_data_LR[i, :]\n",
    "    y_coef_truth = y_coef_truth_test_data_LR[i, :]\n",
    "    \n",
    "    x_lambda_weights = x_lambda_weights.reshape((1, 8362))\n",
    "    \n",
    "    y_coef_pred = model_LR.predict(x=x_lambda_weights,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    model_groundTruth = get_LR(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = model_groundTruth.classes_\n",
    "    \n",
    "    y_coef_pred = y_coef_pred[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_coef_truth, y_coef_pred\n",
    "    )\n",
    "    \n",
    "    score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_truth_set, y_pred_set, labels=[1,0]).ravel()\n",
    "    \n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    fone = f1(tp, fp, tn, fn)\n",
    "    \n",
    "    #results.append([i, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone])\n",
    "    \n",
    "    return i+1, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8652d2-dcfe-4e1c-a229-815a6d07d8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "result_list = parallel(delayed(evaluateSingleSampleOnValidSet)(i) for i in range(x_lambda_weights_test_data_LR.shape[0]))\n",
    "                   \n",
    "    \n",
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"], data=result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249ab04-1e24-495f-98e6-4586c48ac107",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae48075-0b96-4a56-94d0-ce671d424428",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated = pd.DataFrame(results.mean(numeric_only=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7ed92-fdbe-4aff-ac6e-a5db6922c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bdc67-d6f9-4541-8b39-df29e5097315",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated.at[0, \"index_0=aggregated\"] = 0\n",
    "results = pd.concat([aggragated, results], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646255c-ceba-40a4-b978-677a3a550b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_res(df):\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = df.to_csv(path + '/evalRes.csv')\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f12ca-01c0-4c52-b80d-735f9971b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_res(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd40050-793a-4b8c-ba03-be6f1dd9865a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30b02a-be38-46f5-ae81-a57fba36dd98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
