{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89b2195-8060-4d85-8330-19aa6cb1e3ac",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a20ce18-baf7-46be-a2a2-79238d9a1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "\n",
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467f817-f968-4b2c-84d5-e8f8d4627666",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config (2 I-Nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eeaa4-4c27-41aa-aad8-8fd75ce0a3a7",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c0ab8e-2451-424f-8fca-f26d1a761c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_LR = {\n",
    "    'data': {\n",
    "        'n_datasets': 9_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 4_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 10, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 8,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 3.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 42,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.3,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae', #tf.keras.losses.get(config['lambda_net']['loss_lambda']),\n",
    "            'metrics': ['mae']\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.3,\n",
    "                'val_size': 0.2,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'eval': {\n",
    "        'n_datasets': 9_000,\n",
    "        'n_samples_train': 2000,\n",
    "        'n_samples_queryLambda': 1000, # _forLogRegBaseModel\n",
    "        'n_samples_comparison': 1000 # compare inet and basemodel\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 38,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '31',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576d3d5-a533-46c5-8590-3b1ab0a932c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5fe06c-e82f-4e2a-a23f-a717c2ce3b74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "config_DT = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'vanilla', #'SDT', 'vanilla'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 15, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        \n",
    "        'use_distribution_list': True,\n",
    "        'random_parameters_distribution': True, ##MAKEPATH DIFFERENT FILES\n",
    "        'max_distributions_per_class': 1, # None; 0; int >= 1  \n",
    "        'exclude_linearly_seperable': True,\n",
    "        'data_generation_filtering': False,\n",
    "        'fixed_class_probability': False,\n",
    "        'balanced_data': True,\n",
    "        'weighted_data_generation': False,\n",
    "        'shift_distrib': False,\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': 3, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'distribution',# 'make_classification_distribution', 'make_classification_distribution_trained', 'distribution', 'distribution_trained', 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'distrib_by_feature': True,\n",
    "        'distribution_list': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'exponential', 'beta', 'binomial', 'poisson'], \n",
    "        'distribution_list_eval': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'beta', 'poisson'],\n",
    "        \n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        'number_of_generated_datasets': 100,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "        \n",
    "        'data_noise': 0, #None or float\n",
    "        \n",
    "        'distrib_param_max': 5,\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-3,\n",
    "        'restore_best_weights': True,\n",
    "        'patience_lambda': 50,\n",
    "        \n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'use_batchnorm_lambda': False,\n",
    "        \n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 100,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        #'dense_layers': [1024, 1024, 256, 2048, 2048],\n",
    "        'dense_layers': [1792, 512, 512],\n",
    "        #'dense_layers': [1792, 512, 512],\n",
    "        \n",
    "        #'dropout': [0, 0, 0, 0, 0.3],#[0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "        'dropout': [0, 0, 0.5],\n",
    "        #'dropout': [0, 0, 0.5],\n",
    "\n",
    "        #'hidden_activation': 'relu',\n",
    "        'hidden_activation': 'sigmoid',\n",
    "        #'hidden_activation': 'swish',\n",
    "\n",
    "        #'optimizer': 'rmsprop', \n",
    "        'optimizer': 'adam', \n",
    "        #'optimizer': 'adam', \n",
    "        \n",
    "        #'learning_rate': 0.001,\n",
    "        'learning_rate': 0.001,\n",
    "        #'learning_rate': 0.001, \n",
    "        \n",
    "        'separate_weight_bias': False,\n",
    "        \n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,        \n",
    "        'additional_hidden': False,\n",
    "        \n",
    "        'loss': 'binary_crossentropy', #mse; binary_crossentropy; 'binary_accuracy'\n",
    "        'metrics': ['binary_accuracy'], #soft_ or _penalized\n",
    "        \n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 5, #Float for fraction, Int for number 0\n",
    "        'evaluate_distribution': True,\n",
    "        'force_evaluate_real_world': False,\n",
    "        \n",
    "        'function_representation_type': 5, # 1=standard representation; 2=sparse representation with classification for variables; 3=softmax to select classes (n top probabilities)\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2,3) #3=autoencoder dimensionality reduction\n",
    "        \n",
    "        'resampling_strategy': None,#'ADASYN', #'SMOTE', None\n",
    "        'resampling_threshold': 0.25,#0.2,\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 60,\n",
    "        'nas_optimizer': 'greedy' #'hyperband',#\"bayesian\",'greedy', 'random'\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        'number_of_random_evaluations_per_distribution': 10,\n",
    "        'random_evaluation_dataset_size_per_distribution': 10_000, \n",
    "        'optimize_sampling': True,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'random_evaluation_dataset_distribution': 'uniform', \n",
    "        \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        #'sklearn_dt_benchmark': False,\n",
    "        #'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': False,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 15,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ba22e7-b06d-4e66-9484-dabfca32d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_config = {\n",
    "        'n_jobs': 38,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '3',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558ba97-34f2-4429-bad1-647654431a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfa43a3-0f84-4a98-b2e6-2cdb39becb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c4ee38-f93f-40be-9c54-1a9005c0f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = computation_config['gpu_numbers'] if computation_config['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if computation_config['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if computation_config['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if computation_config['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5239304d-59da-4fb9-93a5-5d8c7c4613e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ca24f1-bd4c-402d-8cb5-9d8bd94ca6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_model = generate_base_model(config_DT)#generate_base_model(config_DT, disable_batchnorm=True)\n",
    "\n",
    "np.random.seed(config_DT['computation']['RANDOM_SEED'])\n",
    "        \n",
    "random_network_parameters = random_model.get_weights()\n",
    "network_parameters_structure = [network_parameter.shape for network_parameter in random_network_parameters]  \n",
    "\n",
    "\n",
    "try:\n",
    "    use_distribution_list = config_DT['data']['use_distribution_list'] if config_DT['data']['max_distributions_per_class'] is not None else False\n",
    "except:\n",
    "    use_distribution_list = False if config_DT['data']['max_distributions_per_class'] is None else True\n",
    "\n",
    "metrics = []\n",
    "loss_function = None\n",
    "\n",
    "if config_DT['i_net']['function_value_loss']:\n",
    "    if config_DT['i_net']['function_representation_type'] == 1:\n",
    "        pass\n",
    "        #metrics.append(tf.keras.losses.get('mae'))\n",
    "    if config_DT['i_net']['optimize_decision_function']:\n",
    "        loss_function = inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list)\n",
    "        #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "            #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "    else:\n",
    "        loss_function = inet_target_function_fv_loss_wrapper(config_DT)\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "else:\n",
    "    if config_DT['i_net']['function_representation_type'] >= 3:\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            \n",
    "            loss_function = inet_decision_function_fv_loss_wrapper_parameters(config_DT)\n",
    "            \n",
    "            metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "            for metric in config_DT['i_net']['metrics']:\n",
    "                metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))    \n",
    "            if False:\n",
    "                metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "                #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "                for metric in config_DT['i_net']['metrics']:\n",
    "                    metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "                    #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))                  \n",
    "    else:\n",
    "        raise SystemExit('Coefficient Loss not implemented for config_DTuration')\n",
    "    \n",
    "    if False:\n",
    "        metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            raise SystemExit('Coefficient Loss not implemented for decision function optimization')            \n",
    "        else:\n",
    "            if config_DT['i_net']['function_representation_type'] == 1:\n",
    "                loss_function = tf.keras.losses.get('mae') #inet_coefficient_loss_wrapper(inet_loss)\n",
    "            else:\n",
    "                raise SystemExit('Coefficient Loss not implemented for selected function representation')\n",
    "\n",
    "                \n",
    "# dill.dumps(loss_function)\n",
    "# dill.dumps(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7b6c-1f54-4d2d-b640-63794af75774",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81d71d09-8723-45d9-a164-754a9a57753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_loss(y_coef_dataset_index_true, y_coef_pred):\n",
    "    \n",
    "    #print(\"true\", type(y_coef_dataset_index_true))\n",
    "    #print(\"true\", y_coef_dataset_index_true)\n",
    "    #\n",
    "    #print(\"pred\", type(y_coef_pred))\n",
    "    #print(\"pred\", y_coef_pred)\n",
    "    #\n",
    "    #print(y_coef_dataset_index_true.flatten().shape)\n",
    "    #y_coef_dataset_index_true = y_coef_dataset_index_true.reshape([config['data']['n_targets']+1])\n",
    "    index = y_coef_dataset_index_true[:, 0]\n",
    "    y_coef_true = y_coef_dataset_index_true[:, 1:]\n",
    "    \n",
    "    #print(\"true\", type(y_coef_true))\n",
    "    #print(\"true\", y_coef_true)\n",
    "    \n",
    "    index = tf.cast(index, tf.int32)\n",
    "    #tf.print(index, output_stream=sys.stderr)\n",
    "    \n",
    "    valid_feature_data_sample = tf.gather(valid_feature_data, index)\n",
    "    \n",
    "    #y_true = tf.round(tf.matmul(valid_feature_data_sample, y_coef_true))\n",
    "    #y_pred = tf.round(tf.matmul(valid_feature_data_sample, y_coef_pred))\n",
    "    \n",
    "    y_true = tf.math.sigmoid(tf.linalg.matvec(valid_feature_data_sample, y_coef_true))\n",
    "    y_pred = tf.math.sigmoid(tf.linalg.matvec(valid_feature_data_sample, y_coef_pred))\n",
    "    \n",
    "    #tf.print(y_true, output_stream=sys.stderr)\n",
    "    #tf.print(y_pred, output_stream=sys.stderr)\n",
    "\n",
    "    #metric = tf.keras.metrics.binary_crossentropy(y_true, y_pred, from_logits=True, label_smoothing=0.0, axis=-1)\n",
    "    metric = tf.keras.losses.BinaryCrossentropy(\n",
    "                                from_logits=True,\n",
    "                                label_smoothing=0.0,\n",
    "                                axis=-1,\n",
    "                                reduction='auto',\n",
    "                                name='binary_crossentropy')\n",
    "    val = metric(y_true, y_pred)\n",
    "    #tf.print(val, output_stream=sys.stderr)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b7cbf86-55d4-4bb4-9d96-7b9461484129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LR_inet():\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = keras.models.load_model(path + '/modelKeras', custom_objects={'custom_loss': custom_loss})\n",
    "    print(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9513f469-9f28-4dcc-a9ff-fcebd07df0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_DT_inet_custom():\n",
    "    \n",
    "    loss_function_local = dill.dumps(loss_function)\n",
    "    metrics_local = dill.dumps(metrics)\n",
    "    \n",
    "    path = './data/saved_models/lNetSize5000_numLNets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.001_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1792-512-512_drop0-0-0.5e500b256_adam_funcRep5_reshapeNone_depth3_beta1_decisionSpars1_vanilla_reshapeNone'    \n",
    "    model = []\n",
    "    from tensorflow.keras.utils import CustomObjectScope\n",
    "    loss_function_local = dill.loads(loss_function_local)\n",
    "    metrics_local = dill.loads(metrics_local)       \n",
    "\n",
    "    #with CustomObjectScope({'custom_loss': loss_function}):\n",
    "    custom_object_dict = {}\n",
    "    custom_object_dict[loss_function.__name__] = loss_function_local\n",
    "    for metric in  metrics_local:\n",
    "        custom_object_dict[metric.__name__] = metrics_local        \n",
    "        \n",
    "    model = tf.keras.models.load_model(path, custom_objects=custom_object_dict) # #, compile=False\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9270b5-e9bc-4979-b415-76152d44e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = load_DT_inet_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e8e007c-ed95-4adb-afe4-8340f8766374",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_LR \u001b[38;5;241m=\u001b[39m \u001b[43mload_LR_inet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mload_LR_inet\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_LR_inet\u001b[39m():\n\u001b[1;32m      2\u001b[0m     path \u001b[38;5;241m=\u001b[39m utilities_LR\u001b[38;5;241m.\u001b[39minet_path_LR(config_LR)\n\u001b[0;32m----> 4\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/modelKeras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustom_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/saving/save.py:207\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m--> 207\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaved_model_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/saving/saved_model/load.py:141\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_id, loaded_node \u001b[38;5;129;01min\u001b[39;00m keras_loader\u001b[38;5;241m.\u001b[39mloaded_nodes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    140\u001b[0m   nodes_to_load[keras_loader\u001b[38;5;241m.\u001b[39mget_path(node_id)] \u001b[38;5;241m=\u001b[39m loaded_node\n\u001b[0;32m--> 141\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m keras_loader\u001b[38;5;241m.\u001b[39mfinalize_objects()\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:842\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.saved_model.load_partial\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_partial\u001b[39m(export_dir, filters, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    749\u001b[0m   \u001b[38;5;124;03m\"\"\"Partially load a SavedModel (saved from V2).\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \n\u001b[1;32m    751\u001b[0m \u001b[38;5;124;03m  Similar to `tf.saved_model.load`, but with an additional argument that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;124;03m    A dictionary mapping node paths from the filter to loaded objects.\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:974\u001b[0m, in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[1;32m    973\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mloader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_graph_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_model_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mckpt_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    979\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:187\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_all()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_options\u001b[38;5;241m.\u001b[39mexperimental_skip_checkpoint:\n\u001b[0;32m--> 187\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes:\n\u001b[1;32m    189\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, tracking\u001b[38;5;241m.\u001b[39mCapturableResource):\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:560\u001b[0m, in \u001b[0;36mLoader._restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m   load_status\u001b[38;5;241m.\u001b[39massert_nontrivial_match()\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m   load_status \u001b[38;5;241m=\u001b[39m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkpoint_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   load_status\u001b[38;5;241m.\u001b[39massert_existing_objects_matched()\n\u001b[1;32m    562\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m load_status\u001b[38;5;241m.\u001b[39m_checkpoint\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/tensorflow/python/training/tracking/util.py:1351\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1349\u001b[0m   dtype_map \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mget_variable_to_dtype_map()\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1351\u001b[0m   object_graph_string \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOBJECT_GRAPH_PROTO_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError:\n\u001b[1;32m   1353\u001b[0m   \u001b[38;5;66;03m# The object graph proto does not exist in this checkpoint. Try the\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m   \u001b[38;5;66;03m# name-based compatibility mode.\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m   restore_coordinator \u001b[38;5;241m=\u001b[39m _NameBasedRestoreCoordinator(\n\u001b[1;32m   1356\u001b[0m       save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m   1357\u001b[0m       dtype_map\u001b[38;5;241m=\u001b[39mdtype_map)\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py:66\u001b[0m, in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"Get the tensor from the Checkpoint object.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointReader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCheckpointReader_GetTensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_LR = load_LR_inet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70b789-58f8-429a-86b4-e22a4e89fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8573-c8e5-44aa-8be7-5de1d6c79eeb",
   "metadata": {},
   "source": [
    "# Load Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ed8a8-4c1d-4669-a842-441c3eaca74e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0bf79-b725-48e6-bb3a-755f90dce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.data_path_LR(config_LR)\n",
    "#\n",
    "## with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "#y_coef_truth_test_data_LR = np.load(directory + '/coef_list_targetForInet.npy', allow_pickle=True)\n",
    "#\n",
    "\n",
    "#y_coef_truth_test_data_LR =  np.load(utilities_LR.lambda_path_LR(config_LR) + '/lambda_generated_coef_list_target_for_inet.npy') ## get coef from lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b8c92-2b14-4ed6-ab88-165bbf0da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.lambda_path_LR(config_LR)\n",
    "#\n",
    "##with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "#x_lambda_weights_test_data_LR = np.load(directory + '/lambda_weights_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f1988-46ec-4bf9-9256-4931282060ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_features']])\n",
    "#\n",
    "#if  config_LR['data']['n_targets'] < 2:\n",
    "#    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], ])\n",
    "#    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "#else:\n",
    "#    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_targets']])\n",
    "#    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], config_LR['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef52c5-4c16-4390-9e4a-945fb9901ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config_LR)\n",
    "\n",
    "with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    X_datasets_list_LR_test = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    y_datasets_list_LR_test = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d961df0-5df2-487f-acec-7388358cddd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4b0b5-f797-4c58-8838-4eaec1af843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals().update(generate_paths(config_DT, path_type='lambda_net'))\n",
    "\n",
    "# directory = './data/saved_function_lists/functions_' + path_identifier_function_data + '.csv'\n",
    "\n",
    "#directory = './data/saved_function_lists/functions_lNetSize5000_numDatasets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown.csv'\n",
    "#\n",
    "#function_df = pd.read_csv(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6d7cc-8204-43dd-87ba-81a9cbd00a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893bef4e-f46c-4443-afed-df92c39c085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_data_DT = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906643c-f0be-4c45-ad6c-7798682c5e0d",
   "metadata": {},
   "source": [
    "# Evaluate Inet for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ad615-be9a-49e4-9a5c-5946088f5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp, tn, fn):\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797feda-655a-420e-8ad4-c826a2edacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fp, tn, fn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37144d24-d5b0-4f55-9531-dec28cafbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp, fp, tn, fn):\n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    return 2 * (pre * rec) / (pre + rec) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6d452-5dc6-4555-9be5-69cd4495a575",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation on already known data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aee75d-a445-4460-bee6-de0f06684bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_weights = np.zeros([config_LR['eval']['n_datasets'], 8301, ])\n",
    "\n",
    "X_queryLambda_test = np.zeros([config_LR['eval']['n_datasets'], config_LR['eval']['n_samples_queryLambda'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 2:\n",
    "    y_test_coefs = np.zeros([config_LR['eval']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    print(\"#################### NOT YET IMPLEMENTED ######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7855e-b7a0-450e-8006-fc2ac6b40b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                        random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                        shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                        stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "                                                    y_test, \n",
    "                                                    test_size=config_LR['lambda']['data_prep']['train_test_val_split']['val_size'] / (config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size']), \n",
    "                                                    random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43f096-bdec-49cf-94ba-1a4d32de87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(X, y):\n",
    "    model = LogisticRegression(penalty='l2',\n",
    "        dual=False,\n",
    "        tol=0.0001,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        intercept_scaling=1,\n",
    "        class_weight=None,\n",
    "        random_state=None,\n",
    "        solver='lbfgs',\n",
    "        max_iter=100,\n",
    "        multi_class='auto',\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        n_jobs=None,\n",
    "        l1_ratio=None\n",
    "                              )\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba2c93-14c6-4019-833c-993d867c6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y):\n",
    "    # Data Prep\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "                                                                          y)\n",
    "    \n",
    "    # Model Def\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config_LR['data']['n_features']))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config_LR['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=config_LR['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config_LR['lambda']['model_compile']['loss'],\n",
    "                  metrics=config_LR['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    # Model fit\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=config_LR['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config_LR['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config_LR['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=config_LR['lambda']['model_fit']['callbacks'],\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        shuffle=config_LR['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config_LR['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config_LR['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config_LR['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config_LR['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config_LR['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config_LR['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config_LR['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d700d-d8ef-4614-b06c-c0cd5b9376ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_reg(i):\n",
    "    \n",
    "    ### TRAIN LAMBDA\n",
    "    \n",
    "    X = X_datasets_list_LR_test[i]\n",
    "    y = y_datasets_list_LR_test[i]\n",
    "    \n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=config_LR['eval']['n_samples_queryLambda'] + config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    X_queryLambda, X_comparison, _, _ = train_test_split(X_temp, y_temp, test_size=config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    \n",
    "    model_lambda = train_nn(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    ### GET COEF FROM LAMBDA\n",
    "    \n",
    "    y_pred = model_lambda.predict(X_queryLambda)\n",
    "    \n",
    "    y_pred = [1.0 if y>=0.5 else 0.0 for y in y_pred]\n",
    "    \n",
    "    log_reg_test = get_LR(X_queryLambda, y_pred)\n",
    "    \n",
    "    return np.concatenate([x.flatten() for x in model_lambda.get_weights()]), log_reg_test.coef_, X_queryLambda ## test_weights, test_coef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295fb1a-0211-4e01-8ceb-c7facc5f5218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "#parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "if config_LR['eval']['n_datasets'] > config_LR['data']['n_datasets']:\n",
    "    print(\"Error: eval-n_datasets > data-n_datasets\")\n",
    "\n",
    "weights_coef = parallel(delayed(create_test_reg)(i) for i in range(config_LR['eval']['n_datasets']))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c946e-48db-4d9c-8332-e859e51ab71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config_LR['eval']['n_datasets']):\n",
    "    X_test_weights[i] = weights_coef[i][0]\n",
    "    y_test_coefs[i] = weights_coef[i][1]\n",
    "    X_queryLambda_test[i] = weights_coef[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a828cb-f716-425f-b30a-d02549f939ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSingleSampleOnTestSet(i):\n",
    "    X_test_weight = X_test_weights[i, :]\n",
    "    y_test_coef = y_test_coefs[i, :]\n",
    "    X_queryLambda_test_instance = X_queryLambda_test[i, :]\n",
    "    \n",
    "    \n",
    "    X_test_weight = X_test_weight.reshape((1, 8301))\n",
    "    \n",
    "    y_test_coef_pred = model_LR.predict(x=X_test_weight,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    y_test_coef = y_test_coef.reshape([1, 20])\n",
    "    \n",
    "    model_truth = LogisticRegression()\n",
    "    model_truth.coef_ = y_test_coef\n",
    "    model_truth.intercept_ = 0\n",
    "    model_truth.classes_ = np.array([0, 1])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_test_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = np.array([0, 1])\n",
    "    \n",
    "    y_class_truth = model_truth.predict(X_queryLambda_test_instance)\n",
    "    y_class_pred = model_pred.predict(X_queryLambda_test_instance)\n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_class_truth, y_class_pred\n",
    "    )\n",
    "    \n",
    "    #score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    #y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_class_truth, y_class_pred, labels=[1,0]).ravel()\n",
    "    \n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    fone = f1(tp, fp, tn, fn)\n",
    "    \n",
    "    #results.append([i, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone])\n",
    "    \n",
    "    return i+1, -1, -1, mse, tp, fn, fp, tn, pre, rec, fone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae944e-35e5-48db-b8bb-ffcaddf88f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if os.path.isdir(\"/tmp\") :\n",
    "#    os.system('rm -R /tmp/*')\n",
    "#\n",
    "#os.environ['JOBLIB_TEMP_FOLDER'] = '/tmp'\n",
    "\n",
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], \n",
    "                    verbose=10, backend='loky', \n",
    "                    timeout=None,\n",
    "                    pre_dispatch='2 * n_jobs',\n",
    "                    batch_size='auto',\n",
    "                    temp_folder='/dev/shm',\n",
    "                    max_nbytes='25M',\n",
    "                    mmap_mode='r',\n",
    "                    prefer=None,\n",
    "                    require=None) #loky\n",
    "#parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "result_list = parallel(delayed(evaluateSingleSampleOnTestSet)(i) for i in range(config_LR['eval']['n_datasets']))\n",
    "                   \n",
    "    \n",
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"], data=result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25785450-52b2-4ca0-bd44-8b21d828fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_res_test(df):\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = df.to_csv(path + '/evalRes_test.csv')\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f24f4-43ab-47fb-b2c9-338fb03779c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_res_test(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52978a-c03a-4c5f-bd73-fe33945d0f0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate on arbitrary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5929c-62cd-4464-81df-fc4bff16ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_weights = np.zeros([config_LR['eval']['n_datasets'], 8301, ])\n",
    "\n",
    "X_queryLambda = np.zeros([config_LR['eval']['n_datasets'], config_LR['eval']['n_samples_queryLambda'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 2:\n",
    "    y_valid_coefs = np.zeros([config_LR['eval']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    print(\"#################### NOT YET IMPLEMENTED ######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e857c-9cae-47c4-9132-e26d2cfbd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(X, y):\n",
    "    model = LogisticRegression(penalty='l2',\n",
    "        dual=False,\n",
    "        tol=0.0001,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        intercept_scaling=1,\n",
    "        class_weight=None,\n",
    "        random_state=None,\n",
    "        solver='lbfgs',\n",
    "        max_iter=100,\n",
    "        multi_class='auto',\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        n_jobs=None,\n",
    "        l1_ratio=None\n",
    "                              )\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60586aeb-321d-4305-8a17-18d69d503556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                        random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                        shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                        stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "                                                    y_test, \n",
    "                                                    test_size=config_LR['lambda']['data_prep']['train_test_val_split']['val_size'] / (config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size']), \n",
    "                                                    random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b54a5e-afa4-4295-815e-279a67ba694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y):\n",
    "    # Data Prep\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "                                                                          y)\n",
    "    \n",
    "    # Model Def\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config_LR['data']['n_features']))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config_LR['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=config_LR['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config_LR['lambda']['model_compile']['loss'],\n",
    "                  metrics=config_LR['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    # Model fit\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=config_LR['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config_LR['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config_LR['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=config_LR['lambda']['model_fit']['callbacks'],\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        shuffle=config_LR['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config_LR['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config_LR['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config_LR['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config_LR['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config_LR['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config_LR['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config_LR['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef84628-c36e-4b01-b837-ffa2d87fedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid_reg():\n",
    "    \n",
    "    ### TRAIN LAMBDA\n",
    "    \n",
    "    X, y =  sklearn.datasets.make_classification(n_samples=config_LR['eval']['n_samples_train'] + config_LR['eval']['n_samples_queryLambda'] + config_LR['eval']['n_samples_comparison'], \n",
    "                                                                                         n_features=config_LR['data']['n_features'],\n",
    "                                                                                         n_informative=config_LR['data']['n_informative'],\n",
    "                                                                                         n_redundant=config_LR['data']['n_features']-config_LR['data']['n_informative'],\n",
    "                                                                                         n_repeated=0,\n",
    "                                                                                         n_classes=config_LR['data']['n_targets']+1, \n",
    "                                                                                         n_clusters_per_class=config_LR['data']['n_clusters_per_class'],\n",
    "                                                                                         weights=None,\n",
    "                                                                                         flip_y=config_LR['data']['noise'],\n",
    "                                                                                         class_sep=config_LR['data']['class_sep'],\n",
    "                                                                                         shuffle=config_LR['data']['shuffle'],\n",
    "                                                                                         random_state=config_LR['data']['random_state'])\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=config_LR['eval']['n_samples_queryLambda'] + config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    X_queryLambda, X_comparison, _, _ = train_test_split(X_temp, y_temp, test_size=config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    \n",
    "    model_lambda = train_nn(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    ### GET COEF FROM LAMBDA\n",
    "    \n",
    "    y_pred = model_lambda.predict(X_queryLambda)\n",
    "    \n",
    "    y_pred = [1.0 if y>=0.5 else 0.0 for y in y_pred]\n",
    "    \n",
    "    log_reg_test = get_LR(X_queryLambda, y_pred)\n",
    "    \n",
    "    return np.concatenate([x.flatten() for x in model_lambda.get_weights()]), log_reg_test.coef_, X_queryLambda ## test_weights, test_coef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2acf3-52fb-47b4-8be1-256f671df495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "#parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "\n",
    "weights_coef = parallel(delayed(create_valid_reg)() for i in range(config_LR['eval']['n_datasets']))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ff2f2-5e74-4f1f-9d12-cfb1bbdb75fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(config_LR['eval']['n_datasets']):\n",
    "    X_valid_weights[i] = weights_coef[i][0]\n",
    "    y_valid_coefs[i] = weights_coef[i][1]\n",
    "    X_queryLambda[i] = weights_coef[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ce7a5-10b9-45dd-b815-fd29797ba890",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27034654-f43b-4460-a45a-1c60df10e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339ad59-d878-45b9-b95d-1c95c1df405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_queryLambda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f13f94-f13c-4309-9ad9-f4410f0cb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSingleSampleOnValidSet(i):\n",
    "    X_valid_weight = X_valid_weights[i, :]\n",
    "    y_valid_coef = y_valid_coefs[i, :]\n",
    "    X_queryLambda_instance = X_queryLambda[i, :]\n",
    "    \n",
    "    \n",
    "    X_valid_weight = X_valid_weight.reshape((1, 8301))\n",
    "    \n",
    "    y_valid_coef_pred = model_LR.predict(x=X_valid_weight,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    y_valid_coef = y_valid_coef.reshape([1, 20])\n",
    "    \n",
    "    model_truth = LogisticRegression()\n",
    "    model_truth.coef_ = y_valid_coef\n",
    "    model_truth.intercept_ = 0\n",
    "    model_truth.classes_ = np.array([0, 1])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_valid_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = np.array([0, 1])\n",
    "    \n",
    "    y_class_truth = model_truth.predict(X_queryLambda_instance)\n",
    "    y_class_pred = model_pred.predict(X_queryLambda_instance)\n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_class_truth, y_class_pred\n",
    "    )\n",
    "    \n",
    "    #score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    #y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_class_truth, y_class_pred, labels=[1,0]).ravel()\n",
    "    \n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    fone = f1(tp, fp, tn, fn)\n",
    "    \n",
    "    #results.append([i, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone])\n",
    "    \n",
    "    return i+1, -1, -1, mse, tp, fn, fp, tn, pre, rec, fone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8652d2-dcfe-4e1c-a229-815a6d07d8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "result_list = parallel(delayed(evaluateSingleSampleOnValidSet)(i) for i in range(config_LR['eval']['n_datasets']))\n",
    "                   \n",
    "    \n",
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"], data=result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249ab04-1e24-495f-98e6-4586c48ac107",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae48075-0b96-4a56-94d0-ce671d424428",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated = pd.DataFrame(results.mean(numeric_only=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7ed92-fdbe-4aff-ac6e-a5db6922c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bdc67-d6f9-4541-8b39-df29e5097315",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated.at[0, \"index_0=aggregated\"] = 0\n",
    "results = pd.concat([aggragated, results], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646255c-ceba-40a4-b978-677a3a550b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_res_valid(df):\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = df.to_csv(path + '/evalRes_valid.csv')\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f12ca-01c0-4c52-b80d-735f9971b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_res_valid(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0a389-1b69-4533-a1a0-57df79d62fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d7dfe-46b9-404f-8b4f-6691fd4a36b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
