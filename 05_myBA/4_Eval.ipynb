{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89b2195-8060-4d85-8330-19aa6cb1e3ac",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a20ce18-baf7-46be-a2a2-79238d9a1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "\n",
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467f817-f968-4b2c-84d5-e8f8d4627666",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config (2 I-Nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eeaa4-4c27-41aa-aad8-8fd75ce0a3a7",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c0ab8e-2451-424f-8fca-f26d1a761c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_LR = {\n",
    "    'data': {\n",
    "        'n_datasets': 10, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 20, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 10,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0.01,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': None,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },    \n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae', #tf.keras.losses.get(config['lambda_net']['loss_lambda']),\n",
    "            'metrics': ['mae']\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 150,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'eval': {\n",
    "        'n_datasets': 10,\n",
    "        'n_samples_train': 5_000,\n",
    "        'n_samples_queryLambda': 4_500, # _forLogRegBaseModel\n",
    "        'n_samples_comparison': 4_000 # compare inet and basemodel\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '4',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576d3d5-a533-46c5-8590-3b1ab0a932c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5fe06c-e82f-4e2a-a23f-a717c2ce3b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "config_DT = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'vanilla', #'SDT', 'vanilla'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 15, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        \n",
    "        'use_distribution_list': True,\n",
    "        'random_parameters_distribution': True, ##MAKEPATH DIFFERENT FILES\n",
    "        'max_distributions_per_class': 1, # None; 0; int >= 1  \n",
    "        'exclude_linearly_seperable': True,\n",
    "        'data_generation_filtering': False,\n",
    "        'fixed_class_probability': False,\n",
    "        'balanced_data': True,\n",
    "        'weighted_data_generation': False,\n",
    "        'shift_distrib': False,\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': 3, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'distribution',# 'make_classification_distribution', 'make_classification_distribution_trained', 'distribution', 'distribution_trained', 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'distrib_by_feature': True,\n",
    "        'distribution_list': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'exponential', 'beta', 'binomial', 'poisson'], \n",
    "        'distribution_list_eval': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'beta', 'poisson'],\n",
    "        \n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        'number_of_generated_datasets': 100,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "        \n",
    "        'data_noise': 0, #None or float\n",
    "        \n",
    "        'distrib_param_max': 5,\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-3,\n",
    "        'restore_best_weights': True,\n",
    "        'patience_lambda': 50,\n",
    "        \n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'use_batchnorm_lambda': False,\n",
    "        \n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 100,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        #'dense_layers': [1024, 1024, 256, 2048, 2048],\n",
    "        'dense_layers': [1792, 512, 512],\n",
    "        #'dense_layers': [1792, 512, 512],\n",
    "        \n",
    "        #'dropout': [0, 0, 0, 0, 0.3],#[0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "        'dropout': [0, 0, 0.5],\n",
    "        #'dropout': [0, 0, 0.5],\n",
    "\n",
    "        #'hidden_activation': 'relu',\n",
    "        'hidden_activation': 'sigmoid',\n",
    "        #'hidden_activation': 'swish',\n",
    "\n",
    "        #'optimizer': 'rmsprop', \n",
    "        'optimizer': 'adam', \n",
    "        #'optimizer': 'adam', \n",
    "        \n",
    "        #'learning_rate': 0.001,\n",
    "        'learning_rate': 0.001,\n",
    "        #'learning_rate': 0.001, \n",
    "        \n",
    "        'separate_weight_bias': False,\n",
    "        \n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,        \n",
    "        'additional_hidden': False,\n",
    "        \n",
    "        'loss': 'binary_crossentropy', #mse; binary_crossentropy; 'binary_accuracy'\n",
    "        'metrics': ['binary_accuracy'], #soft_ or _penalized\n",
    "        \n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 5, #Float for fraction, Int for number 0\n",
    "        'evaluate_distribution': True,\n",
    "        'force_evaluate_real_world': False,\n",
    "        \n",
    "        'function_representation_type': 5, # 1=standard representation; 2=sparse representation with classification for variables; 3=softmax to select classes (n top probabilities)\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2,3) #3=autoencoder dimensionality reduction\n",
    "        \n",
    "        'resampling_strategy': None,#'ADASYN', #'SMOTE', None\n",
    "        'resampling_threshold': 0.25,#0.2,\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 60,\n",
    "        'nas_optimizer': 'greedy' #'hyperband',#\"bayesian\",'greedy', 'random'\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        'number_of_random_evaluations_per_distribution': 10,\n",
    "        'random_evaluation_dataset_size_per_distribution': 10_000, \n",
    "        'optimize_sampling': True,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'random_evaluation_dataset_distribution': 'uniform', \n",
    "        \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        #'sklearn_dt_benchmark': False,\n",
    "        #'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': False,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 15,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ba22e7-b06d-4e66-9484-dabfca32d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_config = {\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558ba97-34f2-4429-bad1-647654431a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c4ee38-f93f-40be-9c54-1a9005c0f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = computation_config['gpu_numbers'] if computation_config['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if computation_config['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if computation_config['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if computation_config['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5239304d-59da-4fb9-93a5-5d8c7c4613e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ca24f1-bd4c-402d-8cb5-9d8bd94ca6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_model = generate_base_model(config_DT)#generate_base_model(config_DT, disable_batchnorm=True)\n",
    "\n",
    "np.random.seed(config_DT['computation']['RANDOM_SEED'])\n",
    "        \n",
    "random_network_parameters = random_model.get_weights()\n",
    "network_parameters_structure = [network_parameter.shape for network_parameter in random_network_parameters]  \n",
    "\n",
    "\n",
    "try:\n",
    "    use_distribution_list = config_DT['data']['use_distribution_list'] if config_DT['data']['max_distributions_per_class'] is not None else False\n",
    "except:\n",
    "    use_distribution_list = False if config_DT['data']['max_distributions_per_class'] is None else True\n",
    "\n",
    "metrics = []\n",
    "loss_function = None\n",
    "\n",
    "if config_DT['i_net']['function_value_loss']:\n",
    "    if config_DT['i_net']['function_representation_type'] == 1:\n",
    "        pass\n",
    "        #metrics.append(tf.keras.losses.get('mae'))\n",
    "    if config_DT['i_net']['optimize_decision_function']:\n",
    "        loss_function = inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list)\n",
    "        #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "            #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "    else:\n",
    "        loss_function = inet_target_function_fv_loss_wrapper(config_DT)\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "else:\n",
    "    if config_DT['i_net']['function_representation_type'] >= 3:\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            \n",
    "            loss_function = inet_decision_function_fv_loss_wrapper_parameters(config_DT)\n",
    "            \n",
    "            metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "            for metric in config_DT['i_net']['metrics']:\n",
    "                metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))    \n",
    "            if False:\n",
    "                metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "                #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "                for metric in config_DT['i_net']['metrics']:\n",
    "                    metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "                    #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))                  \n",
    "    else:\n",
    "        raise SystemExit('Coefficient Loss not implemented for config_DTuration')\n",
    "    \n",
    "    if False:\n",
    "        metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            raise SystemExit('Coefficient Loss not implemented for decision function optimization')            \n",
    "        else:\n",
    "            if config_DT['i_net']['function_representation_type'] == 1:\n",
    "                loss_function = tf.keras.losses.get('mae') #inet_coefficient_loss_wrapper(inet_loss)\n",
    "            else:\n",
    "                raise SystemExit('Coefficient Loss not implemented for selected function representation')\n",
    "\n",
    "                \n",
    "# dill.dumps(loss_function)\n",
    "# dill.dumps(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7b6c-1f54-4d2d-b640-63794af75774",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7cbf86-55d4-4bb4-9d96-7b9461484129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LR_inet():\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = keras.models.load_model(path + '/modelKeras')\n",
    "    print(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9513f469-9f28-4dcc-a9ff-fcebd07df0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_DT_inet_custom():\n",
    "    \n",
    "    loss_function_local = dill.dumps(loss_function)\n",
    "    metrics_local = dill.dumps(metrics)\n",
    "    \n",
    "    path = './data/saved_models/lNetSize5000_numLNets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.001_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1792-512-512_drop0-0-0.5e500b256_adam_funcRep5_reshapeNone_depth3_beta1_decisionSpars1_vanilla_reshapeNone'    \n",
    "    model = []\n",
    "    from tensorflow.keras.utils import CustomObjectScope\n",
    "    loss_function_local = dill.loads(loss_function_local)\n",
    "    metrics_local = dill.loads(metrics_local)       \n",
    "\n",
    "    #with CustomObjectScope({'custom_loss': loss_function}):\n",
    "    custom_object_dict = {}\n",
    "    custom_object_dict[loss_function.__name__] = loss_function_local\n",
    "    for metric in  metrics_local:\n",
    "        custom_object_dict[metric.__name__] = metrics_local        \n",
    "        \n",
    "    model = tf.keras.models.load_model(path, custom_objects=custom_object_dict) # #, compile=False\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9270b5-e9bc-4979-b415-76152d44e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = load_DT_inet_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8e007c-ed95-4adb-afe4-8340f8766374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_LR/nda10_nsa5000_nfe20_nin10_nta1_ncc2_sep1.0_noi0.01_shuTrue_ranNone/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo150_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo1000_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1\n"
     ]
    }
   ],
   "source": [
    "model_LR = load_LR_inet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff70b789-58f8-429a-86b4-e22a4e89fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 2177)]       0           []                               \n",
      "                                                                                                  \n",
      " hidden1_1792 (Dense)           (None, 1792)         3902976     ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " activation1_sigmoid (Activatio  (None, 1792)        0           ['hidden1_1792[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " hidden2_512 (Dense)            (None, 512)          918016      ['activation1_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " activation2_sigmoid (Activatio  (None, 512)         0           ['hidden2_512[0][0]']            \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " hidden3_512 (Dense)            (None, 512)          262656      ['activation2_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " activation3_sigmoid (Activatio  (None, 512)         0           ['hidden3_512[0][0]']            \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dropout3_0.5 (Dropout)         (None, 512)          0           ['activation3_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " output_coeff_105 (Dense)       (None, 105)          53865       ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_1 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_2 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_3 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_4 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_5 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_6 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_7 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_leaf_node_8 (Dense)     (None, 8)            4104        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_combined (Concatenate)  (None, 218)          0           ['output_coeff_105[0][0]',       \n",
      "                                                                  'output_identifier_1[0][0]',    \n",
      "                                                                  'output_identifier_2[0][0]',    \n",
      "                                                                  'output_identifier_3[0][0]',    \n",
      "                                                                  'output_identifier_4[0][0]',    \n",
      "                                                                  'output_identifier_5[0][0]',    \n",
      "                                                                  'output_identifier_6[0][0]',    \n",
      "                                                                  'output_identifier_7[0][0]',    \n",
      "                                                                  'output_leaf_node_8[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,195,482\n",
      "Trainable params: 5,195,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_DT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33922ff4-e0a5-41d6-9dce-23be674d60f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_3',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 8301),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'dense_10_input'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_10',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 6000,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_11',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 6000,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_12',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 3500,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_13',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 20,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8573-c8e5-44aa-8be7-5de1d6c79eeb",
   "metadata": {},
   "source": [
    "# Load Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ed8a8-4c1d-4669-a842-441c3eaca74e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e0bf79-b725-48e6-bb3a-755f90dce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.data_path_LR(config_LR)\n",
    "#\n",
    "## with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "#y_coef_truth_test_data_LR = np.load(directory + '/coef_list_targetForInet.npy', allow_pickle=True)\n",
    "#\n",
    "\n",
    "#y_coef_truth_test_data_LR =  np.load(utilities_LR.lambda_path_LR(config_LR) + '/lambda_generated_coef_list_target_for_inet.npy') ## get coef from lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "666b8c92-2b14-4ed6-ab88-165bbf0da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.lambda_path_LR(config_LR)\n",
    "#\n",
    "## with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "#x_lambda_weights_test_data_LR = np.load(directory + '/lambda_weights_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787f1988-46ec-4bf9-9256-4931282060ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_features']])\n",
    "#\n",
    "#if  config_LR['data']['n_targets'] < 2:\n",
    "#    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], ])\n",
    "#    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "#else:\n",
    "#    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_targets']])\n",
    "#    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], config_LR['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cef52c5-4c16-4390-9e4a-945fb9901ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.data_path_LR(config_LR)\n",
    "#\n",
    "#with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "#    X_datasets_list_LR = np.load(f, allow_pickle=True)\n",
    "#with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "#    y_datasets_list_LR = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d961df0-5df2-487f-acec-7388358cddd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19b4b0b5-f797-4c58-8838-4eaec1af843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals().update(generate_paths(config_DT, path_type='lambda_net'))\n",
    "\n",
    "# directory = './data/saved_function_lists/functions_' + path_identifier_function_data + '.csv'\n",
    "\n",
    "#directory = './data/saved_function_lists/functions_lNetSize5000_numDatasets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown.csv'\n",
    "#\n",
    "#function_df = pd.read_csv(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa6d7cc-8204-43dd-87ba-81a9cbd00a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "893bef4e-f46c-4443-afed-df92c39c085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_data_DT = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906643c-f0be-4c45-ad6c-7798682c5e0d",
   "metadata": {},
   "source": [
    "# Evaluate Inet for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "898ad615-be9a-49e4-9a5c-5946088f5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp, tn, fn):\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3797feda-655a-420e-8ad4-c826a2edacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fp, tn, fn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37144d24-d5b0-4f55-9531-dec28cafbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp, fp, tn, fn):\n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    return 2 * (pre * rec) / (pre + rec) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6d452-5dc6-4555-9be5-69cd4495a575",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation on already known data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "592923d0-f576-4549-9a6b-616fae44fd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def confusionMatrixAggregated_SingleSample(i):\n",
    "#    x_lambda_weights = x_lambda_weights_test_data_LR[i, :]\n",
    "#    y_coef_truth = y_coef_truth_test_data_LR[i, :]\n",
    "#    x_lambda_weights = x_lambda_weights.reshape((1, 8362))\n",
    "#    y_coef_pred = model_LR.predict(x=x_lambda_weights,\n",
    "#        batch_size=None,\n",
    "#        verbose=0,\n",
    "#        steps=None,\n",
    "#        callbacks=None,\n",
    "#        max_queue_size=10,\n",
    "#        workers=1,\n",
    "#        use_multiprocessing=False,\n",
    "#                    )\n",
    "#    \n",
    "#    model_groundTruth = get_LR(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "#    \n",
    "#    model_pred = LogisticRegression()\n",
    "#    model_pred.coef_ = y_coef_pred\n",
    "#    model_pred.intercept_ = 0\n",
    "#    model_pred.classes_ = model_groundTruth.classes_\n",
    "#    \n",
    "#    y_coef_pred = y_coef_pred[0]\n",
    "#    \n",
    "#    mse = sklearn.metrics.mean_squared_error(\n",
    "#        y_coef_truth, y_coef_pred\n",
    "#    )\n",
    "#    \n",
    "#    score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "#    score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "#    y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "#    y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "#    \n",
    "#    tn, fp, fn, tp = confusion_matrix(y_truth_set, y_pred_set).ravel()\n",
    "#    \n",
    "#    return tn, fp, fn, tp\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8215d17d-737c-489f-9bec-071026ea10bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#parallel = Parallel(n_jobs=config_LR['n_jobs'], verbose=10, backend='loky') #loky\n",
    "##parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "#\n",
    "#confusion_Matrix_Array = parallel(delayed(confusionMatrixAggregated_SingleSample)(i) for i in range(x_lambda_weights_test_data_LR.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3e5563d-f159-4a79-8a35-3c7535f71b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = np.array(confusion_Matrix_Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a65db1-c49f-4cfc-a898-2245d32ff2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a55c5da-82d5-424c-841d-e20353688ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "868e8e91-0de1-4f0c-ab67-a123e6d64ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.reshape([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad0728bc-d162-4c79-a614-97869e6b30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disp = ConfusionMatrixDisplay(confusion_Matrix_Array)\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52978a-c03a-4c5f-bd73-fe33945d0f0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate on arbitrary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89b5929c-62cd-4464-81df-fc4bff16ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_weights = np.zeros([config_LR['eval']['n_datasets'], 8301, ])\n",
    "\n",
    "X_queryLambda = np.zeros([config_LR['eval']['n_datasets'], config_LR['eval']['n_samples_queryLambda'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 3:\n",
    "    y_valid_coefs = np.zeros([config_LR['eval']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    print(\"#################### NOT YET IMPLEMENTED ######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec0e857c-9cae-47c4-9132-e26d2cfbd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(X, y):\n",
    "    model = LogisticRegression(penalty='l2',\n",
    "        dual=False,\n",
    "        tol=0.0001,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        intercept_scaling=1,\n",
    "        class_weight=None,\n",
    "        random_state=None,\n",
    "        solver='lbfgs',\n",
    "        max_iter=100,\n",
    "        multi_class='auto',\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        n_jobs=None,\n",
    "        l1_ratio=None\n",
    "                              )\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60586aeb-321d-4305-8a17-18d69d503556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                        random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                        shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                        stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "                                                    y_test, \n",
    "                                                    test_size=config_LR['lambda']['data_prep']['train_test_val_split']['val_size'] / (config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size']), \n",
    "                                                    random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4b54a5e-afa4-4295-815e-279a67ba694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y):\n",
    "    # Data Prep\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "                                                                          y)\n",
    "    \n",
    "    # Model Def\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config_LR['data']['n_features']))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config_LR['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=config_LR['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config_LR['lambda']['model_compile']['loss'],\n",
    "                  metrics=config_LR['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    # Model fit\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=config_LR['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config_LR['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config_LR['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=config_LR['lambda']['model_fit']['callbacks'],\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        shuffle=config_LR['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config_LR['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config_LR['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config_LR['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config_LR['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config_LR['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config_LR['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config_LR['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ef84628-c36e-4b01-b837-ffa2d87fedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid_reg():\n",
    "    \n",
    "    ### TRAIN LAMBDA\n",
    "    \n",
    "    X, y =  sklearn.datasets.make_classification(n_samples=config_LR['eval']['n_samples_train'] + config_LR['eval']['n_samples_queryLambda'] + config_LR['eval']['n_samples_comparison'], \n",
    "                                                                                         n_features=config_LR['data']['n_features'],\n",
    "                                                                                         n_informative=config_LR['data']['n_informative'],\n",
    "                                                                                         n_redundant=config_LR['data']['n_features']-config_LR['data']['n_informative'],\n",
    "                                                                                         n_repeated=0,\n",
    "                                                                                         n_classes=config_LR['data']['n_targets']+1, \n",
    "                                                                                         n_clusters_per_class=config_LR['data']['n_clusters_per_class'],\n",
    "                                                                                         weights=None,\n",
    "                                                                                         flip_y=config_LR['data']['noise'],\n",
    "                                                                                         class_sep=config_LR['data']['class_sep'],\n",
    "                                                                                         shuffle=config_LR['data']['shuffle'],\n",
    "                                                                                         random_state=config_LR['data']['random_state'])\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=config_LR['eval']['n_samples_queryLambda'] + config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    X_queryLambda, X_comparison, _, _ = train_test_split(X_temp, y_temp, test_size=config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    \n",
    "    model_lambda = train_nn(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    ### GET COEF FROM LAMBDA\n",
    "    \n",
    "    y_pred = model_lambda.predict(X_queryLambda)\n",
    "    \n",
    "    y_pred = [1.0 if y>=0.5 else 0.0 for y in y_pred]\n",
    "    \n",
    "    log_reg_test = get_LR(X_queryLambda, y_pred)\n",
    "    \n",
    "    return np.concatenate([x.flatten() for x in model_lambda.get_weights()]), log_reg_test.coef_, X_queryLambda ## test_weights, test_coef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fc2acf3-52fb-47b4-8be1-256f671df495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=100)]: Using backend LokyBackend with 100 concurrent workers.\n",
      "[Parallel(n_jobs=100)]: Done   3 out of  10 | elapsed:  2.8min remaining:  6.6min\n",
      "[Parallel(n_jobs=100)]: Done   5 out of  10 | elapsed:  3.2min remaining:  3.2min\n",
      "[Parallel(n_jobs=100)]: Done   7 out of  10 | elapsed:  3.2min remaining:  1.4min\n",
      "[Parallel(n_jobs=100)]: Done  10 out of  10 | elapsed:  4.1min finished\n"
     ]
    }
   ],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "#parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "\n",
    "weights_coef = parallel(delayed(create_valid_reg)() for i in range(config_LR['eval']['n_datasets']))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b9ff2f2-5e74-4f1f-9d12-cfb1bbdb75fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(config_LR['eval']['n_datasets']):\n",
    "    X_valid_weights[i] = weights_coef[i][0]\n",
    "    y_valid_coefs[i] = weights_coef[i][1]\n",
    "    X_queryLambda[i] = weights_coef[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c3ce7a5-10b9-45dd-b815-fd29797ba890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8301)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27034654-f43b-4460-a45a-1c60df10e7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9339ad59-d878-45b9-b95d-1c95c1df405a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4500, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_queryLambda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3f13f94-f13c-4309-9ad9-f4410f0cb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSingleSampleOnValidSet(i):\n",
    "    X_valid_weight = X_valid_weights[i, :]\n",
    "    y_valid_coef = y_valid_coefs[i, :]\n",
    "    X_queryLambda_instance = X_queryLambda[i, :]\n",
    "    \n",
    "    \n",
    "    X_valid_weight = X_valid_weight.reshape((1, 8301))\n",
    "    \n",
    "    y_valid_coef_pred = model_LR.predict(x=X_valid_weight,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    y_valid_coef = y_valid_coef.reshape([1, 20])\n",
    "    \n",
    "    model_truth = LogisticRegression()\n",
    "    model_truth.coef_ = y_valid_coef\n",
    "    model_truth.intercept_ = 0\n",
    "    model_truth.classes_ = np.array([0, 1])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_valid_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = np.array([0, 1])\n",
    "    \n",
    "    y_class_truth = model_truth.predict(X_queryLambda_instance)\n",
    "    y_class_pred = model_pred.predict(X_queryLambda_instance)\n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_class_truth, y_class_pred\n",
    "    )\n",
    "    \n",
    "    #score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    #y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_class_truth, y_class_pred, labels=[1,0]).ravel()\n",
    "    \n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    fone = f1(tp, fp, tn, fn)\n",
    "    \n",
    "    #results.append([i, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone])\n",
    "    \n",
    "    return i+1, -1, -1, mse, tp, fn, fp, tn, pre, rec, fone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f8652d2-dcfe-4e1c-a229-815a6d07d8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    2.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    3.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    3.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "#parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "result_list = parallel(delayed(evaluateSingleSampleOnValidSet)(i) for i in range(config_LR['eval']['n_datasets']))\n",
    "                   \n",
    "    \n",
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"], data=result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d249ab04-1e24-495f-98e6-4586c48ac107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_0=aggregated</th>\n",
       "      <th>scoreOnClassfication_BaseModel</th>\n",
       "      <th>scoreOnClassfication_PredictedModel</th>\n",
       "      <th>mse</th>\n",
       "      <th>tp</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.432889</td>\n",
       "      <td>1450</td>\n",
       "      <td>1493</td>\n",
       "      <td>455</td>\n",
       "      <td>1102</td>\n",
       "      <td>0.761155</td>\n",
       "      <td>0.492695</td>\n",
       "      <td>0.598185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.449111</td>\n",
       "      <td>1392</td>\n",
       "      <td>1106</td>\n",
       "      <td>915</td>\n",
       "      <td>1087</td>\n",
       "      <td>0.603381</td>\n",
       "      <td>0.557246</td>\n",
       "      <td>0.579396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.402889</td>\n",
       "      <td>927</td>\n",
       "      <td>681</td>\n",
       "      <td>1132</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.450219</td>\n",
       "      <td>0.576493</td>\n",
       "      <td>0.505590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.592444</td>\n",
       "      <td>951</td>\n",
       "      <td>1385</td>\n",
       "      <td>1281</td>\n",
       "      <td>883</td>\n",
       "      <td>0.426075</td>\n",
       "      <td>0.407106</td>\n",
       "      <td>0.416375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.371778</td>\n",
       "      <td>1358</td>\n",
       "      <td>820</td>\n",
       "      <td>853</td>\n",
       "      <td>1469</td>\n",
       "      <td>0.614202</td>\n",
       "      <td>0.623508</td>\n",
       "      <td>0.618820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.532222</td>\n",
       "      <td>625</td>\n",
       "      <td>1052</td>\n",
       "      <td>1343</td>\n",
       "      <td>1480</td>\n",
       "      <td>0.317581</td>\n",
       "      <td>0.372689</td>\n",
       "      <td>0.342936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.420667</td>\n",
       "      <td>817</td>\n",
       "      <td>1205</td>\n",
       "      <td>688</td>\n",
       "      <td>1790</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.404055</td>\n",
       "      <td>0.463283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.547111</td>\n",
       "      <td>1343</td>\n",
       "      <td>1106</td>\n",
       "      <td>1356</td>\n",
       "      <td>695</td>\n",
       "      <td>0.497592</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.521756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.209111</td>\n",
       "      <td>1635</td>\n",
       "      <td>438</td>\n",
       "      <td>503</td>\n",
       "      <td>1924</td>\n",
       "      <td>0.764733</td>\n",
       "      <td>0.788712</td>\n",
       "      <td>0.776538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.441778</td>\n",
       "      <td>970</td>\n",
       "      <td>1141</td>\n",
       "      <td>847</td>\n",
       "      <td>1542</td>\n",
       "      <td>0.533847</td>\n",
       "      <td>0.459498</td>\n",
       "      <td>0.493890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_0=aggregated  scoreOnClassfication_BaseModel  \\\n",
       "0                   1                              -1   \n",
       "1                   2                              -1   \n",
       "2                   3                              -1   \n",
       "3                   4                              -1   \n",
       "4                   5                              -1   \n",
       "5                   6                              -1   \n",
       "6                   7                              -1   \n",
       "7                   8                              -1   \n",
       "8                   9                              -1   \n",
       "9                  10                              -1   \n",
       "\n",
       "   scoreOnClassfication_PredictedModel       mse    tp    fn    fp    tn  \\\n",
       "0                                   -1  0.432889  1450  1493   455  1102   \n",
       "1                                   -1  0.449111  1392  1106   915  1087   \n",
       "2                                   -1  0.402889   927   681  1132  1760   \n",
       "3                                   -1  0.592444   951  1385  1281   883   \n",
       "4                                   -1  0.371778  1358   820   853  1469   \n",
       "5                                   -1  0.532222   625  1052  1343  1480   \n",
       "6                                   -1  0.420667   817  1205   688  1790   \n",
       "7                                   -1  0.547111  1343  1106  1356   695   \n",
       "8                                   -1  0.209111  1635   438   503  1924   \n",
       "9                                   -1  0.441778   970  1141   847  1542   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.761155  0.492695  0.598185  \n",
       "1   0.603381  0.557246  0.579396  \n",
       "2   0.450219  0.576493  0.505590  \n",
       "3   0.426075  0.407106  0.416375  \n",
       "4   0.614202  0.623508  0.618820  \n",
       "5   0.317581  0.372689  0.342936  \n",
       "6   0.542857  0.404055  0.463283  \n",
       "7   0.497592  0.548387  0.521756  \n",
       "8   0.764733  0.788712  0.776538  \n",
       "9   0.533847  0.459498  0.493890  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae48075-0b96-4a56-94d0-ce671d424428",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated = pd.DataFrame(results.mean(numeric_only=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7c7ed92-fdbe-4aff-ac6e-a5db6922c088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_0=aggregated</th>\n",
       "      <th>scoreOnClassfication_BaseModel</th>\n",
       "      <th>scoreOnClassfication_PredictedModel</th>\n",
       "      <th>mse</th>\n",
       "      <th>tp</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1146.8</td>\n",
       "      <td>1042.7</td>\n",
       "      <td>937.3</td>\n",
       "      <td>1373.2</td>\n",
       "      <td>0.551164</td>\n",
       "      <td>0.523039</td>\n",
       "      <td>0.531677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_0=aggregated  scoreOnClassfication_BaseModel  \\\n",
       "0                 5.5                            -1.0   \n",
       "\n",
       "   scoreOnClassfication_PredictedModel   mse      tp      fn     fp      tn  \\\n",
       "0                                 -1.0  0.44  1146.8  1042.7  937.3  1373.2   \n",
       "\n",
       "   precision    recall        f1  \n",
       "0   0.551164  0.523039  0.531677  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggragated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d43bdc67-d6f9-4541-8b39-df29e5097315",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated.at[0, \"index_0=aggregated\"] = 0\n",
    "results = pd.concat([aggragated, results], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4646255c-ceba-40a4-b978-677a3a550b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_res(df):\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = df.to_csv(path + '/evalRes.csv')\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d15f12ca-01c0-4c52-b80d-735f9971b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_LR/nda10_nsa5000_nfe20_nin10_nta1_ncc2_sep1.0_noi0.01_shuTrue_ranNone/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo150_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo1000_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1\n"
     ]
    }
   ],
   "source": [
    "save_eval_res(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c78a12-7bee-4124-8d38-0ca6240c76b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb2668-d031-4bdb-9dba-75156cba9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7728121-949b-486d-8be1-502fa10fc1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743de906-ea72-43c6-9db4-49af847a6b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13704348-b36f-4836-bd6d-914ad5a84fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
