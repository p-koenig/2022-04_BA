{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d467f817-f968-4b2c-84d5-e8f8d4627666",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config (2 I-Nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eeaa4-4c27-41aa-aad8-8fd75ce0a3a7",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20ce18-baf7-46be-a2a2-79238d9a1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "\n",
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c0ab8e-2451-424f-8fca-f26d1a761c7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_datasets\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;66;03m# the number of datasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5_000\u001b[39m, \u001b[38;5;66;03m# the number of samples per dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_features\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20\u001b[39m, \n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# The total number of features. \u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_informative\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \u001b[39;00m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \u001b[39;00m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# placed on the vertices of the hypercube.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_targets\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# The number of targets (or labels) of the classification problem.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_clusters_per_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# The number of clusters per class.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         \n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_sep\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# class_sepfloat, default=1.0\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# easier.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         \n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoise\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# flip_y (fraction of samples whose class is assigned randomly)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# Shuffle the samples and the features.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         \n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     },    \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_prep\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_test_val_split\u001b[39m\u001b[38;5;124m'\u001b[39m: { \u001b[38;5;66;03m# refer to sklearn doc\u001b[39;00m\n\u001b[1;32m     41\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     42\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     43\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstratify\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m             }\n\u001b[1;32m     47\u001b[0m         },\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_compile\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m#tf.keras.losses.get(config['lambda_net']['loss_lambda']),\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRootMeanSquaredError()]\n\u001b[1;32m     52\u001b[0m         },\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: { \u001b[38;5;66;03m# refer to keras API\u001b[39;00m\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps_per_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_freq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     66\u001b[0m         }\n\u001b[1;32m     67\u001b[0m     },\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minets\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_prep\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_test_val_split\u001b[39m\u001b[38;5;124m'\u001b[39m: { \u001b[38;5;66;03m# refer to sklearn doc\u001b[39;00m\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstratify\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m             }\n\u001b[1;32m     77\u001b[0m         },\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_compile\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     79\u001b[0m             \n\u001b[1;32m     80\u001b[0m         },\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: { \u001b[38;5;66;03m# refer to keras API\u001b[39;00m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps_per_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_steps\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_freq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m         }\n\u001b[1;32m     95\u001b[0m     },\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputation\u001b[39m\u001b[38;5;124m'\u001b[39m:{\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_gpu\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_numbers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANDOM_SEED\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,   \n\u001b[1;32m    101\u001b[0m     }\n\u001b[1;32m    102\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'n_datasets': 10, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 20, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 10,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 2,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0.01,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': None,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },    \n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae', #tf.keras.losses.get(config['lambda_net']['loss_lambda']),\n",
    "            'metrics': ['mae', keras.metrics.RootMeanSquaredError()]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 150,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '4',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576d3d5-a533-46c5-8590-3b1ab0a932c5",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fe06c-e82f-4e2a-a23f-a717c2ce3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_DT = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'vanilla', #'SDT', 'vanilla'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 15, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        \n",
    "        'use_distribution_list': True,\n",
    "        'random_parameters_distribution': True, ##MAKEPATH DIFFERENT FILES\n",
    "        'max_distributions_per_class': 1, # None; 0; int >= 1  \n",
    "        'exclude_linearly_seperable': True,\n",
    "        'data_generation_filtering': False,\n",
    "        'fixed_class_probability': False,\n",
    "        'balanced_data': True,\n",
    "        'weighted_data_generation': False,\n",
    "        'shift_distrib': False,\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': 3, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'distribution',# 'make_classification_distribution', 'make_classification_distribution_trained', 'distribution', 'distribution_trained', 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'distrib_by_feature': True,\n",
    "        'distribution_list': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'exponential', 'beta', 'binomial', 'poisson'], \n",
    "        'distribution_list_eval': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'beta', 'poisson'],\n",
    "        \n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        'number_of_generated_datasets': 100,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "        \n",
    "        'data_noise': 0, #None or float\n",
    "        \n",
    "        'distrib_param_max': 5,\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-3,\n",
    "        'restore_best_weights': True,\n",
    "        'patience_lambda': 50,\n",
    "        \n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'use_batchnorm_lambda': False,\n",
    "        \n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 100,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        #'dense_layers': [1024, 1024, 256, 2048, 2048],\n",
    "        'dense_layers': [1792, 512, 512],\n",
    "        #'dense_layers': [1792, 512, 512],\n",
    "        \n",
    "        #'dropout': [0, 0, 0, 0, 0.3],#[0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "        'dropout': [0, 0, 0.5],\n",
    "        #'dropout': [0, 0, 0.5],\n",
    "\n",
    "        #'hidden_activation': 'relu',\n",
    "        'hidden_activation': 'sigmoid',\n",
    "        #'hidden_activation': 'swish',\n",
    "\n",
    "        #'optimizer': 'rmsprop', \n",
    "        'optimizer': 'adam', \n",
    "        #'optimizer': 'adam', \n",
    "        \n",
    "        #'learning_rate': 0.001,\n",
    "        'learning_rate': 0.001,\n",
    "        #'learning_rate': 0.001, \n",
    "        \n",
    "        'separate_weight_bias': False,\n",
    "        \n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,        \n",
    "        'additional_hidden': False,\n",
    "        \n",
    "        'loss': 'binary_crossentropy', #mse; binary_crossentropy; 'binary_accuracy'\n",
    "        'metrics': ['binary_accuracy'], #soft_ or _penalized\n",
    "        \n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 5, #Float for fraction, Int for number 0\n",
    "        'evaluate_distribution': True,\n",
    "        'force_evaluate_real_world': False,\n",
    "        \n",
    "        'function_representation_type': 5, # 1=standard representation; 2=sparse representation with classification for variables; 3=softmax to select classes (n top probabilities)\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2,3) #3=autoencoder dimensionality reduction\n",
    "        \n",
    "        'resampling_strategy': None,#'ADASYN', #'SMOTE', None\n",
    "        'resampling_threshold': 0.25,#0.2,\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 60,\n",
    "        'nas_optimizer': 'greedy' #'hyperband',#\"bayesian\",'greedy', 'random'\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        'number_of_random_evaluations_per_distribution': 10,\n",
    "        'random_evaluation_dataset_size_per_distribution': 10_000, \n",
    "        'optimize_sampling': True,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'random_evaluation_dataset_distribution': 'uniform', \n",
    "        \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        #'sklearn_dt_benchmark': False,\n",
    "        #'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': False,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 15,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba22e7-b06d-4e66-9484-dabfca32d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_config = {\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558ba97-34f2-4429-bad1-647654431a0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4ee38-f93f-40be-9c54-1a9005c0f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = computation_config['gpu_numbers'] if computation_config['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if computation_config['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if computation_config['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if computation_config['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239304d-59da-4fb9-93a5-5d8c7c4613e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca24f1-bd4c-402d-8cb5-9d8bd94ca6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_model = generate_base_model(config_DT)#generate_base_model(config_DT, disable_batchnorm=True)\n",
    "\n",
    "np.random.seed(config_DT['computation']['RANDOM_SEED'])\n",
    "        \n",
    "random_network_parameters = random_model.get_weights()\n",
    "network_parameters_structure = [network_parameter.shape for network_parameter in random_network_parameters]  \n",
    "\n",
    "\n",
    "try:\n",
    "    use_distribution_list = config_DT['data']['use_distribution_list'] if config_DT['data']['max_distributions_per_class'] is not None else False\n",
    "except:\n",
    "    use_distribution_list = False if config_DT['data']['max_distributions_per_class'] is None else True\n",
    "\n",
    "metrics = []\n",
    "loss_function = None\n",
    "\n",
    "if config_DT['i_net']['function_value_loss']:\n",
    "    if config_DT['i_net']['function_representation_type'] == 1:\n",
    "        pass\n",
    "        #metrics.append(tf.keras.losses.get('mae'))\n",
    "    if config_DT['i_net']['optimize_decision_function']:\n",
    "        loss_function = inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list)\n",
    "        #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "            #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "    else:\n",
    "        loss_function = inet_target_function_fv_loss_wrapper(config_DT)\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "else:\n",
    "    if config_DT['i_net']['function_representation_type'] >= 3:\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            \n",
    "            loss_function = inet_decision_function_fv_loss_wrapper_parameters(config_DT)\n",
    "            \n",
    "            metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "            for metric in config_DT['i_net']['metrics']:\n",
    "                metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))    \n",
    "            if False:\n",
    "                metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "                #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "                for metric in config_DT['i_net']['metrics']:\n",
    "                    metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "                    #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))                  \n",
    "    else:\n",
    "        raise SystemExit('Coefficient Loss not implemented for config_DTuration')\n",
    "    \n",
    "    if False:\n",
    "        metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            raise SystemExit('Coefficient Loss not implemented for decision function optimization')            \n",
    "        else:\n",
    "            if config_DT['i_net']['function_representation_type'] == 1:\n",
    "                loss_function = tf.keras.losses.get('mae') #inet_coefficient_loss_wrapper(inet_loss)\n",
    "            else:\n",
    "                raise SystemExit('Coefficient Loss not implemented for selected function representation')\n",
    "\n",
    "                \n",
    "# dill.dumps(loss_function)\n",
    "# dill.dumps(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7b6c-1f54-4d2d-b640-63794af75774",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cbf86-55d4-4bb4-9d96-7b9461484129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LR_inet():\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = keras.models.load_model(path + '/modelKeras')\n",
    "    print(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0bfcf-f020-4ddb-8abe-96bced025e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_DT_inet():\n",
    "#    model = load_inet(loss_function=dill.dumps(loss_function), metrics=dill.dumps(metrics), config=config_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513f469-9f28-4dcc-a9ff-fcebd07df0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inet_custom():\n",
    "    \n",
    "    loss_function_local = dill.dumps(loss_function)\n",
    "    metrics_local = dill.dumps(metrics)\n",
    "    \n",
    "    path = './data/saved_models/lNetSize5000_numLNets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.001_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1792-512-512_drop0-0-0.5e500b256_adam_funcRep5_reshapeNone_depth3_beta1_decisionSpars1_vanilla_reshapeNone'    \n",
    "    model = []\n",
    "    from tensorflow.keras.utils import CustomObjectScope\n",
    "    loss_function_local = dill.loads(loss_function_local)\n",
    "    metrics_local = dill.loads(metrics_local)       \n",
    "\n",
    "    #with CustomObjectScope({'custom_loss': loss_function}):\n",
    "    custom_object_dict = {}\n",
    "    custom_object_dict[loss_function.__name__] = loss_function_local\n",
    "    for metric in  metrics_local:\n",
    "        custom_object_dict[metric.__name__] = metrics_local        \n",
    "        \n",
    "    model = tf.keras.models.load_model(path, custom_objects=custom_object_dict) # #, compile=False\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9270b5-e9bc-4979-b415-76152d44e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = load_inet_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e007c-ed95-4adb-afe4-8340f8766374",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR = load_LR_inet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70b789-58f8-429a-86b4-e22a4e89fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33922ff4-e0a5-41d6-9dce-23be674d60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8573-c8e5-44aa-8be7-5de1d6c79eeb",
   "metadata": {},
   "source": [
    "# Load Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ed8a8-4c1d-4669-a842-441c3eaca74e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0bf79-b725-48e6-bb3a-755f90dce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config_LR)\n",
    "\n",
    "# with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "y_coef_truth_test_data_LR = np.load(directory + '/coef_list_targetForInet.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b8c92-2b14-4ed6-ab88-165bbf0da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.lambda_path_LR(config_LR)\n",
    "\n",
    "# with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "x_lambda_weights_test_data_LR = np.load(directory + '/lambda_weights_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540f4d0-d1a0-4f78-a63e-23c2ca1d9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lambda_weights_test_data_LR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28443bc8-3a6b-4785-9fea-eceefffd30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lambda_weights_test_data_LR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f1988-46ec-4bf9-9256-4931282060ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 2:\n",
    "    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], ])\n",
    "    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_targets']])\n",
    "    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], config_LR['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef52c5-4c16-4390-9e4a-945fb9901ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config_LR)\n",
    "\n",
    "with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    X_datasets_list_LR = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    y_datasets_list_LR = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a84f54-d361-406d-b0cc-3e7337e80428",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list_LR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54dd4c4-e685-4411-b8b1-cc764338929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_datasets_list_LR[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d961df0-5df2-487f-acec-7388358cddd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4b0b5-f797-4c58-8838-4eaec1af843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals().update(generate_paths(config_DT, path_type='lambda_net'))\n",
    "\n",
    "# directory = './data/saved_function_lists/functions_' + path_identifier_function_data + '.csv'\n",
    "\n",
    "#directory = './data/saved_function_lists/functions_lNetSize5000_numDatasets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown.csv'\n",
    "#\n",
    "#function_df = pd.read_csv(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6d7cc-8204-43dd-87ba-81a9cbd00a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893bef4e-f46c-4443-afed-df92c39c085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_data_DT = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906643c-f0be-4c45-ad6c-7798682c5e0d",
   "metadata": {},
   "source": [
    "# Evaluate Inet for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ad615-be9a-49e4-9a5c-5946088f5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp, tn, fn):\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797feda-655a-420e-8ad4-c826a2edacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fp, tn, fn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37144d24-d5b0-4f55-9531-dec28cafbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp, fp, tn, fn):\n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    return 2 * (pre * rec) / (pre + rec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9f0d8-f719-4463-8729-6201287d62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_LR.evaluate(x=x_lambda_weights_test_data_LR,\n",
    "    y=y_coef_truth_test_data_LR,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    sample_weight=None,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=10,\n",
    "    use_multiprocessing=True,\n",
    "    return_dict=False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e857c-9cae-47c4-9132-e26d2cfbd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(X, y):\n",
    "    model = LogisticRegression(penalty='l2',\n",
    "        dual=False,\n",
    "        tol=0.0001,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        intercept_scaling=1,\n",
    "        class_weight=None,\n",
    "        random_state=None,\n",
    "        solver='lbfgs',\n",
    "        max_iter=100,\n",
    "        multi_class='auto',\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        n_jobs=None,\n",
    "        l1_ratio=None\n",
    "                              )\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592923d0-f576-4549-9a6b-616fae44fd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confusionMatrixAggregated_SingleSample(i):\n",
    "    x_lambda_weights = x_lambda_weights_test_data_LR[i, :]\n",
    "    y_coef_truth = y_coef_truth_test_data_LR[i, :]\n",
    "    x_lambda_weights = x_lambda_weights.reshape((1, 8362))\n",
    "    y_coef_pred = model_LR.predict(x=x_lambda_weights,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    model_groundTruth = get_LR(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = model_groundTruth.classes_\n",
    "    \n",
    "    y_coef_pred = y_coef_pred[0]\n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_coef_truth, y_coef_pred\n",
    "    )\n",
    "    \n",
    "    score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_truth_set, y_pred_set).ravel()\n",
    "    \n",
    "    return tn, fp, fn, tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215d17d-737c-489f-9bec-071026ea10bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#parallel = Parallel(n_jobs=config_LR['n_jobs'], verbose=10, backend='loky') #loky\n",
    "##parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "#\n",
    "#confusion_Matrix_Array = parallel(delayed(confusionMatrixAggregated_SingleSample)(i) for i in range(x_lambda_weights_test_data_LR.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5563d-f159-4a79-8a35-3c7535f71b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = np.array(confusion_Matrix_Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a65db1-c49f-4cfc-a898-2245d32ff2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55c5da-82d5-424c-841d-e20353688ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e8e91-0de1-4f0c-ab67-a123e6d64ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.reshape([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0728bc-d162-4c79-a614-97869e6b30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disp = ConfusionMatrixDisplay(confusion_Matrix_Array)\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52978a-c03a-4c5f-bd73-fe33945d0f0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate on arbitrary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5929c-62cd-4464-81df-fc4bff16ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 3:\n",
    "    y_datasets_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], ])\n",
    "    coef_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    y_datasets_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_targets']])\n",
    "    coef_list_valid = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], config_LR['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef84628-c36e-4b01-b837-ffa2d87fedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid_data():\n",
    "    X_datasets_list_valid, y_datasets_list_valid = sklearn.datasets.make_classification(n_samples=config_LR['data']['n_samples'], \n",
    "                                                                                         n_features=config_LR['data']['n_features'],\n",
    "                                                                                         n_informative=config_LR['data']['n_informative'], \n",
    "                                                                                         n_classes=config_LR['data']['n_targets'], \n",
    "                                                                                         n_clusters_per_class=config_LR['data']['n_clusters_per_class'],\n",
    "                                                                                         weights=None,\n",
    "                                                                                         flip_y=config_LR['data']['noise'],\n",
    "                                                                                         class_sep=config_LR['data']['class_sep'],\n",
    "                                                                                         shuffle=config_LR['data']['shuffle'],\n",
    "                                                                                         random_state=config_LR['data']['random_state'])\n",
    "    model_valid = LogisticRegression()\n",
    "    model_valid.fit(X_datasets_list_valid, y_datasets_list_valid)\n",
    "    return X_datasets_list_valid, y_datasets_list_valid, model_valid.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2acf3-52fb-47b4-8be1-256f671df495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "\n",
    "results = parallel(delayed(create_valid_data)() for i in range(config_LR['data']['n_datasets']))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ff2f2-5e74-4f1f-9d12-cfb1bbdb75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(config_LR['data']['n_datasets']):\n",
    "    X_datasets_list_valid[i] = results[i][0]\n",
    "    y_datasets_list_valid[i] = results[i][1]\n",
    "    coef_list_valid[i] = results[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feac40a-ebc2-40fa-87fa-19e1cc5c2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(config_LR['data']['n_datasets']):\n",
    "#    X_datasets_list_valid[i], y_datasets_list_valid[i] = sklearn.datasets.make_classification(n_samples=config_LR['data']['n_samples'], \n",
    "#                                                                                         n_features=config_LR['data']['n_features'],\n",
    "#                                                                                         n_informative=config_LR['data']['n_informative'], \n",
    "#                                                                                         n_classes=config_LR['data']['n_targets'], \n",
    "#                                                                                         n_clusters_per_class=config_LR['data']['n_clusters_per_class'],\n",
    "#                                                                                         weights=None,\n",
    "#                                                                                         flip_y=config_LR['data']['noise'],\n",
    "#                                                                                         class_sep=config_LR['data']['class_sep'],\n",
    "#                                                                                         shuffle=config_LR['data']['shuffle'],\n",
    "#                                                                                         random_state=config_LR['data']['random_state'])\n",
    "#    model_valid = LogisticRegression()\n",
    "#    model_valid.fit(X_datasets_list_valid[i], y_datasets_list_valid[i])\n",
    "#    coef_list_valid[i] = model_valid.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f85b77-50ca-4609-98b6-1e8ea9e48e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9b754-8457-46cd-ad6f-dd2fb489794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_model(config):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config_LR['data']['n_features']))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config_LR['data']['n_targets'], activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def shape_flat_network_parameters(flat_network_parameters, target_network_parameters):\n",
    "               \n",
    "    shaped_network_parameters =[]\n",
    "    start = 0  \n",
    "    \n",
    "    for parameters in target_network_parameters:\n",
    "        target_shape = parameters.shape\n",
    "        size = np.prod(target_shape)\n",
    "        shaped_parameters = np.reshape(flat_network_parameters[start:start+size], target_shape)\n",
    "        shaped_network_parameters.append(shaped_parameters)\n",
    "        start += size\n",
    "\n",
    "    return shaped_network_parameters\n",
    "\n",
    "def network_parameters_to_network(network_parameters, config):\n",
    "    \n",
    "    model = generate_base_model(config)    \n",
    "\n",
    "    model_network_parameters = model.get_weights()    \n",
    " \n",
    "\n",
    "    # Shape weights (flat) into correct model structure\n",
    "    shaped_network_parameters = shape_flat_network_parameters(network_parameters, model_network_parameters)\n",
    "    \n",
    "    model.set_weights(shaped_network_parameters)\n",
    "    \n",
    "    model.compile(optimizer=config_LR['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config_LR['lambda']['model_compile']['loss'],\n",
    "                  metrics=config_LR['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240502cd-2958-4097-9ed6-83835e661c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSingleSampleOnValidSet_withLambdaNetPredictions(i):\n",
    "    x_lambda_weights = x_lambda_weights_test_data_LR[i, :]\n",
    "    y_coef_truth = y_coef_truth_test_data_LR[i, :]\n",
    "    \n",
    "    x_lambda_weights = x_lambda_weights.reshape((1, 8362))\n",
    "    \n",
    "    y_coef_pred = model_LR.predict(x=x_lambda_weights,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    model_groundTruth = get_LR(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = model_groundTruth.classes_\n",
    "    \n",
    "    y_coef_pred = y_coef_pred[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_coef_truth, y_coef_pred\n",
    "    )\n",
    "    \n",
    "    lambda_net = network_parameters_to_network(x_lambda_weights, config_LR)\n",
    "    \n",
    "    predicted_coef_from_lambda = lambda_net.predict(X_datasets_list_LR[i])\n",
    "    \n",
    "    score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], predicted_coef_from_lambda)\n",
    "    score_predModel = model_pred.score(X_datasets_list_LR[i], predicted_coef_from_lambda)\n",
    "    y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_truth_set, y_pred_set, labels=[1,0]).ravel()\n",
    "    \n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    fone = f1(tp, fp, tn, fn)\n",
    "    \n",
    "    #results.append([i, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone])\n",
    "    \n",
    "    return i+1, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f13f94-f13c-4309-9ad9-f4410f0cb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSingleSampleOnValidSet(i):\n",
    "    x_lambda_weights = x_lambda_weights_test_data_LR[i, :]\n",
    "    y_coef_truth = y_coef_truth_test_data_LR[i, :]\n",
    "    \n",
    "    x_lambda_weights = x_lambda_weights.reshape((1, 8362))\n",
    "    \n",
    "    y_coef_pred = model_LR.predict(x=x_lambda_weights,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    model_groundTruth = get_LR(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = model_groundTruth.classes_\n",
    "    \n",
    "    y_coef_pred = y_coef_pred[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_coef_truth, y_coef_pred\n",
    "    )\n",
    "    \n",
    "    score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_truth_set, y_pred_set, labels=[1,0]).ravel()\n",
    "    \n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    fone = f1(tp, fp, tn, fn)\n",
    "    \n",
    "    #results.append([i, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone])\n",
    "    \n",
    "    return i+1, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8652d2-dcfe-4e1c-a229-815a6d07d8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "result_list = parallel(delayed(evaluateSingleSampleOnValidSet)(i) for i in range(x_lambda_weights_test_data_LR.shape[0]))\n",
    "                   \n",
    "    \n",
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"], data=result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249ab04-1e24-495f-98e6-4586c48ac107",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae48075-0b96-4a56-94d0-ce671d424428",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated = pd.DataFrame(results.mean(numeric_only=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7ed92-fdbe-4aff-ac6e-a5db6922c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bdc67-d6f9-4541-8b39-df29e5097315",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated.at[0, \"index_0=aggregated\"] = 0\n",
    "results = pd.concat([aggragated, results], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646255c-ceba-40a4-b978-677a3a550b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_res(df):\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = df.to_csv(path + '/evalRes.csv')\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f12ca-01c0-4c52-b80d-735f9971b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_res(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd40050-793a-4b8c-ba03-be6f1dd9865a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30b02a-be38-46f5-ae81-a57fba36dd98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
