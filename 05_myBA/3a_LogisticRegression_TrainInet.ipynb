{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd1549c-3ec6-40dd-a7cf-69cd5beeca85",
   "metadata": {},
   "source": [
    "# Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24522e9e-6237-4c8e-b502-e555da2fef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.regularizers import L1L2\n",
    "from keras.regularizers import L2\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f8e753-3ed4-47c2-a508-794c5ccff2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "   'data': {\n",
    "        'n_datasets': 10_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 4_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 10, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 8,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 3.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 42,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': -1, # currently not used\n",
    "                'val_size': 0.25,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae', #tf.keras.losses.get(config['lambda_net']['loss_lambda']),\n",
    "            'metrics': ['mae']\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.3,\n",
    "                'val_size': 0.2,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 256,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 30,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '5',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85fab86-b65d-445f-a4ff-9ce3f11d1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44bbe3e-16e1-45f1-92cf-88dbd8a87318",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['computation']['gpu_numbers'] if config['computation']['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if config['computation']['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if config['computation']['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if config['computation']['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ca8427-e1ef-419f-a901-2f010ffea5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d80c2-ba75-403e-a3d1-6611a228d86f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21aa2853-27a3-40e7-94ee-7458467f7adc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_lambda():\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    return np.load(directory + '/lambda_weights_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51ec205-d98c-40dc-9f79-0cfb6a5b5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_data():\n",
    "    directory = utilities_LR.data_path_LR(config)\n",
    "    return np.load(directory + '/X_datasets_list_dataForLambda.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b05333f3-925a-4849-93f8-ab79d01ffc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred_data():\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    return np.load(directory + '/lambda_preds_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75fc6c1a-e279-438d-81dd-3000dd58b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_test_val_split(X, y):\n",
    "#    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "#                                                        y, \n",
    "#                                                        test_size=config['inets']['data_prep']['train_test_val_split']['test_size'] + config['inets']['data_prep']['train_test_val_split']['val_size'], \n",
    "#                                                        random_state=config['inets']['data_prep']['train_test_val_split']['random_state'], \n",
    "#                                                        shuffle=config['inets']['data_prep']['train_test_val_split']['shuffle'], \n",
    "#                                                        stratify=config['inets']['data_prep']['train_test_val_split']['stratify'])\n",
    "#    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "#                                                    y_test, \n",
    "#                                                    test_size=config['inets']['data_prep']['train_test_val_split']['val_size'] / (config['inets']['data_prep']['train_test_val_split']['test_size'] + config['inets']['data_prep']['train_test_val_split']['val_size']), \n",
    "#                                                    random_state=config['inets']['data_prep']['train_test_val_split']['random_state'], \n",
    "#                                                    shuffle=config['inets']['data_prep']['train_test_val_split']['shuffle'], \n",
    "#                                                    stratify=config['inets']['data_prep']['train_test_val_split']['stratify'])\n",
    "#    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "#    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974df36-5052-4bbc-a191-4aadc84d02c1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d47754df-11d9-4365-ba85-5d5ceb10bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32794f7-c1f7-4523-b282-8d06e49cd305",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = get_y_pred_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4d3f8f5-20e5-4c61-ac1f-623c204c2d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_feature_data = get_X_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aebe75c-5ddf-406f-b30b-218bf8476a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b05597-3d15-45da-8500-033b8629fd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_feature_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f1fc041-6629-4e1e-befc-dec63874e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_feature_data = np.hstack(valid_feature_data)\n",
    "indexcol = np.array([np.arange(config['data']['n_datasets'], dtype=int)], ndmin=2).reshape(config['data']['n_datasets'], 1)\n",
    "# print(indexcol.shape)\n",
    "y_predictions = np.concatenate([indexcol, y_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58209e39-b22f-4166-81f9-587f401e7a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4001)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bbc7589-810a-4720-8b55-6caedabe8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_feature_data = tf.convert_to_tensor(valid_feature_data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f63019-5b93-4df8-bb78-1ebbc45b732e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92cd5eda-a804-4edd-8946-d5da64fd664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def custom_loss(y_predictions_index, y_coef_pred):\n",
    "    \n",
    "    index = y_predictions_index[:, 0]\n",
    "    y_true = y_predictions_index[:, 1:]\n",
    "    \n",
    "    index = tf.cast(index, tf.int32)\n",
    "    \n",
    "    X_feature_data_samples = tf.gather(valid_feature_data, index)\n",
    "    \n",
    "    #y_pred = tf.math.sigmoid(tf.linalg.matvec(valid_feature_data_sample, y_coef_pred))\n",
    "    y_pred = tf.linalg.matvec(X_feature_data_samples, y_coef_pred)\n",
    "\n",
    "    metric = tf.keras.losses.BinaryCrossentropy(\n",
    "                                from_logits=True,\n",
    "                                label_smoothing=0.0,\n",
    "                                axis=-1,\n",
    "                                reduction='auto',\n",
    "                                name='binary_crossentropy')\n",
    "    loss = metric(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e618a02-4ccd-4d44-a88d-066f91ffe2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "#def custom_loss(y_coef_dataset_index_true, y_coef_pred):\n",
    "#    \n",
    "#    #print(\"true\", type(y_coef_dataset_index_true))\n",
    "#    #print(\"true\", y_coef_dataset_index_true)\n",
    "#    #\n",
    "#    #print(\"pred\", type(y_coef_pred))\n",
    "#    #print(\"pred\", y_coef_pred)\n",
    "#    #\n",
    "#    #print(y_coef_dataset_index_true.flatten().shape)\n",
    "#    #y_coef_dataset_index_true = y_coef_dataset_index_true.reshape([config['data']['n_targets']+1])\n",
    "#    index = y_coef_dataset_index_true[:, 0]\n",
    "#    y_coef_true = y_coef_dataset_index_true[:, 1:]\n",
    "#    \n",
    "#    #print(\"true\", type(y_coef_true))\n",
    "#    #print(\"true\", y_coef_true)\n",
    "#    \n",
    "#    index = tf.cast(index, tf.int32)\n",
    "#    #tf.print(index, output_stream=sys.stderr)\n",
    "#    \n",
    "#    valid_feature_data_sample = tf.gather(valid_feature_data, index)\n",
    "#    \n",
    "#    #y_true = tf.round(tf.matmul(valid_feature_data_sample, y_coef_true))\n",
    "#    #y_pred = tf.round(tf.matmul(valid_feature_data_sample, y_coef_pred))\n",
    "#    \n",
    "#    y_true = tf.math.sigmoid(tf.linalg.matvec(valid_feature_data_sample, y_coef_true))\n",
    "#    y_pred = tf.math.sigmoid(tf.linalg.matvec(valid_feature_data_sample, y_coef_pred))\n",
    "#    \n",
    "#    #tf.print(y_true, output_stream=sys.stderr)\n",
    "#    #tf.print(y_pred, output_stream=sys.stderr)\n",
    "#\n",
    "#    #metric = tf.keras.metrics.binary_crossentropy(y_true, y_pred, from_logits=True, label_smoothing=0.0, axis=-1)\n",
    "#    metric = tf.keras.losses.BinaryCrossentropy(\n",
    "#                                from_logits=True,\n",
    "#                                label_smoothing=0.0,\n",
    "#                                axis=-1,\n",
    "#                                reduction='auto',\n",
    "#                                name='binary_crossentropy')\n",
    "#    val = metric(y_true, y_pred)\n",
    "#    #tf.print(val, output_stream=sys.stderr)\n",
    "#    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35a9a546-bca2-4392-9d84-fe9592f4dad6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (8000, 4096)              29745152  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (8000, 2048)              8390656   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (8000, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (8000, 512)               524800    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (8000, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,763,914\n",
      "Trainable params: 40,763,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "32/32 [==============================] - 5s 55ms/step - loss: 1.0019 - val_loss: 0.7950\n",
      "Epoch 2/1000\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.6365 - val_loss: 0.7586\n",
      "Epoch 3/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.5157 - val_loss: 0.7736\n",
      "Epoch 4/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.4243 - val_loss: 0.7960\n",
      "Epoch 5/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3514 - val_loss: 0.8269\n",
      "Epoch 6/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3010 - val_loss: 0.8571\n",
      "Epoch 7/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2680 - val_loss: 0.8968\n",
      "Epoch 8/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2459 - val_loss: 0.9458\n",
      "Epoch 9/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2264 - val_loss: 0.9708\n",
      "Epoch 10/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2095 - val_loss: 1.0221\n",
      "Epoch 11/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1892 - val_loss: 1.0339\n",
      "Epoch 12/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.1722 - val_loss: 1.0524\n",
      "Epoch 13/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1619 - val_loss: 1.0790\n",
      "Epoch 14/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1514 - val_loss: 1.0835\n",
      "Epoch 15/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1443 - val_loss: 1.1248\n",
      "Epoch 16/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1357 - val_loss: 1.1552\n",
      "Epoch 17/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1309 - val_loss: 1.1885\n",
      "Epoch 18/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1282 - val_loss: 1.1863\n",
      "Epoch 19/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1205 - val_loss: 1.2304\n",
      "Epoch 20/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1125 - val_loss: 1.2472\n",
      "Epoch 21/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1084 - val_loss: 1.2783\n",
      "Epoch 22/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1058 - val_loss: 1.2734\n",
      "Epoch 23/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1044 - val_loss: 1.2789\n",
      "Epoch 24/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1003 - val_loss: 1.2869\n",
      "Epoch 25/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0985 - val_loss: 1.3303\n",
      "Epoch 26/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0996 - val_loss: 1.3171\n",
      "Epoch 27/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0978 - val_loss: 1.3426\n",
      "Epoch 28/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0968 - val_loss: 1.3619\n",
      "Epoch 29/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0957 - val_loss: 1.3560\n",
      "Epoch 30/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0918 - val_loss: 1.3825\n",
      "Epoch 31/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0898 - val_loss: 1.3863\n",
      "Epoch 32/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0885 - val_loss: 1.4571\n",
      "Epoch 33/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0962 - val_loss: 1.3989\n",
      "Epoch 34/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0905 - val_loss: 1.4212\n",
      "Epoch 35/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0875 - val_loss: 1.4517\n",
      "Epoch 36/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0882 - val_loss: 1.4192\n",
      "Epoch 37/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0849 - val_loss: 1.4554\n",
      "Epoch 38/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0822 - val_loss: 1.5132\n",
      "Epoch 39/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0837 - val_loss: 1.4780\n",
      "Epoch 40/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0825 - val_loss: 1.5055\n",
      "Epoch 41/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0813 - val_loss: 1.5085\n",
      "Epoch 42/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0837 - val_loss: 1.4692\n",
      "Epoch 43/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0805 - val_loss: 1.4826\n",
      "Epoch 44/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0785 - val_loss: 1.4952\n",
      "Epoch 45/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0804 - val_loss: 1.5086\n",
      "Epoch 46/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0779 - val_loss: 1.5130\n",
      "Epoch 47/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0743 - val_loss: 1.5445\n",
      "Epoch 48/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0727 - val_loss: 1.5274\n",
      "Epoch 49/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0714 - val_loss: 1.5512\n",
      "Epoch 50/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0722 - val_loss: 1.5496\n",
      "Epoch 51/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0711 - val_loss: 1.5837\n",
      "Epoch 52/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0709 - val_loss: 1.5657\n",
      "Epoch 53/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0705 - val_loss: 1.6101\n",
      "Epoch 54/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0701 - val_loss: 1.5563\n",
      "Epoch 55/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0723 - val_loss: 1.5848\n",
      "Epoch 56/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0733 - val_loss: 1.5988\n",
      "Epoch 57/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0733 - val_loss: 1.5767\n",
      "Epoch 58/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0744 - val_loss: 1.6001\n",
      "Epoch 59/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0741 - val_loss: 1.6065\n",
      "Epoch 60/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0745 - val_loss: 1.5713\n",
      "Epoch 61/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0715 - val_loss: 1.6153\n",
      "Epoch 62/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0690 - val_loss: 1.6062\n",
      "Epoch 63/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0697 - val_loss: 1.6256\n",
      "Epoch 64/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0681 - val_loss: 1.6826\n",
      "Epoch 65/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0685 - val_loss: 1.6588\n",
      "Epoch 66/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0679 - val_loss: 1.6396\n",
      "Epoch 67/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0662 - val_loss: 1.6755\n",
      "Epoch 68/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0665 - val_loss: 1.6567\n",
      "Epoch 69/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0670 - val_loss: 1.6838\n",
      "Epoch 70/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0677 - val_loss: 1.6602\n",
      "Epoch 71/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0663 - val_loss: 1.6958\n",
      "Epoch 72/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0651 - val_loss: 1.6716\n",
      "Epoch 73/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0666 - val_loss: 1.6793\n",
      "Epoch 74/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0671 - val_loss: 1.6965\n",
      "Epoch 75/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0682 - val_loss: 1.6790\n",
      "Epoch 76/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0686 - val_loss: 1.7139\n",
      "Epoch 77/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0691 - val_loss: 1.6758\n",
      "Epoch 78/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0691 - val_loss: 1.6740\n",
      "Epoch 79/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0671 - val_loss: 1.6928\n",
      "Epoch 80/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0653 - val_loss: 1.7000\n",
      "Epoch 81/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0653 - val_loss: 1.7119\n",
      "Epoch 82/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0642 - val_loss: 1.7195\n",
      "Epoch 83/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0635 - val_loss: 1.7517\n",
      "Epoch 84/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0628 - val_loss: 1.7436\n",
      "Epoch 85/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0635 - val_loss: 1.7265\n",
      "Epoch 86/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0620 - val_loss: 1.7480\n",
      "Epoch 87/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0618 - val_loss: 1.7812\n",
      "Epoch 88/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0617 - val_loss: 1.8000\n",
      "Epoch 89/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0618 - val_loss: 1.7863\n",
      "Epoch 90/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0624 - val_loss: 1.8045\n",
      "Epoch 91/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0627 - val_loss: 1.7673\n",
      "Epoch 92/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0643 - val_loss: 1.7770\n",
      "Epoch 93/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0625 - val_loss: 1.7885\n",
      "Epoch 94/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0618 - val_loss: 1.7596\n",
      "Epoch 95/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0613 - val_loss: 1.7809\n",
      "Epoch 96/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0613 - val_loss: 1.7846\n",
      "Epoch 97/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0612 - val_loss: 1.8194\n",
      "Epoch 98/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0614 - val_loss: 1.7977\n",
      "Epoch 99/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0604 - val_loss: 1.8056\n",
      "Epoch 100/1000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.0596 - val_loss: 1.8228\n",
      "Epoch 101/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.0595 - val_loss: 1.8415\n",
      "Epoch 102/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0590 - val_loss: 1.8620\n",
      "Epoch 103/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0596 - val_loss: 1.8340\n",
      "Epoch 104/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0595 - val_loss: 1.8580\n",
      "Epoch 105/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0596 - val_loss: 1.8252\n",
      "Epoch 106/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0596 - val_loss: 1.8585\n",
      "Epoch 107/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0596 - val_loss: 1.8368\n",
      "Epoch 108/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0591 - val_loss: 1.8342\n",
      "Epoch 109/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0591 - val_loss: 1.8457\n",
      "Epoch 110/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 1.8838\n",
      "Epoch 111/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0592 - val_loss: 1.8579\n",
      "Epoch 112/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0591 - val_loss: 1.8897\n",
      "Epoch 113/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0619 - val_loss: 1.8933\n",
      "Epoch 114/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0640 - val_loss: 1.8339\n",
      "Epoch 115/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0646 - val_loss: 1.8171\n",
      "Epoch 116/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0636 - val_loss: 1.8295\n",
      "Epoch 117/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0627 - val_loss: 1.8657\n",
      "Epoch 118/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0646 - val_loss: 1.8497\n",
      "Epoch 119/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0650 - val_loss: 1.7692\n",
      "Epoch 120/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0656 - val_loss: 1.8158\n",
      "Epoch 121/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0642 - val_loss: 1.8238\n",
      "Epoch 122/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0631 - val_loss: 1.8653\n",
      "Epoch 123/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0646 - val_loss: 1.7866\n",
      "Epoch 124/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0620 - val_loss: 1.8092\n",
      "Epoch 125/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0607 - val_loss: 1.8758\n",
      "Epoch 126/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 1.9265\n",
      "Epoch 127/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 1.9178\n",
      "Epoch 128/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0558 - val_loss: 1.9434\n",
      "Epoch 129/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0553 - val_loss: 1.9441\n",
      "Epoch 130/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0544 - val_loss: 1.9581\n",
      "Epoch 131/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0537 - val_loss: 1.9492\n",
      "Epoch 132/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0538 - val_loss: 1.9799\n",
      "Epoch 133/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0542 - val_loss: 1.9992\n",
      "Epoch 134/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0536 - val_loss: 1.9969\n",
      "Epoch 135/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0545 - val_loss: 1.9990\n",
      "Epoch 136/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0550 - val_loss: 1.9511\n",
      "Epoch 137/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0556 - val_loss: 2.0072\n",
      "Epoch 138/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0558 - val_loss: 1.9894\n",
      "Epoch 139/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0563 - val_loss: 1.9784\n",
      "Epoch 140/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 1.9699\n",
      "Epoch 141/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0598 - val_loss: 1.9230\n",
      "Epoch 142/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0629 - val_loss: 1.9422\n",
      "Epoch 143/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0660 - val_loss: 1.8990\n",
      "Epoch 144/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0643 - val_loss: 1.8814\n",
      "Epoch 145/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0629 - val_loss: 1.8671\n",
      "Epoch 146/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0608 - val_loss: 1.9077\n",
      "Epoch 147/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0594 - val_loss: 1.9192\n",
      "Epoch 148/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 1.9553\n",
      "Epoch 149/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0562 - val_loss: 1.9690\n",
      "Epoch 150/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0549 - val_loss: 1.9947\n",
      "Epoch 151/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0551 - val_loss: 2.0300\n",
      "Epoch 152/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0558 - val_loss: 1.9922\n",
      "Epoch 153/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0552 - val_loss: 2.0186\n",
      "Epoch 154/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0562 - val_loss: 2.0376\n",
      "Epoch 155/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0556 - val_loss: 2.0589\n",
      "Epoch 156/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0560 - val_loss: 2.0268\n",
      "Epoch 157/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 2.0349\n",
      "Epoch 158/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 2.0280\n",
      "Epoch 159/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0571 - val_loss: 2.0397\n",
      "Epoch 160/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0570 - val_loss: 1.9998\n",
      "Epoch 161/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 2.0173\n",
      "Epoch 162/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0556 - val_loss: 2.0745\n",
      "Epoch 163/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0554 - val_loss: 2.0280\n",
      "Epoch 164/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0545 - val_loss: 2.0350\n",
      "Epoch 165/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0540 - val_loss: 2.0549\n",
      "Epoch 166/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0538 - val_loss: 2.0993\n",
      "Epoch 167/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0532 - val_loss: 2.0899\n",
      "Epoch 168/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0529 - val_loss: 2.1183\n",
      "Epoch 169/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0529 - val_loss: 2.0999\n",
      "Epoch 170/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0535 - val_loss: 2.1103\n",
      "Epoch 171/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0542 - val_loss: 2.0792\n",
      "Epoch 172/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0540 - val_loss: 2.1341\n",
      "Epoch 173/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0558 - val_loss: 2.0539\n",
      "Epoch 174/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 2.0741\n",
      "Epoch 175/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0595 - val_loss: 2.0713\n",
      "Epoch 176/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0587 - val_loss: 2.0563\n",
      "Epoch 177/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0584 - val_loss: 2.0125\n",
      "Epoch 178/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 2.0281\n",
      "Epoch 179/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0580 - val_loss: 2.0146\n",
      "Epoch 180/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 2.0683\n",
      "Epoch 181/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 2.0822\n",
      "Epoch 182/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0560 - val_loss: 2.0364\n",
      "Epoch 183/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0556 - val_loss: 2.0923\n",
      "Epoch 184/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0555 - val_loss: 2.0448\n",
      "Epoch 185/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0547 - val_loss: 2.0775\n",
      "Epoch 186/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0558 - val_loss: 2.0925\n",
      "Epoch 187/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 2.0925\n",
      "Epoch 188/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0579 - val_loss: 2.0682\n",
      "Epoch 189/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 2.0992\n",
      "Epoch 190/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 2.0866\n",
      "Epoch 191/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0560 - val_loss: 2.0573\n",
      "Epoch 192/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0544 - val_loss: 2.1315\n",
      "Epoch 193/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0535 - val_loss: 2.1477\n",
      "Epoch 194/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0526 - val_loss: 2.1659\n",
      "Epoch 195/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0518 - val_loss: 2.1530\n",
      "Epoch 196/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0514 - val_loss: 2.1758\n",
      "Epoch 197/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0512 - val_loss: 2.1833\n",
      "Epoch 198/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0513 - val_loss: 2.1970\n",
      "Epoch 199/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0528 - val_loss: 2.1682\n",
      "Epoch 200/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0524 - val_loss: 2.1557\n",
      "Epoch 201/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0518 - val_loss: 2.1883\n",
      "Epoch 202/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0528 - val_loss: 2.1555\n",
      "Epoch 203/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0530 - val_loss: 2.1721\n",
      "Epoch 204/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0526 - val_loss: 2.1491\n",
      "Epoch 205/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0528 - val_loss: 2.1755\n",
      "Epoch 206/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0535 - val_loss: 2.1627\n",
      "Epoch 207/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0532 - val_loss: 2.1914\n",
      "Epoch 208/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0530 - val_loss: 2.1830\n",
      "Epoch 209/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0536 - val_loss: 2.1846\n",
      "Epoch 210/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0544 - val_loss: 2.1759\n",
      "Epoch 211/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0540 - val_loss: 2.1856\n",
      "Epoch 212/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0545 - val_loss: 2.1636\n",
      "Epoch 213/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0553 - val_loss: 2.1581\n",
      "Epoch 214/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 2.0867\n",
      "Epoch 215/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 2.0489\n",
      "Epoch 216/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 2.1116\n",
      "Epoch 217/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0595 - val_loss: 2.0412\n",
      "Epoch 218/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0593 - val_loss: 1.9750\n",
      "Epoch 219/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0588 - val_loss: 2.0997\n",
      "Epoch 220/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0606 - val_loss: 2.0810\n",
      "Epoch 221/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0623 - val_loss: 2.0273\n",
      "Epoch 222/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0642 - val_loss: 2.0426\n",
      "Epoch 223/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0620 - val_loss: 2.0432\n",
      "Epoch 224/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0582 - val_loss: 2.0563\n",
      "Epoch 225/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0558 - val_loss: 2.0584\n",
      "Epoch 226/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0536 - val_loss: 2.1427\n",
      "Epoch 227/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0525 - val_loss: 2.1492\n",
      "Epoch 228/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0513 - val_loss: 2.2021\n",
      "Epoch 229/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0503 - val_loss: 2.1967\n",
      "Epoch 230/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.2185\n",
      "Epoch 231/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0495 - val_loss: 2.2409\n",
      "Epoch 232/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 2.2696\n",
      "Epoch 233/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.2872\n",
      "Epoch 234/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.3042\n",
      "Epoch 235/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.3030\n",
      "Epoch 236/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.3163\n",
      "Epoch 237/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3305\n",
      "Epoch 238/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3407\n",
      "Epoch 239/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.3394\n",
      "Epoch 240/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.3395\n",
      "Epoch 241/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3518\n",
      "Epoch 242/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0495 - val_loss: 2.3308\n",
      "Epoch 243/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.3361\n",
      "Epoch 244/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0498 - val_loss: 2.3145\n",
      "Epoch 245/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.3469\n",
      "Epoch 246/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.3570\n",
      "Epoch 247/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 2.3459\n",
      "Epoch 248/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0512 - val_loss: 2.3207\n",
      "Epoch 249/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0517 - val_loss: 2.3156\n",
      "Epoch 250/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0525 - val_loss: 2.3219\n",
      "Epoch 251/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0536 - val_loss: 2.3044\n",
      "Epoch 252/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0564 - val_loss: 2.1898\n",
      "Epoch 253/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 2.1251\n",
      "Epoch 254/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0594 - val_loss: 2.1553\n",
      "Epoch 255/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0597 - val_loss: 2.1367\n",
      "Epoch 256/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0603 - val_loss: 2.0543\n",
      "Epoch 257/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0609 - val_loss: 2.1198\n",
      "Epoch 258/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0612 - val_loss: 2.1117\n",
      "Epoch 259/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0602 - val_loss: 2.0746\n",
      "Epoch 260/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0605 - val_loss: 2.0783\n",
      "Epoch 261/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0615 - val_loss: 2.0689\n",
      "Epoch 262/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0604 - val_loss: 2.0815\n",
      "Epoch 263/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0593 - val_loss: 2.1209\n",
      "Epoch 264/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0574 - val_loss: 2.1810\n",
      "Epoch 265/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0554 - val_loss: 2.1775\n",
      "Epoch 266/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0540 - val_loss: 2.1776\n",
      "Epoch 267/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0528 - val_loss: 2.2373\n",
      "Epoch 268/1000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0518 - val_loss: 2.2513\n",
      "Epoch 269/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0505 - val_loss: 2.2596\n",
      "Epoch 270/1000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0499 - val_loss: 2.2752\n",
      "Epoch 271/1000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.0494 - val_loss: 2.2993\n",
      "Epoch 272/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.3254\n",
      "Epoch 273/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.3434\n",
      "Epoch 274/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.3573\n",
      "Epoch 275/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3753\n",
      "Epoch 276/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.3681\n",
      "Epoch 277/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3798\n",
      "Epoch 278/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.4095\n",
      "Epoch 279/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.4005\n",
      "Epoch 280/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.4039\n",
      "Epoch 281/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.4104\n",
      "Epoch 282/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4241\n",
      "Epoch 283/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.4100\n",
      "Epoch 284/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.4165\n",
      "Epoch 285/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 2.4295\n",
      "Epoch 286/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.4030\n",
      "Epoch 287/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.4333\n",
      "Epoch 288/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0498 - val_loss: 2.4113\n",
      "Epoch 289/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0510 - val_loss: 2.3466\n",
      "Epoch 290/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0518 - val_loss: 2.3811\n",
      "Epoch 291/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0536 - val_loss: 2.3313\n",
      "Epoch 292/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0557 - val_loss: 2.2471\n",
      "Epoch 293/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 2.2617\n",
      "Epoch 294/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0584 - val_loss: 2.1379\n",
      "Epoch 295/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0598 - val_loss: 2.1224\n",
      "Epoch 296/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0601 - val_loss: 2.1583\n",
      "Epoch 297/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0589 - val_loss: 2.1144\n",
      "Epoch 298/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0577 - val_loss: 2.1462\n",
      "Epoch 299/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0575 - val_loss: 2.1853\n",
      "Epoch 300/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0560 - val_loss: 2.2157\n",
      "Epoch 301/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0550 - val_loss: 2.1562\n",
      "Epoch 302/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0531 - val_loss: 2.2282\n",
      "Epoch 303/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0516 - val_loss: 2.2486\n",
      "Epoch 304/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0506 - val_loss: 2.2979\n",
      "Epoch 305/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497 - val_loss: 2.3024\n",
      "Epoch 306/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 2.3334\n",
      "Epoch 307/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.3586\n",
      "Epoch 308/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3679\n",
      "Epoch 309/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.3843\n",
      "Epoch 310/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0487 - val_loss: 2.4113\n",
      "Epoch 311/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.4296\n",
      "Epoch 312/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4264\n",
      "Epoch 313/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0484 - val_loss: 2.4354\n",
      "Epoch 314/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4316\n",
      "Epoch 315/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4562\n",
      "Epoch 316/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0485 - val_loss: 2.4482\n",
      "Epoch 317/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.4379\n",
      "Epoch 318/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4418\n",
      "Epoch 319/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4570\n",
      "Epoch 320/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.4371\n",
      "Epoch 321/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0494 - val_loss: 2.4187\n",
      "Epoch 322/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0499 - val_loss: 2.4011\n",
      "Epoch 323/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 2.4125\n",
      "Epoch 324/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0509 - val_loss: 2.4498\n",
      "Epoch 325/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0513 - val_loss: 2.2888\n",
      "Epoch 326/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0515 - val_loss: 2.3671\n",
      "Epoch 327/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0530 - val_loss: 2.3160\n",
      "Epoch 328/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0536 - val_loss: 2.3440\n",
      "Epoch 329/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0547 - val_loss: 2.2984\n",
      "Epoch 330/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0572 - val_loss: 2.3207\n",
      "Epoch 331/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0580 - val_loss: 2.1829\n",
      "Epoch 332/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0585 - val_loss: 2.2348\n",
      "Epoch 333/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 2.1369\n",
      "Epoch 334/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0552 - val_loss: 2.2418\n",
      "Epoch 335/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0546 - val_loss: 2.2351\n",
      "Epoch 336/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0539 - val_loss: 2.2053\n",
      "Epoch 337/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0525 - val_loss: 2.2347\n",
      "Epoch 338/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0513 - val_loss: 2.2862\n",
      "Epoch 339/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0506 - val_loss: 2.3328\n",
      "Epoch 340/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0501 - val_loss: 2.3417\n",
      "Epoch 341/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0498 - val_loss: 2.3673\n",
      "Epoch 342/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.3895\n",
      "Epoch 343/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.4130\n",
      "Epoch 344/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4330\n",
      "Epoch 345/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4316\n",
      "Epoch 346/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.4622\n",
      "Epoch 347/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.4476\n",
      "Epoch 348/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4518\n",
      "Epoch 349/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.4616\n",
      "Epoch 350/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 2.4174\n",
      "Epoch 351/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4556\n",
      "Epoch 352/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.4724\n",
      "Epoch 353/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4668\n",
      "Epoch 354/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4486\n",
      "Epoch 355/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.5030\n",
      "Epoch 356/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4440\n",
      "Epoch 357/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4854\n",
      "Epoch 358/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4836\n",
      "Epoch 359/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.4644\n",
      "Epoch 360/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0495 - val_loss: 2.4796\n",
      "Epoch 361/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.4494\n",
      "Epoch 362/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.4587\n",
      "Epoch 363/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0512 - val_loss: 2.4027\n",
      "Epoch 364/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0514 - val_loss: 2.4063\n",
      "Epoch 365/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0521 - val_loss: 2.4157\n",
      "Epoch 366/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0542 - val_loss: 2.3807\n",
      "Epoch 367/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0558 - val_loss: 2.2839\n",
      "Epoch 368/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0583 - val_loss: 2.2511\n",
      "Epoch 369/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0616 - val_loss: 2.0703\n",
      "Epoch 370/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0661 - val_loss: 2.0898\n",
      "Epoch 371/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0658 - val_loss: 2.0544\n",
      "Epoch 372/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0625 - val_loss: 2.0563\n",
      "Epoch 373/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0598 - val_loss: 2.0958\n",
      "Epoch 374/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0561 - val_loss: 2.1236\n",
      "Epoch 375/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0536 - val_loss: 2.2093\n",
      "Epoch 376/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0518 - val_loss: 2.2861\n",
      "Epoch 377/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.3211\n",
      "Epoch 378/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0492 - val_loss: 2.3430\n",
      "Epoch 379/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0491 - val_loss: 2.3729\n",
      "Epoch 380/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0486 - val_loss: 2.3726\n",
      "Epoch 381/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.4087\n",
      "Epoch 382/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.4330\n",
      "Epoch 383/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.4443\n",
      "Epoch 384/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4467\n",
      "Epoch 385/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4584\n",
      "Epoch 386/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0479 - val_loss: 2.4795\n",
      "Epoch 387/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4839\n",
      "Epoch 388/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4934\n",
      "Epoch 389/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5014\n",
      "Epoch 390/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0479 - val_loss: 2.5151\n",
      "Epoch 391/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0479 - val_loss: 2.5206\n",
      "Epoch 392/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.5349\n",
      "Epoch 393/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5333\n",
      "Epoch 394/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5321\n",
      "Epoch 395/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5478\n",
      "Epoch 396/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.5473\n",
      "Epoch 397/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5504\n",
      "Epoch 398/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5527\n",
      "Epoch 399/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.5107\n",
      "Epoch 400/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.5291\n",
      "Epoch 401/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0500 - val_loss: 2.4385\n",
      "Epoch 402/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0503 - val_loss: 2.4665\n",
      "Epoch 403/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0509 - val_loss: 2.4420\n",
      "Epoch 404/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0513 - val_loss: 2.5088\n",
      "Epoch 405/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0515 - val_loss: 2.4132\n",
      "Epoch 406/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0518 - val_loss: 2.4216\n",
      "Epoch 407/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0520 - val_loss: 2.3916\n",
      "Epoch 408/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0525 - val_loss: 2.3953\n",
      "Epoch 409/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0529 - val_loss: 2.3827\n",
      "Epoch 410/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0527 - val_loss: 2.3874\n",
      "Epoch 411/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0526 - val_loss: 2.4005\n",
      "Epoch 412/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0521 - val_loss: 2.3620\n",
      "Epoch 413/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0514 - val_loss: 2.3773\n",
      "Epoch 414/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0509 - val_loss: 2.3433\n",
      "Epoch 415/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0510 - val_loss: 2.4521\n",
      "Epoch 416/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0501 - val_loss: 2.4362\n",
      "Epoch 417/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.4409\n",
      "Epoch 418/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 2.4691\n",
      "Epoch 419/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.5049\n",
      "Epoch 420/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4956\n",
      "Epoch 421/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0486 - val_loss: 2.5189\n",
      "Epoch 422/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4977\n",
      "Epoch 423/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.5182\n",
      "Epoch 424/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.5172\n",
      "Epoch 425/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.5725\n",
      "Epoch 426/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5339\n",
      "Epoch 427/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.5041\n",
      "Epoch 428/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.5630\n",
      "Epoch 429/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0499 - val_loss: 2.5075\n",
      "Epoch 430/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.4749\n",
      "Epoch 431/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.4703\n",
      "Epoch 432/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.4738\n",
      "Epoch 433/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.4878\n",
      "Epoch 434/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0503 - val_loss: 2.4643\n",
      "Epoch 435/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.4579\n",
      "Epoch 436/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.4332\n",
      "Epoch 437/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0500 - val_loss: 2.5173\n",
      "Epoch 438/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0510 - val_loss: 2.3948\n",
      "Epoch 439/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0516 - val_loss: 2.4183\n",
      "Epoch 440/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511 - val_loss: 2.4129\n",
      "Epoch 441/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0510 - val_loss: 2.4166\n",
      "Epoch 442/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0512 - val_loss: 2.4504\n",
      "Epoch 443/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0509 - val_loss: 2.4005\n",
      "Epoch 444/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.4165\n",
      "Epoch 445/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.4047\n",
      "Epoch 446/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0504 - val_loss: 2.4894\n",
      "Epoch 447/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.4263\n",
      "Epoch 448/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0500 - val_loss: 2.4577\n",
      "Epoch 449/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497 - val_loss: 2.4612\n",
      "Epoch 450/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.4595\n",
      "Epoch 451/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0495 - val_loss: 2.4646\n",
      "Epoch 452/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.4978\n",
      "Epoch 453/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.4862\n",
      "Epoch 454/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0500 - val_loss: 2.4497\n",
      "Epoch 455/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.3924\n",
      "Epoch 456/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 2.4530\n",
      "Epoch 457/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511 - val_loss: 2.4376\n",
      "Epoch 458/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0515 - val_loss: 2.3448\n",
      "Epoch 459/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0522 - val_loss: 2.2767\n",
      "Epoch 460/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0518 - val_loss: 2.2897\n",
      "Epoch 461/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0529 - val_loss: 2.1273\n",
      "Epoch 462/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0524 - val_loss: 2.2989\n",
      "Epoch 463/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0539 - val_loss: 2.2763\n",
      "Epoch 464/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0531 - val_loss: 2.2367\n",
      "Epoch 465/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0526 - val_loss: 2.2831\n",
      "Epoch 466/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0530 - val_loss: 2.2635\n",
      "Epoch 467/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0535 - val_loss: 2.3001\n",
      "Epoch 468/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0522 - val_loss: 2.2663\n",
      "Epoch 469/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0512 - val_loss: 2.2834\n",
      "Epoch 470/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 2.3592\n",
      "Epoch 471/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.3584\n",
      "Epoch 472/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3605\n",
      "Epoch 473/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.3757\n",
      "Epoch 474/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4414\n",
      "Epoch 475/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0482 - val_loss: 2.4482\n",
      "Epoch 476/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.4553\n",
      "Epoch 477/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4956\n",
      "Epoch 478/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4950\n",
      "Epoch 479/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5011\n",
      "Epoch 480/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5232\n",
      "Epoch 481/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5273\n",
      "Epoch 482/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5388\n",
      "Epoch 483/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5457\n",
      "Epoch 484/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5559\n",
      "Epoch 485/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5623\n",
      "Epoch 486/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.5707\n",
      "Epoch 487/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.5613\n",
      "Epoch 488/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5720\n",
      "Epoch 489/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0481 - val_loss: 2.5744\n",
      "Epoch 490/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0483 - val_loss: 2.5626\n",
      "Epoch 491/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5954\n",
      "Epoch 492/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0488 - val_loss: 2.5773\n",
      "Epoch 493/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.5459\n",
      "Epoch 494/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.5549\n",
      "Epoch 495/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497 - val_loss: 2.5394\n",
      "Epoch 496/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0507 - val_loss: 2.4491\n",
      "Epoch 497/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0526 - val_loss: 2.4582\n",
      "Epoch 498/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0552 - val_loss: 2.2000\n",
      "Epoch 499/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0569 - val_loss: 2.1788\n",
      "Epoch 500/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0581 - val_loss: 2.1617\n",
      "Epoch 501/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0572 - val_loss: 2.1071\n",
      "Epoch 502/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0566 - val_loss: 2.0289\n",
      "Epoch 503/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0560 - val_loss: 2.2890\n",
      "Epoch 504/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0535 - val_loss: 2.2891\n",
      "Epoch 505/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0513 - val_loss: 2.2855\n",
      "Epoch 506/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0497 - val_loss: 2.3378\n",
      "Epoch 507/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.3650\n",
      "Epoch 508/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4137\n",
      "Epoch 509/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.4216\n",
      "Epoch 510/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0481 - val_loss: 2.4569\n",
      "Epoch 511/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0479 - val_loss: 2.4659\n",
      "Epoch 512/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4881\n",
      "Epoch 513/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5057\n",
      "Epoch 514/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5043\n",
      "Epoch 515/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5159\n",
      "Epoch 516/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5299\n",
      "Epoch 517/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5428\n",
      "Epoch 518/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5482\n",
      "Epoch 519/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5536\n",
      "Epoch 520/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5621\n",
      "Epoch 521/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5727\n",
      "Epoch 522/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5722\n",
      "Epoch 523/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 2.5914\n",
      "Epoch 524/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5883\n",
      "Epoch 525/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.5875\n",
      "Epoch 526/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5819\n",
      "Epoch 527/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.5874\n",
      "Epoch 528/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.5680\n",
      "Epoch 529/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5768\n",
      "Epoch 530/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.5671\n",
      "Epoch 531/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.5675\n",
      "Epoch 532/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.5040\n",
      "Epoch 533/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.5700\n",
      "Epoch 534/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0501 - val_loss: 2.5181\n",
      "Epoch 535/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0503 - val_loss: 2.4636\n",
      "Epoch 536/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0508 - val_loss: 2.4577\n",
      "Epoch 537/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0506 - val_loss: 2.3982\n",
      "Epoch 538/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0510 - val_loss: 2.3958\n",
      "Epoch 539/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0510 - val_loss: 2.4325\n",
      "Epoch 540/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0512 - val_loss: 2.4127\n",
      "Epoch 541/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0520 - val_loss: 2.3250\n",
      "Epoch 542/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0519 - val_loss: 2.3321\n",
      "Epoch 543/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0517 - val_loss: 2.3446\n",
      "Epoch 544/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0521 - val_loss: 2.3029\n",
      "Epoch 545/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0525 - val_loss: 2.2351\n",
      "Epoch 546/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0527 - val_loss: 2.2764\n",
      "Epoch 547/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0523 - val_loss: 2.2875\n",
      "Epoch 548/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0516 - val_loss: 2.2820\n",
      "Epoch 549/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0513 - val_loss: 2.3835\n",
      "Epoch 550/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.3293\n",
      "Epoch 551/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.4123\n",
      "Epoch 552/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3893\n",
      "Epoch 553/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.4501\n",
      "Epoch 554/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.4559\n",
      "Epoch 555/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4804\n",
      "Epoch 556/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4777\n",
      "Epoch 557/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 2.5024\n",
      "Epoch 558/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5166\n",
      "Epoch 559/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5221\n",
      "Epoch 560/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.5394\n",
      "Epoch 561/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5407\n",
      "Epoch 562/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5658\n",
      "Epoch 563/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5559\n",
      "Epoch 564/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5657\n",
      "Epoch 565/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5733\n",
      "Epoch 566/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5800\n",
      "Epoch 567/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 2.5953\n",
      "Epoch 568/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5826\n",
      "Epoch 569/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.6056\n",
      "Epoch 570/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0480 - val_loss: 2.5901\n",
      "Epoch 571/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.5947\n",
      "Epoch 572/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5908\n",
      "Epoch 573/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.5553\n",
      "Epoch 574/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.5259\n",
      "Epoch 575/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.4502\n",
      "Epoch 576/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 2.4208\n",
      "Epoch 577/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511 - val_loss: 2.4166\n",
      "Epoch 578/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0523 - val_loss: 2.2856\n",
      "Epoch 579/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0528 - val_loss: 2.3803\n",
      "Epoch 580/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0533 - val_loss: 2.2641\n",
      "Epoch 581/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0534 - val_loss: 2.2258\n",
      "Epoch 582/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0537 - val_loss: 2.3181\n",
      "Epoch 583/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0525 - val_loss: 2.2914\n",
      "Epoch 584/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0516 - val_loss: 2.3212\n",
      "Epoch 585/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0512 - val_loss: 2.2969\n",
      "Epoch 586/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0503 - val_loss: 2.3772\n",
      "Epoch 587/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.3784\n",
      "Epoch 588/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.4199\n",
      "Epoch 589/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.4063\n",
      "Epoch 590/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.4639\n",
      "Epoch 591/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4790\n",
      "Epoch 592/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0480 - val_loss: 2.4790\n",
      "Epoch 593/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5080\n",
      "Epoch 594/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 2.5224\n",
      "Epoch 595/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5255\n",
      "Epoch 596/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5391\n",
      "Epoch 597/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5442\n",
      "Epoch 598/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5703\n",
      "Epoch 599/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5628\n",
      "Epoch 600/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5711\n",
      "Epoch 601/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5732\n",
      "Epoch 602/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 2.5838\n",
      "Epoch 603/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5863\n",
      "Epoch 604/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5925\n",
      "Epoch 605/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.5820\n",
      "Epoch 606/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5960\n",
      "Epoch 607/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.5936\n",
      "Epoch 608/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5790\n",
      "Epoch 609/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5687\n",
      "Epoch 610/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.5389\n",
      "Epoch 611/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.5482\n",
      "Epoch 612/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.4795\n",
      "Epoch 613/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0498 - val_loss: 2.4806\n",
      "Epoch 614/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0507 - val_loss: 2.4909\n",
      "Epoch 615/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 2.4441\n",
      "Epoch 616/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0515 - val_loss: 2.3040\n",
      "Epoch 617/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0535 - val_loss: 2.2990\n",
      "Epoch 618/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0562 - val_loss: 1.9383\n",
      "Epoch 619/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0586 - val_loss: 2.0197\n",
      "Epoch 620/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0603 - val_loss: 1.8949\n",
      "Epoch 621/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0578 - val_loss: 1.9929\n",
      "Epoch 622/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0567 - val_loss: 1.9927\n",
      "Epoch 623/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0542 - val_loss: 2.0209\n",
      "Epoch 624/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0517 - val_loss: 2.1745\n",
      "Epoch 625/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.2297\n",
      "Epoch 626/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0486 - val_loss: 2.2855\n",
      "Epoch 627/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.3202\n",
      "Epoch 628/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.3524\n",
      "Epoch 629/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.3776\n",
      "Epoch 630/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.3973\n",
      "Epoch 631/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.4153\n",
      "Epoch 632/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4252\n",
      "Epoch 633/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4429\n",
      "Epoch 634/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4562\n",
      "Epoch 635/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.4668\n",
      "Epoch 636/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.4752\n",
      "Epoch 637/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4846\n",
      "Epoch 638/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4969\n",
      "Epoch 639/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.4902\n",
      "Epoch 640/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5150\n",
      "Epoch 641/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5064\n",
      "Epoch 642/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5248\n",
      "Epoch 643/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5359\n",
      "Epoch 644/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5380\n",
      "Epoch 645/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5425\n",
      "Epoch 646/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5481\n",
      "Epoch 647/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5567\n",
      "Epoch 648/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5573\n",
      "Epoch 649/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5566\n",
      "Epoch 650/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5725\n",
      "Epoch 651/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0479 - val_loss: 2.5646\n",
      "Epoch 652/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.5356\n",
      "Epoch 653/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.5596\n",
      "Epoch 654/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5372\n",
      "Epoch 655/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5570\n",
      "Epoch 656/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5582\n",
      "Epoch 657/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5403\n",
      "Epoch 658/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5418\n",
      "Epoch 659/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.5379\n",
      "Epoch 660/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0487 - val_loss: 2.5257\n",
      "Epoch 661/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.5526\n",
      "Epoch 662/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.5471\n",
      "Epoch 663/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5530\n",
      "Epoch 664/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5854\n",
      "Epoch 665/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0484 - val_loss: 2.5617\n",
      "Epoch 666/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0484 - val_loss: 2.5399\n",
      "Epoch 667/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5662\n",
      "Epoch 668/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.5630\n",
      "Epoch 669/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0485 - val_loss: 2.5504\n",
      "Epoch 670/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.5314\n",
      "Epoch 671/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0487 - val_loss: 2.5751\n",
      "Epoch 672/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.5353\n",
      "Epoch 673/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.5433\n",
      "Epoch 674/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.5344\n",
      "Epoch 675/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.5243\n",
      "Epoch 676/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.5187\n",
      "Epoch 677/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.5621\n",
      "Epoch 678/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.5109\n",
      "Epoch 679/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0494 - val_loss: 2.4930\n",
      "Epoch 680/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497 - val_loss: 2.5120\n",
      "Epoch 681/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.5091\n",
      "Epoch 682/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.3874\n",
      "Epoch 683/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0527 - val_loss: 2.3966\n",
      "Epoch 684/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0552 - val_loss: 2.1346\n",
      "Epoch 685/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0553 - val_loss: 2.2295\n",
      "Epoch 686/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0546 - val_loss: 2.0715\n",
      "Epoch 687/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0538 - val_loss: 2.2262\n",
      "Epoch 688/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0523 - val_loss: 2.2156\n",
      "Epoch 689/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0520 - val_loss: 2.0540\n",
      "Epoch 690/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0503 - val_loss: 2.3417\n",
      "Epoch 691/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.2646\n",
      "Epoch 692/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.3230\n",
      "Epoch 693/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.3979\n",
      "Epoch 694/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.3949\n",
      "Epoch 695/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4412\n",
      "Epoch 696/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 2.4461\n",
      "Epoch 697/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4660\n",
      "Epoch 698/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.4857\n",
      "Epoch 699/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.4973\n",
      "Epoch 700/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0476 - val_loss: 2.5130\n",
      "Epoch 701/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5194\n",
      "Epoch 702/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5321\n",
      "Epoch 703/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0476 - val_loss: 2.5317\n",
      "Epoch 704/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0476 - val_loss: 2.5457\n",
      "Epoch 705/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0476 - val_loss: 2.5493\n",
      "Epoch 706/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0476 - val_loss: 2.5607\n",
      "Epoch 707/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5726\n",
      "Epoch 708/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5779\n",
      "Epoch 709/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5800\n",
      "Epoch 710/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.5879\n",
      "Epoch 711/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5896\n",
      "Epoch 712/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0477 - val_loss: 2.5895\n",
      "Epoch 713/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5705\n",
      "Epoch 714/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.6040\n",
      "Epoch 715/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5721\n",
      "Epoch 716/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.6062\n",
      "Epoch 717/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5882\n",
      "Epoch 718/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5801\n",
      "Epoch 719/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5573\n",
      "Epoch 720/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.5982\n",
      "Epoch 721/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.5293\n",
      "Epoch 722/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.5633\n",
      "Epoch 723/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.5659\n",
      "Epoch 724/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.5374\n",
      "Epoch 725/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.5594\n",
      "Epoch 726/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4560\n",
      "Epoch 727/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.4925\n",
      "Epoch 728/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.3942\n",
      "Epoch 729/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497 - val_loss: 2.2905\n",
      "Epoch 730/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.4561\n",
      "Epoch 731/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.4011\n",
      "Epoch 732/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0509 - val_loss: 2.2822\n",
      "Epoch 733/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511 - val_loss: 2.3198\n",
      "Epoch 734/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0510 - val_loss: 2.2965\n",
      "Epoch 735/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0505 - val_loss: 2.2882\n",
      "Epoch 736/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0502 - val_loss: 2.3723\n",
      "Epoch 737/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0513 - val_loss: 2.2974\n",
      "Epoch 738/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0504 - val_loss: 2.3848\n",
      "Epoch 739/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3828\n",
      "Epoch 740/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4219\n",
      "Epoch 741/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0485 - val_loss: 2.4427\n",
      "Epoch 742/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.4460\n",
      "Epoch 743/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4825\n",
      "Epoch 744/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.4933\n",
      "Epoch 745/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5153\n",
      "Epoch 746/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5130\n",
      "Epoch 747/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5436\n",
      "Epoch 748/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5432\n",
      "Epoch 749/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5565\n",
      "Epoch 750/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5497\n",
      "Epoch 751/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5675\n",
      "Epoch 752/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5771\n",
      "Epoch 753/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5720\n",
      "Epoch 754/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5862\n",
      "Epoch 755/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5902\n",
      "Epoch 756/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.6021\n",
      "Epoch 757/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5857\n",
      "Epoch 758/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.6077\n",
      "Epoch 759/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5902\n",
      "Epoch 760/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5865\n",
      "Epoch 761/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5666\n",
      "Epoch 762/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.5525\n",
      "Epoch 763/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5650\n",
      "Epoch 764/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5488\n",
      "Epoch 765/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.5743\n",
      "Epoch 766/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0494 - val_loss: 2.5012\n",
      "Epoch 767/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.3511\n",
      "Epoch 768/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0510 - val_loss: 2.3788\n",
      "Epoch 769/1000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0524 - val_loss: 2.1523\n",
      "Epoch 770/1000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0541 - val_loss: 2.0576\n",
      "Epoch 771/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0549 - val_loss: 2.1422\n",
      "Epoch 772/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0540 - val_loss: 2.1548\n",
      "Epoch 773/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0532 - val_loss: 2.0910\n",
      "Epoch 774/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0529 - val_loss: 2.0689\n",
      "Epoch 775/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511 - val_loss: 2.3159\n",
      "Epoch 776/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0498 - val_loss: 2.2886\n",
      "Epoch 777/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.2846\n",
      "Epoch 778/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.3294\n",
      "Epoch 779/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.3706\n",
      "Epoch 780/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3615\n",
      "Epoch 781/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.3998\n",
      "Epoch 782/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4309\n",
      "Epoch 783/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.4323\n",
      "Epoch 784/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4678\n",
      "Epoch 785/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4705\n",
      "Epoch 786/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4925\n",
      "Epoch 787/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4991\n",
      "Epoch 788/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5110\n",
      "Epoch 789/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5137\n",
      "Epoch 790/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5248\n",
      "Epoch 791/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5259\n",
      "Epoch 792/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5465\n",
      "Epoch 793/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5453\n",
      "Epoch 794/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5528\n",
      "Epoch 795/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5458\n",
      "Epoch 796/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5677\n",
      "Epoch 797/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5555\n",
      "Epoch 798/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5730\n",
      "Epoch 799/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5733\n",
      "Epoch 800/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5995\n",
      "Epoch 801/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5699\n",
      "Epoch 802/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5909\n",
      "Epoch 803/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5893\n",
      "Epoch 804/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5866\n",
      "Epoch 805/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0481 - val_loss: 2.5802\n",
      "Epoch 806/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.5595\n",
      "Epoch 807/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.5543\n",
      "Epoch 808/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.5716\n",
      "Epoch 809/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.5451\n",
      "Epoch 810/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.4256\n",
      "Epoch 811/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.4770\n",
      "Epoch 812/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 2.3744\n",
      "Epoch 813/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 2.4385\n",
      "Epoch 814/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 2.3670\n",
      "Epoch 815/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0504 - val_loss: 2.3988\n",
      "Epoch 816/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0501 - val_loss: 2.3922\n",
      "Epoch 817/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497 - val_loss: 2.3731\n",
      "Epoch 818/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.4044\n",
      "Epoch 819/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4382\n",
      "Epoch 820/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4064\n",
      "Epoch 821/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4615\n",
      "Epoch 822/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.4830\n",
      "Epoch 823/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4967\n",
      "Epoch 824/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5088\n",
      "Epoch 825/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5269\n",
      "Epoch 826/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5250\n",
      "Epoch 827/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5335\n",
      "Epoch 828/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5414\n",
      "Epoch 829/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5500\n",
      "Epoch 830/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5699\n",
      "Epoch 831/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5672\n",
      "Epoch 832/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5785\n",
      "Epoch 833/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5409\n",
      "Epoch 834/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5967\n",
      "Epoch 835/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.5800\n",
      "Epoch 836/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5712\n",
      "Epoch 837/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.5596\n",
      "Epoch 838/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.5738\n",
      "Epoch 839/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.5315\n",
      "Epoch 840/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.4883\n",
      "Epoch 841/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.4127\n",
      "Epoch 842/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.4515\n",
      "Epoch 843/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.4112\n",
      "Epoch 844/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.4487\n",
      "Epoch 845/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.4178\n",
      "Epoch 846/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.4540\n",
      "Epoch 847/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.3907\n",
      "Epoch 848/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.3921\n",
      "Epoch 849/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0495 - val_loss: 2.4069\n",
      "Epoch 850/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3818\n",
      "Epoch 851/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3618\n",
      "Epoch 852/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 2.4232\n",
      "Epoch 853/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.3723\n",
      "Epoch 854/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.3932\n",
      "Epoch 855/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3512\n",
      "Epoch 856/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0495 - val_loss: 2.3262\n",
      "Epoch 857/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0498 - val_loss: 2.2029\n",
      "Epoch 858/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.3039\n",
      "Epoch 859/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3222\n",
      "Epoch 860/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.3737\n",
      "Epoch 861/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.4074\n",
      "Epoch 862/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.4061\n",
      "Epoch 863/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.3976\n",
      "Epoch 864/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4565\n",
      "Epoch 865/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4496\n",
      "Epoch 866/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.4652\n",
      "Epoch 867/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.4785\n",
      "Epoch 868/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4894\n",
      "Epoch 869/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5105\n",
      "Epoch 870/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5207\n",
      "Epoch 871/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5267\n",
      "Epoch 872/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5316\n",
      "Epoch 873/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5320\n",
      "Epoch 874/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5497\n",
      "Epoch 875/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5637\n",
      "Epoch 876/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0476 - val_loss: 2.5586\n",
      "Epoch 877/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5633\n",
      "Epoch 878/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5573\n",
      "Epoch 879/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5855\n",
      "Epoch 880/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5759\n",
      "Epoch 881/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5934\n",
      "Epoch 882/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.5419\n",
      "Epoch 883/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.5209\n",
      "Epoch 884/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.5354\n",
      "Epoch 885/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4681\n",
      "Epoch 886/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4651\n",
      "Epoch 887/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0495 - val_loss: 2.3786\n",
      "Epoch 888/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0499 - val_loss: 2.4578\n",
      "Epoch 889/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0504 - val_loss: 2.4120\n",
      "Epoch 890/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0512 - val_loss: 2.3860\n",
      "Epoch 891/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0513 - val_loss: 2.2892\n",
      "Epoch 892/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0523 - val_loss: 2.3364\n",
      "Epoch 893/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0523 - val_loss: 2.1352\n",
      "Epoch 894/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0517 - val_loss: 2.1186\n",
      "Epoch 895/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0521 - val_loss: 2.1781\n",
      "Epoch 896/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0508 - val_loss: 2.1822\n",
      "Epoch 897/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 2.2253\n",
      "Epoch 898/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.2550\n",
      "Epoch 899/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.3022\n",
      "Epoch 900/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.2642\n",
      "Epoch 901/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.3396\n",
      "Epoch 902/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.3349\n",
      "Epoch 903/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.3830\n",
      "Epoch 904/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.3798\n",
      "Epoch 905/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4116\n",
      "Epoch 906/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4203\n",
      "Epoch 907/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4302\n",
      "Epoch 908/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4461\n",
      "Epoch 909/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4465\n",
      "Epoch 910/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4628\n",
      "Epoch 911/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4701\n",
      "Epoch 912/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4724\n",
      "Epoch 913/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4877\n",
      "Epoch 914/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4906\n",
      "Epoch 915/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4970\n",
      "Epoch 916/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5120\n",
      "Epoch 917/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5121\n",
      "Epoch 918/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5130\n",
      "Epoch 919/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.5213\n",
      "Epoch 920/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5117\n",
      "Epoch 921/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5243\n",
      "Epoch 922/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.5152\n",
      "Epoch 923/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.5311\n",
      "Epoch 924/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5183\n",
      "Epoch 925/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5332\n",
      "Epoch 926/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.5257\n",
      "Epoch 927/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0481 - val_loss: 2.4972\n",
      "Epoch 928/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4771\n",
      "Epoch 929/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4675\n",
      "Epoch 930/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0492 - val_loss: 2.4210\n",
      "Epoch 931/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3677\n",
      "Epoch 932/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.3799\n",
      "Epoch 933/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0502 - val_loss: 2.2757\n",
      "Epoch 934/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0510 - val_loss: 2.3432\n",
      "Epoch 935/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0509 - val_loss: 2.2078\n",
      "Epoch 936/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511 - val_loss: 2.2783\n",
      "Epoch 937/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0521 - val_loss: 1.8496\n",
      "Epoch 938/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0515 - val_loss: 2.1898\n",
      "Epoch 939/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0511 - val_loss: 2.0209\n",
      "Epoch 940/1000\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.0506 - val_loss: 2.0304\n",
      "Epoch 941/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0522 - val_loss: 2.1417\n",
      "Epoch 942/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0507 - val_loss: 2.1147\n",
      "Epoch 943/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0497 - val_loss: 2.1749\n",
      "Epoch 944/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.1807\n",
      "Epoch 945/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.2059\n",
      "Epoch 946/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.2648\n",
      "Epoch 947/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0478 - val_loss: 2.3007\n",
      "Epoch 948/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.3256\n",
      "Epoch 949/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.3389\n",
      "Epoch 950/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.3545\n",
      "Epoch 951/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.3695\n",
      "Epoch 952/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.3839\n",
      "Epoch 953/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.3949\n",
      "Epoch 954/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4040\n",
      "Epoch 955/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4165\n",
      "Epoch 956/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0475 - val_loss: 2.4198\n",
      "Epoch 957/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0475 - val_loss: 2.4306\n",
      "Epoch 958/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0475 - val_loss: 2.4325\n",
      "Epoch 959/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4471\n",
      "Epoch 960/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4489\n",
      "Epoch 961/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4590\n",
      "Epoch 962/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4602\n",
      "Epoch 963/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4668\n",
      "Epoch 964/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4826\n",
      "Epoch 965/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4654\n",
      "Epoch 966/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0476 - val_loss: 2.4702\n",
      "Epoch 967/1000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0477 - val_loss: 2.4886\n",
      "Epoch 968/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0477 - val_loss: 2.4822\n",
      "Epoch 969/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.5034\n",
      "Epoch 970/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4847\n",
      "Epoch 971/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4619\n",
      "Epoch 972/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4725\n",
      "Epoch 973/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.4770\n",
      "Epoch 974/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.4585\n",
      "Epoch 975/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 2.4356\n",
      "Epoch 976/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4945\n",
      "Epoch 977/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.4290\n",
      "Epoch 978/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.3714\n",
      "Epoch 979/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0490 - val_loss: 2.4253\n",
      "Epoch 980/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 2.4081\n",
      "Epoch 981/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.4266\n",
      "Epoch 982/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.4484\n",
      "Epoch 983/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0483 - val_loss: 2.4221\n",
      "Epoch 984/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.4497\n",
      "Epoch 985/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0481 - val_loss: 2.4700\n",
      "Epoch 986/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4690\n",
      "Epoch 987/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4894\n",
      "Epoch 988/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0480 - val_loss: 2.4705\n",
      "Epoch 989/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 2.4886\n",
      "Epoch 990/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.4851\n",
      "Epoch 991/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0486 - val_loss: 2.4125\n",
      "Epoch 992/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0488 - val_loss: 2.4236\n",
      "Epoch 993/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0489 - val_loss: 2.4255\n",
      "Epoch 994/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0493 - val_loss: 2.3906\n",
      "Epoch 995/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.3055\n",
      "Epoch 996/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0494 - val_loss: 2.3178\n",
      "Epoch 997/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0491 - val_loss: 2.3634\n",
      "Epoch 998/1000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0488 - val_loss: 2.4121\n",
      "Epoch 999/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0485 - val_loss: 2.3542\n",
      "Epoch 1000/1000\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0482 - val_loss: 2.4082\n",
      "[0.7950201630592346, 0.7586485147476196, 0.7735586762428284, 0.7959569692611694, 0.8268746733665466, 0.8570789694786072, 0.8968186974525452, 0.9458156824111938, 0.9708298444747925, 1.0220791101455688, 1.0339391231536865, 1.0523719787597656, 1.0790143013000488, 1.0834649801254272, 1.1248457431793213, 1.1551779508590698, 1.188523292541504, 1.1862624883651733, 1.2303816080093384, 1.247168779373169, 1.278268575668335, 1.2734090089797974, 1.2789312601089478, 1.2869404554367065, 1.330253005027771, 1.3171169757843018, 1.3425827026367188, 1.3618987798690796, 1.3560255765914917, 1.3825414180755615, 1.3863359689712524, 1.4571326971054077, 1.3989053964614868, 1.421245813369751, 1.4517472982406616, 1.4191875457763672, 1.4553545713424683, 1.5131679773330688, 1.477975606918335, 1.5055369138717651, 1.5084725618362427, 1.469232201576233, 1.4826043844223022, 1.4952465295791626, 1.508586049079895, 1.512951374053955, 1.5445215702056885, 1.5274194478988647, 1.5512495040893555, 1.5496097803115845, 1.5836544036865234, 1.5657031536102295, 1.6100798845291138, 1.5563035011291504, 1.5847723484039307, 1.5987812280654907, 1.576676845550537, 1.6000689268112183, 1.6064509153366089, 1.571347951889038, 1.6152644157409668, 1.6062294244766235, 1.6256071329116821, 1.6825729608535767, 1.6588029861450195, 1.6395760774612427, 1.6754518747329712, 1.6566566228866577, 1.6837700605392456, 1.6601754426956177, 1.6958072185516357, 1.6715571880340576, 1.6793097257614136, 1.6964507102966309, 1.6790411472320557, 1.713884711265564, 1.6757745742797852, 1.674046516418457, 1.6927868127822876, 1.6999653577804565, 1.7119131088256836, 1.719490647315979, 1.7517353296279907, 1.7436435222625732, 1.7264692783355713, 1.748012900352478, 1.7811667919158936, 1.8000273704528809, 1.7863285541534424, 1.8044829368591309, 1.7672882080078125, 1.7769949436187744, 1.7885420322418213, 1.759648084640503, 1.7809243202209473, 1.7846381664276123, 1.8194222450256348, 1.797654390335083, 1.8055601119995117, 1.8228068351745605, 1.8414658308029175, 1.8620010614395142, 1.834000825881958, 1.8580167293548584, 1.8251758813858032, 1.8584774732589722, 1.8367741107940674, 1.8342164754867554, 1.8456851243972778, 1.8837529420852661, 1.8579018115997314, 1.8896814584732056, 1.8932712078094482, 1.833909511566162, 1.8170959949493408, 1.8295397758483887, 1.8657180070877075, 1.8497315645217896, 1.769195556640625, 1.8157950639724731, 1.8238009214401245, 1.865312099456787, 1.7865667343139648, 1.8091919422149658, 1.875786542892456, 1.9264836311340332, 1.9178078174591064, 1.9434027671813965, 1.9441416263580322, 1.95806086063385, 1.9491771459579468, 1.9798986911773682, 1.9992010593414307, 1.996887445449829, 1.9990026950836182, 1.9510672092437744, 2.0072312355041504, 1.9893568754196167, 1.9783743619918823, 1.9698777198791504, 1.923004150390625, 1.942249059677124, 1.8990483283996582, 1.8813570737838745, 1.867123007774353, 1.9077420234680176, 1.9191590547561646, 1.9552525281906128, 1.9689654111862183, 1.994696855545044, 2.030041456222534, 1.9922024011611938, 2.018561363220215, 2.03759503364563, 2.058929681777954, 2.026787757873535, 2.034893751144409, 2.028000831604004, 2.0396599769592285, 1.9997680187225342, 2.017308235168457, 2.0744986534118652, 2.0279793739318848, 2.0349605083465576, 2.054917573928833, 2.0993258953094482, 2.089923620223999, 2.1183278560638428, 2.0998892784118652, 2.110302209854126, 2.079188585281372, 2.134089946746826, 2.053886890411377, 2.074075222015381, 2.071338415145874, 2.0562710762023926, 2.0124597549438477, 2.0281147956848145, 2.014629364013672, 2.068333387374878, 2.0821943283081055, 2.0364372730255127, 2.09230375289917, 2.0448427200317383, 2.077456474304199, 2.0925142765045166, 2.092477321624756, 2.0681536197662354, 2.0992133617401123, 2.0866334438323975, 2.057340621948242, 2.131549835205078, 2.147709608078003, 2.165872812271118, 2.1529688835144043, 2.175807476043701, 2.183295726776123, 2.197046995162964, 2.1682379245758057, 2.1556668281555176, 2.1883175373077393, 2.155468463897705, 2.172128915786743, 2.149129629135132, 2.175509214401245, 2.1627464294433594, 2.19140625, 2.1830360889434814, 2.1846446990966797, 2.1759352684020996, 2.1855835914611816, 2.1635642051696777, 2.158081531524658, 2.0867438316345215, 2.048914909362793, 2.1115803718566895, 2.0412025451660156, 1.9750298261642456, 2.0997281074523926, 2.08097243309021, 2.0273423194885254, 2.0426409244537354, 2.0431509017944336, 2.056288719177246, 2.058373212814331, 2.1426920890808105, 2.149237871170044, 2.202066659927368, 2.1967248916625977, 2.2185490131378174, 2.2408690452575684, 2.269606351852417, 2.287158727645874, 2.3042333126068115, 2.3030121326446533, 2.316317558288574, 2.330545425415039, 2.3407206535339355, 2.339350461959839, 2.339524269104004, 2.351771354675293, 2.330826997756958, 2.336097478866577, 2.314506769180298, 2.3468775749206543, 2.3569839000701904, 2.345937967300415, 2.320720911026001, 2.3156137466430664, 2.321901559829712, 2.304386854171753, 2.189819812774658, 2.1250994205474854, 2.155292510986328, 2.1366939544677734, 2.0542831420898438, 2.1197636127471924, 2.1116740703582764, 2.0746331214904785, 2.078296661376953, 2.0689072608947754, 2.081540822982788, 2.120863914489746, 2.1810123920440674, 2.177522659301758, 2.1776161193847656, 2.237257957458496, 2.251307487487793, 2.259608268737793, 2.2751786708831787, 2.2992756366729736, 2.3254222869873047, 2.34338116645813, 2.357281446456909, 2.3753457069396973, 2.3680946826934814, 2.3797664642333984, 2.409468412399292, 2.400498151779175, 2.403857469558716, 2.4103546142578125, 2.4240942001342773, 2.410027265548706, 2.416506052017212, 2.429547071456909, 2.403045654296875, 2.4333105087280273, 2.4112894535064697, 2.3466241359710693, 2.3811044692993164, 2.331251382827759, 2.2470831871032715, 2.2617266178131104, 2.13791561126709, 2.1223948001861572, 2.1582887172698975, 2.1144068241119385, 2.146167516708374, 2.1853315830230713, 2.215742588043213, 2.1562204360961914, 2.2281548976898193, 2.248610734939575, 2.297914981842041, 2.3023717403411865, 2.333371877670288, 2.3585996627807617, 2.3678793907165527, 2.384291172027588, 2.4112603664398193, 2.429644823074341, 2.4264333248138428, 2.435424327850342, 2.4316489696502686, 2.45615816116333, 2.4481966495513916, 2.437948703765869, 2.441814661026001, 2.4570400714874268, 2.4371135234832764, 2.41870379447937, 2.4011363983154297, 2.4125280380249023, 2.449784994125366, 2.2888436317443848, 2.36714506149292, 2.316009283065796, 2.3440067768096924, 2.298403263092041, 2.3207435607910156, 2.1828863620758057, 2.2348408699035645, 2.1369082927703857, 2.241811990737915, 2.2350552082061768, 2.205327033996582, 2.2347280979156494, 2.2862372398376465, 2.332763433456421, 2.341709613800049, 2.3672754764556885, 2.389536142349243, 2.413007974624634, 2.4330034255981445, 2.4315543174743652, 2.4621715545654297, 2.447648525238037, 2.451819658279419, 2.461596965789795, 2.4174439907073975, 2.455571413040161, 2.472437620162964, 2.466820001602173, 2.4485576152801514, 2.5029947757720947, 2.4440319538116455, 2.4854326248168945, 2.4835684299468994, 2.464367628097534, 2.4795515537261963, 2.44936203956604, 2.458651065826416, 2.4026708602905273, 2.4063339233398438, 2.4157114028930664, 2.3806545734405518, 2.2839102745056152, 2.251075267791748, 2.0702903270721436, 2.089820384979248, 2.054417133331299, 2.0562915802001953, 2.095808982849121, 2.123633623123169, 2.2093303203582764, 2.286149024963379, 2.3211474418640137, 2.3430328369140625, 2.3729379177093506, 2.372588634490967, 2.4087255001068115, 2.43304705619812, 2.444279909133911, 2.4467289447784424, 2.45835280418396, 2.479457378387451, 2.483863115310669, 2.4934229850769043, 2.501389741897583, 2.515110969543457, 2.520625352859497, 2.534921169281006, 2.533292293548584, 2.5321059226989746, 2.547816514968872, 2.547328233718872, 2.5503780841827393, 2.5526866912841797, 2.5106637477874756, 2.5290842056274414, 2.4385430812835693, 2.4664525985717773, 2.442026138305664, 2.5087854862213135, 2.4131503105163574, 2.42164945602417, 2.39160418510437, 2.395271062850952, 2.3826589584350586, 2.3873507976531982, 2.4005229473114014, 2.3619799613952637, 2.377258062362671, 2.343268632888794, 2.452099323272705, 2.4361653327941895, 2.4409375190734863, 2.4691317081451416, 2.504899024963379, 2.4956109523773193, 2.5189316272735596, 2.4976933002471924, 2.5181634426116943, 2.517240285873413, 2.5725162029266357, 2.5338540077209473, 2.504117965698242, 2.562985897064209, 2.5074942111968994, 2.4748520851135254, 2.4702823162078857, 2.473785877227783, 2.487766981124878, 2.4642891883850098, 2.457949638366699, 2.433211326599121, 2.517301321029663, 2.394835948944092, 2.418252944946289, 2.4129090309143066, 2.4165797233581543, 2.4504127502441406, 2.4005424976348877, 2.416541814804077, 2.4046709537506104, 2.4893558025360107, 2.426300048828125, 2.457672595977783, 2.461223602294922, 2.4595260620117188, 2.464571952819824, 2.4977922439575195, 2.4861948490142822, 2.449666738510132, 2.3924174308776855, 2.4530186653137207, 2.4375669956207275, 2.3447558879852295, 2.2767369747161865, 2.2896904945373535, 2.127305269241333, 2.2989044189453125, 2.2763073444366455, 2.2367117404937744, 2.2830684185028076, 2.2635483741760254, 2.3001136779785156, 2.2663331031799316, 2.283406972885132, 2.3591554164886475, 2.358396053314209, 2.3605105876922607, 2.3757026195526123, 2.44136643409729, 2.448221445083618, 2.455338954925537, 2.4956438541412354, 2.4950485229492188, 2.5011138916015625, 2.5231661796569824, 2.5273497104644775, 2.538823127746582, 2.5457019805908203, 2.5559237003326416, 2.562291145324707, 2.5707321166992188, 2.5613298416137695, 2.5719962120056152, 2.574399471282959, 2.56256365776062, 2.595412492752075, 2.577341318130493, 2.545949935913086, 2.554924488067627, 2.539367198944092, 2.4491193294525146, 2.4581573009490967, 2.1999776363372803, 2.1788110733032227, 2.1617183685302734, 2.107083559036255, 2.0289013385772705, 2.288954734802246, 2.2890772819519043, 2.2854819297790527, 2.3378379344940186, 2.364959239959717, 2.4137091636657715, 2.4216442108154297, 2.45686411857605, 2.465925931930542, 2.4880669116973877, 2.5057120323181152, 2.5042765140533447, 2.515923023223877, 2.5298702716827393, 2.5427539348602295, 2.5482263565063477, 2.553588390350342, 2.562089681625366, 2.572723150253296, 2.572180986404419, 2.591359853744507, 2.588297128677368, 2.587479829788208, 2.5818560123443604, 2.5874269008636475, 2.5679893493652344, 2.5767769813537598, 2.5670785903930664, 2.5675017833709717, 2.5039937496185303, 2.570000410079956, 2.518132209777832, 2.463622808456421, 2.4576680660247803, 2.398216724395752, 2.395796775817871, 2.432459592819214, 2.4126548767089844, 2.32503080368042, 2.3321127891540527, 2.3445944786071777, 2.302920341491699, 2.2350893020629883, 2.276431083679199, 2.2874820232391357, 2.281989574432373, 2.383455276489258, 2.329275131225586, 2.4122705459594727, 2.389322280883789, 2.450052261352539, 2.455921173095703, 2.480354070663452, 2.4777398109436035, 2.502396821975708, 2.5165820121765137, 2.5220799446105957, 2.5393621921539307, 2.5406534671783447, 2.5658037662506104, 2.555919885635376, 2.5657479763031006, 2.5732901096343994, 2.579979419708252, 2.5953352451324463, 2.582559585571289, 2.6055641174316406, 2.590123176574707, 2.594668388366699, 2.5907671451568604, 2.5553176403045654, 2.525941848754883, 2.4501841068267822, 2.420849323272705, 2.4166207313537598, 2.285580635070801, 2.38029408454895, 2.2640902996063232, 2.2258377075195312, 2.3181052207946777, 2.291414499282837, 2.321239471435547, 2.296931266784668, 2.3772199153900146, 2.3783936500549316, 2.4198882579803467, 2.406336784362793, 2.4639453887939453, 2.478966236114502, 2.4790358543395996, 2.5079574584960938, 2.5224363803863525, 2.5255165100097656, 2.5390636920928955, 2.544160842895508, 2.570291757583618, 2.5628299713134766, 2.571086883544922, 2.5732266902923584, 2.5838279724121094, 2.5863358974456787, 2.592545986175537, 2.5820369720458984, 2.596031904220581, 2.593611240386963, 2.579047679901123, 2.5686843395233154, 2.5389254093170166, 2.548166036605835, 2.479536533355713, 2.480590343475342, 2.49088978767395, 2.444103240966797, 2.3040337562561035, 2.2989799976348877, 1.9383442401885986, 2.0196638107299805, 1.894879937171936, 1.9928667545318604, 1.9926923513412476, 2.0208654403686523, 2.1745355129241943, 2.2296576499938965, 2.2854597568511963, 2.3202223777770996, 2.35244083404541, 2.377647876739502, 2.397347927093506, 2.4152824878692627, 2.4251832962036133, 2.442939043045044, 2.4562184810638428, 2.4667515754699707, 2.4752330780029297, 2.4846458435058594, 2.496889591217041, 2.490187644958496, 2.5150198936462402, 2.5064022541046143, 2.524808406829834, 2.535888910293579, 2.538025379180908, 2.542520761489868, 2.548103094100952, 2.556673049926758, 2.5572924613952637, 2.556647777557373, 2.5724830627441406, 2.564574718475342, 2.535594940185547, 2.5596189498901367, 2.5372090339660645, 2.557008981704712, 2.5581607818603516, 2.5403378009796143, 2.5418388843536377, 2.537874221801758, 2.525700569152832, 2.552598237991333, 2.54709529876709, 2.5529661178588867, 2.585383415222168, 2.5617270469665527, 2.539933443069458, 2.5662388801574707, 2.5629642009735107, 2.5503528118133545, 2.531421184539795, 2.5751187801361084, 2.535318613052368, 2.543295383453369, 2.5344061851501465, 2.5242600440979004, 2.518725872039795, 2.562072992324829, 2.5108673572540283, 2.4930191040039062, 2.5120344161987305, 2.509103536605835, 2.38736629486084, 2.396559476852417, 2.1345577239990234, 2.229530096054077, 2.071505546569824, 2.22619366645813, 2.2156264781951904, 2.054013252258301, 2.3417396545410156, 2.2645750045776367, 2.3229804039001465, 2.3978960514068604, 2.3949191570281982, 2.4411745071411133, 2.446053981781006, 2.4660072326660156, 2.4856560230255127, 2.4973175525665283, 2.51297926902771, 2.51937198638916, 2.53206467628479, 2.531745195388794, 2.545703172683716, 2.5492639541625977, 2.56065034866333, 2.5725972652435303, 2.5779309272766113, 2.580000162124634, 2.5879387855529785, 2.5896482467651367, 2.5894970893859863, 2.5704851150512695, 2.604020357131958, 2.572101354598999, 2.6062045097351074, 2.5882279872894287, 2.5800859928131104, 2.5573267936706543, 2.5981557369232178, 2.5293025970458984, 2.563291549682617, 2.5659048557281494, 2.5373706817626953, 2.55942440032959, 2.4559621810913086, 2.4925038814544678, 2.394238233566284, 2.2905361652374268, 2.4561328887939453, 2.4010865688323975, 2.2822203636169434, 2.3197884559631348, 2.2964727878570557, 2.288245916366577, 2.3722712993621826, 2.2973921298980713, 2.3848111629486084, 2.382817029953003, 2.421870708465576, 2.442748785018921, 2.445960760116577, 2.4824981689453125, 2.4933457374572754, 2.515267848968506, 2.513000249862671, 2.543639659881592, 2.5431764125823975, 2.5564708709716797, 2.5497350692749023, 2.567460060119629, 2.5770769119262695, 2.5720276832580566, 2.5861644744873047, 2.5901949405670166, 2.602100372314453, 2.5857229232788086, 2.6077377796173096, 2.5902395248413086, 2.5864768028259277, 2.566561460494995, 2.552515745162964, 2.5649771690368652, 2.5488178730010986, 2.5742714405059814, 2.5012385845184326, 2.351128101348877, 2.378772497177124, 2.1522786617279053, 2.0575618743896484, 2.1421689987182617, 2.154787302017212, 2.0910093784332275, 2.0689423084259033, 2.3159117698669434, 2.2886099815368652, 2.2846479415893555, 2.3294379711151123, 2.370587110519409, 2.3615152835845947, 2.399813175201416, 2.4309136867523193, 2.4322633743286133, 2.467813491821289, 2.470529556274414, 2.4924960136413574, 2.499101161956787, 2.5109808444976807, 2.5137360095977783, 2.524848461151123, 2.525850772857666, 2.546548366546631, 2.5452523231506348, 2.5527725219726562, 2.5458245277404785, 2.567680835723877, 2.555453300476074, 2.5729520320892334, 2.5733275413513184, 2.5994503498077393, 2.569931983947754, 2.5909063816070557, 2.589315414428711, 2.5865557193756104, 2.5801661014556885, 2.5595123767852783, 2.554286003112793, 2.5716135501861572, 2.545133352279663, 2.4255948066711426, 2.4769744873046875, 2.3743529319763184, 2.4384751319885254, 2.3670287132263184, 2.3988006114959717, 2.3921866416931152, 2.373147487640381, 2.4043757915496826, 2.438204765319824, 2.406445264816284, 2.461549997329712, 2.482987403869629, 2.4966719150543213, 2.508795738220215, 2.526916980743408, 2.5249884128570557, 2.533475160598755, 2.5414326190948486, 2.5500309467315674, 2.5698893070220947, 2.567171335220337, 2.5784618854522705, 2.5408735275268555, 2.5967485904693604, 2.5800094604492188, 2.57123064994812, 2.5595641136169434, 2.5738370418548584, 2.5315017700195312, 2.4882917404174805, 2.4127197265625, 2.4515490531921387, 2.4112277030944824, 2.4486684799194336, 2.4177536964416504, 2.453993797302246, 2.390707015991211, 2.3921022415161133, 2.4068753719329834, 2.38177227973938, 2.361760139465332, 2.4232394695281982, 2.3722949028015137, 2.393192768096924, 2.351238489151001, 2.3262083530426025, 2.2029271125793457, 2.303919553756714, 2.322169542312622, 2.3736565113067627, 2.407360792160034, 2.406062126159668, 2.3976380825042725, 2.4565470218658447, 2.4496097564697266, 2.4651684761047363, 2.478506088256836, 2.489434003829956, 2.5104613304138184, 2.5206549167633057, 2.526705503463745, 2.531611442565918, 2.5320208072662354, 2.5496864318847656, 2.563746213912964, 2.5586137771606445, 2.5632808208465576, 2.5573344230651855, 2.5855491161346436, 2.5758864879608154, 2.5933566093444824, 2.541938066482544, 2.5209367275238037, 2.5354161262512207, 2.4681124687194824, 2.4651198387145996, 2.3785760402679443, 2.457785129547119, 2.4119982719421387, 2.3860185146331787, 2.2892353534698486, 2.3363893032073975, 2.1351747512817383, 2.118622064590454, 2.178067207336426, 2.182219982147217, 2.225344181060791, 2.2550337314605713, 2.3021697998046875, 2.2641799449920654, 2.3396475315093994, 2.3349292278289795, 2.383028030395508, 2.3798389434814453, 2.411595344543457, 2.420257329940796, 2.4302217960357666, 2.446131467819214, 2.446460485458374, 2.462773561477661, 2.470134973526001, 2.472411870956421, 2.4876902103424072, 2.490635871887207, 2.496981620788574, 2.511958122253418, 2.5121383666992188, 2.5129663944244385, 2.5213468074798584, 2.5116939544677734, 2.5242722034454346, 2.515150785446167, 2.531064987182617, 2.518313407897949, 2.5332181453704834, 2.5257067680358887, 2.497194290161133, 2.4771153926849365, 2.4674735069274902, 2.421002149581909, 2.367710590362549, 2.379851818084717, 2.2756855487823486, 2.3431661128997803, 2.2077503204345703, 2.2783467769622803, 1.849595308303833, 2.1898365020751953, 2.0209052562713623, 2.0304393768310547, 2.141740560531616, 2.114680290222168, 2.174877643585205, 2.1806702613830566, 2.205899238586426, 2.2648096084594727, 2.3007428646087646, 2.325605630874634, 2.3388755321502686, 2.35451078414917, 2.3694725036621094, 2.3839468955993652, 2.394854784011841, 2.4039690494537354, 2.4164962768554688, 2.4198012351989746, 2.4305849075317383, 2.432513952255249, 2.4470691680908203, 2.4488637447357178, 2.458956718444824, 2.4602479934692383, 2.466809034347534, 2.482573986053467, 2.4653775691986084, 2.4701671600341797, 2.4885575771331787, 2.4821908473968506, 2.5033693313598633, 2.4847443103790283, 2.4619266986846924, 2.472470998764038, 2.476954221725464, 2.458535671234131, 2.4355781078338623, 2.4945123195648193, 2.429023027420044, 2.3713834285736084, 2.4252703189849854, 2.4080698490142822, 2.426605701446533, 2.4484293460845947, 2.422128915786743, 2.4497148990631104, 2.470033884048462, 2.4690237045288086, 2.4893617630004883, 2.470510244369507, 2.4886422157287598, 2.485119581222534, 2.4124741554260254, 2.423630714416504, 2.425471305847168, 2.390638589859009, 2.30545973777771, 2.3177783489227295, 2.3634066581726074, 2.4120805263519287, 2.3542349338531494, 2.4082117080688477]\n"
     ]
    }
   ],
   "source": [
    "# Data Prep\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                min_delta=0.001,\n",
    "                                patience=25,\n",
    "                                verbose=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True)\n",
    "\n",
    "#X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "#                                                                      y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                    y_predictions, \n",
    "                                                    test_size=config['inets']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                    random_state=config['inets']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config['inets']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config['inets']['data_prep']['train_test_val_split']['stratify'])\n",
    "\n",
    "\n",
    "# Model Def\n",
    "model = Sequential()\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(2500, activation='relu'))\n",
    "#model.add(Dense(1500, activation='linear'))\n",
    "#model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(config['data']['n_features'], activation='linear'))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam',)\n",
    "\n",
    "#opt = tf.keras.optimizers.SGD(\n",
    "#    learning_rate=0.01,\n",
    "#    momentum=0.0,\n",
    "#    nesterov=False,\n",
    "#    name='SGD'\n",
    "#)\n",
    "\n",
    "model.compile(optimizer=opt, loss=custom_loss, metrics=[])\n",
    "model.build(input_shape=X_train.shape)\n",
    "model.summary()\n",
    "\n",
    "# Model fit\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=config['inets']['model_fit']['batch_size'],\n",
    "                    epochs=config['inets']['model_fit']['epochs'],\n",
    "                    verbose=config['inets']['model_fit']['verbose'],\n",
    "                    #callbacks=[early_stopping],\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    shuffle=config['inets']['model_fit']['shuffle'],\n",
    "                    class_weight=config['inets']['model_fit']['class_weight'],\n",
    "                    sample_weight=config['inets']['model_fit']['sample_weight'],\n",
    "                    initial_epoch=config['inets']['model_fit']['initial_epoch'],\n",
    "                    steps_per_epoch=config['inets']['model_fit']['steps_per_epoch'],\n",
    "                    validation_steps=config['inets']['model_fit']['validation_steps'],\n",
    "                    validation_batch_size=config['inets']['model_fit']['validation_batch_size'],\n",
    "                    validation_freq=config['inets']['model_fit']['validation_freq'],\n",
    "                    workers=config['computation']['n_jobs'],\n",
    "                    #use_multiprocessing=True\n",
    "                   )\n",
    "print(history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "753420b6-eee9-4d22-88fe-3d651a35c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    path = utilities_LR.inet_path_LR(config)\n",
    "    \n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    model.save(path + '/modelKeras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f1961b-2c3d-4a8d-b9e1-2cace3d8f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data_LR/nda10000_nsa4000_nfe10_nin8_nta1_ncc2_sep3.0_noi0_shuTrue_ran42/tsi-1_vsi0.25_ranNone_shuTrue_strNone_bat32_epo100_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1/tsi0.3_vsi0.2_ranNone_shuTrue_strNone_bat256_epo1000_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1/modelKeras/assets\n"
     ]
    }
   ],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a99f3c-6351-4952-9a65-fbfb44cfb825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
