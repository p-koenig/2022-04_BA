{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd1549c-3ec6-40dd-a7cf-69cd5beeca85",
   "metadata": {},
   "source": [
    "# Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f8e753-3ed4-47c2-a508-794c5ccff2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'n_datasets': 10_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 20, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 10,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 2,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0.01,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': None,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },    \n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 150,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '4',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24522e9e-6237-4c8e-b502-e555da2fef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 13:53:41.144317: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-31 13:53:41.144367: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ReLU\n",
    "\n",
    "from keras.regularizers import L1L2\n",
    "from keras.regularizers import L2\n",
    "\n",
    "import utilities_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44bbe3e-16e1-45f1-92cf-88dbd8a87318",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['computation']['gpu_numbers'] if config['computation']['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if config['computation']['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if config['computation']['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if config['computation']['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ca8427-e1ef-419f-a901-2f010ffea5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 13:54:08.248198: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-31 13:54:08.248238: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-31 13:54:08.248273: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dws-02): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d80c2-ba75-403e-a3d1-6611a228d86f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6ef2fa-cee5-42ca-b1a5-b5fcf14185d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_model(datasetIndex):\n",
    " #   # with open(utilities_LR.lambda_path(config, datasetIndex), 'r') as f:\n",
    "  #  #     model = keras.models.load_model(f)\n",
    "   # model = keras.models.load_model(utilities_LR.lambda_path(config, datasetIndex))\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81c7b58-f1b9-468d-acfd-403f48b9febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_weightdata(datasetIndex):\n",
    "  #   weights_list = load_model(datasetIndex).get_weights()\n",
    "    # return np.concatenate([x.flatten() for x in weights_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21aa2853-27a3-40e7-94ee-7458467f7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lambda():\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    with open(directory + '/lambda_weights_list.npy', \"rb\") as f:\n",
    "        return np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d13a520-a726-4262-87db-6a0b41dd767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_models():\n",
    "#    # return np.array([get_weights(i) for i in range(config['data']['n_datasets'])])\n",
    "#    # return np.array([get_weightdata(i) for i in range(10)])\n",
    "#    with open(utilities_LR.lambda_path_LR(config, 0) + '.npy', 'rb') as f:\n",
    "#        weights_list = np.load(f, allow_pickle=True)\n",
    "#    for i in range(1, config['data']['n_datasets']):\n",
    "#        with open(utilities_LR.lambda_path(config, i) + '.npy', 'rb') as f:\n",
    "#            weights = np.load(f, allow_pickle=True)\n",
    "#        weights_list = np.vstack([weights_list, weights])\n",
    "#    return weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e2ee23-b5a6-40a9-ad60-216984958f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coef():\n",
    "    return np.load(utilities_LR.data_path_LR(config) + '/coef_list_targetForInet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75fc6c1a-e279-438d-81dd-3000dd58b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=config['inets']['data_prep']['train_test_val_split']['test_size'] + config['inets']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                        random_state=config['inets']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                        shuffle=config['inets']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                        stratify=config['inets']['data_prep']['train_test_val_split']['stratify'])\n",
    "    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "                                                    y_test, \n",
    "                                                    test_size=config['inets']['data_prep']['train_test_val_split']['val_size'] / (config['inets']['data_prep']['train_test_val_split']['test_size'] + config['inets']['data_prep']['train_test_val_split']['val_size']), \n",
    "                                                    random_state=config['inets']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config['inets']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config['inets']['data_prep']['train_test_val_split']['stratify'])\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16620e0f-0109-4177-b0a4-23afaf92a886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    path = utilities_LR.inet_path_LR(config)\n",
    "    \n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # weights_list = model.get_weights()\n",
    "    \n",
    "    # weights_linearized = np.concatenate([x.flatten() for x in weights_list])\n",
    "    \n",
    "    # with open(path + '/inet_weights.npy', \"wb\") as f:\n",
    "        # np.save(f, weights_linearized, allow_pickle=True)\n",
    "    model.save(path + '/modelKeras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974df36-5052-4bbc-a191-4aadc84d02c1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47754df-11d9-4365-ba85-5d5ceb10bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e0a7289-6d7b-4817-88e1-393afcd91b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8362)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32794f7-c1f7-4523-b282-8d06e49cd305",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = get_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2dabb49-66da-4f6c-92cb-90aa22645467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00495198, -0.01854766, -0.03919359,  0.04854344,  0.0091744 ,\n",
       "       -0.07450365, -0.04318035,  0.03160309, -0.11410224, -0.01132964,\n",
       "        0.00093653,  0.15812977, -0.24470446, -0.2362686 ,  0.00529975,\n",
       "       -0.03122651, -0.50247647, -0.02148568,  0.04833143, -0.04019827])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb245c6-7f0c-4214-8655-e73ea7fb1963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9a546-bca2-4392-9d84-fe9592f4dad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 13:54:12.405390: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (6999, 1000)              8363000   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (6999, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (6999, 20)                20020     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,383,020\n",
      "Trainable params: 8,383,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "219/219 [==============================] - 14s 59ms/step - loss: 1.3197 - mse: 0.0821 - root_mean_squared_error: 0.2866 - val_loss: 1.3128 - val_mse: 0.0767 - val_root_mean_squared_error: 0.2770\n",
      "Epoch 2/1000\n",
      "219/219 [==============================] - 12s 54ms/step - loss: 1.3120 - mse: 0.0774 - root_mean_squared_error: 0.2781 - val_loss: 1.3096 - val_mse: 0.0764 - val_root_mean_squared_error: 0.2764\n",
      "Epoch 3/1000\n",
      "219/219 [==============================] - 12s 56ms/step - loss: 1.3089 - mse: 0.0772 - root_mean_squared_error: 0.2778 - val_loss: 1.3066 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2763\n",
      "Epoch 4/1000\n",
      "219/219 [==============================] - 14s 63ms/step - loss: 1.3059 - mse: 0.0771 - root_mean_squared_error: 0.2777 - val_loss: 1.3036 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 5/1000\n",
      "219/219 [==============================] - 12s 57ms/step - loss: 1.3029 - mse: 0.0771 - root_mean_squared_error: 0.2777 - val_loss: 1.3007 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 6/1000\n",
      "219/219 [==============================] - 12s 55ms/step - loss: 1.3000 - mse: 0.0771 - root_mean_squared_error: 0.2776 - val_loss: 1.2977 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 7/1000\n",
      "219/219 [==============================] - 13s 57ms/step - loss: 1.2970 - mse: 0.0770 - root_mean_squared_error: 0.2775 - val_loss: 1.2948 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 8/1000\n",
      "219/219 [==============================] - 12s 55ms/step - loss: 1.2941 - mse: 0.0770 - root_mean_squared_error: 0.2775 - val_loss: 1.2919 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 9/1000\n",
      "219/219 [==============================] - 12s 55ms/step - loss: 1.2912 - mse: 0.0770 - root_mean_squared_error: 0.2774 - val_loss: 1.2890 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 10/1000\n",
      "219/219 [==============================] - 13s 57ms/step - loss: 1.2882 - mse: 0.0770 - root_mean_squared_error: 0.2774 - val_loss: 1.2861 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 11/1000\n",
      "219/219 [==============================] - 12s 57ms/step - loss: 1.2853 - mse: 0.0769 - root_mean_squared_error: 0.2774 - val_loss: 1.2832 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 12/1000\n",
      "219/219 [==============================] - 13s 57ms/step - loss: 1.2824 - mse: 0.0769 - root_mean_squared_error: 0.2773 - val_loss: 1.2803 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 13/1000\n",
      "219/219 [==============================] - 12s 56ms/step - loss: 1.2795 - mse: 0.0769 - root_mean_squared_error: 0.2773 - val_loss: 1.2774 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 14/1000\n",
      "219/219 [==============================] - 13s 57ms/step - loss: 1.2766 - mse: 0.0769 - root_mean_squared_error: 0.2773 - val_loss: 1.2746 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 15/1000\n",
      "219/219 [==============================] - 12s 56ms/step - loss: 1.2737 - mse: 0.0769 - root_mean_squared_error: 0.2773 - val_loss: 1.2717 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 16/1000\n",
      "219/219 [==============================] - 12s 56ms/step - loss: 1.2708 - mse: 0.0769 - root_mean_squared_error: 0.2772 - val_loss: 1.2688 - val_mse: 0.0763 - val_root_mean_squared_error: 0.2762\n",
      "Epoch 17/1000\n",
      "183/219 [========================>.....] - ETA: 1s - loss: 1.2683 - mse: 0.0770 - root_mean_squared_error: 0.2776"
     ]
    }
   ],
   "source": [
    "# Data Prep\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "                                                                      y)\n",
    "\n",
    "# Model Def\n",
    "model = Sequential()\n",
    "# model.add(BatchNormalization(axis=1,\n",
    "#    momentum=0.99,\n",
    "#    epsilon=0.001,\n",
    "#    center=True,\n",
    "#    scale=True,\n",
    "#    beta_initializer='zeros',\n",
    "#    gamma_initializer='ones',\n",
    "#    moving_mean_initializer='zeros',\n",
    "#    moving_variance_initializer='ones',\n",
    "#    beta_regularizer=None,\n",
    "#    gamma_regularizer=None,\n",
    "#    beta_constraint=None))\n",
    "model.add(Dense(\n",
    "    units=1000,\n",
    "    kernel_regularizer=L1L2(l1=1e-5, l2=1e-4),\n",
    "    bias_regularizer=L2(1e-4),\n",
    "    activity_regularizer=L2(1e-5)\n",
    "))\n",
    "model.add(ReLU())\n",
    "#model.add(Dense(6500, activation='linear'))\n",
    "#model.add(Dense(6000, activation='relu'))\n",
    "#model.add(Dense(5500, activation='linear'))\n",
    "#model.add(Dense(3500, activation='relu'))\n",
    "#model.add(Dense(2500, activation='relu'))\n",
    "#model.add(Dense(1500, activation='linear'))\n",
    "#model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='relu'))\n",
    "\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(\n",
    "#    learning_rate=3e-4,\n",
    "#    beta_1=0.999999,\n",
    "#    beta_2=0.999,\n",
    "#    epsilon=1e-07,\n",
    "#    amsgrad=False,\n",
    "#    name=\"Adam\")\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.0,\n",
    "    nesterov=False,\n",
    "    name='SGD'\n",
    ")\n",
    "\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mse', keras.metrics.RootMeanSquaredError()])\n",
    "model.build(input_shape=X_train.shape)\n",
    "model.summary()\n",
    "\n",
    "# Model fit\n",
    "history = model.fit(x=X_train,\n",
    "                    y=y_train,\n",
    "                    batch_size=config['inets']['model_fit']['batch_size'],\n",
    "                    epochs=config['inets']['model_fit']['epochs'],\n",
    "                    verbose=config['inets']['model_fit']['verbose'],\n",
    "                    callbacks=config['inets']['model_fit']['callbacks'],\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    shuffle=config['inets']['model_fit']['shuffle'],\n",
    "                    class_weight=config['inets']['model_fit']['class_weight'],\n",
    "                    sample_weight=config['inets']['model_fit']['sample_weight'],\n",
    "                    initial_epoch=config['inets']['model_fit']['initial_epoch'],\n",
    "                    steps_per_epoch=config['inets']['model_fit']['steps_per_epoch'],\n",
    "                    validation_steps=config['inets']['model_fit']['validation_steps'],\n",
    "                    validation_batch_size=config['inets']['model_fit']['validation_batch_size'],\n",
    "                    validation_freq=config['inets']['model_fit']['validation_freq'],\n",
    "                   )\n",
    "print(history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1961b-2c3d-4a8d-b9e1-2cace3d8f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a489da-c45b-4679-8bb8-aa0cc2545ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6acda-dd96-41f9-b841-16462ad6c4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
