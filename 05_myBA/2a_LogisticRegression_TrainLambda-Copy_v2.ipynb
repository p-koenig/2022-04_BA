{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7413b1-5b9f-472e-9045-ee14068be971",
   "metadata": {},
   "source": [
    "# Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d269d23f-33f7-456b-b4fc-a457414ed778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ef29c7-245e-4770-b29d-0045b6b0b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'n_datasets': 10_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 15, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        # or random\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 3.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 42,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': -1, # currently not used\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 32,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '5',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8c0cc-8b52-4637-aaeb-e16a16634b89",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9518e9f9-6a5d-45e8-9bea-95333bb436d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['computation']['gpu_numbers'] if config['computation']['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if config['computation']['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if config['computation']['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if config['computation']['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d525d7-df11-4db7-bd88-211471e5a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae09b938-bfec-4ac3-b5cd-a33ce494c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76077315-f3bc-495f-92b9-df69649f3eed",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91501bce-140a-4c6c-89c4-94317079c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_features']])\n",
    "\n",
    "if  config['data']['n_targets'] < 2:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], ])\n",
    "    #coef_list = np.zeros([config['data']['n_datasets'], config['data']['n_features'], ])\n",
    "else:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_targets']])\n",
    "    #coef_list = np.zeros([config['data']['n_datasets'], config['data']['n_features'], config['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f258543-80c3-4faa-a6f8-04f4a48e09cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config)\n",
    "\n",
    "with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    X_datasets_list = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    y_datasets_list = np.load(f, allow_pickle=True)\n",
    "#with open(directory + '/coef_list_targetForInet.npy', \"rb\") as f:\n",
    "#    coef_list = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65510a-d70f-4277-800b-75ed4df35315",
   "metadata": {},
   "source": [
    "# Save Model & Metrics (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5754bd-00ae-472c-827e-fc2f807f93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_predictions(weights_list, y_pred_list):\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(directory + '/lambda_weights_list.npy', weights_list, allow_pickle=True)\n",
    "    np.save(directory + '/lambda_preds_list.npy', y_pred_list, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77189e30-fcd7-4410-a593-dfc50360347f",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf1bd26-92b3-4826-9601-e8c4b6b77654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, index):\n",
    "    # Data Prep\n",
    "    #X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "    #                                                                      y)\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                min_delta=0.001,\n",
    "                                patience=35,\n",
    "                                verbose=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    # Model Def\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config['data']['n_features']))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=config['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config['lambda']['model_compile']['loss'],\n",
    "                  metrics=config['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    # Model fit\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=config['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=[early_stopping],\n",
    "                        #validation_data=(X_val, y_val),\n",
    "                        validation_split=config['lambda']['data_prep']['train_test_val_split']['val_size'],\n",
    "                        shuffle=config['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    \n",
    "    lambda_weights = np.concatenate([x.flatten() for x in model.get_weights()])\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    #print(history.history['val_loss'])\n",
    "    \n",
    "    #print(history.history['loss'])\n",
    "    \n",
    "    return lambda_weights, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ab4a7a-cae7-4fe8-b9a4-277357c64c86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done   8 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=32)]: Done  21 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=32)]: Done  34 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=32)]: Done  49 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=32)]: Done  64 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=32)]: Done  81 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=32)]: Done  98 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=32)]: Done 117 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=32)]: Done 157 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=32)]: Done 178 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=32)]: Done 201 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=32)]: Done 224 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=32)]: Done 249 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=32)]: Done 274 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=32)]: Done 301 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=32)]: Done 328 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=32)]: Done 357 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=32)]: Done 417 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=32)]: Done 448 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=32)]: Done 481 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=32)]: Done 514 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=32)]: Done 549 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=32)]: Done 584 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=32)]: Done 621 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=32)]: Done 658 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=32)]: Done 697 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=32)]: Done 736 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=32)]: Done 777 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=32)]: Done 818 tasks      | elapsed: 23.5min\n",
      "[Parallel(n_jobs=32)]: Done 861 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=32)]: Done 904 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=32)]: Done 949 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=32)]: Done 994 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=32)]: Done 1041 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=32)]: Done 1088 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=32)]: Done 1137 tasks      | elapsed: 31.9min\n",
      "[Parallel(n_jobs=32)]: Done 1186 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=32)]: Done 1237 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=32)]: Done 1288 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=32)]: Done 1341 tasks      | elapsed: 41.1min\n",
      "[Parallel(n_jobs=32)]: Done 1394 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=32)]: Done 1449 tasks      | elapsed: 44.1min\n",
      "[Parallel(n_jobs=32)]: Done 1504 tasks      | elapsed: 45.7min\n",
      "[Parallel(n_jobs=32)]: Done 1561 tasks      | elapsed: 47.3min\n",
      "[Parallel(n_jobs=32)]: Done 1618 tasks      | elapsed: 48.7min\n",
      "[Parallel(n_jobs=32)]: Done 1677 tasks      | elapsed: 50.1min\n",
      "[Parallel(n_jobs=32)]: Done 1736 tasks      | elapsed: 51.7min\n",
      "[Parallel(n_jobs=32)]: Done 1797 tasks      | elapsed: 54.8min\n",
      "[Parallel(n_jobs=32)]: Done 1858 tasks      | elapsed: 56.9min\n",
      "[Parallel(n_jobs=32)]: Done 1921 tasks      | elapsed: 59.4min\n",
      "[Parallel(n_jobs=32)]: Done 1984 tasks      | elapsed: 61.5min\n",
      "[Parallel(n_jobs=32)]: Done 2049 tasks      | elapsed: 63.3min\n",
      "[Parallel(n_jobs=32)]: Done 2114 tasks      | elapsed: 65.1min\n",
      "[Parallel(n_jobs=32)]: Done 2181 tasks      | elapsed: 67.0min\n",
      "[Parallel(n_jobs=32)]: Done 2248 tasks      | elapsed: 69.0min\n",
      "[Parallel(n_jobs=32)]: Done 2317 tasks      | elapsed: 71.8min\n",
      "[Parallel(n_jobs=32)]: Done 2386 tasks      | elapsed: 74.1min\n",
      "[Parallel(n_jobs=32)]: Done 2457 tasks      | elapsed: 76.9min\n",
      "[Parallel(n_jobs=32)]: Done 2528 tasks      | elapsed: 78.9min\n",
      "[Parallel(n_jobs=32)]: Done 2601 tasks      | elapsed: 81.1min\n",
      "[Parallel(n_jobs=32)]: Done 2674 tasks      | elapsed: 83.0min\n",
      "[Parallel(n_jobs=32)]: Done 2749 tasks      | elapsed: 85.2min\n",
      "[Parallel(n_jobs=32)]: Done 2824 tasks      | elapsed: 87.4min\n",
      "[Parallel(n_jobs=32)]: Done 2901 tasks      | elapsed: 90.5min\n",
      "[Parallel(n_jobs=32)]: Done 2978 tasks      | elapsed: 93.3min\n",
      "[Parallel(n_jobs=32)]: Done 3057 tasks      | elapsed: 97.2min\n",
      "[Parallel(n_jobs=32)]: Done 3217 tasks      | elapsed: 103.0min\n",
      "[Parallel(n_jobs=32)]: Done 3298 tasks      | elapsed: 105.6min\n",
      "[Parallel(n_jobs=32)]: Done 3381 tasks      | elapsed: 108.0min\n",
      "[Parallel(n_jobs=32)]: Done 3464 tasks      | elapsed: 110.6min\n",
      "[Parallel(n_jobs=32)]: Done 3549 tasks      | elapsed: 113.5min\n",
      "[Parallel(n_jobs=32)]: Done 3634 tasks      | elapsed: 116.8min\n",
      "[Parallel(n_jobs=32)]: Done 3721 tasks      | elapsed: 119.5min\n",
      "[Parallel(n_jobs=32)]: Done 3808 tasks      | elapsed: 122.0min\n",
      "[Parallel(n_jobs=32)]: Done 3897 tasks      | elapsed: 124.7min\n",
      "[Parallel(n_jobs=32)]: Done 3986 tasks      | elapsed: 127.5min\n",
      "[Parallel(n_jobs=32)]: Done 4077 tasks      | elapsed: 130.6min\n",
      "[Parallel(n_jobs=32)]: Done 4168 tasks      | elapsed: 136.6min\n",
      "[Parallel(n_jobs=32)]: Done 4261 tasks      | elapsed: 139.3min\n",
      "[Parallel(n_jobs=32)]: Done 4354 tasks      | elapsed: 142.0min\n",
      "[Parallel(n_jobs=32)]: Done 4449 tasks      | elapsed: 146.1min\n",
      "[Parallel(n_jobs=32)]: Done 4544 tasks      | elapsed: 148.8min\n",
      "[Parallel(n_jobs=32)]: Done 4641 tasks      | elapsed: 152.2min\n",
      "[Parallel(n_jobs=32)]: Done 4738 tasks      | elapsed: 156.3min\n",
      "[Parallel(n_jobs=32)]: Done 4837 tasks      | elapsed: 160.5min\n",
      "[Parallel(n_jobs=32)]: Done 4936 tasks      | elapsed: 163.6min\n",
      "[Parallel(n_jobs=32)]: Done 5037 tasks      | elapsed: 166.6min\n",
      "[Parallel(n_jobs=32)]: Done 5138 tasks      | elapsed: 170.0min\n",
      "[Parallel(n_jobs=32)]: Done 5241 tasks      | elapsed: 173.2min\n",
      "[Parallel(n_jobs=32)]: Done 5344 tasks      | elapsed: 177.8min\n",
      "[Parallel(n_jobs=32)]: Done 5449 tasks      | elapsed: 180.9min\n",
      "[Parallel(n_jobs=32)]: Done 5661 tasks      | elapsed: 187.6min\n",
      "[Parallel(n_jobs=32)]: Done 5768 tasks      | elapsed: 191.0min\n",
      "[Parallel(n_jobs=32)]: Done 5877 tasks      | elapsed: 194.6min\n",
      "[Parallel(n_jobs=32)]: Done 5986 tasks      | elapsed: 198.9min\n",
      "[Parallel(n_jobs=32)]: Done 6097 tasks      | elapsed: 202.1min\n",
      "[Parallel(n_jobs=32)]: Done 6208 tasks      | elapsed: 205.0min\n",
      "[Parallel(n_jobs=32)]: Done 6321 tasks      | elapsed: 207.6min\n",
      "[Parallel(n_jobs=32)]: Done 6434 tasks      | elapsed: 225.0min\n",
      "[Parallel(n_jobs=32)]: Done 6549 tasks      | elapsed: 227.7min\n",
      "[Parallel(n_jobs=32)]: Done 6664 tasks      | elapsed: 230.8min\n",
      "[Parallel(n_jobs=32)]: Done 6781 tasks      | elapsed: 234.1min\n",
      "[Parallel(n_jobs=32)]: Done 6898 tasks      | elapsed: 237.5min\n",
      "[Parallel(n_jobs=32)]: Done 7017 tasks      | elapsed: 244.0min\n",
      "[Parallel(n_jobs=32)]: Done 7136 tasks      | elapsed: 249.1min\n",
      "[Parallel(n_jobs=32)]: Done 7257 tasks      | elapsed: 252.5min\n",
      "[Parallel(n_jobs=32)]: Done 7378 tasks      | elapsed: 256.0min\n",
      "[Parallel(n_jobs=32)]: Done 7501 tasks      | elapsed: 259.4min\n",
      "[Parallel(n_jobs=32)]: Done 7624 tasks      | elapsed: 265.4min\n",
      "[Parallel(n_jobs=32)]: Done 7749 tasks      | elapsed: 270.1min\n",
      "[Parallel(n_jobs=32)]: Done 7874 tasks      | elapsed: 274.0min\n",
      "[Parallel(n_jobs=32)]: Done 8001 tasks      | elapsed: 277.7min\n",
      "[Parallel(n_jobs=32)]: Done 8128 tasks      | elapsed: 281.6min\n",
      "[Parallel(n_jobs=32)]: Done 8257 tasks      | elapsed: 287.9min\n",
      "[Parallel(n_jobs=32)]: Done 8386 tasks      | elapsed: 292.5min\n",
      "[Parallel(n_jobs=32)]: Done 8517 tasks      | elapsed: 296.6min\n",
      "[Parallel(n_jobs=32)]: Done 8648 tasks      | elapsed: 300.6min\n",
      "[Parallel(n_jobs=32)]: Done 8781 tasks      | elapsed: 306.2min\n",
      "[Parallel(n_jobs=32)]: Done 8914 tasks      | elapsed: 311.8min\n",
      "[Parallel(n_jobs=32)]: Done 9049 tasks      | elapsed: 315.9min\n",
      "[Parallel(n_jobs=32)]: Done 9184 tasks      | elapsed: 319.8min\n",
      "[Parallel(n_jobs=32)]: Done 9321 tasks      | elapsed: 324.6min\n",
      "[Parallel(n_jobs=32)]: Done 9458 tasks      | elapsed: 327.7min\n",
      "[Parallel(n_jobs=32)]: Done 9597 tasks      | elapsed: 330.6min\n",
      "[Parallel(n_jobs=32)]: Done 9736 tasks      | elapsed: 333.5min\n",
      "[Parallel(n_jobs=32)]: Done 9877 tasks      | elapsed: 336.8min\n",
      "[Parallel(n_jobs=32)]: Done 10000 out of 10000 | elapsed: 339.6min finished\n"
     ]
    }
   ],
   "source": [
    "parallel = Parallel(n_jobs=config['computation']['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "weights_ypred_list = parallel(delayed(train_nn)(X_data, y_data, index) for index, (X_data, y_data) in enumerate(zip(X_datasets_list, y_datasets_list)))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f76eaafe-64ae-492c-aca2-cf5db356c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = np.stack([np.array(x[0]) for x in weights_ypred_list])\n",
    "y_pred_list = np.stack([x[1] for x in weights_ypred_list])\n",
    "y_pred_list = y_pred_list.reshape([config['data']['n_datasets'], config['data']['n_samples']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778c426-b6e2-421c-aa06-17507dfa3a32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfcd173a-cba2-440d-a63d-ebd1f8b8579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2237)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92e041fd-5704-498f-9a40-489636bf503c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e440930-b8ca-4b24-bbdd-56c8a5ed1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a2f702f-483b-4f55-8141-e4167eac7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_pred_list).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4dbf02a-2f09-40ee-a620-51aa1822d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(weights_list).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334925c3-84fb-4005-a6c5-da5915a5708a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save Models without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32ac9b49-0daa-4851-8f77-56354ff8d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_predictions(weights_list, y_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fab2b2-43cd-4d92-adba-784c5c7523f9",
   "metadata": {},
   "source": [
    "# Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "701375dd-ba6c-42a0-867a-2d4dc362bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc508f9c-501c-4477-bb1f-4b686df10a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(y_pred_list.shape[0], int(y_pred_list.shape[0] * noise), replace=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "235661f8-72d0-4e03-b9eb-567e42452e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list[index] = [abs(1 - x) for x in y_pred_list[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346c860-7d1b-4d55-aad8-5f55424f90a8",
   "metadata": {},
   "source": [
    "# Save Models with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35494f25-b2ed-45c4-b7cf-fdba74aa249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_predictions_with_noise(weights_list, y_pred_list):\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(directory + f'/lambda_weights_list_with{noise}_noise.npy', weights_list, allow_pickle=True)\n",
    "    np.save(directory + f'/lambda_preds_list_with{noise}_noise.npy', y_pred_list, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9f833c6-c0e5-4f3d-9298-31559b2dc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_predictions_with_noise(weights_list, y_pred_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
