{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7413b1-5b9f-472e-9045-ee14068be971",
   "metadata": {},
   "source": [
    "# Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d269d23f-33f7-456b-b4fc-a457414ed778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ef29c7-245e-4770-b29d-0045b6b0b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'n_datasets': 10_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 25, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        #'n_informative': random.randint(2, 10),\n",
    "        'n_informative': 'random',\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        # or random\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 1,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 3.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': 42,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },\n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': -1, # currently not used\n",
    "                'val_size': 0.15,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': keras.losses.BinaryCrossentropy(from_logits=False), #tf.keras.losses.get(config['lambda_net']['loss_lambda']), # 'mae'\n",
    "            'metrics': ['mae', keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 64,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 8,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8c0cc-8b52-4637-aaeb-e16a16634b89",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9518e9f9-6a5d-45e8-9bea-95333bb436d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['computation']['gpu_numbers'] if config['computation']['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if config['computation']['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if config['computation']['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if config['computation']['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d525d7-df11-4db7-bd88-211471e5a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae09b938-bfec-4ac3-b5cd-a33ce494c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76077315-f3bc-495f-92b9-df69649f3eed",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91501bce-140a-4c6c-89c4-94317079c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_features']])\n",
    "\n",
    "if  config['data']['n_targets'] < 2:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], ])\n",
    "    #coef_list = np.zeros([config['data']['n_datasets'], config['data']['n_features'], ])\n",
    "else:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_targets']])\n",
    "    #coef_list = np.zeros([config['data']['n_datasets'], config['data']['n_features'], config['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f258543-80c3-4faa-a6f8-04f4a48e09cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config)\n",
    "\n",
    "with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    X_datasets_list = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    y_datasets_list = np.load(f, allow_pickle=True)\n",
    "#with open(directory + '/coef_list_targetForInet.npy', \"rb\") as f:\n",
    "#    coef_list = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65510a-d70f-4277-800b-75ed4df35315",
   "metadata": {},
   "source": [
    "# Save Model & Metrics (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5754bd-00ae-472c-827e-fc2f807f93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_predictions(weights_list, y_pred_list):\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(directory + '/lambda_weights_list.npy', weights_list, allow_pickle=True)\n",
    "    np.save(directory + '/lambda_preds_list.npy', y_pred_list, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77189e30-fcd7-4410-a593-dfc50360347f",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf1bd26-92b3-4826-9601-e8c4b6b77654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, index):\n",
    "    # Data Prep\n",
    "    #X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "    #                                                                      y)\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                min_delta=0.001,\n",
    "                                patience=35,\n",
    "                                verbose=0,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    # Model Def\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config['data']['n_features']))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=config['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config['lambda']['model_compile']['loss'],\n",
    "                  metrics=config['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    # Model fit\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=config['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=[early_stopping],\n",
    "                        #validation_data=(X_val, y_val),\n",
    "                        validation_split=config['lambda']['data_prep']['train_test_val_split']['val_size'],\n",
    "                        shuffle=config['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    \n",
    "    lambda_weights = np.concatenate([x.flatten() for x in model.get_weights()])\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    #print(history.history['val_loss'])\n",
    "    \n",
    "    #print(history.history['loss'])\n",
    "    \n",
    "    return lambda_weights, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8ab4a7a-cae7-4fe8-b9a4-277357c64c86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=8)]: Done  45 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=8)]: Done  69 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=8)]: Done  82 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=8)]: Done  97 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=8)]: Done 129 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=8)]: Done 165 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=8)]: Done 205 tasks      | elapsed: 23.2min\n",
      "[Parallel(n_jobs=8)]: Done 226 tasks      | elapsed: 25.1min\n",
      "[Parallel(n_jobs=8)]: Done 249 tasks      | elapsed: 27.4min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=8)]: Done 297 tasks      | elapsed: 33.5min\n",
      "[Parallel(n_jobs=8)]: Done 322 tasks      | elapsed: 35.9min\n",
      "[Parallel(n_jobs=8)]: Done 349 tasks      | elapsed: 39.0min\n",
      "[Parallel(n_jobs=8)]: Done 376 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=8)]: Done 405 tasks      | elapsed: 45.9min\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed: 49.9min\n",
      "[Parallel(n_jobs=8)]: Done 465 tasks      | elapsed: 53.2min\n",
      "[Parallel(n_jobs=8)]: Done 496 tasks      | elapsed: 56.4min\n",
      "[Parallel(n_jobs=8)]: Done 529 tasks      | elapsed: 59.8min\n",
      "[Parallel(n_jobs=8)]: Done 562 tasks      | elapsed: 64.3min\n",
      "[Parallel(n_jobs=8)]: Done 597 tasks      | elapsed: 68.6min\n",
      "[Parallel(n_jobs=8)]: Done 632 tasks      | elapsed: 72.8min\n",
      "[Parallel(n_jobs=8)]: Done 669 tasks      | elapsed: 77.5min\n",
      "[Parallel(n_jobs=8)]: Done 706 tasks      | elapsed: 83.5min\n",
      "[Parallel(n_jobs=8)]: Done 745 tasks      | elapsed: 87.7min\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed: 91.9min\n",
      "[Parallel(n_jobs=8)]: Done 825 tasks      | elapsed: 97.2min\n",
      "[Parallel(n_jobs=8)]: Done 866 tasks      | elapsed: 101.9min\n",
      "[Parallel(n_jobs=8)]: Done 909 tasks      | elapsed: 106.9min\n",
      "[Parallel(n_jobs=8)]: Done 952 tasks      | elapsed: 113.3min\n",
      "[Parallel(n_jobs=8)]: Done 997 tasks      | elapsed: 118.9min\n",
      "[Parallel(n_jobs=8)]: Done 1042 tasks      | elapsed: 124.9min\n",
      "[Parallel(n_jobs=8)]: Done 1089 tasks      | elapsed: 131.4min\n",
      "[Parallel(n_jobs=8)]: Done 1136 tasks      | elapsed: 137.8min\n",
      "[Parallel(n_jobs=8)]: Done 1185 tasks      | elapsed: 143.1min\n",
      "[Parallel(n_jobs=8)]: Done 1234 tasks      | elapsed: 149.7min\n",
      "[Parallel(n_jobs=8)]: Done 1285 tasks      | elapsed: 156.3min\n",
      "[Parallel(n_jobs=8)]: Done 1336 tasks      | elapsed: 161.7min\n",
      "[Parallel(n_jobs=8)]: Done 1389 tasks      | elapsed: 169.2min\n",
      "[Parallel(n_jobs=8)]: Done 1442 tasks      | elapsed: 175.4min\n",
      "[Parallel(n_jobs=8)]: Done 1497 tasks      | elapsed: 182.1min\n",
      "[Parallel(n_jobs=8)]: Done 1552 tasks      | elapsed: 203.8min\n",
      "[Parallel(n_jobs=8)]: Done 1609 tasks      | elapsed: 209.6min\n",
      "[Parallel(n_jobs=8)]: Done 1666 tasks      | elapsed: 216.3min\n",
      "[Parallel(n_jobs=8)]: Done 1725 tasks      | elapsed: 223.5min\n",
      "[Parallel(n_jobs=8)]: Done 1784 tasks      | elapsed: 230.3min\n",
      "[Parallel(n_jobs=8)]: Done 1845 tasks      | elapsed: 237.9min\n",
      "[Parallel(n_jobs=8)]: Done 1906 tasks      | elapsed: 244.5min\n",
      "[Parallel(n_jobs=8)]: Done 1969 tasks      | elapsed: 252.4min\n",
      "[Parallel(n_jobs=8)]: Done 2032 tasks      | elapsed: 259.9min\n",
      "[Parallel(n_jobs=8)]: Done 2097 tasks      | elapsed: 268.2min\n",
      "[Parallel(n_jobs=8)]: Done 2229 tasks      | elapsed: 284.6min\n",
      "[Parallel(n_jobs=8)]: Done 2296 tasks      | elapsed: 292.3min\n",
      "[Parallel(n_jobs=8)]: Done 2365 tasks      | elapsed: 301.1min\n",
      "[Parallel(n_jobs=8)]: Done 2434 tasks      | elapsed: 305.9min\n",
      "[Parallel(n_jobs=8)]: Done 2505 tasks      | elapsed: 311.3min\n",
      "[Parallel(n_jobs=8)]: Done 2576 tasks      | elapsed: 316.4min\n",
      "[Parallel(n_jobs=8)]: Done 2649 tasks      | elapsed: 320.0min\n",
      "[Parallel(n_jobs=8)]: Done 2722 tasks      | elapsed: 323.2min\n",
      "[Parallel(n_jobs=8)]: Done 2797 tasks      | elapsed: 326.6min\n",
      "[Parallel(n_jobs=8)]: Done 2872 tasks      | elapsed: 329.4min\n",
      "[Parallel(n_jobs=8)]: Done 2949 tasks      | elapsed: 332.8min\n",
      "[Parallel(n_jobs=8)]: Done 3026 tasks      | elapsed: 336.1min\n",
      "[Parallel(n_jobs=8)]: Done 3105 tasks      | elapsed: 339.2min\n",
      "[Parallel(n_jobs=8)]: Done 3184 tasks      | elapsed: 341.3min\n",
      "[Parallel(n_jobs=8)]: Done 3265 tasks      | elapsed: 343.3min\n",
      "[Parallel(n_jobs=8)]: Done 3346 tasks      | elapsed: 345.3min\n",
      "[Parallel(n_jobs=8)]: Done 3429 tasks      | elapsed: 347.3min\n",
      "[Parallel(n_jobs=8)]: Done 3512 tasks      | elapsed: 349.2min\n",
      "[Parallel(n_jobs=8)]: Done 3597 tasks      | elapsed: 351.3min\n",
      "[Parallel(n_jobs=8)]: Done 3682 tasks      | elapsed: 353.3min\n",
      "[Parallel(n_jobs=8)]: Done 3769 tasks      | elapsed: 355.4min\n",
      "[Parallel(n_jobs=8)]: Done 3856 tasks      | elapsed: 357.5min\n",
      "[Parallel(n_jobs=8)]: Done 3945 tasks      | elapsed: 359.7min\n",
      "[Parallel(n_jobs=8)]: Done 4034 tasks      | elapsed: 361.8min\n",
      "[Parallel(n_jobs=8)]: Done 4125 tasks      | elapsed: 364.0min\n",
      "[Parallel(n_jobs=8)]: Done 4216 tasks      | elapsed: 366.2min\n",
      "[Parallel(n_jobs=8)]: Done 4309 tasks      | elapsed: 368.4min\n",
      "[Parallel(n_jobs=8)]: Done 4497 tasks      | elapsed: 373.0min\n",
      "[Parallel(n_jobs=8)]: Done 4592 tasks      | elapsed: 375.1min\n",
      "[Parallel(n_jobs=8)]: Done 4689 tasks      | elapsed: 377.0min\n",
      "[Parallel(n_jobs=8)]: Done 4786 tasks      | elapsed: 378.9min\n",
      "[Parallel(n_jobs=8)]: Done 4885 tasks      | elapsed: 380.8min\n",
      "[Parallel(n_jobs=8)]: Done 4984 tasks      | elapsed: 382.7min\n",
      "[Parallel(n_jobs=8)]: Done 5085 tasks      | elapsed: 384.7min\n",
      "[Parallel(n_jobs=8)]: Done 5186 tasks      | elapsed: 386.6min\n",
      "[Parallel(n_jobs=8)]: Done 5289 tasks      | elapsed: 388.5min\n",
      "[Parallel(n_jobs=8)]: Done 5392 tasks      | elapsed: 390.5min\n",
      "[Parallel(n_jobs=8)]: Done 5497 tasks      | elapsed: 392.4min\n",
      "[Parallel(n_jobs=8)]: Done 5602 tasks      | elapsed: 394.4min\n",
      "[Parallel(n_jobs=8)]: Done 5709 tasks      | elapsed: 396.5min\n",
      "[Parallel(n_jobs=8)]: Done 5816 tasks      | elapsed: 398.5min\n",
      "[Parallel(n_jobs=8)]: Done 5925 tasks      | elapsed: 400.6min\n",
      "[Parallel(n_jobs=8)]: Done 6034 tasks      | elapsed: 402.6min\n",
      "[Parallel(n_jobs=8)]: Done 6145 tasks      | elapsed: 404.8min\n",
      "[Parallel(n_jobs=8)]: Done 6256 tasks      | elapsed: 407.5min\n",
      "[Parallel(n_jobs=8)]: Done 6369 tasks      | elapsed: 410.7min\n",
      "[Parallel(n_jobs=8)]: Done 6482 tasks      | elapsed: 413.9min\n",
      "[Parallel(n_jobs=8)]: Done 6597 tasks      | elapsed: 417.0min\n",
      "[Parallel(n_jobs=8)]: Done 6712 tasks      | elapsed: 420.2min\n",
      "[Parallel(n_jobs=8)]: Done 6829 tasks      | elapsed: 423.3min\n",
      "[Parallel(n_jobs=8)]: Done 6946 tasks      | elapsed: 426.6min\n",
      "[Parallel(n_jobs=8)]: Done 7065 tasks      | elapsed: 430.0min\n",
      "[Parallel(n_jobs=8)]: Done 7184 tasks      | elapsed: 433.4min\n",
      "[Parallel(n_jobs=8)]: Done 7305 tasks      | elapsed: 436.7min\n",
      "[Parallel(n_jobs=8)]: Done 7426 tasks      | elapsed: 440.0min\n",
      "[Parallel(n_jobs=8)]: Done 7549 tasks      | elapsed: 443.4min\n",
      "[Parallel(n_jobs=8)]: Done 7672 tasks      | elapsed: 446.6min\n",
      "[Parallel(n_jobs=8)]: Done 7797 tasks      | elapsed: 449.6min\n",
      "[Parallel(n_jobs=8)]: Done 7922 tasks      | elapsed: 452.1min\n",
      "[Parallel(n_jobs=8)]: Done 8049 tasks      | elapsed: 454.4min\n",
      "[Parallel(n_jobs=8)]: Done 8176 tasks      | elapsed: 457.2min\n",
      "[Parallel(n_jobs=8)]: Done 8305 tasks      | elapsed: 459.7min\n",
      "[Parallel(n_jobs=8)]: Done 8434 tasks      | elapsed: 462.1min\n",
      "[Parallel(n_jobs=8)]: Done 8565 tasks      | elapsed: 464.5min\n",
      "[Parallel(n_jobs=8)]: Done 8696 tasks      | elapsed: 467.0min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#loky\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m weights_ypred_list \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_nn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_datasets_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_datasets_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m parallel\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parallel = Parallel(n_jobs=config['computation']['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "weights_ypred_list = parallel(delayed(train_nn)(X_data, y_data, index) for index, (X_data, y_data) in enumerate(zip(X_datasets_list, y_datasets_list)))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76eaafe-64ae-492c-aca2-cf5db356c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = np.stack([np.array(x[0]) for x in weights_ypred_list])\n",
    "y_pred_list = np.stack([x[1] for x in weights_ypred_list])\n",
    "y_pred_list = y_pred_list.reshape([config['data']['n_datasets'], config['data']['n_samples']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778c426-b6e2-421c-aa06-17507dfa3a32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd173a-cba2-440d-a63d-ebd1f8b8579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e041fd-5704-498f-9a40-489636bf503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e440930-b8ca-4b24-bbdd-56c8a5ed1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f702f-483b-4f55-8141-e4167eac7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_list).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbf02a-2f09-40ee-a620-51aa1822d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weights_list).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fab2b2-43cd-4d92-adba-784c5c7523f9",
   "metadata": {},
   "source": [
    "# Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701375dd-ba6c-42a0-867a-2d4dc362bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc508f9c-501c-4477-bb1f-4b686df10a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(y_pred_list.shape[0], int(y_pred_list.shape[0] * noise), replace=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235661f8-72d0-4e03-b9eb-567e42452e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list[index] = [abs(1 - x) for x in y_pred_list[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334925c3-84fb-4005-a6c5-da5915a5708a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac9b49-0daa-4851-8f77-56354ff8d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_predictions(weights_list, y_pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f094b6-c9ad-43a0-9553-efecd1901b82",
   "metadata": {},
   "source": [
    "# Save Models without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16455a18-fcce-4cac-881b-2650171cb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_predictions_without_noise(weights_list, y_pred_list):\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(directory + '/lambda_weights_list_without_noise.npy', weights_list, allow_pickle=True)\n",
    "    np.save(directory + '/lambda_preds_list_without_noise.npy', y_pred_list, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca754d4-04e6-48d7-8016-6be40dfe31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = np.stack([np.array(x[0]) for x in weights_ypred_list])\n",
    "y_pred_list = np.stack([x[1] for x in weights_ypred_list])\n",
    "y_pred_list = y_pred_list.reshape([config['data']['n_datasets'], config['data']['n_samples']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c282605-f3ef-4c6e-9867-95676efff846",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_predictions(weights_list, y_pred_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
