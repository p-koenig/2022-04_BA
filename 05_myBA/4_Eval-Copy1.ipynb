{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89b2195-8060-4d85-8330-19aa6cb1e3ac",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a20ce18-baf7-46be-a2a2-79238d9a1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "\n",
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467f817-f968-4b2c-84d5-e8f8d4627666",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config (2 I-Nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eeaa4-4c27-41aa-aad8-8fd75ce0a3a7",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c0ab8e-2451-424f-8fca-f26d1a761c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_LR = {\n",
    "    'data': {\n",
    "        'n_datasets': 15_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 5_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 20, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 10,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 1,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0.01,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': None,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },    \n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            'optimizer_lambda': 'adam',\n",
    "            'loss': 'mae', #tf.keras.losses.get(config['lambda_net']['loss_lambda']),\n",
    "            'metrics': ['mae']\n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 150,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'inets': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 1000,\n",
    "            'verbose': 'auto',\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'eval': {\n",
    "        'n_datasets': 10,\n",
    "        'n_samples_train': 5_000,\n",
    "        'n_samples_queryLambda': 4_500, # _forLogRegBaseModel\n",
    "        'n_samples_comparison': 4_000 # compare inet and basemodel\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '4',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576d3d5-a533-46c5-8590-3b1ab0a932c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5fe06c-e82f-4e2a-a23f-a717c2ce3b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "config_DT = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'vanilla', #'SDT', 'vanilla'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 15, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        \n",
    "        'use_distribution_list': True,\n",
    "        'random_parameters_distribution': True, ##MAKEPATH DIFFERENT FILES\n",
    "        'max_distributions_per_class': 1, # None; 0; int >= 1  \n",
    "        'exclude_linearly_seperable': True,\n",
    "        'data_generation_filtering': False,\n",
    "        'fixed_class_probability': False,\n",
    "        'balanced_data': True,\n",
    "        'weighted_data_generation': False,\n",
    "        'shift_distrib': False,\n",
    "        \n",
    "        'dt_type_train': 'vanilla', # (None, 'vanilla', 'SDT')\n",
    "        'maximum_depth_train': 3, #None or int\n",
    "        'decision_sparsity_train': 1, #None or int\n",
    "        \n",
    "        'function_generation_type': 'distribution',# 'make_classification_distribution', 'make_classification_distribution_trained', 'distribution', 'distribution_trained', 'make_classification', 'make_classification_trained', 'random_decision_tree', 'random_decision_tree_trained'\n",
    "        'distrib_by_feature': True,\n",
    "        'distribution_list': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'exponential', 'beta', 'binomial', 'poisson'], \n",
    "        'distribution_list_eval': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],#['uniform', 'gamma', 'poisson', 'exponential', 'weibull'],#['uniform', 'normal', 'gamma', 'beta', 'poisson'],\n",
    "        \n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        'number_of_generated_datasets': 100,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "        \n",
    "        'data_noise': 0, #None or float\n",
    "        \n",
    "        'distrib_param_max': 5,\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-3,\n",
    "        'restore_best_weights': True,\n",
    "        'patience_lambda': 50,\n",
    "        \n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'use_batchnorm_lambda': False,\n",
    "        \n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 100,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        #'dense_layers': [1024, 1024, 256, 2048, 2048],\n",
    "        'dense_layers': [1792, 512, 512],\n",
    "        #'dense_layers': [1792, 512, 512],\n",
    "        \n",
    "        #'dropout': [0, 0, 0, 0, 0.3],#[0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "        'dropout': [0, 0, 0.5],\n",
    "        #'dropout': [0, 0, 0.5],\n",
    "\n",
    "        #'hidden_activation': 'relu',\n",
    "        'hidden_activation': 'sigmoid',\n",
    "        #'hidden_activation': 'swish',\n",
    "\n",
    "        #'optimizer': 'rmsprop', \n",
    "        'optimizer': 'adam', \n",
    "        #'optimizer': 'adam', \n",
    "        \n",
    "        #'learning_rate': 0.001,\n",
    "        'learning_rate': 0.001,\n",
    "        #'learning_rate': 0.001, \n",
    "        \n",
    "        'separate_weight_bias': False,\n",
    "        \n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,        \n",
    "        'additional_hidden': False,\n",
    "        \n",
    "        'loss': 'binary_crossentropy', #mse; binary_crossentropy; 'binary_accuracy'\n",
    "        'metrics': ['binary_accuracy'], #soft_ or _penalized\n",
    "        \n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 5, #Float for fraction, Int for number 0\n",
    "        'evaluate_distribution': True,\n",
    "        'force_evaluate_real_world': False,\n",
    "        \n",
    "        'function_representation_type': 5, # 1=standard representation; 2=sparse representation with classification for variables; 3=softmax to select classes (n top probabilities)\n",
    "        'normalize_lambda_nets': False,\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2,3) #3=autoencoder dimensionality reduction\n",
    "        \n",
    "        'resampling_strategy': None,#'ADASYN', #'SMOTE', None\n",
    "        'resampling_threshold': 0.25,#0.2,\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 60,\n",
    "        'nas_optimizer': 'greedy' #'hyperband',#\"bayesian\",'greedy', 'random'\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        'number_of_random_evaluations_per_distribution': 10,\n",
    "        'random_evaluation_dataset_size_per_distribution': 10_000, \n",
    "        'optimize_sampling': True,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'random_evaluation_dataset_distribution': 'uniform', \n",
    "        \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        #'sklearn_dt_benchmark': False,\n",
    "        #'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': False,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 5000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 100,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 100,\n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        'n_jobs': 15,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ba22e7-b06d-4e66-9484-dabfca32d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "computation_config = {\n",
    "        'n_jobs': 100,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "        'verbosity': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558ba97-34f2-4429-bad1-647654431a0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c4ee38-f93f-40be-9c54-1a9005c0f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = computation_config['gpu_numbers'] if computation_config['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if computation_config['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if computation_config['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if computation_config['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5239304d-59da-4fb9-93a5-5d8c7c4613e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ca24f1-bd4c-402d-8cb5-9d8bd94ca6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_model = generate_base_model(config_DT)#generate_base_model(config_DT, disable_batchnorm=True)\n",
    "\n",
    "np.random.seed(config_DT['computation']['RANDOM_SEED'])\n",
    "        \n",
    "random_network_parameters = random_model.get_weights()\n",
    "network_parameters_structure = [network_parameter.shape for network_parameter in random_network_parameters]  \n",
    "\n",
    "\n",
    "try:\n",
    "    use_distribution_list = config_DT['data']['use_distribution_list'] if config_DT['data']['max_distributions_per_class'] is not None else False\n",
    "except:\n",
    "    use_distribution_list = False if config_DT['data']['max_distributions_per_class'] is None else True\n",
    "\n",
    "metrics = []\n",
    "loss_function = None\n",
    "\n",
    "if config_DT['i_net']['function_value_loss']:\n",
    "    if config_DT['i_net']['function_representation_type'] == 1:\n",
    "        pass\n",
    "        #metrics.append(tf.keras.losses.get('mae'))\n",
    "    if config_DT['i_net']['optimize_decision_function']:\n",
    "        loss_function = inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list)\n",
    "        #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "            #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "    else:\n",
    "        loss_function = inet_target_function_fv_loss_wrapper(config_DT)\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        for metric in config_DT['i_net']['metrics']:\n",
    "            metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))  \n",
    "            metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "else:\n",
    "    if config_DT['i_net']['function_representation_type'] >= 3:\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            \n",
    "            loss_function = inet_decision_function_fv_loss_wrapper_parameters(config_DT)\n",
    "            \n",
    "            metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "            for metric in config_DT['i_net']['metrics']:\n",
    "                metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))    \n",
    "            if False:\n",
    "                metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "                #metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "                for metric in config_DT['i_net']['metrics']:\n",
    "                    metrics.append(inet_decision_function_fv_metric_wrapper(random_model, network_parameters_structure, config_DT, metric, use_distribution_list=use_distribution_list))  \n",
    "                    #metrics.append(inet_target_function_fv_metric_wrapper(config_DT, metric))                  \n",
    "    else:\n",
    "        raise SystemExit('Coefficient Loss not implemented for config_DTuration')\n",
    "    \n",
    "    if False:\n",
    "        metrics.append(inet_target_function_fv_loss_wrapper(config_DT))\n",
    "        metrics.append(inet_decision_function_fv_loss_wrapper(random_model, network_parameters_structure, config_DT, use_distribution_list=use_distribution_list))\n",
    "        if config_DT['i_net']['optimize_decision_function']:\n",
    "            raise SystemExit('Coefficient Loss not implemented for decision function optimization')            \n",
    "        else:\n",
    "            if config_DT['i_net']['function_representation_type'] == 1:\n",
    "                loss_function = tf.keras.losses.get('mae') #inet_coefficient_loss_wrapper(inet_loss)\n",
    "            else:\n",
    "                raise SystemExit('Coefficient Loss not implemented for selected function representation')\n",
    "\n",
    "                \n",
    "# dill.dumps(loss_function)\n",
    "# dill.dumps(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7b6c-1f54-4d2d-b640-63794af75774",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7cbf86-55d4-4bb4-9d96-7b9461484129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_LR_inet():\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = keras.models.load_model(path + '/modelKeras')\n",
    "    print(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9513f469-9f28-4dcc-a9ff-fcebd07df0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_DT_inet_custom():\n",
    "    \n",
    "    loss_function_local = dill.dumps(loss_function)\n",
    "    metrics_local = dill.dumps(metrics)\n",
    "    \n",
    "    path = './data/saved_models/lNetSize5000_numLNets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown/128_e1000ES0.001_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1792-512-512_drop0-0-0.5e500b256_adam_funcRep5_reshapeNone_depth3_beta1_decisionSpars1_vanilla_reshapeNone'    \n",
    "    model = []\n",
    "    from tensorflow.keras.utils import CustomObjectScope\n",
    "    loss_function_local = dill.loads(loss_function_local)\n",
    "    metrics_local = dill.loads(metrics_local)       \n",
    "\n",
    "    #with CustomObjectScope({'custom_loss': loss_function}):\n",
    "    custom_object_dict = {}\n",
    "    custom_object_dict[loss_function.__name__] = loss_function_local\n",
    "    for metric in  metrics_local:\n",
    "        custom_object_dict[metric.__name__] = metrics_local        \n",
    "        \n",
    "    model = tf.keras.models.load_model(path, custom_objects=custom_object_dict) # #, compile=False\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9270b5-e9bc-4979-b415-76152d44e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DT = load_DT_inet_custom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e8e007c-ed95-4adb-afe4-8340f8766374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_LR/nda10000_nsa5000_nfe20_nin10_nta1_ncc2_sep1.0_noi0.01_shuTrue_ranNone/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo150_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1/tsi0.2_vsi0.1_ranNone_shuTrue_strNone_bat32_epo1000_shuTrue_claNone_samNone_ini0_steNone_vstNone_vbsNone_vfr1\n"
     ]
    }
   ],
   "source": [
    "model_LR = load_LR_inet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff70b789-58f8-429a-86b4-e22a4e89fb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 2177)]       0           []                               \n",
      "                                                                                                  \n",
      " hidden1_1792 (Dense)           (None, 1792)         3902976     ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " activation1_sigmoid (Activatio  (None, 1792)        0           ['hidden1_1792[0][0]']           \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " hidden2_512 (Dense)            (None, 512)          918016      ['activation1_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " activation2_sigmoid (Activatio  (None, 512)         0           ['hidden2_512[0][0]']            \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " hidden3_512 (Dense)            (None, 512)          262656      ['activation2_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " activation3_sigmoid (Activatio  (None, 512)         0           ['hidden3_512[0][0]']            \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " dropout3_0.5 (Dropout)         (None, 512)          0           ['activation3_sigmoid[0][0]']    \n",
      "                                                                                                  \n",
      " output_coeff_105 (Dense)       (None, 105)          53865       ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_1 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_2 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_3 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_4 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_5 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_6 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_identifier_7 (Dense)    (None, 15)           7695        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_leaf_node_8 (Dense)     (None, 8)            4104        ['dropout3_0.5[0][0]']           \n",
      "                                                                                                  \n",
      " output_combined (Concatenate)  (None, 218)          0           ['output_coeff_105[0][0]',       \n",
      "                                                                  'output_identifier_1[0][0]',    \n",
      "                                                                  'output_identifier_2[0][0]',    \n",
      "                                                                  'output_identifier_3[0][0]',    \n",
      "                                                                  'output_identifier_4[0][0]',    \n",
      "                                                                  'output_identifier_5[0][0]',    \n",
      "                                                                  'output_identifier_6[0][0]',    \n",
      "                                                                  'output_identifier_7[0][0]',    \n",
      "                                                                  'output_leaf_node_8[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,195,482\n",
      "Trainable params: 5,195,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_DT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33922ff4-e0a5-41d6-9dce-23be674d60f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 8362),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'dense_input'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 1000,\n",
       "    'activation': 'linear',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': {'class_name': 'L1L2',\n",
       "     'config': {'l1': 9.999999747378752e-06, 'l2': 9.999999747378752e-05}},\n",
       "    'bias_regularizer': {'class_name': 'L2',\n",
       "     'config': {'l2': 9.999999747378752e-05}},\n",
       "    'activity_regularizer': {'class_name': 'L2',\n",
       "     'config': {'l2': 9.999999747378752e-06}},\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'ReLU',\n",
       "   'config': {'name': 're_lu',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'max_value': None,\n",
       "    'negative_slope': array(0., dtype=float32),\n",
       "    'threshold': array(0., dtype=float32)}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 20,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "     'config': {'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LR.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a8573-c8e5-44aa-8be7-5de1d6c79eeb",
   "metadata": {},
   "source": [
    "# Load Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ed8a8-4c1d-4669-a842-441c3eaca74e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e0bf79-b725-48e6-bb3a-755f90dce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.data_path_LR(config_LR)\n",
    "#\n",
    "## with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "#y_coef_truth_test_data_LR = np.load(directory + '/coef_list_targetForInet.npy', allow_pickle=True)\n",
    "#\n",
    "\n",
    "#y_coef_truth_test_data_LR =  np.load(utilities_LR.lambda_path_LR(config_LR) + '/lambda_generated_coef_list_target_for_inet.npy') ## get coef from lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "666b8c92-2b14-4ed6-ab88-165bbf0da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.lambda_path_LR(config_LR)\n",
    "#\n",
    "## with open(directory + '/coef_list_LR_targetForInet.npy', \"rb\") as f:\n",
    "#x_lambda_weights_test_data_LR = np.load(directory + '/lambda_weights_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787f1988-46ec-4bf9-9256-4931282060ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_features']])\n",
    "#\n",
    "#if  config_LR['data']['n_targets'] < 2:\n",
    "#    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], ])\n",
    "#    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "#else:\n",
    "#    y_datasets_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_samples'], config_LR['data']['n_targets']])\n",
    "#    coef_list_LR = np.zeros([config_LR['data']['n_datasets'], config_LR['data']['n_features'], config_LR['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cef52c5-4c16-4390-9e4a-945fb9901ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = utilities_LR.data_path_LR(config_LR)\n",
    "#\n",
    "#with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "#    X_datasets_list_LR = np.load(f, allow_pickle=True)\n",
    "#with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "#    y_datasets_list_LR = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d961df0-5df2-487f-acec-7388358cddd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19b4b0b5-f797-4c58-8838-4eaec1af843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals().update(generate_paths(config_DT, path_type='lambda_net'))\n",
    "\n",
    "# directory = './data/saved_function_lists/functions_' + path_identifier_function_data + '.csv'\n",
    "\n",
    "#directory = './data/saved_function_lists/functions_lNetSize5000_numDatasets100_var15_class2_distribution_xMax1_xMin0_xDistuniform_dNoise0_randParamDist_maxDistClass1_distribParamMax5_randClassProb_exLinSepun-no-ga-be-po_depth3_beta1_decisionSpars1_vanilla_fullyGrown.csv'\n",
    "#\n",
    "#function_df = pd.read_csv(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa6d7cc-8204-43dd-87ba-81a9cbd00a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "893bef4e-f46c-4443-afed-df92c39c085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_data_DT = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906643c-f0be-4c45-ad6c-7798682c5e0d",
   "metadata": {},
   "source": [
    "# Evaluate Inet for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "898ad615-be9a-49e4-9a5c-5946088f5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp, tn, fn):\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3797feda-655a-420e-8ad4-c826a2edacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fp, tn, fn):\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37144d24-d5b0-4f55-9531-dec28cafbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(tp, fp, tn, fn):\n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    return 2 * (pre * rec) / (pre + rec) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6d452-5dc6-4555-9be5-69cd4495a575",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation on already known data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "592923d0-f576-4549-9a6b-616fae44fd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def confusionMatrixAggregated_SingleSample(i):\n",
    "#    x_lambda_weights = x_lambda_weights_test_data_LR[i, :]\n",
    "#    y_coef_truth = y_coef_truth_test_data_LR[i, :]\n",
    "#    x_lambda_weights = x_lambda_weights.reshape((1, 8362))\n",
    "#    y_coef_pred = model_LR.predict(x=x_lambda_weights,\n",
    "#        batch_size=None,\n",
    "#        verbose=0,\n",
    "#        steps=None,\n",
    "#        callbacks=None,\n",
    "#        max_queue_size=10,\n",
    "#        workers=1,\n",
    "#        use_multiprocessing=False,\n",
    "#                    )\n",
    "#    \n",
    "#    model_groundTruth = get_LR(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "#    \n",
    "#    model_pred = LogisticRegression()\n",
    "#    model_pred.coef_ = y_coef_pred\n",
    "#    model_pred.intercept_ = 0\n",
    "#    model_pred.classes_ = model_groundTruth.classes_\n",
    "#    \n",
    "#    y_coef_pred = y_coef_pred[0]\n",
    "#    \n",
    "#    mse = sklearn.metrics.mean_squared_error(\n",
    "#        y_coef_truth, y_coef_pred\n",
    "#    )\n",
    "#    \n",
    "#    score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "#    score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "#    y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "#    y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "#    \n",
    "#    tn, fp, fn, tp = confusion_matrix(y_truth_set, y_pred_set).ravel()\n",
    "#    \n",
    "#    return tn, fp, fn, tp\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8215d17d-737c-489f-9bec-071026ea10bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#parallel = Parallel(n_jobs=config_LR['n_jobs'], verbose=10, backend='loky') #loky\n",
    "##parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "#\n",
    "#confusion_Matrix_Array = parallel(delayed(confusionMatrixAggregated_SingleSample)(i) for i in range(x_lambda_weights_test_data_LR.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3e5563d-f159-4a79-8a35-3c7535f71b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = np.array(confusion_Matrix_Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a65db1-c49f-4cfc-a898-2245d32ff2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a55c5da-82d5-424c-841d-e20353688ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "868e8e91-0de1-4f0c-ab67-a123e6d64ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_Matrix_Array = confusion_Matrix_Array.reshape([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad0728bc-d162-4c79-a614-97869e6b30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disp = ConfusionMatrixDisplay(confusion_Matrix_Array)\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52978a-c03a-4c5f-bd73-fe33945d0f0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate on arbitrary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89b5929c-62cd-4464-81df-fc4bff16ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_weights = np.zeros([config_LR['eval']['n_datasets'], 8301, ])\n",
    "\n",
    "X_queryLambda = np.zeros([config_LR['eval']['n_datasets'], config_LR['eval']['n_samples_queryLambda'], config_LR['data']['n_features']])\n",
    "\n",
    "if  config_LR['data']['n_targets'] < 3:\n",
    "    y_valid_coefs = np.zeros([config_LR['eval']['n_datasets'], config_LR['data']['n_features'], ])\n",
    "else:\n",
    "    print(\"#################### NOT YET IMPLEMENTED ######################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec0e857c-9cae-47c4-9132-e26d2cfbd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR(X, y):\n",
    "    model = LogisticRegression(penalty='l2',\n",
    "        dual=False,\n",
    "        tol=0.0001,\n",
    "        C=1.0,\n",
    "        fit_intercept=True,\n",
    "        intercept_scaling=1,\n",
    "        class_weight=None,\n",
    "        random_state=None,\n",
    "        solver='lbfgs',\n",
    "        max_iter=100,\n",
    "        multi_class='auto',\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        n_jobs=None,\n",
    "        l1_ratio=None\n",
    "                              )\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60586aeb-321d-4305-8a17-18d69d503556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                        random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                        shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                        stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "                                                    y_test, \n",
    "                                                    test_size=config_LR['lambda']['data_prep']['train_test_val_split']['val_size'] / (config_LR['lambda']['data_prep']['train_test_val_split']['test_size'] + config_LR['lambda']['data_prep']['train_test_val_split']['val_size']), \n",
    "                                                    random_state=config_LR['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config_LR['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config_LR['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4b54a5e-afa4-4295-815e-279a67ba694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y):\n",
    "    # Data Prep\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "                                                                          y)\n",
    "    \n",
    "    # Model Def\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config_LR['data']['n_features']))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config_LR['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=config_LR['lambda']['model_compile']['optimizer_lambda'],\n",
    "                  loss=config_LR['lambda']['model_compile']['loss'],\n",
    "                  metrics=config_LR['lambda']['model_compile']['metrics']\n",
    "                 )\n",
    "    \n",
    "    # Model fit\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=config_LR['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config_LR['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config_LR['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=config_LR['lambda']['model_fit']['callbacks'],\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        shuffle=config_LR['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config_LR['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config_LR['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config_LR['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config_LR['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config_LR['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config_LR['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config_LR['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ef84628-c36e-4b01-b837-ffa2d87fedee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid_reg():\n",
    "    \n",
    "    ### TRAIN LAMBDA\n",
    "    \n",
    "    X, y =  sklearn.datasets.make_classification(n_samples=config_LR['eval']['n_samples_train'] + config_LR['eval']['n_samples_queryLambda'] + config_LR['eval']['n_samples_comparison'], \n",
    "                                                                                         n_features=config_LR['data']['n_features'],\n",
    "                                                                                         n_informative=config_LR['data']['n_informative'],\n",
    "                                                                                         n_redundant=config_LR['data']['n_features']-config_LR['data']['n_informative'],\n",
    "                                                                                         n_repeated=0,\n",
    "                                                                                         n_classes=config_LR['data']['n_targets']+1, \n",
    "                                                                                         n_clusters_per_class=config_LR['data']['n_clusters_per_class'],\n",
    "                                                                                         weights=None,\n",
    "                                                                                         flip_y=config_LR['data']['noise'],\n",
    "                                                                                         class_sep=config_LR['data']['class_sep'],\n",
    "                                                                                         shuffle=config_LR['data']['shuffle'],\n",
    "                                                                                         random_state=config_LR['data']['random_state'])\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=config_LR['eval']['n_samples_queryLambda'] + config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    X_queryLambda, X_comparison, _, _ = train_test_split(X_temp, y_temp, test_size=config_LR['eval']['n_samples_comparison'])\n",
    "    \n",
    "    \n",
    "    model_lambda = train_nn(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    ### GET COEF FROM LAMBDA\n",
    "    \n",
    "    y_pred = model_lambda.predict(X_queryLambda)\n",
    "    \n",
    "    y_pred = [1.0 if y>=0.5 else 0.0 for y in y_pred]\n",
    "    \n",
    "    log_reg_test = get_LR(X_queryLambda, y_pred)\n",
    "    \n",
    "    return np.concatenate([x.flatten() for x in model_lambda.get_weights()]), log_reg_test.coef_, X_queryLambda ## test_weights, test_coef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fc2acf3-52fb-47b4-8be1-256f671df495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=100)]: Using backend LokyBackend with 100 concurrent workers.\n",
      "[Parallel(n_jobs=100)]: Done   3 out of  10 | elapsed:   29.6s remaining:  1.2min\n",
      "[Parallel(n_jobs=100)]: Done   5 out of  10 | elapsed:   29.8s remaining:   29.8s\n",
      "[Parallel(n_jobs=100)]: Done   7 out of  10 | elapsed:   29.9s remaining:   12.8s\n",
      "[Parallel(n_jobs=100)]: Done  10 out of  10 | elapsed:   30.3s finished\n"
     ]
    }
   ],
   "source": [
    "parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "#parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "\n",
    "weights_coef = parallel(delayed(create_valid_reg)() for i in range(config_LR['eval']['n_datasets']))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b9ff2f2-5e74-4f1f-9d12-cfb1bbdb75fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(config_LR['eval']['n_datasets']):\n",
    "    X_valid_weights[i] = weights_coef[i][0]\n",
    "    y_valid_coefs[i] = weights_coef[i][1]\n",
    "    X_queryLambda[i] = weights_coef[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c3ce7a5-10b9-45dd-b815-fd29797ba890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8301)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27034654-f43b-4460-a45a-1c60df10e7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9339ad59-d878-45b9-b95d-1c95c1df405a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4500, 20)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_queryLambda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3f13f94-f13c-4309-9ad9-f4410f0cb6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateSingleSampleOnValidSet(i):\n",
    "    X_valid_weight = X_valid_weights[i, :]\n",
    "    y_valid_coef = y_valid_coefs[i, :]\n",
    "    X_queryLambda_instance = X_queryLambda[i, :]\n",
    "    \n",
    "    \n",
    "    X_valid_weight = X_valid_weight.reshape((1, 8301))\n",
    "    \n",
    "    y_valid_coef_pred = model_LR.predict(x=X_valid_weight,\n",
    "        batch_size=None,\n",
    "        verbose=0,\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "                    )\n",
    "    \n",
    "    y_valid_coef = y_valid_coef.reshape([1, 20])\n",
    "    \n",
    "    model_truth = LogisticRegression()\n",
    "    model_truth.coef_ = y_valid_coef\n",
    "    model_truth.intercept_ = 0\n",
    "    model_truth.classes_ = np.array([0, 1])\n",
    "    \n",
    "    model_pred = LogisticRegression()\n",
    "    model_pred.coef_ = y_valid_coef_pred\n",
    "    model_pred.intercept_ = 0\n",
    "    model_pred.classes_ = np.array([0, 1])\n",
    "    \n",
    "    y_class_truth = model_truth.predict(X_queryLambda_instance)\n",
    "    y_class_pred = model_pred.predict(X_queryLambda_instance)\n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(\n",
    "        y_class_truth, y_class_pred\n",
    "    )\n",
    "    \n",
    "    #score_groundTruthModel = model_groundTruth.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #score_predModel = model_pred.score(X_datasets_list_LR[i], y_datasets_list_LR[i])\n",
    "    #y_truth_set = model_groundTruth.predict(X_datasets_list_LR[i])\n",
    "    #y_pred_set  = model_pred.predict(X_datasets_list_LR[i])\n",
    "    tn, fp, fn, tp = confusion_matrix(y_class_truth, y_class_pred, labels=[1,0]).ravel()\n",
    "    \n",
    "    pre = precision(tp, fp, tn, fn)\n",
    "    rec = recall(tp, fp, tn, fn)\n",
    "    fone = f1(tp, fp, tn, fn)\n",
    "    \n",
    "    #results.append([i, score_groundTruthModel, score_predModel, mse, tp, fn, fp, tn, pre, rec, fone])\n",
    "    \n",
    "    return i+1, -1, -1, mse, tp, fn, fp, tn, pre, rec, fone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f8652d2-dcfe-4e1c-a229-815a6d07d8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 8362), found shape=(None, 8301)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\u001b[39;00m\n\u001b[1;32m      2\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#loky\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m result_list \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluateSingleSampleOnValidSet\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_LR\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_datasets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_0=aggregated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscoreOnClassfication_BaseModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscoreOnClassfication_PredictedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m], data\u001b[38;5;241m=\u001b[39mresult_list)\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mevaluateSingleSampleOnValidSet\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m      4\u001b[0m X_queryLambda_instance \u001b[38;5;241m=\u001b[39m X_queryLambda[i, :]\n\u001b[1;32m      7\u001b[0m X_valid_weight \u001b[38;5;241m=\u001b[39m X_valid_weight\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8301\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m y_valid_coef_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_LR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_valid_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m y_valid_coef \u001b[38;5;241m=\u001b[39m y_valid_coef\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m     21\u001b[0m model_truth \u001b[38;5;241m=\u001b[39m LogisticRegression()\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/work/pkoenig/miniconda3/envs/myBA/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 8362), found shape=(None, 8301)\n"
     ]
    }
   ],
   "source": [
    "#parallel = Parallel(n_jobs=computation_config['n_jobs'], verbose=10, backend='loky') #loky\n",
    "parallel = Parallel(n_jobs=1, verbose=10, backend='loky') #loky\n",
    "\n",
    "result_list = parallel(delayed(evaluateSingleSampleOnValidSet)(i) for i in range(config_LR['eval']['n_datasets']))\n",
    "                   \n",
    "    \n",
    "results = pd.DataFrame(columns=[\"index_0=aggregated\", \"scoreOnClassfication_BaseModel\", \"scoreOnClassfication_PredictedModel\" , \"mse\",  \"tp\", \"fn\", \"fp\", \"tn\", \"precision\", \"recall\", \"f1\"], data=result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249ab04-1e24-495f-98e6-4586c48ac107",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae48075-0b96-4a56-94d0-ce671d424428",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated = pd.DataFrame(results.mean(numeric_only=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7ed92-fdbe-4aff-ac6e-a5db6922c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bdc67-d6f9-4541-8b39-df29e5097315",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggragated.at[0, \"index_0=aggregated\"] = 0\n",
    "results = pd.concat([aggragated, results], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646255c-ceba-40a4-b978-677a3a550b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_res(df):\n",
    "    path = utilities_LR.inet_path_LR(config_LR)\n",
    "    \n",
    "    model = df.to_csv(path + '/evalRes.csv')\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f12ca-01c0-4c52-b80d-735f9971b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_res(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c78a12-7bee-4124-8d38-0ca6240c76b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb2668-d031-4bdb-9dba-75156cba9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7728121-949b-486d-8be1-502fa10fc1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743de906-ea72-43c6-9db4-49af847a6b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13704348-b36f-4836-bd6d-914ad5a84fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
