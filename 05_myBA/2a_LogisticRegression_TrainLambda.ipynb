{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7413b1-5b9f-472e-9045-ee14068be971",
   "metadata": {},
   "source": [
    "# Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ef29c7-245e-4770-b29d-0045b6b0b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': {\n",
    "        'n_datasets': 20_000, # the number of datasets\n",
    "        \n",
    "        'n_samples': 10_000, # the number of samples per dataset\n",
    "        \n",
    "        'n_features': 30, \n",
    "        # The total number of features. \n",
    "        # These comprise n_informative informative features, n_redundant redundant features, n_repeated duplicated features and \n",
    "        # n_features-n_informative-n_redundant-n_repeated useless features drawn at random.\n",
    "        \n",
    "        'n_informative': 25,\n",
    "        # The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices \n",
    "        # of a hypercube in a subspace of dimension n_informative. For each cluster, informative features are drawn independently \n",
    "        # from N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then \n",
    "        # placed on the vertices of the hypercube.\n",
    "        \n",
    "        'n_targets': 2,\n",
    "        # The number of targets (or labels) of the classification problem.\n",
    "    \n",
    "        'n_clusters_per_class': 2,\n",
    "        # The number of clusters per class.\n",
    "        \n",
    "        'class_sep': 1.0,\n",
    "        # class_sepfloat, default=1.0\n",
    "        # The factor multiplying the hypercube size. Larger values spread out the clusters/classes and make the classification task \n",
    "        # easier.\n",
    "        \n",
    "        'noise': 0.01,\n",
    "        # flip_y (fraction of samples whose class is assigned randomly)\n",
    "        \n",
    "        'shuffle': True,\n",
    "        # Shuffle the samples and the features.\n",
    "        \n",
    "        'random_state': None,\n",
    "        # Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls.\n",
    "    },    \n",
    "    'lambda': {\n",
    "        'data_prep': {\n",
    "            'train_test_val_split': { # refer to sklearn doc\n",
    "                'test_size': 0.2,\n",
    "                'val_size': 0.1,\n",
    "                'random_state': None,\n",
    "                'shuffle': True,\n",
    "                'stratify': None\n",
    "            }\n",
    "        },\n",
    "        'model_compile': {\n",
    "            \n",
    "        },\n",
    "        'model_fit': { # refer to keras API\n",
    "            'batch_size': 32,\n",
    "            'epochs': 150,\n",
    "            'verbose': 0,\n",
    "            'callbacks': None,\n",
    "            'shuffle': True,\n",
    "            'class_weight': None,\n",
    "            'sample_weight': None,\n",
    "            'initial_epoch': 0,\n",
    "            'steps_per_epoch': None,\n",
    "            'validation_steps': None,\n",
    "            'validation_batch_size': None,\n",
    "            'validation_freq': 1\n",
    "        }\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 30,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '4',\n",
    "        'RANDOM_SEED': 1,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d269d23f-33f7-456b-b4fc-a457414ed778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['computation']['gpu_numbers'] if config['computation']['use_gpu'] else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if config['computation']['use_gpu'] else ''\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if config['computation']['use_gpu'] else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if config['computation']['use_gpu'] else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "import utilities_LR\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae09b938-bfec-4ac3-b5cd-a33ce494c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76077315-f3bc-495f-92b9-df69649f3eed",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91501bce-140a-4c6c-89c4-94317079c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_features']])\n",
    "\n",
    "if  config['data']['n_targets'] < 2:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], ])\n",
    "    coef_list = np.zeros([config['data']['n_datasets'], config['data']['n_features'], ])\n",
    "else:\n",
    "    y_datasets_list = np.zeros([config['data']['n_datasets'], config['data']['n_samples'], config['data']['n_targets']])\n",
    "    coef_list = np.zeros([config['data']['n_datasets'], config['data']['n_features'], config['data']['n_targets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f258543-80c3-4faa-a6f8-04f4a48e09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = utilities_LR.data_path_LR(config)\n",
    "\n",
    "with open(directory + '/X_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    X_datasets_list = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/y_datasets_list_dataForLambda.npy', \"rb\") as f:\n",
    "    y_datasets_list = np.load(f, allow_pickle=True)\n",
    "with open(directory + '/coef_list_targetForInet.npy', \"rb\") as f:\n",
    "    coef_list = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00746f77-9a97-42be-837b-e5f9fb78cde1",
   "metadata": {},
   "source": [
    "# Prepare Data (Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6ed8ef-cb7e-40de-a669-6f79f753809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=config['lambda']['data_prep']['train_test_val_split']['test_size'] + config['lambda']['data_prep']['train_test_val_split']['val_size'], \n",
    "                                                        random_state=config['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                        shuffle=config['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                        stratify=config['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    X_test, X_val, y__test, y_val = train_test_split(X_test, \n",
    "                                                    y_test, \n",
    "                                                    test_size=config['lambda']['data_prep']['train_test_val_split']['val_size'] / (config['lambda']['data_prep']['train_test_val_split']['test_size'] + config['lambda']['data_prep']['train_test_val_split']['val_size']), \n",
    "                                                    random_state=config['lambda']['data_prep']['train_test_val_split']['random_state'], \n",
    "                                                    shuffle=config['lambda']['data_prep']['train_test_val_split']['shuffle'], \n",
    "                                                    stratify=config['lambda']['data_prep']['train_test_val_split']['stratify'])\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65510a-d70f-4277-800b-75ed4df35315",
   "metadata": {},
   "source": [
    "# Save Model & Metrics (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5754bd-00ae-472c-827e-fc2f807f93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(weights_list):\n",
    "    directory = utilities_LR.lambda_path_LR(config)\n",
    "    \n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(directory + '/lambda_weights_list.npy', \"wb\") as f:\n",
    "        np.save(f, weights_list, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77189e30-fcd7-4410-a593-dfc50360347f",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf1bd26-92b3-4826-9601-e8c4b6b77654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X, y, index):\n",
    "    # Data Prep\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,\n",
    "                                                                          y)\n",
    "    \n",
    "    # Model Def\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_dim=config['data']['n_features']))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(60, activation='relu'))\n",
    "    model.add(Dense(config['data']['n_targets'], activation='sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mae', keras.metrics.RootMeanSquaredError()])\n",
    "    # model.summary()\n",
    "    \n",
    "    # Model fit\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=config['lambda']['model_fit']['batch_size'],\n",
    "                        epochs=config['lambda']['model_fit']['epochs'],\n",
    "                        verbose=config['lambda']['model_fit']['verbose'],\n",
    "                        callbacks=config['lambda']['model_fit']['callbacks'],\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        shuffle=config['lambda']['model_fit']['shuffle'],\n",
    "                        class_weight=config['lambda']['model_fit']['class_weight'],\n",
    "                        sample_weight=config['lambda']['model_fit']['sample_weight'],\n",
    "                        initial_epoch=config['lambda']['model_fit']['initial_epoch'],\n",
    "                        steps_per_epoch=config['lambda']['model_fit']['steps_per_epoch'],\n",
    "                        validation_steps=config['lambda']['model_fit']['validation_steps'],\n",
    "                        validation_batch_size=config['lambda']['model_fit']['validation_batch_size'],\n",
    "                        validation_freq=config['lambda']['model_fit']['validation_freq'],\n",
    "                       )\n",
    "    # print(history.history['val_loss'])\n",
    "    # save_model(model, index)\n",
    "    return np.concatenate([x.flatten() for x in model.get_weights()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab4a7a-cae7-4fe8-b9a4-277357c64c86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=30)]: Using backend LokyBackend with 30 concurrent workers.\n",
      "2022-06-06 15:02:27.965945: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.024320: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.040628: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.072812: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.112064: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.136357: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.200070: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.242580: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.242608: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.268795: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.270197: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.279451: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.279479: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.279915: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.298612: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.341724: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.343743: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.348548: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.364950: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.389075: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.395565: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.407659: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.450806: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.454071: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.454192: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.454891: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.505102: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.538117: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.538462: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:28.774352: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:02:30.820123: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:30.820219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22630 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:30.829096: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:30.829162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22512 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:30.865864: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:30.865960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22326 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:30.918567: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:30.918702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22122 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.135592: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.135671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20992 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.206782: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.206863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20754 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.219186: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.219389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20708 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.232279: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.244790: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.244966: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.247218: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.259228: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.259339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20504 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.261866: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.261945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20478 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.262861: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.262958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20472 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.264293: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.264368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20466 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.268303: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.268365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20458 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.272899: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.272990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20430 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.274134: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.274214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20426 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.277856: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.277930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20420 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.279947: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.280137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20410 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.281500: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.281566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20406 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.282355: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.282472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20400 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.282865: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.282935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20400 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.287933: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.288063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20384 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.292890: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.292985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20378 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.295961: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.296042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20372 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.296337: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.296411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20374 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.299147: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.299257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20360 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.300081: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.300291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20356 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.302314: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.302385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20340 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.304554: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.304682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20308 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.316313: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.316457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20280 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.326403: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.326539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20274 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.366184: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:02:31.366321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20254 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:02:31.388849: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.431920: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.487328: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.490781: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.498236: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.513765: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.519887: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.523572: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.532397: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.540659: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.542061: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.547517: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.549580: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.559071: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.561234: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.561745: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.561894: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.567463: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.586416: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.587299: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.592147: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.602305: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.605455: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.607083: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fdb500098b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.607124: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.614134: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.621347: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2db00097d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.621393: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.627900: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.635970: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc8740092a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.636020: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.641413: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.643807: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7b2401a470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.643842: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.646991: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.648897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.655208: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.702940: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff0c80098e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.702979: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.708523: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.709825: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:02:31.746238: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fcd78009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.746295: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.755593: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.799082: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f182c009650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.799132: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.803446: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2de4009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.803488: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.807380: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.810723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.853081: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7834008f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.853144: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.860554: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.869246: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7efe00009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.869298: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.877444: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.885438: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7effbc009440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.885533: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.889022: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f15e80096e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.889063: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.893560: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.893874: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6cdc018be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.893929: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.894944: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3b740098c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.895002: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.896167: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.900929: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.901601: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.912724: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb69c009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.912765: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.913268: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f88e4018d50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.913323: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.918138: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.920256: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.922662: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe28c009420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.922705: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.927862: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.946798: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f90f0009650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.946846: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.952358: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.957977: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd1000095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.958027: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.960789: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f43800096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.960842: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.964111: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.964385: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9f800095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.964415: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.967846: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.969596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.989864: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7edc009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.989906: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.993092: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f282c009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.993138: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.995543: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:31.997139: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f54240093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:31.997205: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:31.998878: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:32.005905: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:32.007709: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6a98009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:32.007762: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:32.012924: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f13f0009600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:32.012970: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:32.012961: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:32.013037: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7f0c0093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:32.013112: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:32.019202: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:32.022376: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:32.044465: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f03cc0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:32.044521: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:32.046860: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff45c009400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:32.046905: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:32.050754: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:32.052275: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:32.120311: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7efa58009750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:02:32.120365: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:02:32.127266: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:02:33.563012: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:33.563187: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:33.579395: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:33.591498: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:33.881851: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:33.999776: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.025531: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.109122: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.241643: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.261424: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.291376: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.346279: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.377812: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.415636: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.416334: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.421985: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.452823: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.454011: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.501209: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.519513: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.523092: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.547651: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.555511: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.567574: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.577707: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.626410: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.685563: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.717285: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.720734: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:34.750377: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:02:35.959874: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:36.115783: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:36.161780: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:36.886937: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:37.156546: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:37.505562: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:37.571991: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:37.682685: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:37.915464: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:37.998131: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.024911: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.113950: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.166974: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.220852: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.220872: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.254997: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.258542: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.260905: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.295992: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.315914: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.319751: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.349078: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.351734: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.356303: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.358468: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.395701: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.441843: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.452078: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.463187: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:38.476951: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:02:46.167693: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:46.177418: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:46.177401: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:46.289129: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:46.458399: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:46.735996: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:46.823448: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:46.890234: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.039337: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.141218: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.197882: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.260230: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.471218: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.557901: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.616253: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.660140: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.693934: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.751301: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.755354: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.780200: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.781770: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.793719: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.794109: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.810459: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.823359: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.836939: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.844953: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.849059: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.857875: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:02:47.876566: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done   1 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=30)]: Done  12 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=30)]: Done  25 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=30)]: Done  38 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=30)]: Done  53 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=30)]: Done  68 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=30)]: Done  85 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=30)]: Done 102 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=30)]: Done 121 tasks      | elapsed: 32.8min\n",
      "[Parallel(n_jobs=30)]: Done 140 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=30)]: Done 161 tasks      | elapsed: 39.3min\n",
      "[Parallel(n_jobs=30)]: Done 182 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=30)]: Done 205 tasks      | elapsed: 45.8min\n",
      "2022-06-06 15:47:49.289292: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:49.305866: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:49.306448: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:49.333263: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:49.334023: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:49.334316: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:50.241606: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:50.527812: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:50.527886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11911 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:50.575968: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:50.576044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11845 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:50.590252: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:50.590309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11829 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:50.598328: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:50.598384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11825 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:50.602871: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:50.602930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11823 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:50.625089: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:50.625167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11811 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:50.770780: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:50.832602: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:50.840854: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:50.850956: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:50.860111: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:50.881151: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:50.997721: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:50.997775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12270 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:51.082980: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f649c009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:51.083024: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:51.088512: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:51.170512: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff6400095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:51.170549: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:51.170668: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fbf9c009140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:51.170709: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:51.171426: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc244009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:51.171449: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:51.175501: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:51.175808: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:51.175840: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:51.197608: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa118009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:51.197646: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:51.203847: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:51.204792: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:51.220671: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe1480092b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:51.220714: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:51.225750: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:51.268845: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:51.503976: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7efc440096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:51.504018: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:51.509225: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:51.952259: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:52.211208: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:52.211267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10309 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:52.423543: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:52.772676: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:52.799335: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f97500095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:52.799373: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:52.805806: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:52.855102: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:52.876361: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:52.876426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9782 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:52.886015: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:52.896751: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:52.984561: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:53.004072: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:53.026204: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:53.175981: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:53.223472: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:53.490593: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3b28018d50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:53.490638: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:53.497701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:54.045204: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:54.105541: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:54.105643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9784 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:54.394442: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:54.738000: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9230009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:54.738056: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:54.743904: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:54.744148: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:54.808265: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:54.851435: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:54.895311: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:54.897696: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:54.992332: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:55.025471: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:55.033157: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:55.115541: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:55.115608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9756 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:55.226335: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:55.368895: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:55.702335: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3e00009550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:55.702370: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:55.707315: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:55.747450: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:55.972427: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:56.207495: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:56.207558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9481 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:56.489110: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:56.863105: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:56.892239: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe4280093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:56.892280: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:56.897881: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:56.991501: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:57.048785: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:57.233422: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:57.233481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9473 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:57.496012: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:57.676677: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:57.889087: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff320009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:57.889139: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:57.896297: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:58.106507: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:58.106578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9547 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:58.167891: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:58.251005: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:58.330475: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:58.620007: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:47:58.668896: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7efb600093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:58.668970: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:58.678438: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:59.006578: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:59.006650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9667 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:47:59.259477: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:47:59.381223: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:47:59.562909: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:47:59.596150: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f37800095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:47:59.596185: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:47:59.601685: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:47:59.875685: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:47:59.875762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9914 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:48:00.025728: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:48:00.116804: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:48:00.474670: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:00.486069: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb1c8009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:48:00.486110: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:48:00.491581: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:48:00.726208: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:00.957035: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:01.138034: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:48:01.209262: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:01.247456: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:01.348696: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:01.366437: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:01.393342: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:48:01.393411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9495 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:48:01.467996: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:01.516685: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:01.672077: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:48:01.733332: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:01.887561: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:48:01.917594: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:02.051837: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7ad4009100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:48:02.051871: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:48:02.058940: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:48:02.300009: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:02.415862: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:48:02.415942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9433 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:48:02.686334: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:48:02.960886: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:48:03.037444: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f4bc40093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:48:03.037507: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:48:03.044357: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:48:03.153445: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:03.226474: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:03.375427: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:48:03.375524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8642 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:48:03.698974: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:48:03.937322: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:03.940466: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:04.067186: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fefa00092b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:48:04.067224: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:48:04.073049: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:48:04.282145: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:48:04.448762: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:48:04.448828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8015 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:48:04.687471: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:48:04.968876: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:05.046809: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f465c009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:48:05.046851: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:48:05.052271: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:48:05.223244: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:05.307056: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:48:05.381813: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:05.840360: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:48:05.840429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6718 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:48:06.105893: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:06.112953: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:48:06.367674: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:06.448877: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f183c018ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:48:06.448937: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:48:06.455115: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:48:06.886053: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:48:06.886118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6120 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:48:07.088285: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:48:07.182127: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:07.226898: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:07.407929: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6abc009550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:48:07.407979: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:48:07.413254: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:48:08.177360: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:08.293143: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:08.739677: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:09.499932: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:09.674989: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:10.485753: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:10.596272: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:48:10.677263: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:11.753052: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:12.196375: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:13.036953: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:13.268544: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:14.172503: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:48:14.590820: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:15.663606: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:17.707201: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:18.860125: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:20.021258: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:21.005038: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:22.463934: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:48:23.381023: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:53:37.392196: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:53:37.405191: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:53:39.025260: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:53:39.025330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6957 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:53:39.073599: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:53:39.073702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6953 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:53:39.317442: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:53:39.401486: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:53:39.636820: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2fb0009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:53:39.636871: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:53:39.643761: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:53:39.757193: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f460c009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:53:39.757235: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:53:39.763422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:53:40.817160: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:53:42.322902: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:53:42.322974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5658 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:53:42.621938: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:53:42.729820: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:53:42.804554: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:53:42.991069: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff764009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:53:42.991120: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:53:42.998662: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:53:46.032208: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:53:46.133302: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:53:46.140401: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:53:49.495090: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:53:54.025533: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:53:54.676976: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:53:54.697653: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:53:55.366357: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:53:55.544586: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:53:55.544644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6439 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:53:55.843769: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:53:56.216416: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb474018bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:53:56.216463: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:53:56.221288: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:53:56.950612: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:53:56.950711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5842 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:53:57.264958: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:53:57.634595: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f73a00095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:53:57.634657: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:53:57.642197: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:53:57.894608: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:53:59.192527: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:54:00.602422: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:54:02.372162: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:54:02.996954: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:54:03.759119: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 15:54:03.818080: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:54:04.616068: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:54:04.616204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6114 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:54:04.861320: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:54:05.190150: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9f2c009780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:54:05.190208: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:54:05.195920: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:54:05.332872: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 15:54:05.332955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5892 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 15:54:05.640686: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 15:54:05.978616: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff570009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 15:54:05.978666: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 15:54:05.985150: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 15:54:08.299922: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:54:09.182229: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 15:54:11.168492: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 15:54:11.635070: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:54:12.515653: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 15:54:12.654604: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 253 tasks      | elapsed: 59.0min\n",
      "[Parallel(n_jobs=30)]: Done 278 tasks      | elapsed: 65.4min\n",
      "[Parallel(n_jobs=30)]: Done 305 tasks      | elapsed: 71.6min\n",
      "[Parallel(n_jobs=30)]: Done 332 tasks      | elapsed: 77.7min\n",
      "[Parallel(n_jobs=30)]: Done 361 tasks      | elapsed: 84.1min\n",
      "[Parallel(n_jobs=30)]: Done 390 tasks      | elapsed: 85.2min\n",
      "2022-06-06 16:33:06.165170: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:06.183271: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:06.809276: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:07.562902: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:07.562981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9495 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:07.590263: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:07.590328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9491 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:07.715357: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:07.857959: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:07.858052: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:08.071486: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:08.071567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9376 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:08.198989: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd5b00095b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:08.199026: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:08.205049: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:08.224569: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f44ac0095b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:08.224620: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:08.230233: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:08.346630: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:08.573829: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:08.715560: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f8dd0009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:08.715602: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:08.721654: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:08.941404: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:08.941462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9334 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:09.210146: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:09.602218: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f4e54009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:09.602260: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:09.608514: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:09.770209: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:09.842583: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:09.842647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9273 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:10.132144: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:10.459235: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f87c40095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:10.459276: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:10.464997: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:10.621951: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:10.655523: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:11.021781: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:11.021844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8919 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:11.241042: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:11.273266: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:11.445028: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:11.592469: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6bb0009620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:11.592514: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:11.597474: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:12.172058: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:12.447539: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:12.827032: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:12.827112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9219 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:13.104995: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:13.151677: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:13.241744: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:13.290355: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:13.503128: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f0e48009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:13.503180: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:13.509674: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:13.670117: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:13.670187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9105 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:13.798866: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:13.862062: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:13.968301: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:14.164385: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:14.291441: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fcec8008000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:14.291490: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:14.298042: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:14.686459: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:15.120744: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:15.151528: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:15.151620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9722 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:15.419916: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:15.710111: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:15.732639: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9e380092b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:15.732673: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:15.737411: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:16.103456: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:16.148525: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:16.453721: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:16.453781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9507 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:16.742534: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:16.849352: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:16.905742: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:17.160912: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd4a40093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:17.160947: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:17.166031: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:17.168842: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:17.523629: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:17.523691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9467 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:17.860861: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:18.251578: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f40cc0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:18.251630: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:18.259257: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:18.318435: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:18.494000: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:18.625454: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:18.625510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8588 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:18.961845: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:18.977055: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:19.350750: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f73e8009100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:19.350791: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:19.357539: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:19.750126: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:19.750244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8879 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:19.760137: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:19.901321: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:20.064963: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:20.244813: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:20.383599: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:20.419466: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fad600095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:20.419504: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:20.426034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:20.508144: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:20.952284: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:21.091097: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:21.202257: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:21.309969: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:21.702206: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:21.702275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7409 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:21.982010: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:22.157268: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:22.270417: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:22.319993: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6a3c0093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:22.320055: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:22.327043: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:22.745698: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:22.745778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6871 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:22.954609: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:23.063617: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:23.143136: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:33:23.356804: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:23.365545: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:23.428239: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f85b8009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:23.428283: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:23.434244: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:24.031575: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:24.701275: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:24.810274: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:33:24.810336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5856 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:33:25.081950: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:33:25.467628: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:25.476745: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff1b8009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:33:25.476779: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:33:25.481603: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:33:25.518746: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:26.608153: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:26.698994: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:27.256468: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:28.182407: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:28.666195: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:33:28.890567: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:30.087307: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:30.154493: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:32.010624: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:32.176328: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:33:33.175320: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:34.743259: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:35.940736: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:38.093609: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:39.254211: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:33:41.293400: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 421 tasks      | elapsed: 96.8min\n",
      "2022-06-06 16:38:55.905404: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:38:55.924052: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:38:57.363726: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:38:57.547731: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:38:57.547804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6688 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:38:57.550131: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:38:57.550199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6688 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:38:57.843818: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:38:57.869815: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:38:58.196180: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa81c0098d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:38:58.196228: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:38:58.203770: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:38:58.218587: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f194c0093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:38:58.218632: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:38:58.223988: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:38:58.843948: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:38:58.844014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6184 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:38:59.110030: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:38:59.485715: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f262c0096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:38:59.485766: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:38:59.493117: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:01.206373: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:01.281666: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:02.566672: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:04.530156: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:04.593943: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:05.924793: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:10.322264: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:11.853035: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:11.853120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5506 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:12.106432: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:12.486378: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f5844009ab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:12.486436: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:12.492046: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:13.158801: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:13.218227: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:14.478817: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:15.508493: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:18.735017: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:18.787995: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:20.353570: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:20.353635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6441 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:20.590291: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:20.936484: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f1a40009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:20.936535: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:20.943047: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:22.071369: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:23.638345: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:23.638431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5570 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:23.881543: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:24.090896: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:24.250279: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f685c009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:24.250337: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:24.257076: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:27.413874: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:27.422155: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:27.427661: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:30.816869: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:35.096174: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:35.887928: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:36.386193: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:36.575736: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:36.575842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7117 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:36.863216: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:37.242609: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9644009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:37.242675: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:37.248893: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:37.250157: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:37.879189: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:37.879276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6754 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:38.227995: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:38.592505: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6478009870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:38.592587: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:38.601673: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:38.779261: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:38.779331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7045 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:38.962542: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:39.055932: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:39.408643: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f5aa4009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:39.408691: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:39.414679: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:40.192270: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:41.490147: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:41.671037: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:42.335181: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:43.224596: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:43.224709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6451 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:43.408585: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:43.442960: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:43.534157: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:43.930497: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3f8c009540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:43.930568: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:43.938479: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:44.752789: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:45.007999: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:45.008070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5810 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:45.306944: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:45.600508: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:45.640528: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f1f74009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:45.640570: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:45.646134: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:46.995970: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:48.673161: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:50.293916: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:50.421839: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:39:51.962753: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:39:51.962835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5522 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:39:51.962914: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:39:52.088818: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:52.280920: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:39:52.647675: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f85f0009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:39:52.647723: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:39:52.652715: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:39:53.413309: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:54.276074: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:55.673674: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:39:58.778320: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:39:58.833568: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:40:00.378589: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:40:02.340916: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:40:03.923464: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:40:03.923546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:40:04.311843: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:40:04.704111: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f56e00093f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:40:04.704165: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:40:04.710189: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:40:07.232479: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:40:07.700032: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 16:40:11.001298: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:40:19.723836: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 16:45:38.278880: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 16:45:39.849865: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 16:45:39.849928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5522 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 16:45:40.186161: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 16:45:40.543981: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f56a4009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 16:45:40.544050: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 16:45:40.551065: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 16:45:43.577971: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "[Parallel(n_jobs=30)]: Done 452 tasks      | elapsed: 103.7min\n",
      "2022-06-06 16:45:46.856393: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 16:45:55.551889: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 485 tasks      | elapsed: 110.4min\n",
      "[Parallel(n_jobs=30)]: Done 518 tasks      | elapsed: 117.1min\n",
      "[Parallel(n_jobs=30)]: Done 553 tasks      | elapsed: 123.5min\n",
      "[Parallel(n_jobs=30)]: Done 588 tasks      | elapsed: 130.0min\n",
      "2022-06-06 17:18:24.479640: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:24.489279: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:24.764406: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:25.898984: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:25.899071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22912 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:25.903459: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:25.903563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22912 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:26.157817: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:26.166446: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:26.166527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23449 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:26.171096: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:26.443942: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:26.482508: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:26.531165: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2fbc009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:26.531218: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:26.533264: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f5a60009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:26.533301: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:26.538477: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:26.539367: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:26.818491: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f484c0081a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:26.818525: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:26.824104: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:27.397002: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:27.737037: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:27.737119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22922 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:28.137871: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:28.497292: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9a1c009550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:28.497339: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:28.503356: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:28.649881: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:28.649952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22180 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:28.864560: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:29.117701: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:29.127533: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:29.177672: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9a04009550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:29.177719: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:29.184478: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:29.384537: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:29.862198: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:31.168491: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:31.278717: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:31.370359: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:31.370439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21347 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:31.591564: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:31.918217: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fde180095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:31.918257: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:31.924201: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:32.017876: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:32.021390: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:32.032504: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=30)]: Done 625 tasks      | elapsed: 136.5min\n",
      "2022-06-06 17:18:32.333208: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:32.574879: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:32.574953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20903 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:32.869271: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:33.257187: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f76280092a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:33.257242: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:33.263807: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:34.288113: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:34.809539: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:34.844083: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:35.022073: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:36.236328: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:36.313122: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:36.313188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20563 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:36.701468: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:37.085640: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f5a8c0092a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:37.085695: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:37.092275: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:37.925239: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:37.930394: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:39.326195: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:39.393248: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:39.393360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20659 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:39.616840: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:39.933871: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f46f8009a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:39.933922: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:39.941170: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:39.983415: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:40.125078: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:40.131748: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:40.348445: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:18:40.489382: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:41.868460: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:18:41.868528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19820 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:18:42.140024: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:18:42.530952: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:42.533119: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fda480082f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:18:42.533174: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:18:42.540873: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:18:42.943954: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:43.148590: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:43.361003: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:45.637504: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:18:46.294546: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:46.489622: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:48.029703: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:49.040927: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:18:52.222510: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:55.380545: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:18:58.071143: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:24:13.962928: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:14.663418: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:15.494136: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:15.494189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20218 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:15.794503: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:16.161367: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2b08018d50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:16.161424: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:16.168172: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:16.197559: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:16.197641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20138 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:16.491477: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:16.823628: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f13d8009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:16.823689: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:16.831557: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:19.193479: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:24:19.918713: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:24:22.489075: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:24:23.221888: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:24:31.268307: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:24:31.946536: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:24:40.220166: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:41.788271: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:41.788336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19628 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:42.118208: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:42.474593: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9fbc0093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:42.474679: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:42.483810: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:45.607358: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:24:48.802059: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:24:50.244786: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:51.714937: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:51.715015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21648 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:52.026636: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:52.155408: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:52.386565: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f02bc0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:52.386612: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:52.394535: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:53.540635: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:53.540723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22711 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:53.718432: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:53.828388: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:54.210626: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f67400095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:54.210661: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:54.216800: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:55.100452: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:55.100506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23107 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:55.103759: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:24:55.373479: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:55.436126: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:55.731158: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6d380096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:55.731206: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:55.738868: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:56.193393: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:24:56.716553: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:56.787768: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:56.787853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24611 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:56.891602: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:24:57.061124: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:57.402235: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9d90009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:57.402274: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:57.408384: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:57.597365: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:57.870206: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:24:58.076968: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:58.077037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24392 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:58.362214: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:58.420610: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:24:58.511337: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:58.716306: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f0ae40093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:58.716348: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:58.722105: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:58.938767: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:58.938830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23665 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:24:59.239253: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:24:59.249505: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:24:59.598446: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fcec0009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:24:59.598487: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:24:59.604212: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:24:59.628634: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:24:59.916057: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:24:59.916158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23701 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:25:00.007800: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:25:00.143259: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:00.189770: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:25:00.510691: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe1d4009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:25:00.510736: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:25:00.517316: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:25:00.665997: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:25:00.666055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22922 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:25:00.926715: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:25:00.941391: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:25:01.271294: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:01.323017: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa488009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:25:01.323072: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:25:01.329657: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:25:01.487165: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:25:01.487251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22067 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:25:01.560442: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:01.768522: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:25:02.124618: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc45c009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:25:02.124682: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:25:02.131019: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:25:02.440999: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:25:02.441055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22340 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:25:02.490431: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:02.663131: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:25:02.981048: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9978009610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:25:02.981106: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:25:02.987057: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:25:03.125265: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:03.160124: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:25:03.371871: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:04.249718: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:04.668151: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:04.672251: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:25:04.672325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20847 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:25:04.908933: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:25:05.069219: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:05.235418: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9ff8018a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:25:05.235483: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:25:05.244038: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:25:05.523510: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:25:05.654185: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:05.736717: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:06.000810: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:06.653194: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:07.133135: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:25:07.133211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19838 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:25:07.434879: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:25:07.589238: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:07.810615: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f041c009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:25:07.810668: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:25:07.818390: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:25:07.892163: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:08.384836: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:08.388384: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:09.366418: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:09.824751: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:11.047208: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:25:11.821622: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:12.002569: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:13.539167: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:14.333586: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:25:14.551889: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:15.515795: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:16.383617: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:17.208297: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:18.180847: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:20.428115: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:25:23.076514: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:30:35.640614: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:30:37.239343: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:30:37.239478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19652 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:30:37.527086: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:30:37.863154: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f89b80093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:30:37.863201: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:30:37.868056: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:30:40.936624: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "[Parallel(n_jobs=30)]: Done 662 tasks      | elapsed: 148.7min\n",
      "2022-06-06 17:30:44.278347: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:30:48.950302: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:30:50.514531: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:30:50.514609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20589 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:30:50.786051: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:30:51.128391: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f27e00095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:30:51.128419: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:30:51.133600: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:30:52.751975: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:30:53.406254: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:30:54.100525: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:30:54.898903: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:30:54.898999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20591 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:30:55.141289: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:30:55.492538: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f52440096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:30:55.492593: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:30:55.498268: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:30:57.156885: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:30:57.279562: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:30:58.532502: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:30:58.736338: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:30:58.736469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19658 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:30:59.034361: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:30:59.389124: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f000c009140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:30:59.389198: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:30:59.396786: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:31:01.863081: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:31:02.508452: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:31:05.803433: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:31:06.074768: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:31:10.595305: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:31:14.605794: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 17:31:44.028012: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 17:31:45.625896: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 17:31:45.625989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19660 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 17:31:46.002253: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 17:31:46.374278: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe2d8009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 17:31:46.374316: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 17:31:46.379506: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 17:31:49.248874: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 17:31:52.356208: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 17:32:00.805913: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 701 tasks      | elapsed: 156.0min\n",
      "[Parallel(n_jobs=30)]: Done 740 tasks      | elapsed: 162.6min\n",
      "[Parallel(n_jobs=30)]: Done 781 tasks      | elapsed: 174.6min\n",
      "[Parallel(n_jobs=30)]: Done 822 tasks      | elapsed: 181.7min\n",
      "2022-06-06 18:03:43.041250: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:03:43.047124: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:03:44.566429: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:03:44.566492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22020 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:03:44.588242: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:03:44.588336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22018 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:03:44.825282: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:03:44.886953: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:03:45.061683: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:03:45.153829: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2e9c009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:03:45.153876: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:03:45.161782: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:03:45.221223: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd4040095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:03:45.221273: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:03:45.228767: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:03:46.451403: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:03:46.451472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21089 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:03:46.701561: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:03:47.036001: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff568018c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:03:47.036060: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:03:47.042127: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:03:47.963771: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:03:48.021552: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:03:48.044320: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:03:49.428084: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:03:49.428150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20661 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:03:49.685302: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:03:49.889221: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:03:50.012871: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ef8c0008000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:03:50.012915: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:03:50.020608: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:03:51.015586: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:03:51.045079: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:03:52.805555: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:03:52.864587: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:03:52.933387: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:03:54.434849: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:03:54.434924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19642 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:03:54.697746: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:03:55.011459: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f02a00093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:03:55.011508: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:03:55.018178: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:03:56.042900: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:03:57.976997: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:03:59.259551: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:03:59.265955: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:03:59.586983: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:04:01.143791: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:04:01.143851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19634 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:04:01.162650: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:04:01.398050: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:04:01.417759: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:04:01.762101: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff2740098e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:04:01.762184: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:04:01.774558: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:04:04.552790: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:04:04.861866: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:04:08.201638: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:04:10.121227: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:04:17.127170: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:09:32.465746: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:09:34.009208: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:09:34.009305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19632 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:09:34.317326: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:09:34.661205: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa0640092a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:09:34.661259: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:09:34.668134: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:09:37.812184: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:09:41.105302: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:09:49.857574: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:09:58.697836: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:00.283037: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:00.283114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19628 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:00.581266: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:00.944626: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe1640095b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:00.944687: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:00.951531: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:03.923977: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:05.709208: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:07.157543: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:07.198527: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:07.198597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19994 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:07.493289: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:07.814587: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa704008000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:07.814641: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:07.821513: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:10.634122: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:10.801883: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:11.705075: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:12.212155: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:12.212212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23043 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:12.448008: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:12.630989: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:12.752895: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f1754008000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:12.752937: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:12.758652: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:13.067990: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:13.068053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22746 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:13.366185: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:13.434694: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:13.732331: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f8d54009140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:13.732372: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:13.738603: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:13.855898: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:13.993305: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:13.993361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23204 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:14.256253: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:14.631042: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3be8009100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:14.631081: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:14.635919: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:14.636746: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:14.868278: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:15.174461: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:15.174533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22778 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:15.377243: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:15.386557: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:15.744181: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc020008000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:15.744224: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:15.750368: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:16.118311: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:16.118367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23091 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:16.288707: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:16.363079: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:16.688051: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7dd0008000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:16.688080: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:16.693160: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:16.755105: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:17.138930: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:17.595795: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:17.896192: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:18.137455: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:18.137531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23415 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:18.370571: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:18.396658: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:18.743599: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd4d4009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:18.743656: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:18.750264: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:19.012335: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:19.012423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23850 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:19.068898: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:19.071812: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:19.300162: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:19.463205: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:19.653257: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f46380095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:19.653293: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:19.659961: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:19.971912: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:19.992137: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:20.437366: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:20.539080: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:20.539145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22582 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:20.769346: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:21.046658: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:21.118351: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f5acc0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:21.118389: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:21.124095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:21.240868: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:21.404261: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:21.404337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22089 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:21.566374: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:21.675548: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:22.060904: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f4b98009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:22.060962: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:22.068614: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:22.115908: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:22.409297: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:22.526196: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:22.575597: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:22.575689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21807 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:22.824928: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:23.158623: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff1ac009540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:23.158670: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:23.164498: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:23.614657: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:23.614717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21211 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:23.881360: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:24.137088: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:24.196715: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fde44009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:24.196760: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:24.202765: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:24.756870: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:25.068266: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:25.414470: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:10:25.731931: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:25.884733: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:26.168925: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:26.964124: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:10:26.964207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19706 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:10:27.237840: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:10:27.322943: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:10:27.345937: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:27.433451: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:27.557676: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f87c8009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:10:27.557721: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:10:27.563956: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:10:28.379559: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:28.411921: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:29.591600: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:29.907241: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:30.703550: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:30.800223: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "[Parallel(n_jobs=30)]: Done 865 tasks      | elapsed: 188.5min\n",
      "2022-06-06 18:10:31.163441: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:33.614535: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:34.117216: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:10:34.625195: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:36.378699: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:37.320963: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:38.414686: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:39.561844: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:10:43.051453: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:15:54.399339: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:15:54.536642: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:15:56.006839: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:15:56.006934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20162 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:15:56.086716: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:15:56.086829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20156 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:15:56.312957: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:15:56.318447: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:15:56.663414: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd19c0098e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:15:56.663484: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:15:56.670751: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:15:56.675728: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3f900093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:15:56.675808: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:15:56.685923: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:15:59.729619: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:15:59.745352: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:16:03.020396: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:16:03.097206: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:16:06.157089: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:16:07.662397: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:16:07.662495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19650 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:16:07.961777: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:16:08.297351: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f00cc0095e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:16:08.297402: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:16:08.303314: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:16:11.335716: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:16:11.621379: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:16:11.696275: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:16:14.532120: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:16:15.047595: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:16:16.638639: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:16:16.638696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19652 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:16:16.897594: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:16:17.244552: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fdd0c009100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:16:17.244594: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:16:17.249681: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:16:20.382005: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:16:23.404198: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:16:23.708550: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:16:32.523735: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:16:37.098227: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:16:38.665917: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:16:38.665980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20589 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:16:38.977551: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:16:39.342479: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa8fc009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:16:39.342533: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:16:39.349280: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:16:39.843207: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:16:41.388655: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:16:41.388707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19820 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:16:41.637053: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:16:41.978404: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9a2c009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:16:41.978448: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:16:41.985332: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:16:42.433575: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:16:45.011361: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:16:45.665949: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:16:46.498779: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:16:48.118461: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:16:48.118537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19658 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:16:48.292732: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:16:48.415692: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:16:48.795432: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f81100096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:16:48.795472: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:16:48.800669: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:16:51.905403: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:16:54.444220: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:16:55.194115: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:16:56.951234: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:17:02.859066: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:17:03.530030: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:17:04.416367: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:17:04.416496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19658 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:17:04.771733: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:17:05.152958: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fcb0c01a1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:17:05.153016: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:17:05.159268: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:17:08.234745: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:17:11.459051: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:17:19.982500: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:22:36.216053: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:22:37.776983: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:22:37.777083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19660 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:22:38.114354: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:22:38.471935: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3628009430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:22:38.471998: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:22:38.483457: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:22:41.480563: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:22:44.823087: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:22:53.503222: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 908 tasks      | elapsed: 201.3min\n",
      "[Parallel(n_jobs=30)]: Done 953 tasks      | elapsed: 207.9min\n",
      "[Parallel(n_jobs=30)]: Done 998 tasks      | elapsed: 220.5min\n",
      "2022-06-06 18:49:02.176227: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:49:03.662802: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:49:03.662863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20583 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:49:03.922612: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:49:04.299918: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f80980098e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:49:04.299966: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:49:04.306759: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:49:07.020411: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:49:07.174179: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:49:08.525940: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:49:08.526006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19648 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:49:08.772825: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:49:09.090996: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fdd200095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:49:09.091053: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:49:09.097173: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:49:10.292607: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:49:12.073196: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "[Parallel(n_jobs=30)]: Done 1045 tasks      | elapsed: 227.2min\n",
      "2022-06-06 18:49:15.253400: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:49:18.623847: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:49:21.446121: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:49:23.024729: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:49:23.024813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19632 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:49:23.415294: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:49:23.638430: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:49:23.789126: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f91bc009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:49:23.789210: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:49:23.797514: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:49:26.922515: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:49:30.285435: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:49:39.163683: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:54:53.093140: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:54:54.689381: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:54:54.689452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19628 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:54:54.961316: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:54:55.320232: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9f8c0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:54:55.320279: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:54:55.326565: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:54:58.457793: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:01.775093: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:10.506744: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:24.342320: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:25.853442: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:25.853510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21496 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:26.125392: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:26.444941: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2bf80095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:26.444982: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:26.449570: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:26.548221: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:27.894781: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:28.029614: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:28.029723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21675 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:28.342560: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:28.788844: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa63c0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:28.788891: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:28.794475: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:29.223907: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:29.320488: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:29.320544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21834 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:29.597822: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:29.944551: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fbe200095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:29.944595: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:29.949317: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:30.106343: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:31.514151: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:31.514211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21726 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:31.567137: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:31.785476: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:32.067605: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:32.116448: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:32.119412: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f37d80096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:32.119447: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:32.125512: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:32.700056: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:33.589625: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:33.589679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21716 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:33.861141: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:34.196421: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f4d8801a040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:34.196461: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:34.202365: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:34.308060: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:34.538265: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:35.002312: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:35.580031: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:35.747487: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:35.747542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22653 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:36.009823: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:36.066023: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:36.350158: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff660009100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:36.350206: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:36.355908: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:36.979852: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:37.464617: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:37.464694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22779 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:37.694080: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:37.746532: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:37.806377: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:38.091877: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f78d0009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:38.091946: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:38.099994: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:38.776398: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:39.147015: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:39.147094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22456 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:39.196626: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:39.394740: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:39.577315: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:39.727146: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f030c0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:39.727204: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:39.734454: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:39.849568: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:40.193048: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:40.193146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21956 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:40.441367: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:40.557053: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:40.811523: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f2e740095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:40.811596: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:40.822517: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:40.925287: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:41.599228: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:42.037766: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:42.037833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21581 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:42.120586: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:42.161394: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:42.265532: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:42.564568: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:42.597875: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f73540095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:42.597915: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:42.607614: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:43.133522: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:43.133613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21039 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:43.262228: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:43.430665: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:43.764727: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7f9c009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:43.764789: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:43.771425: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:43.771502: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:43.994542: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:44.563837: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 18:55:45.582864: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:45.794117: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:45.878002: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:46.156064: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 18:55:46.156190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19742 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 18:55:46.487043: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 18:55:46.812916: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:46.838937: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6ee8009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 18:55:46.839007: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 18:55:46.848624: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 18:55:47.068625: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:48.246010: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:48.974000: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:50.110750: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 18:55:50.257171: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:50.946749: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:53.009360: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:53.523027: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 18:55:54.855373: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:56.123950: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:57.909139: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:55:59.241004: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 18:56:02.472538: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:01:26.709365: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:01:28.275827: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:01:28.275900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19638 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:01:28.545030: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:01:28.886740: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff5340092a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:01:28.886788: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:01:28.891548: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:01:31.993973: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:01:35.066352: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:01:35.200738: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:01:36.565607: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:01:36.565663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19998 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:01:36.853894: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:01:37.186853: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb164009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:01:37.186907: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:01:37.193874: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:01:40.229211: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:01:40.735913: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:01:42.296629: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:01:42.296694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19642 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:01:42.634686: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:01:43.026329: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe270009870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:01:43.026370: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:01:43.032812: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:01:43.457239: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:01:43.714971: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:01:47.080597: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:01:50.333731: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:01:51.579805: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:01:55.367164: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[Parallel(n_jobs=30)]: Done 1092 tasks      | elapsed: 239.9min\n",
      "2022-06-06 19:01:56.808301: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:01:56.808360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22515 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:01:57.059521: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:01:57.420102: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fac34009840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:01:57.420146: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:01:57.426103: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:01:57.440893: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:01:58.254959: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:01:58.279711: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:01:58.879252: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:01:58.879315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22384 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:01:59.138824: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:01:59.495456: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6ac00082f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:01:59.495507: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:01:59.502797: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:01:59.596488: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:01:59.733188: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:01:59.733245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21983 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:01:59.987964: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:02:00.277054: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:00.329344: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f249c018d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:02:00.329399: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:02:00.335223: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:02:00.689107: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:02:01.109741: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:02:01.109821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21731 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:02:01.371870: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:02:01.723724: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f64480096e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:02:01.723779: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:02:01.730969: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:02:02.193762: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:02:02.193832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21149 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:02:02.392636: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:02.449736: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:02:02.779744: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb938009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:02:02.779781: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:02:02.785473: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:02:03.289667: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:03.386555: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:04.546594: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:02:04.664387: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:05.509040: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:05.837231: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:06.157937: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:02:06.158024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20589 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:02:06.497279: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:02:06.518883: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:06.745169: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:02:06.845471: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f5490009470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:02:06.845524: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:02:06.852124: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:02:07.983236: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:08.362244: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:02:08.362349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19908 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:02:08.657485: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:02:09.026819: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f69b40096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:02:09.026880: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:02:09.033750: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:02:09.189388: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:09.937525: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:12.156714: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:02:12.177993: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:13.276827: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:14.378832: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:02:15.334087: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:02:15.446802: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:16.760789: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:02:17.969305: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:02:21.767776: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:02:22.339211: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:02:23.903192: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:02:23.903268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19656 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:02:23.984644: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:02:24.217179: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:02:24.568813: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f081c009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:02:24.568861: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:02:24.575626: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:02:27.651673: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:02:30.903553: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:02:39.564487: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:07:37.073947: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:07:37.075990: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:07:38.668158: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:07:38.668222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20176 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:07:38.699418: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:07:38.699484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20168 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:07:38.916961: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:07:39.045344: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:07:39.256268: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fdc200095b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:07:39.256322: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:07:39.261193: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:07:39.413030: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f96100096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:07:39.413084: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:07:39.419201: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:07:42.342889: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:07:42.457199: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:07:45.689162: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:07:45.800121: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:07:54.473117: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:07:54.568006: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:07:56.381684: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:07:57.934789: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:07:57.934854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19660 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:07:58.161512: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:07:58.496736: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f01b4009820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:07:58.496775: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:07:58.502030: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:08:01.609920: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:08:04.988153: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:08:13.779446: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 1141 tasks      | elapsed: 252.4min\n",
      "[Parallel(n_jobs=30)]: Done 1190 tasks      | elapsed: 259.6min\n",
      "[Parallel(n_jobs=30)]: Done 1241 tasks      | elapsed: 272.3min\n",
      "2022-06-06 19:34:24.838232: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:34:26.348520: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:34:26.348591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19650 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:34:26.645820: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:34:27.009412: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f82b40095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:34:27.009453: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:34:27.014723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:34:29.975661: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:34:33.142293: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:34:38.477627: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:34:40.026217: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:34:40.026408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19640 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:34:40.360158: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:34:40.707838: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fbc54009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:34:40.707875: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:34:40.712760: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:34:41.517478: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:34:43.770363: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:34:47.128643: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:34:55.915730: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:40:11.433364: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:40:12.993398: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:40:12.993499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19642 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:40:13.312744: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:40:13.661577: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f659c009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:40:13.661627: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:40:13.666697: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:40:16.706546: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:40:20.040866: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:40:28.812400: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:40:40.029086: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:40:41.518145: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:40:41.518210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20571 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:40:41.829759: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:40:42.172174: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f34580093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:40:42.172216: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:40:42.178428: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:40:42.950803: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:40:44.462399: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:40:44.462455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20689 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:40:44.723385: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:40:45.060383: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f62ac0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:40:45.060462: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:40:45.067248: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:40:45.118791: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:40:46.754271: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:40:47.994948: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:40:48.213430: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:40:48.213512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19642 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:40:48.232437: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:40:48.529690: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:40:48.875846: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f11a80093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:40:48.875923: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:40:48.882848: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:40:51.080664: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:40:51.807648: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:40:53.957759: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:40:54.833691: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:40:55.447056: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:40:55.447132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21506 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:40:55.710703: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:40:56.051380: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7faa200096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:40:56.051442: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:40:56.057677: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:40:56.293596: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:40:57.741050: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:40:58.345332: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:40:58.894917: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:40:58.985036: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:40:59.287518: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:40:59.287623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21169 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:40:59.570358: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:40:59.910307: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f197c0092b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:40:59.910348: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:40:59.914855: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:40:59.914922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21079 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:40:59.917101: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:41:00.163573: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:41:00.502225: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff06c0093a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:41:00.502297: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:41:00.514707: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:41:02.007917: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:41:02.261817: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:41:02.859875: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:41:02.892531: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:41:03.499901: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:41:03.832427: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:41:03.832527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19636 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:41:04.169453: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:41:04.496555: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f93dc009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:41:04.496615: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:41:04.502458: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:41:06.184609: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:41:06.858176: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:41:07.750454: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:41:10.923310: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:41:11.137341: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:41:15.114361: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:41:15.761529: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:41:19.995676: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 1292 tasks      | elapsed: 284.5min\n",
      "2022-06-06 19:46:44.322781: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:46:45.864710: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:46:45.864771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19634 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:46:46.129112: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:46:46.469218: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fdef4009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:46:46.469256: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:46:46.474990: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:46:49.580134: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:46:52.038238: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:46:52.832709: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:46:53.557208: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:46:53.557261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19634 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:46:53.824705: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:46:54.205669: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fbac80095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:46:54.205727: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:46:54.212411: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:46:57.252860: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:00.570984: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:01.542704: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:06.467635: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:08.069804: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:08.069861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19636 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:08.389964: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:08.760595: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6d00009100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:08.760660: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:08.768557: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:09.012409: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:11.592514: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:12.457591: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:13.546056: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:13.827153: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:13.827208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22166 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:14.040985: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:14.358405: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f265c009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:14.358437: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:14.364730: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:14.514756: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:14.924897: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:14.924960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22783 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:15.179789: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:15.220677: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:15.548560: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc5500093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:15.548607: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:15.553639: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:16.640619: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:16.640681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22809 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:16.822687: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:16.872033: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:17.143288: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:17.207166: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9ea4018ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:17.207210: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:17.213138: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:17.673910: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:18.295803: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:18.295901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22420 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:18.402587: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:18.545743: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:18.892241: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f61600095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:18.892301: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:18.899186: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:19.122781: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:19.169783: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:19.169872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22040 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:19.435044: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:19.795350: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f43bc009550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:19.795419: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:19.805222: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:20.109695: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:20.121931: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:20.573289: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:20.573353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22000 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:20.857244: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:21.174230: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f62c4007f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:21.174302: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:21.182115: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:21.378745: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:21.654224: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:21.752110: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:22.264786: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:22.659426: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:23.160614: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:23.160685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21664 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:23.231139: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:23.453888: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:23.809586: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9e20009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:23.809645: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:23.816165: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:23.848794: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:24.191282: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:24.884612: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:25.446188: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:25.446268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20789 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:25.730128: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:25.823007: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:26.062396: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fed0c009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:26.062470: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:26.070451: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:26.485050: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:26.849707: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:27.442619: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:28.099276: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:28.099339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19806 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:28.441872: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:28.467897: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:28.776113: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3700009440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:28.776167: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:28.783667: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:29.141717: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:29.854885: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:30.187963: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:31.882952: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:31.960810: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:32.452962: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:33.660195: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:34.560485: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:35.209713: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:36.114132: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:38.049951: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:47:38.829229: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:39.617818: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:47:39.617921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19646 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:47:39.884959: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:47:40.244455: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f24d4009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:47:40.244514: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:47:40.250793: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:47:41.143487: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:43.353418: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:47:43.990862: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:47:46.742575: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:47:55.534823: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:52:56.268297: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:52:57.818160: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:52:57.818238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19648 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:52:58.109255: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:52:58.441337: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fdad40096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:52:58.441381: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:52:58.449458: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:53:01.499078: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:53:04.842202: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:53:13.624856: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:53:22.547956: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:53:24.175212: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:53:24.175283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19650 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:53:24.562009: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:53:24.956794: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f4ca4009a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:53:24.956835: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:53:24.963376: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:53:28.030683: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:53:31.358218: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:53:38.201993: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:53:38.968471: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:53:39.719546: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:53:39.739005: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:53:39.739081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21205 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:53:40.074390: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:53:40.450914: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:53:40.451039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21097 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:53:40.455948: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb6bc009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:53:40.456016: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:53:40.463652: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:53:40.688941: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:53:41.027809: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa3c40095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:53:41.027883: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:53:41.042791: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:53:42.007344: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:53:43.505411: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:53:43.522183: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:53:43.522250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19750 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:53:43.760939: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:53:44.084145: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fbe54009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:53:44.084208: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:53:44.090059: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:53:44.091137: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:53:46.797101: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:53:47.228603: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:53:47.398815: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:53:50.618587: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:53:55.685197: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:53:56.251914: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:53:59.279829: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 1345 tasks      | elapsed: 292.2min\n",
      "2022-06-06 19:59:21.802966: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:59:23.380014: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:59:23.380106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19658 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:59:23.793267: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:59:24.145649: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f0a300095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:59:24.145699: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:59:24.152161: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:59:27.252054: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:59:30.567486: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:59:34.208548: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:59:35.751547: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 19:59:35.751606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19660 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 19:59:36.023478: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 19:59:36.371779: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f9538009100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 19:59:36.371839: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 19:59:36.377404: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 19:59:39.190460: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 19:59:39.455262: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 19:59:42.773983: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 19:59:51.576695: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 1398 tasks      | elapsed: 304.9min\n",
      "[Parallel(n_jobs=30)]: Done 1453 tasks      | elapsed: 317.6min\n",
      "2022-06-06 20:19:55.944457: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:19:57.424844: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:19:57.424928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19642 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:19:57.698101: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:19:58.021081: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f72c4009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:19:58.021138: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:19:58.027198: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:20:01.076160: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:20:04.397109: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:20:13.224572: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:25:56.419442: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:25:57.253811: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:25:57.881804: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:25:57.881873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20464 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:25:58.143374: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:25:58.481899: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f6034009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:25:58.481977: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:25:58.488887: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:25:58.755328: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:25:58.755399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21075 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:25:59.026025: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:25:59.351780: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc91c0093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:25:59.351835: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:25:59.357851: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:26:01.321678: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:26:01.519972: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:26:02.188682: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:26:02.490242: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:26:03.010673: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:26:03.010730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20276 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:26:03.216772: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:26:03.547944: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe8a00092b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:26:03.548007: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:26:03.554105: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:26:04.027045: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:26:04.027122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19992 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:26:04.308340: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:26:04.486031: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:26:04.667164: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f820c0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:26:04.667211: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:26:04.673833: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:26:05.273269: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:26:06.525376: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:26:07.601995: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:26:09.631765: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:26:10.708706: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:26:12.042641: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:26:12.664386: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:26:13.444657: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:26:13.548671: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:26:13.548759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20573 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:26:13.898050: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:26:14.220202: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7fa80095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:26:14.220266: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:26:14.227379: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:26:16.902843: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:26:17.148665: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:26:17.835196: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:26:18.432643: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:26:18.432737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19636 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:26:18.742301: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:26:19.041187: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:26:19.087164: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe2500095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:26:19.087238: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:26:19.094647: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:26:20.415411: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:26:22.184787: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:26:25.616382: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:26:29.464597: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:26:34.518983: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:31:52.337103: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:31:53.865164: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:31:53.865256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19632 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:31:54.221520: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:31:54.607057: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb84c009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:31:54.607094: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:31:54.612934: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:31:57.630842: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:31:59.638993: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:00.873481: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:01.206030: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:01.206107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19634 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:01.780072: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:02.169015: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f63280096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:02.169083: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:02.175780: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:05.098777: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:06.163526: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:07.721671: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:07.721735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19636 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:07.973479: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:08.344358: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc8a0019d00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:08.344401: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:08.350631: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:08.373080: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:09.495461: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:11.354566: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:14.675613: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[Parallel(n_jobs=30)]: Done 1508 tasks      | elapsed: 330.2min\n",
      "2022-06-06 20:32:17.130938: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:21.797882: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:23.313348: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:23.313403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19636 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:23.327269: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:23.602228: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:23.956615: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fc34c009780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:23.956658: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:23.962028: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:26.927766: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:27.615499: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:29.093143: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:29.093287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20002 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:29.385755: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:29.729636: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb77c0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:29.729686: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:29.736053: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:29.977012: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:32.414558: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:32.449914: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:33.183749: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:33.812892: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:33.812952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23121 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:34.011342: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:34.185018: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:34.336209: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f3b800095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:34.336253: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:34.341417: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:34.544665: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:34.544746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23455 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:34.780160: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:35.098123: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f44a4009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:35.098174: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:35.105651: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:35.233136: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:35.432236: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:35.502651: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:35.502784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 23976 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:35.796314: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:36.123094: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fcdb0009790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:36.123161: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:36.130358: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:36.436657: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:36.848916: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:36.848979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22686 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:36.988846: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:37.116160: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:37.349091: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:37.466012: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f03dc0095f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:37.466052: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:37.471971: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:37.513349: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:37.766707: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:37.766771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21815 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:37.811279: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:38.047128: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:38.405164: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb4b8018a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:38.405202: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:38.411523: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:38.663504: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:38.958912: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:38.958984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21839 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:38.980817: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:39.211547: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:39.538003: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f8d64009600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:39.538037: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:39.543738: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:39.995797: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:40.154634: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:40.154696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21173 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:40.403526: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:40.424651: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:40.765752: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fe65c0093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:40.765804: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:40.770720: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:40.855459: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:41.382273: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:42.035285: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:42.099429: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:42.493094: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:43.183708: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:43.581376: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:43.581429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19698 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:43.680586: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:43.829674: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:43.845590: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:44.210203: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fd4f0009550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:44.210254: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:44.216503: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:44.697666: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:45.812556: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:47.206038: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:47.353687: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:32:48.723713: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:49.667554: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:50.629790: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:32:50.853391: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:52.396865: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:53.353754: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:54.089500: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:32:54.432668: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:55.616244: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:32:55.616326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19646 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:32:55.803239: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:55.900667: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:32:56.259743: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f0ff8009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:32:56.259800: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:32:56.265472: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:32:59.298258: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:32:59.335699: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:33:02.741975: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:33:11.746227: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:38:11.339062: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:38:12.895354: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:38:12.895431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19648 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:38:13.229692: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:38:13.617857: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f1398009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:38:13.617904: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:38:13.623897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:38:16.750219: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:38:20.101564: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:38:28.889437: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:38:38.885055: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:38:40.510595: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:38:40.510670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19648 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:38:40.797744: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:38:41.176044: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb488009110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:38:41.176112: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:38:41.182250: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:38:44.283871: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:38:47.599204: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:38:51.868598: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:38:53.292756: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:38:53.292816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20585 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:38:53.542380: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:38:53.875058: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fcfb8018a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:38:53.875103: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:38:53.881980: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:38:54.986603: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:38:55.806408: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:38:56.436550: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:38:56.436611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20653 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:38:56.746442: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:38:56.838426: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:38:57.097132: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f8f28019f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:38:57.097187: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:38:57.103152: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:38:58.327093: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:38:59.879596: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:38:59.879671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20076 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:39:00.080737: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:39:00.158626: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:39:00.209726: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:39:00.551641: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fb4f8009910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:39:00.551693: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:39:00.557485: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:39:03.352184: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:39:03.570347: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:39:03.852927: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:39:05.470408: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:39:05.470499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19656 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:39:05.762397: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:39:06.101552: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f8f6c0096d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:39:06.101644: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:39:06.115300: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:39:06.918466: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:39:08.778907: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:39:09.288213: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:39:12.100331: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:39:12.546560: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:39:15.594593: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:39:21.199113: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:44:36.811243: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:44:38.453366: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:44:38.453476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19656 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:44:38.792072: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:44:39.145373: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f14c4009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:44:39.145452: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:44:39.155137: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:44:42.210890: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:44:45.517536: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:44:54.327602: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 1565 tasks      | elapsed: 343.2min\n",
      "2022-06-06 20:45:22.232903: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:45:23.827579: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:45:23.827634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19658 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:45:24.085739: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:45:24.456035: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fa1700093b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:45:24.456084: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:45:24.460891: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:45:27.611756: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:45:30.903757: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:45:39.501036: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-06-06 20:51:12.824468: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:51:14.334365: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-06 20:51:14.334463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19660 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2022-06-06 20:51:14.605992: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1658] (One-time warning): Not using XLA:CPU for cluster.\n",
      "\n",
      "If you want XLA:CPU, do one of the following:\n",
      "\n",
      " - set the TF_XLA_FLAGS to include \"--tf_xla_cpu_global_jit\", or\n",
      " - set cpu_global_jit to true on this session's OptimizerOptions, or\n",
      " - use experimental_jit_scope, or\n",
      " - use tf.function(jit_compile=True).\n",
      "\n",
      "To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a\n",
      "proper command-line flag, not via TF_XLA_FLAGS).\n",
      "2022-06-06 20:51:14.947078: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f7a64009530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-06 20:51:14.947128: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2022-06-06 20:51:14.952645: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-06 20:51:18.002611: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-06-06 20:51:21.336648: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-06 20:51:30.072682: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "[Parallel(n_jobs=30)]: Done 1622 tasks      | elapsed: 355.8min\n"
     ]
    }
   ],
   "source": [
    "parallel = Parallel(n_jobs=config['computation']['n_jobs'], verbose=10, backend='loky') #loky\n",
    "\n",
    "weights_list = parallel(delayed(train_nn)(X_data, y_data, index) for index, (X_data, y_data) in enumerate(zip(X_datasets_list, y_datasets_list)))\n",
    "                                  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778c426-b6e2-421c-aa06-17507dfa3a32",
   "metadata": {},
   "source": [
    "# Inspect Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0cb7e-bbd1-49c3-bfb6-1be71cfe41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = np.stack([np.array(x) for x in weights_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334925c3-84fb-4005-a6c5-da5915a5708a",
   "metadata": {},
   "source": [
    "# Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac9b49-0daa-4851-8f77-56354ff8d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda06af5-f972-484f-9c01-c87cc5030f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a6e52-e429-4a2d-b6e4-7f1d7c5c9655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myBA",
   "language": "python",
   "name": "myba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
