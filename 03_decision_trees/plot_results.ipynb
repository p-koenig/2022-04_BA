{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "national-channel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:49.373703Z",
     "iopub.status.busy": "2022-01-04T19:49:49.372917Z",
     "iopub.status.idle": "2022-01-04T19:49:55.078229Z",
     "shell.execute_reply": "2022-01-04T19:49:55.076715Z",
     "shell.execute_reply.started": "2022-01-04T19:49:49.373555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "import itertools\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d75633-dbcf-43e6-a72b-45a898caf649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc73da04-8710-49f8-91b9-92fa72840804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:55.080722Z",
     "iopub.status.busy": "2022-01-04T19:49:55.080409Z",
     "iopub.status.idle": "2022-01-04T19:49:56.096059Z",
     "shell.execute_reply": "2022-01-04T19:49:56.094878Z",
     "shell.execute_reply.started": "2022-01-04T19:49:55.080686Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_beta</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_fully_grown</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>function_family_basic_function_representation_length</th>\n",
       "      <th>function_family_function_representation_length</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_num_classes</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_decision_sparsity_train</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_objective</th>\n",
       "      <th>data_x_max</th>\n",
       "      <th>data_x_min</th>\n",
       "      <th>data_x_distrib</th>\n",
       "      <th>data_lambda_dataset_size</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_noise_injected_type</th>\n",
       "      <th>lambda_net_epochs_lambda</th>\n",
       "      <th>lambda_net_early_stopping_lambda</th>\n",
       "      <th>lambda_net_early_stopping_min_delta_lambda</th>\n",
       "      <th>lambda_net_batch_lambda</th>\n",
       "      <th>lambda_net_dropout_lambda</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>lambda_net_loss_lambda</th>\n",
       "      <th>lambda_net_number_of_lambda_weights</th>\n",
       "      <th>lambda_net_number_initializations_lambda</th>\n",
       "      <th>lambda_net_number_of_trained_lambda_nets</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_convolution_layers</th>\n",
       "      <th>i_net_lstm_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_optimizer</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_metrics</th>\n",
       "      <th>i_net_epochs</th>\n",
       "      <th>i_net_early_stopping</th>\n",
       "      <th>i_net_batch_size</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_test_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_normalize_lambda_nets</th>\n",
       "      <th>i_net_optimize_decision_function</th>\n",
       "      <th>i_net_function_value_loss</th>\n",
       "      <th>i_net_soft_labels</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_type</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_random_evaluation_dataset_size</th>\n",
       "      <th>evaluation_per_network_optimization_dataset_size</th>\n",
       "      <th>evaluation_sklearn_dt_benchmark</th>\n",
       "      <th>evaluation_sdt_benchmark</th>\n",
       "      <th>evaluation_different_eval_data</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_lambda_dataset_size</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_number_of_trained_lambda_nets</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_interpretation_dataset_size</th>\n",
       "      <th>computation_load_model</th>\n",
       "      <th>computation_n_jobs</th>\n",
       "      <th>computation_use_gpu</th>\n",
       "      <th>computation_gpu_numbers</th>\n",
       "      <th>computation_RANDOM_SEED</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy_data_random</th>\n",
       "      <th>train_dt_scores_soft_binary_crossentropy_data_random_median</th>\n",
       "      <th>train_dt_scores_binary_crossentropy</th>\n",
       "      <th>train_dt_scores_binary_crossentropy_median</th>\n",
       "      <th>train_dt_scores_binary_crossentropy_data_random</th>\n",
       "      <th>train_dt_scores_binary_crossentropy_data_random_median</th>\n",
       "      <th>train_dt_scores_accuracy</th>\n",
       "      <th>train_dt_scores_accuracy_median</th>\n",
       "      <th>train_dt_scores_accuracy_data_random</th>\n",
       "      <th>train_dt_scores_accuracy_data_random_median</th>\n",
       "      <th>train_dt_scores_f1_score</th>\n",
       "      <th>train_dt_scores_f1_score_median</th>\n",
       "      <th>train_dt_scores_f1_score_data_random</th>\n",
       "      <th>train_dt_scores_f1_score_data_random_median</th>\n",
       "      <th>train_dt_scores_runtime</th>\n",
       "      <th>train_dt_scores_runtime_median</th>\n",
       "      <th>train_inet_scores_soft_binary_crossentropy</th>\n",
       "      <th>train_inet_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>train_inet_scores_binary_crossentropy</th>\n",
       "      <th>train_inet_scores_binary_crossentropy_median</th>\n",
       "      <th>train_inet_scores_accuracy</th>\n",
       "      <th>train_inet_scores_accuracy_median</th>\n",
       "      <th>train_inet_scores_f1_score</th>\n",
       "      <th>train_inet_scores_f1_score_median</th>\n",
       "      <th>train_inet_scores_runtime</th>\n",
       "      <th>train_inet_scores_runtime_median</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy_data_random</th>\n",
       "      <th>valid_dt_scores_soft_binary_crossentropy_data_random_median</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy_median</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy_data_random</th>\n",
       "      <th>valid_dt_scores_binary_crossentropy_data_random_median</th>\n",
       "      <th>valid_dt_scores_accuracy</th>\n",
       "      <th>valid_dt_scores_accuracy_median</th>\n",
       "      <th>valid_dt_scores_accuracy_data_random</th>\n",
       "      <th>valid_dt_scores_accuracy_data_random_median</th>\n",
       "      <th>valid_dt_scores_f1_score</th>\n",
       "      <th>valid_dt_scores_f1_score_median</th>\n",
       "      <th>valid_dt_scores_f1_score_data_random</th>\n",
       "      <th>valid_dt_scores_f1_score_data_random_median</th>\n",
       "      <th>valid_dt_scores_runtime</th>\n",
       "      <th>valid_dt_scores_runtime_median</th>\n",
       "      <th>valid_inet_scores_soft_binary_crossentropy</th>\n",
       "      <th>valid_inet_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_inet_scores_binary_crossentropy</th>\n",
       "      <th>valid_inet_scores_binary_crossentropy_median</th>\n",
       "      <th>valid_inet_scores_accuracy</th>\n",
       "      <th>valid_inet_scores_accuracy_median</th>\n",
       "      <th>valid_inet_scores_f1_score</th>\n",
       "      <th>valid_inet_scores_f1_score_median</th>\n",
       "      <th>valid_inet_scores_runtime</th>\n",
       "      <th>valid_inet_scores_runtime_median</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy_data_random</th>\n",
       "      <th>test_dt_scores_soft_binary_crossentropy_data_random_median</th>\n",
       "      <th>test_dt_scores_binary_crossentropy</th>\n",
       "      <th>test_dt_scores_binary_crossentropy_median</th>\n",
       "      <th>test_dt_scores_binary_crossentropy_data_random</th>\n",
       "      <th>test_dt_scores_binary_crossentropy_data_random_median</th>\n",
       "      <th>test_dt_scores_accuracy</th>\n",
       "      <th>test_dt_scores_accuracy_median</th>\n",
       "      <th>test_dt_scores_accuracy_data_random</th>\n",
       "      <th>test_dt_scores_accuracy_data_random_median</th>\n",
       "      <th>test_dt_scores_f1_score</th>\n",
       "      <th>test_dt_scores_f1_score_median</th>\n",
       "      <th>test_dt_scores_f1_score_data_random</th>\n",
       "      <th>test_dt_scores_f1_score_data_random_median</th>\n",
       "      <th>test_dt_scores_runtime</th>\n",
       "      <th>test_dt_scores_runtime_median</th>\n",
       "      <th>test_inet_scores_soft_binary_crossentropy</th>\n",
       "      <th>test_inet_scores_soft_binary_crossentropy_median</th>\n",
       "      <th>test_inet_scores_binary_crossentropy</th>\n",
       "      <th>test_inet_scores_binary_crossentropy_median</th>\n",
       "      <th>test_inet_scores_accuracy</th>\n",
       "      <th>test_inet_scores_accuracy_median</th>\n",
       "      <th>test_inet_scores_f1_score</th>\n",
       "      <th>test_inet_scores_f1_score_median</th>\n",
       "      <th>test_inet_scores_runtime</th>\n",
       "      <th>test_inet_scores_runtime_median</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_1000</th>\n",
       "      <th>dt_scores_accuracy_adult_1000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_1000</th>\n",
       "      <th>dt_scores_f1_score_adult_1000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_1000</th>\n",
       "      <th>dt_scores_runtime_adult_1000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_1000</th>\n",
       "      <th>inet_scores_accuracy_adult_1000</th>\n",
       "      <th>inet_scores_f1_score_adult_1000</th>\n",
       "      <th>inet_scores_runtime_adult_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_accuracy_titanic_1000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_f1_score_titanic_1000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_1000</th>\n",
       "      <th>dt_scores_runtime_titanic_1000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_1000</th>\n",
       "      <th>inet_scores_accuracy_titanic_1000</th>\n",
       "      <th>inet_scores_f1_score_titanic_1000</th>\n",
       "      <th>inet_scores_runtime_titanic_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_1000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_1000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_1000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_1000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_1000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_1000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_1000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_10000</th>\n",
       "      <th>dt_scores_accuracy_adult_10000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_10000</th>\n",
       "      <th>dt_scores_f1_score_adult_10000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_10000</th>\n",
       "      <th>dt_scores_runtime_adult_10000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_10000</th>\n",
       "      <th>inet_scores_accuracy_adult_10000</th>\n",
       "      <th>inet_scores_f1_score_adult_10000</th>\n",
       "      <th>inet_scores_runtime_adult_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_accuracy_titanic_10000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_f1_score_titanic_10000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_10000</th>\n",
       "      <th>dt_scores_runtime_titanic_10000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_10000</th>\n",
       "      <th>inet_scores_accuracy_titanic_10000</th>\n",
       "      <th>inet_scores_f1_score_titanic_10000</th>\n",
       "      <th>inet_scores_runtime_titanic_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_10000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_10000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_10000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_10000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_10000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_10000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_10000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_100000</th>\n",
       "      <th>dt_scores_accuracy_adult_100000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_100000</th>\n",
       "      <th>dt_scores_f1_score_adult_100000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_100000</th>\n",
       "      <th>dt_scores_runtime_adult_100000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_100000</th>\n",
       "      <th>inet_scores_accuracy_adult_100000</th>\n",
       "      <th>inet_scores_f1_score_adult_100000</th>\n",
       "      <th>inet_scores_runtime_adult_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_accuracy_titanic_100000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_f1_score_titanic_100000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_100000</th>\n",
       "      <th>dt_scores_runtime_titanic_100000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_100000</th>\n",
       "      <th>inet_scores_accuracy_titanic_100000</th>\n",
       "      <th>inet_scores_f1_score_titanic_100000</th>\n",
       "      <th>inet_scores_runtime_titanic_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_100000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_100000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_100000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_100000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_100000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_100000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_100000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_accuracy_adult_1000000</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_f1_score_adult_1000000</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_1000000</th>\n",
       "      <th>dt_scores_runtime_adult_1000000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_1000000</th>\n",
       "      <th>inet_scores_accuracy_adult_1000000</th>\n",
       "      <th>inet_scores_f1_score_adult_1000000</th>\n",
       "      <th>inet_scores_runtime_adult_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_accuracy_titanic_1000000</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_f1_score_titanic_1000000</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_1000000</th>\n",
       "      <th>dt_scores_runtime_titanic_1000000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>inet_scores_accuracy_titanic_1000000</th>\n",
       "      <th>inet_scores_f1_score_titanic_1000000</th>\n",
       "      <th>inet_scores_runtime_titanic_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_1000000</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_1000000</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_1000000</th>\n",
       "      <th>dt_scores_runtime_absenteeism_1000000</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_1000000</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_1000000</th>\n",
       "      <th>inet_scores_runtime_absenteeism_1000000</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_data_random_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_runtime_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>inet_scores_runtime_adult_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_data_random_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>inet_scores_runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_soft_binary_crossentropy_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_binary_crossentropy_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_accuracy_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_f1_score_data_random_absenteeism_TRAIN_DATA</th>\n",
       "      <th>dt_scores_runtime_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>inet_scores_runtime_absenteeism_TRAIN_DATA</th>\n",
       "      <th>z-score_train</th>\n",
       "      <th>z-score_valid</th>\n",
       "      <th>z-score_test</th>\n",
       "      <th>z-score_adult</th>\n",
       "      <th>z-score_titanic</th>\n",
       "      <th>z-score_absenteeism</th>\n",
       "      <th>dist_to_init_train</th>\n",
       "      <th>dist_to_init_valid</th>\n",
       "      <th>dist_to_init_test</th>\n",
       "      <th>dist_to_init_adult</th>\n",
       "      <th>dist_to_init_titanic</th>\n",
       "      <th>dist_to_init_absenteeism</th>\n",
       "      <th>avg_dist_to_train_train</th>\n",
       "      <th>avg_dist_to_train_valid</th>\n",
       "      <th>avg_dist_to_train_test</th>\n",
       "      <th>avg_dist_to_train_adult</th>\n",
       "      <th>avg_dist_to_train_titanic</th>\n",
       "      <th>avg_dist_to_train_absenteeism</th>\n",
       "      <th>min_dist_to_train_sample_train</th>\n",
       "      <th>min_dist_to_train_sample_valid</th>\n",
       "      <th>min_dist_to_train_samplee_test</th>\n",
       "      <th>min_dist_to_train_sample_adult</th>\n",
       "      <th>min_dist_to_train_sample_titanic</th>\n",
       "      <th>min_dist_to_train_sample_absenteeism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>22</td>\n",
       "      <td>134</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1409</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.519338</td>\n",
       "      <td>0.521510</td>\n",
       "      <td>0.513757</td>\n",
       "      <td>0.516722</td>\n",
       "      <td>0.415607</td>\n",
       "      <td>0.424973</td>\n",
       "      <td>0.395052</td>\n",
       "      <td>0.407307</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.823310</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>0.804334</td>\n",
       "      <td>0.803337</td>\n",
       "      <td>0.814989</td>\n",
       "      <td>0.819453</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.019261</td>\n",
       "      <td>0.616474</td>\n",
       "      <td>0.615307</td>\n",
       "      <td>0.569003</td>\n",
       "      <td>0.567067</td>\n",
       "      <td>0.703344</td>\n",
       "      <td>0.7024</td>\n",
       "      <td>0.664482</td>\n",
       "      <td>0.695851</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.515391</td>\n",
       "      <td>0.512567</td>\n",
       "      <td>0.510315</td>\n",
       "      <td>0.509515</td>\n",
       "      <td>0.408172</td>\n",
       "      <td>0.409819</td>\n",
       "      <td>0.389243</td>\n",
       "      <td>0.390487</td>\n",
       "      <td>0.818256</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>0.827992</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.814105</td>\n",
       "      <td>0.817887</td>\n",
       "      <td>0.824589</td>\n",
       "      <td>0.829977</td>\n",
       "      <td>0.021446</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.619904</td>\n",
       "      <td>0.617766</td>\n",
       "      <td>0.574994</td>\n",
       "      <td>0.573605</td>\n",
       "      <td>0.702496</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.673612</td>\n",
       "      <td>0.713898</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.523347</td>\n",
       "      <td>0.524943</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.513713</td>\n",
       "      <td>0.421876</td>\n",
       "      <td>0.427665</td>\n",
       "      <td>0.403941</td>\n",
       "      <td>0.401487</td>\n",
       "      <td>0.807616</td>\n",
       "      <td>0.8064</td>\n",
       "      <td>0.816672</td>\n",
       "      <td>0.8191</td>\n",
       "      <td>0.795271</td>\n",
       "      <td>0.812604</td>\n",
       "      <td>0.804644</td>\n",
       "      <td>0.816798</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>0.021086</td>\n",
       "      <td>0.626373</td>\n",
       "      <td>0.629011</td>\n",
       "      <td>0.583992</td>\n",
       "      <td>0.589657</td>\n",
       "      <td>0.694880</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.669380</td>\n",
       "      <td>0.723074</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.676063</td>\n",
       "      <td>0.333621</td>\n",
       "      <td>10.156703</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>0.592354</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.313421</td>\n",
       "      <td>0.996441</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.590181</td>\n",
       "      <td>0.591582</td>\n",
       "      <td>0.590905</td>\n",
       "      <td>0.434295</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.624773</td>\n",
       "      <td>0.435884</td>\n",
       "      <td>0.472553</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.824417</td>\n",
       "      <td>0.339943</td>\n",
       "      <td>1.386253</td>\n",
       "      <td>0.045065</td>\n",
       "      <td>0.429295</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.572218</td>\n",
       "      <td>0.991844</td>\n",
       "      <td>0.037807</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.556268</td>\n",
       "      <td>0.597483</td>\n",
       "      <td>0.363571</td>\n",
       "      <td>0.473530</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.805050</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.636781</td>\n",
       "      <td>0.440972</td>\n",
       "      <td>0.523597</td>\n",
       "      <td>0.179272</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.9243</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.698286</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.814428</td>\n",
       "      <td>0.341439</td>\n",
       "      <td>1.424239</td>\n",
       "      <td>0.049010</td>\n",
       "      <td>0.477353</td>\n",
       "      <td>0.98186</td>\n",
       "      <td>0.592726</td>\n",
       "      <td>0.990746</td>\n",
       "      <td>0.470477</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.563472</td>\n",
       "      <td>0.603581</td>\n",
       "      <td>0.380740</td>\n",
       "      <td>0.492859</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.76884</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.457628</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.621059</td>\n",
       "      <td>0.443937</td>\n",
       "      <td>0.444783</td>\n",
       "      <td>0.188583</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.92095</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.680516</td>\n",
       "      <td>0.491968</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.831044</td>\n",
       "      <td>0.341260</td>\n",
       "      <td>1.510337</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.419929</td>\n",
       "      <td>0.981878</td>\n",
       "      <td>0.568623</td>\n",
       "      <td>0.990764</td>\n",
       "      <td>6.078736</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.563204</td>\n",
       "      <td>0.604133</td>\n",
       "      <td>0.379736</td>\n",
       "      <td>0.494743</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.766630</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.785663</td>\n",
       "      <td>6.039980</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.631333</td>\n",
       "      <td>0.443815</td>\n",
       "      <td>0.484076</td>\n",
       "      <td>0.188630</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.682028</td>\n",
       "      <td>6.398765</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.509151</td>\n",
       "      <td>0.514619</td>\n",
       "      <td>0.241589</td>\n",
       "      <td>0.232145</td>\n",
       "      <td>0.902349</td>\n",
       "      <td>0.913053</td>\n",
       "      <td>0.882222</td>\n",
       "      <td>0.924979</td>\n",
       "      <td>0.026223</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.501347</td>\n",
       "      <td>0.501361</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.114649</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.606344</td>\n",
       "      <td>0.605437</td>\n",
       "      <td>0.258774</td>\n",
       "      <td>0.275451</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.915433</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>890.099784</td>\n",
       "      <td>874.574718</td>\n",
       "      <td>870.012645</td>\n",
       "      <td>5618.356374</td>\n",
       "      <td>11957.028231</td>\n",
       "      <td>2158.561583</td>\n",
       "      <td>253.216522</td>\n",
       "      <td>247.094642</td>\n",
       "      <td>237.894941</td>\n",
       "      <td>385.6601</td>\n",
       "      <td>175.44424</td>\n",
       "      <td>167.62558</td>\n",
       "      <td>354.663538</td>\n",
       "      <td>349.519353</td>\n",
       "      <td>344.572977</td>\n",
       "      <td>494.338096</td>\n",
       "      <td>320.998343</td>\n",
       "      <td>304.472420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.462495</td>\n",
       "      <td>202.812791</td>\n",
       "      <td>376.585663</td>\n",
       "      <td>202.923064</td>\n",
       "      <td>199.341257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>46</td>\n",
       "      <td>286</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1409</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.495972</td>\n",
       "      <td>0.495769</td>\n",
       "      <td>0.486865</td>\n",
       "      <td>0.485136</td>\n",
       "      <td>0.380874</td>\n",
       "      <td>0.383410</td>\n",
       "      <td>0.339374</td>\n",
       "      <td>0.339764</td>\n",
       "      <td>0.833472</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.850728</td>\n",
       "      <td>0.8509</td>\n",
       "      <td>0.826597</td>\n",
       "      <td>0.829024</td>\n",
       "      <td>0.844733</td>\n",
       "      <td>0.847432</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>0.028576</td>\n",
       "      <td>0.612697</td>\n",
       "      <td>0.613249</td>\n",
       "      <td>0.564528</td>\n",
       "      <td>0.564254</td>\n",
       "      <td>0.705632</td>\n",
       "      <td>0.7028</td>\n",
       "      <td>0.678824</td>\n",
       "      <td>0.705614</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.492361</td>\n",
       "      <td>0.490719</td>\n",
       "      <td>0.484306</td>\n",
       "      <td>0.479981</td>\n",
       "      <td>0.367409</td>\n",
       "      <td>0.366267</td>\n",
       "      <td>0.333593</td>\n",
       "      <td>0.334073</td>\n",
       "      <td>0.837136</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.852082</td>\n",
       "      <td>0.8547</td>\n",
       "      <td>0.834163</td>\n",
       "      <td>0.834104</td>\n",
       "      <td>0.849903</td>\n",
       "      <td>0.849988</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.027170</td>\n",
       "      <td>0.616877</td>\n",
       "      <td>0.620622</td>\n",
       "      <td>0.570432</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.7136</td>\n",
       "      <td>0.688397</td>\n",
       "      <td>0.715229</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.498293</td>\n",
       "      <td>0.496365</td>\n",
       "      <td>0.489628</td>\n",
       "      <td>0.484901</td>\n",
       "      <td>0.382379</td>\n",
       "      <td>0.378958</td>\n",
       "      <td>0.344514</td>\n",
       "      <td>0.342052</td>\n",
       "      <td>0.831040</td>\n",
       "      <td>0.8388</td>\n",
       "      <td>0.847064</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.819418</td>\n",
       "      <td>0.827358</td>\n",
       "      <td>0.836161</td>\n",
       "      <td>0.844203</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.622317</td>\n",
       "      <td>0.622120</td>\n",
       "      <td>0.579808</td>\n",
       "      <td>0.576846</td>\n",
       "      <td>0.698048</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>0.675447</td>\n",
       "      <td>0.710239</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.646430</td>\n",
       "      <td>0.332021</td>\n",
       "      <td>10.300082</td>\n",
       "      <td>0.011146</td>\n",
       "      <td>0.686627</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.998470</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.628368</td>\n",
       "      <td>0.574516</td>\n",
       "      <td>0.670269</td>\n",
       "      <td>0.364126</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.496241</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.645147</td>\n",
       "      <td>0.430846</td>\n",
       "      <td>0.556817</td>\n",
       "      <td>0.120251</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.820717</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.765034</td>\n",
       "      <td>0.336949</td>\n",
       "      <td>0.982626</td>\n",
       "      <td>0.033669</td>\n",
       "      <td>0.466605</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.586527</td>\n",
       "      <td>0.993710</td>\n",
       "      <td>0.059062</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.558729</td>\n",
       "      <td>0.580193</td>\n",
       "      <td>0.361183</td>\n",
       "      <td>0.421473</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.837691</td>\n",
       "      <td>0.048559</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.645694</td>\n",
       "      <td>0.435666</td>\n",
       "      <td>0.624696</td>\n",
       "      <td>0.157809</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.9349</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.737817</td>\n",
       "      <td>0.085963</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.745745</td>\n",
       "      <td>0.338689</td>\n",
       "      <td>0.958959</td>\n",
       "      <td>0.039931</td>\n",
       "      <td>0.544603</td>\n",
       "      <td>0.98464</td>\n",
       "      <td>0.619646</td>\n",
       "      <td>0.992122</td>\n",
       "      <td>0.707219</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.570864</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.453769</td>\n",
       "      <td>0.440922</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.79334</td>\n",
       "      <td>0.675862</td>\n",
       "      <td>0.815627</td>\n",
       "      <td>0.780087</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.631943</td>\n",
       "      <td>0.438652</td>\n",
       "      <td>0.517273</td>\n",
       "      <td>0.167041</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.93158</td>\n",
       "      <td>0.561983</td>\n",
       "      <td>0.740085</td>\n",
       "      <td>0.628368</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.751342</td>\n",
       "      <td>0.338606</td>\n",
       "      <td>0.965319</td>\n",
       "      <td>0.040026</td>\n",
       "      <td>0.560571</td>\n",
       "      <td>0.983952</td>\n",
       "      <td>0.625980</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>8.948903</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.564544</td>\n",
       "      <td>0.585896</td>\n",
       "      <td>0.411363</td>\n",
       "      <td>0.443391</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.792173</td>\n",
       "      <td>0.675862</td>\n",
       "      <td>0.814393</td>\n",
       "      <td>8.988415</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.635391</td>\n",
       "      <td>0.438672</td>\n",
       "      <td>0.511929</td>\n",
       "      <td>0.167892</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.928820</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.727265</td>\n",
       "      <td>8.718359</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.499393</td>\n",
       "      <td>0.504802</td>\n",
       "      <td>0.185063</td>\n",
       "      <td>0.176034</td>\n",
       "      <td>0.934899</td>\n",
       "      <td>0.940567</td>\n",
       "      <td>0.914859</td>\n",
       "      <td>0.945923</td>\n",
       "      <td>0.035989</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.502917</td>\n",
       "      <td>0.501101</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.084286</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.603373</td>\n",
       "      <td>0.602921</td>\n",
       "      <td>0.403781</td>\n",
       "      <td>0.195279</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.942918</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.882096</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>890.099784</td>\n",
       "      <td>874.574718</td>\n",
       "      <td>870.012645</td>\n",
       "      <td>5618.356374</td>\n",
       "      <td>11957.028231</td>\n",
       "      <td>2158.561583</td>\n",
       "      <td>253.216522</td>\n",
       "      <td>247.094642</td>\n",
       "      <td>237.894941</td>\n",
       "      <td>385.6601</td>\n",
       "      <td>175.44424</td>\n",
       "      <td>167.62558</td>\n",
       "      <td>354.663538</td>\n",
       "      <td>349.519353</td>\n",
       "      <td>344.572977</td>\n",
       "      <td>494.338096</td>\n",
       "      <td>320.998343</td>\n",
       "      <td>304.472420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.462495</td>\n",
       "      <td>202.812791</td>\n",
       "      <td>376.585663</td>\n",
       "      <td>202.923064</td>\n",
       "      <td>199.341257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>94</td>\n",
       "      <td>590</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1409</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.477639</td>\n",
       "      <td>0.476955</td>\n",
       "      <td>0.463469</td>\n",
       "      <td>0.459092</td>\n",
       "      <td>0.378772</td>\n",
       "      <td>0.375503</td>\n",
       "      <td>0.286219</td>\n",
       "      <td>0.284103</td>\n",
       "      <td>0.852288</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.878014</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>0.846358</td>\n",
       "      <td>0.845334</td>\n",
       "      <td>0.873359</td>\n",
       "      <td>0.877471</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.615532</td>\n",
       "      <td>0.617586</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>0.568291</td>\n",
       "      <td>0.708408</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.717616</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>0.472170</td>\n",
       "      <td>0.460330</td>\n",
       "      <td>0.455737</td>\n",
       "      <td>0.368566</td>\n",
       "      <td>0.367264</td>\n",
       "      <td>0.278693</td>\n",
       "      <td>0.274248</td>\n",
       "      <td>0.856288</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.881550</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.853689</td>\n",
       "      <td>0.861308</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.883553</td>\n",
       "      <td>0.036905</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>0.617547</td>\n",
       "      <td>0.623748</td>\n",
       "      <td>0.571834</td>\n",
       "      <td>0.579464</td>\n",
       "      <td>0.709880</td>\n",
       "      <td>0.7084</td>\n",
       "      <td>0.692539</td>\n",
       "      <td>0.712673</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.479234</td>\n",
       "      <td>0.471233</td>\n",
       "      <td>0.464623</td>\n",
       "      <td>0.459743</td>\n",
       "      <td>0.376381</td>\n",
       "      <td>0.372756</td>\n",
       "      <td>0.288202</td>\n",
       "      <td>0.280073</td>\n",
       "      <td>0.848352</td>\n",
       "      <td>0.8608</td>\n",
       "      <td>0.876036</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>0.840693</td>\n",
       "      <td>0.844355</td>\n",
       "      <td>0.869102</td>\n",
       "      <td>0.877708</td>\n",
       "      <td>0.033396</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>0.626444</td>\n",
       "      <td>0.628567</td>\n",
       "      <td>0.586809</td>\n",
       "      <td>0.590938</td>\n",
       "      <td>0.693312</td>\n",
       "      <td>0.6928</td>\n",
       "      <td>0.674189</td>\n",
       "      <td>0.703011</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.633102</td>\n",
       "      <td>0.330991</td>\n",
       "      <td>10.818193</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.686627</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.311636</td>\n",
       "      <td>0.998980</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.612476</td>\n",
       "      <td>0.555402</td>\n",
       "      <td>1.938406</td>\n",
       "      <td>0.265439</td>\n",
       "      <td>0.743017</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.900356</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.637858</td>\n",
       "      <td>0.425028</td>\n",
       "      <td>2.884254</td>\n",
       "      <td>0.071208</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.757829</td>\n",
       "      <td>0.334709</td>\n",
       "      <td>1.139937</td>\n",
       "      <td>0.023555</td>\n",
       "      <td>0.531552</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.592058</td>\n",
       "      <td>0.995287</td>\n",
       "      <td>0.058041</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.604942</td>\n",
       "      <td>0.566556</td>\n",
       "      <td>0.513529</td>\n",
       "      <td>0.374040</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.8295</td>\n",
       "      <td>0.859060</td>\n",
       "      <td>0.849952</td>\n",
       "      <td>0.060057</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.656525</td>\n",
       "      <td>0.432403</td>\n",
       "      <td>0.656106</td>\n",
       "      <td>0.138408</td>\n",
       "      <td>0.709459</td>\n",
       "      <td>0.9396</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.765710</td>\n",
       "      <td>0.065024</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.684232</td>\n",
       "      <td>0.336940</td>\n",
       "      <td>0.784067</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.541225</td>\n",
       "      <td>0.98651</td>\n",
       "      <td>0.620812</td>\n",
       "      <td>0.993110</td>\n",
       "      <td>0.781147</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.562121</td>\n",
       "      <td>0.573012</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>0.401912</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.81728</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>0.830827</td>\n",
       "      <td>0.686671</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.631208</td>\n",
       "      <td>0.435106</td>\n",
       "      <td>0.485480</td>\n",
       "      <td>0.150556</td>\n",
       "      <td>0.871622</td>\n",
       "      <td>0.93382</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.739346</td>\n",
       "      <td>0.770864</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.686935</td>\n",
       "      <td>0.337091</td>\n",
       "      <td>0.807044</td>\n",
       "      <td>0.034885</td>\n",
       "      <td>0.557807</td>\n",
       "      <td>0.986639</td>\n",
       "      <td>0.628866</td>\n",
       "      <td>0.993170</td>\n",
       "      <td>10.499663</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.562387</td>\n",
       "      <td>0.573653</td>\n",
       "      <td>0.422405</td>\n",
       "      <td>0.405561</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.814067</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>0.828896</td>\n",
       "      <td>9.653604</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.644520</td>\n",
       "      <td>0.435362</td>\n",
       "      <td>0.540236</td>\n",
       "      <td>0.152760</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.934484</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.733373</td>\n",
       "      <td>10.151225</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.492986</td>\n",
       "      <td>0.498856</td>\n",
       "      <td>0.175076</td>\n",
       "      <td>0.135784</td>\n",
       "      <td>0.943498</td>\n",
       "      <td>0.948713</td>\n",
       "      <td>0.924990</td>\n",
       "      <td>0.953017</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.493192</td>\n",
       "      <td>0.493345</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.977153</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.602736</td>\n",
       "      <td>0.593692</td>\n",
       "      <td>0.614574</td>\n",
       "      <td>0.132693</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>890.099784</td>\n",
       "      <td>874.574718</td>\n",
       "      <td>870.012645</td>\n",
       "      <td>5618.356374</td>\n",
       "      <td>11957.028231</td>\n",
       "      <td>2158.561583</td>\n",
       "      <td>253.216522</td>\n",
       "      <td>247.094642</td>\n",
       "      <td>237.894941</td>\n",
       "      <td>385.6601</td>\n",
       "      <td>175.44424</td>\n",
       "      <td>167.62558</td>\n",
       "      <td>354.663538</td>\n",
       "      <td>349.519353</td>\n",
       "      <td>344.572977</td>\n",
       "      <td>494.338096</td>\n",
       "      <td>320.998343</td>\n",
       "      <td>304.472420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.462495</td>\n",
       "      <td>202.812791</td>\n",
       "      <td>376.585663</td>\n",
       "      <td>202.923064</td>\n",
       "      <td>199.341257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>22</td>\n",
       "      <td>134</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1409</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.502905</td>\n",
       "      <td>0.502870</td>\n",
       "      <td>0.500508</td>\n",
       "      <td>0.501946</td>\n",
       "      <td>0.377181</td>\n",
       "      <td>0.381723</td>\n",
       "      <td>0.364557</td>\n",
       "      <td>0.373574</td>\n",
       "      <td>0.831776</td>\n",
       "      <td>0.8336</td>\n",
       "      <td>0.836462</td>\n",
       "      <td>0.8342</td>\n",
       "      <td>0.823214</td>\n",
       "      <td>0.826249</td>\n",
       "      <td>0.827982</td>\n",
       "      <td>0.831399</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.606874</td>\n",
       "      <td>0.613907</td>\n",
       "      <td>0.561997</td>\n",
       "      <td>0.565275</td>\n",
       "      <td>0.706720</td>\n",
       "      <td>0.6992</td>\n",
       "      <td>0.674549</td>\n",
       "      <td>0.714300</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.494646</td>\n",
       "      <td>0.498007</td>\n",
       "      <td>0.490680</td>\n",
       "      <td>0.494132</td>\n",
       "      <td>0.365627</td>\n",
       "      <td>0.375666</td>\n",
       "      <td>0.349626</td>\n",
       "      <td>0.364636</td>\n",
       "      <td>0.838112</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.844900</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.821327</td>\n",
       "      <td>0.828912</td>\n",
       "      <td>0.828311</td>\n",
       "      <td>0.835338</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.612247</td>\n",
       "      <td>0.613774</td>\n",
       "      <td>0.571419</td>\n",
       "      <td>0.572374</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.7056</td>\n",
       "      <td>0.649521</td>\n",
       "      <td>0.715474</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>0.516472</td>\n",
       "      <td>0.500962</td>\n",
       "      <td>0.511956</td>\n",
       "      <td>0.386779</td>\n",
       "      <td>0.406075</td>\n",
       "      <td>0.367982</td>\n",
       "      <td>0.392357</td>\n",
       "      <td>0.825776</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>0.834160</td>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.813966</td>\n",
       "      <td>0.816357</td>\n",
       "      <td>0.822841</td>\n",
       "      <td>0.824787</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>0.017498</td>\n",
       "      <td>0.611344</td>\n",
       "      <td>0.617777</td>\n",
       "      <td>0.572884</td>\n",
       "      <td>0.580001</td>\n",
       "      <td>0.703008</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.659850</td>\n",
       "      <td>0.701589</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.606532</td>\n",
       "      <td>0.343930</td>\n",
       "      <td>1.194771</td>\n",
       "      <td>0.029702</td>\n",
       "      <td>0.722094</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.473531</td>\n",
       "      <td>0.993296</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.598812</td>\n",
       "      <td>0.507022</td>\n",
       "      <td>2.277555</td>\n",
       "      <td>0.270955</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.891871</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.617050</td>\n",
       "      <td>0.426472</td>\n",
       "      <td>0.507695</td>\n",
       "      <td>0.139921</td>\n",
       "      <td>0.871622</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.773612</td>\n",
       "      <td>0.353635</td>\n",
       "      <td>1.059174</td>\n",
       "      <td>0.069665</td>\n",
       "      <td>0.455550</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.582824</td>\n",
       "      <td>0.984545</td>\n",
       "      <td>0.032153</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.573358</td>\n",
       "      <td>0.512211</td>\n",
       "      <td>0.453806</td>\n",
       "      <td>0.299919</td>\n",
       "      <td>0.849162</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.825806</td>\n",
       "      <td>0.884096</td>\n",
       "      <td>0.036592</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.624068</td>\n",
       "      <td>0.428355</td>\n",
       "      <td>0.497640</td>\n",
       "      <td>0.139317</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.9466</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.899397</td>\n",
       "      <td>0.027850</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.764616</td>\n",
       "      <td>0.355650</td>\n",
       "      <td>1.034835</td>\n",
       "      <td>0.076282</td>\n",
       "      <td>0.484723</td>\n",
       "      <td>0.96746</td>\n",
       "      <td>0.595273</td>\n",
       "      <td>0.983126</td>\n",
       "      <td>0.414240</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.631198</td>\n",
       "      <td>0.513346</td>\n",
       "      <td>0.627517</td>\n",
       "      <td>0.304766</td>\n",
       "      <td>0.692737</td>\n",
       "      <td>0.86083</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.882651</td>\n",
       "      <td>0.473240</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.627679</td>\n",
       "      <td>0.430359</td>\n",
       "      <td>0.522701</td>\n",
       "      <td>0.138477</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.94427</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.896951</td>\n",
       "      <td>0.320349</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.748558</td>\n",
       "      <td>0.355464</td>\n",
       "      <td>0.977142</td>\n",
       "      <td>0.077190</td>\n",
       "      <td>0.489483</td>\n",
       "      <td>0.966564</td>\n",
       "      <td>0.597019</td>\n",
       "      <td>0.982700</td>\n",
       "      <td>6.422559</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.633388</td>\n",
       "      <td>0.514233</td>\n",
       "      <td>0.638453</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.692737</td>\n",
       "      <td>0.860024</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.881758</td>\n",
       "      <td>6.648989</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.627118</td>\n",
       "      <td>0.430960</td>\n",
       "      <td>0.524895</td>\n",
       "      <td>0.140045</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.944159</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.896894</td>\n",
       "      <td>4.305763</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.509151</td>\n",
       "      <td>0.514619</td>\n",
       "      <td>0.241589</td>\n",
       "      <td>0.232145</td>\n",
       "      <td>0.902349</td>\n",
       "      <td>0.913053</td>\n",
       "      <td>0.882222</td>\n",
       "      <td>0.924979</td>\n",
       "      <td>0.043410</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.501347</td>\n",
       "      <td>0.501361</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.114649</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.606344</td>\n",
       "      <td>0.605437</td>\n",
       "      <td>0.258774</td>\n",
       "      <td>0.275451</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.915433</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>952.125841</td>\n",
       "      <td>938.678758</td>\n",
       "      <td>967.163455</td>\n",
       "      <td>1530.924031</td>\n",
       "      <td>815.914129</td>\n",
       "      <td>702.551322</td>\n",
       "      <td>294.860154</td>\n",
       "      <td>288.710905</td>\n",
       "      <td>299.193479</td>\n",
       "      <td>385.6601</td>\n",
       "      <td>175.44424</td>\n",
       "      <td>167.62558</td>\n",
       "      <td>401.357852</td>\n",
       "      <td>397.316608</td>\n",
       "      <td>403.692552</td>\n",
       "      <td>506.776383</td>\n",
       "      <td>342.340437</td>\n",
       "      <td>336.541181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>247.487209</td>\n",
       "      <td>264.935285</td>\n",
       "      <td>369.448476</td>\n",
       "      <td>174.373387</td>\n",
       "      <td>193.871242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>46</td>\n",
       "      <td>286</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>classification</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>uniform</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>1409</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>['soft_binary_crossentropy', 'binary_accuracy']</td>\n",
       "      <td>500</td>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>SEQUENTIAL</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>flip_percentage</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0.474580</td>\n",
       "      <td>0.473859</td>\n",
       "      <td>0.469407</td>\n",
       "      <td>0.470362</td>\n",
       "      <td>0.327691</td>\n",
       "      <td>0.331907</td>\n",
       "      <td>0.298243</td>\n",
       "      <td>0.296453</td>\n",
       "      <td>0.857424</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.866946</td>\n",
       "      <td>0.8698</td>\n",
       "      <td>0.850090</td>\n",
       "      <td>0.857306</td>\n",
       "      <td>0.860619</td>\n",
       "      <td>0.870499</td>\n",
       "      <td>0.021769</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.600036</td>\n",
       "      <td>0.600425</td>\n",
       "      <td>0.549041</td>\n",
       "      <td>0.550977</td>\n",
       "      <td>0.719944</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.690287</td>\n",
       "      <td>0.714838</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.467580</td>\n",
       "      <td>0.470944</td>\n",
       "      <td>0.461695</td>\n",
       "      <td>0.465275</td>\n",
       "      <td>0.320949</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.286682</td>\n",
       "      <td>0.293382</td>\n",
       "      <td>0.862608</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>0.872366</td>\n",
       "      <td>0.8708</td>\n",
       "      <td>0.849012</td>\n",
       "      <td>0.852081</td>\n",
       "      <td>0.859110</td>\n",
       "      <td>0.859674</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.605724</td>\n",
       "      <td>0.609405</td>\n",
       "      <td>0.555553</td>\n",
       "      <td>0.562603</td>\n",
       "      <td>0.714264</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.681811</td>\n",
       "      <td>0.709280</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.480201</td>\n",
       "      <td>0.490897</td>\n",
       "      <td>0.472931</td>\n",
       "      <td>0.484267</td>\n",
       "      <td>0.343678</td>\n",
       "      <td>0.353273</td>\n",
       "      <td>0.307333</td>\n",
       "      <td>0.329922</td>\n",
       "      <td>0.847696</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>0.860920</td>\n",
       "      <td>0.8532</td>\n",
       "      <td>0.839975</td>\n",
       "      <td>0.838571</td>\n",
       "      <td>0.853453</td>\n",
       "      <td>0.853609</td>\n",
       "      <td>0.021356</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.609198</td>\n",
       "      <td>0.605301</td>\n",
       "      <td>0.563761</td>\n",
       "      <td>0.562193</td>\n",
       "      <td>0.714608</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.724678</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.646097</td>\n",
       "      <td>0.339806</td>\n",
       "      <td>2.083472</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.698142</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.400975</td>\n",
       "      <td>0.997424</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.878277</td>\n",
       "      <td>0.486462</td>\n",
       "      <td>14.285952</td>\n",
       "      <td>0.203904</td>\n",
       "      <td>0.357542</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.371585</td>\n",
       "      <td>0.930519</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.424072</td>\n",
       "      <td>1.211867</td>\n",
       "      <td>0.122480</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.908023</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.699088</td>\n",
       "      <td>0.349824</td>\n",
       "      <td>0.740372</td>\n",
       "      <td>0.057663</td>\n",
       "      <td>0.541379</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.616412</td>\n",
       "      <td>0.987396</td>\n",
       "      <td>0.046304</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.585473</td>\n",
       "      <td>0.493887</td>\n",
       "      <td>0.513521</td>\n",
       "      <td>0.249480</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.8851</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.892987</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.634266</td>\n",
       "      <td>0.425474</td>\n",
       "      <td>0.494213</td>\n",
       "      <td>0.121895</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.9471</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.908746</td>\n",
       "      <td>0.027859</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.692830</td>\n",
       "      <td>0.351874</td>\n",
       "      <td>0.721504</td>\n",
       "      <td>0.065143</td>\n",
       "      <td>0.550438</td>\n",
       "      <td>0.97234</td>\n",
       "      <td>0.623069</td>\n",
       "      <td>0.985547</td>\n",
       "      <td>0.559095</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.636763</td>\n",
       "      <td>0.497750</td>\n",
       "      <td>0.703895</td>\n",
       "      <td>0.261732</td>\n",
       "      <td>0.743017</td>\n",
       "      <td>0.88517</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.892607</td>\n",
       "      <td>0.554929</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.640689</td>\n",
       "      <td>0.427252</td>\n",
       "      <td>0.523307</td>\n",
       "      <td>0.121175</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.94939</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.913842</td>\n",
       "      <td>0.363112</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.675718</td>\n",
       "      <td>0.351683</td>\n",
       "      <td>0.675149</td>\n",
       "      <td>0.066051</td>\n",
       "      <td>0.591279</td>\n",
       "      <td>0.973342</td>\n",
       "      <td>0.637428</td>\n",
       "      <td>0.986045</td>\n",
       "      <td>7.925493</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.635056</td>\n",
       "      <td>0.498131</td>\n",
       "      <td>0.668259</td>\n",
       "      <td>0.263208</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.886782</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.894634</td>\n",
       "      <td>7.662928</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.640127</td>\n",
       "      <td>0.427879</td>\n",
       "      <td>0.549056</td>\n",
       "      <td>0.123195</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.948687</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.912878</td>\n",
       "      <td>4.915602</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.499393</td>\n",
       "      <td>0.504802</td>\n",
       "      <td>0.185063</td>\n",
       "      <td>0.176034</td>\n",
       "      <td>0.934899</td>\n",
       "      <td>0.940567</td>\n",
       "      <td>0.914859</td>\n",
       "      <td>0.945923</td>\n",
       "      <td>0.050534</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.502917</td>\n",
       "      <td>0.501101</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.084286</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.603373</td>\n",
       "      <td>0.602921</td>\n",
       "      <td>0.403781</td>\n",
       "      <td>0.195279</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.942918</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.882096</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>952.125841</td>\n",
       "      <td>938.678758</td>\n",
       "      <td>967.163455</td>\n",
       "      <td>1530.924031</td>\n",
       "      <td>815.914129</td>\n",
       "      <td>702.551322</td>\n",
       "      <td>294.860154</td>\n",
       "      <td>288.710905</td>\n",
       "      <td>299.193479</td>\n",
       "      <td>385.6601</td>\n",
       "      <td>175.44424</td>\n",
       "      <td>167.62558</td>\n",
       "      <td>401.357852</td>\n",
       "      <td>397.316608</td>\n",
       "      <td>403.692552</td>\n",
       "      <td>506.776383</td>\n",
       "      <td>342.340437</td>\n",
       "      <td>336.541181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>247.487209</td>\n",
       "      <td>264.935285</td>\n",
       "      <td>369.448476</td>\n",
       "      <td>174.373387</td>\n",
       "      <td>193.871242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   function_family_maximum_depth  function_family_beta  \\\n",
       "0                              3                     1   \n",
       "1                              4                     1   \n",
       "2                              5                     1   \n",
       "3                              3                     1   \n",
       "4                              4                     1   \n",
       "\n",
       "   function_family_decision_sparsity  function_family_fully_grown  \\\n",
       "0                                  1                         True   \n",
       "1                                  1                         True   \n",
       "2                                  1                         True   \n",
       "3                                  1                         True   \n",
       "4                                  1                         True   \n",
       "\n",
       "  function_family_dt_type  \\\n",
       "0                 vanilla   \n",
       "1                 vanilla   \n",
       "2                 vanilla   \n",
       "3                 vanilla   \n",
       "4                 vanilla   \n",
       "\n",
       "   function_family_basic_function_representation_length  \\\n",
       "0                                                 22      \n",
       "1                                                 46      \n",
       "2                                                 94      \n",
       "3                                                 22      \n",
       "4                                                 46      \n",
       "\n",
       "   function_family_function_representation_length  data_number_of_variables  \\\n",
       "0                                             134                         9   \n",
       "1                                             286                         9   \n",
       "2                                             590                         9   \n",
       "3                                             134                         9   \n",
       "4                                             286                         9   \n",
       "\n",
       "   data_num_classes data_categorical_indices data_dt_type_train  \\\n",
       "0                 2                       []            vanilla   \n",
       "1                 2                       []            vanilla   \n",
       "2                 2                       []            vanilla   \n",
       "3                 2             [0, 1, 2, 3]            vanilla   \n",
       "4                 2             [0, 1, 2, 3]            vanilla   \n",
       "\n",
       "   data_maximum_depth_train  data_decision_sparsity_train  \\\n",
       "0                         5                             1   \n",
       "1                         5                             1   \n",
       "2                         5                             1   \n",
       "3                         5                             1   \n",
       "4                         5                             1   \n",
       "\n",
       "  data_function_generation_type  data_objective  data_x_max  data_x_min  \\\n",
       "0   make_classification_trained  classification           1           0   \n",
       "1   make_classification_trained  classification           1           0   \n",
       "2   make_classification_trained  classification           1           0   \n",
       "3   make_classification_trained  classification           1           0   \n",
       "4   make_classification_trained  classification           1           0   \n",
       "\n",
       "  data_x_distrib  data_lambda_dataset_size  data_noise_injected_level  \\\n",
       "0        uniform                      5000                          0   \n",
       "1        uniform                      5000                          0   \n",
       "2        uniform                      5000                          0   \n",
       "3        uniform                      5000                          0   \n",
       "4        uniform                      5000                          0   \n",
       "\n",
       "  data_noise_injected_type  lambda_net_epochs_lambda  \\\n",
       "0          flip_percentage                      1000   \n",
       "1          flip_percentage                      1000   \n",
       "2          flip_percentage                      1000   \n",
       "3          flip_percentage                      1000   \n",
       "4          flip_percentage                      1000   \n",
       "\n",
       "   lambda_net_early_stopping_lambda  \\\n",
       "0                              True   \n",
       "1                              True   \n",
       "2                              True   \n",
       "3                              True   \n",
       "4                              True   \n",
       "\n",
       "   lambda_net_early_stopping_min_delta_lambda  lambda_net_batch_lambda  \\\n",
       "0                                        0.01                       64   \n",
       "1                                        0.01                       64   \n",
       "2                                        0.01                       64   \n",
       "3                                        0.01                       64   \n",
       "4                                        0.01                       64   \n",
       "\n",
       "   lambda_net_dropout_lambda lambda_net_lambda_network_layers  \\\n",
       "0                          0                            [128]   \n",
       "1                          0                            [128]   \n",
       "2                          0                            [128]   \n",
       "3                          0                            [128]   \n",
       "4                          0                            [128]   \n",
       "\n",
       "  lambda_net_optimizer_lambda lambda_net_loss_lambda  \\\n",
       "0                        adam    binary_crossentropy   \n",
       "1                        adam    binary_crossentropy   \n",
       "2                        adam    binary_crossentropy   \n",
       "3                        adam    binary_crossentropy   \n",
       "4                        adam    binary_crossentropy   \n",
       "\n",
       "   lambda_net_number_of_lambda_weights  \\\n",
       "0                                 1409   \n",
       "1                                 1409   \n",
       "2                                 1409   \n",
       "3                                 1409   \n",
       "4                                 1409   \n",
       "\n",
       "   lambda_net_number_initializations_lambda  \\\n",
       "0                                         1   \n",
       "1                                         1   \n",
       "2                                         1   \n",
       "3                                         1   \n",
       "4                                         1   \n",
       "\n",
       "   lambda_net_number_of_trained_lambda_nets             i_net_dense_layers  \\\n",
       "0                                     10000  [1024, 1024, 256, 2048, 2048]   \n",
       "1                                     10000  [1024, 1024, 256, 2048, 2048]   \n",
       "2                                     10000  [1024, 1024, 256, 2048, 2048]   \n",
       "3                                     10000  [1024, 1024, 256, 2048, 2048]   \n",
       "4                                     10000  [1024, 1024, 256, 2048, 2048]   \n",
       "\n",
       "  i_net_convolution_layers i_net_lstm_layers              i_net_dropout  \\\n",
       "0                     None              None  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "1                     None              None  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "2                     None              None  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "3                     None              None  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "4                     None              None  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "\n",
       "  i_net_optimizer  i_net_learning_rate           i_net_loss  \\\n",
       "0            adam               0.0001  binary_crossentropy   \n",
       "1            adam               0.0001  binary_crossentropy   \n",
       "2            adam               0.0001  binary_crossentropy   \n",
       "3            adam               0.0001  binary_crossentropy   \n",
       "4            adam               0.0001  binary_crossentropy   \n",
       "\n",
       "                                     i_net_metrics  i_net_epochs  \\\n",
       "0  ['soft_binary_crossentropy', 'binary_accuracy']           500   \n",
       "1  ['soft_binary_crossentropy', 'binary_accuracy']           500   \n",
       "2  ['soft_binary_crossentropy', 'binary_accuracy']           500   \n",
       "3  ['soft_binary_crossentropy', 'binary_accuracy']           500   \n",
       "4  ['soft_binary_crossentropy', 'binary_accuracy']           500   \n",
       "\n",
       "   i_net_early_stopping  i_net_batch_size  i_net_interpretation_dataset_size  \\\n",
       "0                  True               256                              10000   \n",
       "1                  True               256                              10000   \n",
       "2                  True               256                              10000   \n",
       "3                  True               256                              10000   \n",
       "4                  True               256                              10000   \n",
       "\n",
       "   i_net_test_size  i_net_function_representation_type  \\\n",
       "0               50                                   3   \n",
       "1               50                                   3   \n",
       "2               50                                   3   \n",
       "3               50                                   3   \n",
       "4               50                                   3   \n",
       "\n",
       "   i_net_normalize_lambda_nets  i_net_optimize_decision_function  \\\n",
       "0                        False                              True   \n",
       "1                        False                              True   \n",
       "2                        False                              True   \n",
       "3                        False                              True   \n",
       "4                        False                              True   \n",
       "\n",
       "   i_net_function_value_loss  i_net_soft_labels i_net_data_reshape_version  \\\n",
       "0                       True              False                       None   \n",
       "1                       True              False                       None   \n",
       "2                       True              False                       None   \n",
       "3                       True              False                       None   \n",
       "4                       True              False                       None   \n",
       "\n",
       "   i_net_nas i_net_nas_type  i_net_nas_trials  \\\n",
       "0      False     SEQUENTIAL               100   \n",
       "1      False     SEQUENTIAL               100   \n",
       "2      False     SEQUENTIAL               100   \n",
       "3      False     SEQUENTIAL               100   \n",
       "4      False     SEQUENTIAL               100   \n",
       "\n",
       "   evaluation_random_evaluation_dataset_size  \\\n",
       "0                                        500   \n",
       "1                                        500   \n",
       "2                                        500   \n",
       "3                                        500   \n",
       "4                                        500   \n",
       "\n",
       "   evaluation_per_network_optimization_dataset_size  \\\n",
       "0                                              5000   \n",
       "1                                              5000   \n",
       "2                                              5000   \n",
       "3                                              5000   \n",
       "4                                              5000   \n",
       "\n",
       "   evaluation_sklearn_dt_benchmark  evaluation_sdt_benchmark  \\\n",
       "0                            False                     False   \n",
       "1                            False                     False   \n",
       "2                            False                     False   \n",
       "3                            False                     False   \n",
       "4                            False                     False   \n",
       "\n",
       "   evaluation_different_eval_data  \\\n",
       "0                           False   \n",
       "1                           False   \n",
       "2                           False   \n",
       "3                           False   \n",
       "4                           False   \n",
       "\n",
       "  evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                make_classification                    \n",
       "1                                make_classification                    \n",
       "2                                make_classification                    \n",
       "3                                make_classification                    \n",
       "4                                make_classification                    \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_lambda_dataset_size  \\\n",
       "0                                               5000                \n",
       "1                                               5000                \n",
       "2                                               5000                \n",
       "3                                               5000                \n",
       "4                                               5000                \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                  0                 \n",
       "1                                                  0                 \n",
       "2                                                  0                 \n",
       "3                                                  0                 \n",
       "4                                                  0                 \n",
       "\n",
       "  evaluation_eval_data_description_eval_data_noise_injected_type  \\\n",
       "0                                    flip_percentage               \n",
       "1                                    flip_percentage               \n",
       "2                                    flip_percentage               \n",
       "3                                    flip_percentage               \n",
       "4                                    flip_percentage               \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_number_of_trained_lambda_nets  \\\n",
       "0                                                100                          \n",
       "1                                                100                          \n",
       "2                                                100                          \n",
       "3                                                100                          \n",
       "4                                                100                          \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_interpretation_dataset_size  \\\n",
       "0                                                100                        \n",
       "1                                                100                        \n",
       "2                                                100                        \n",
       "3                                                100                        \n",
       "4                                                100                        \n",
       "\n",
       "   computation_load_model  computation_n_jobs  computation_use_gpu  \\\n",
       "0                   False                   7                False   \n",
       "1                   False                   7                False   \n",
       "2                   False                   7                False   \n",
       "3                   False                   7                False   \n",
       "4                   False                   7                False   \n",
       "\n",
       "   computation_gpu_numbers  computation_RANDOM_SEED  \\\n",
       "0                        2                       42   \n",
       "1                        2                       42   \n",
       "2                        2                       42   \n",
       "3                        2                       42   \n",
       "4                        2                       42   \n",
       "\n",
       "   train_dt_scores_soft_binary_crossentropy  \\\n",
       "0                                  0.519338   \n",
       "1                                  0.495972   \n",
       "2                                  0.477639   \n",
       "3                                  0.502905   \n",
       "4                                  0.474580   \n",
       "\n",
       "   train_dt_scores_soft_binary_crossentropy_median  \\\n",
       "0                                         0.521510   \n",
       "1                                         0.495769   \n",
       "2                                         0.476955   \n",
       "3                                         0.502870   \n",
       "4                                         0.473859   \n",
       "\n",
       "   train_dt_scores_soft_binary_crossentropy_data_random  \\\n",
       "0                                           0.513757      \n",
       "1                                           0.486865      \n",
       "2                                           0.463469      \n",
       "3                                           0.500508      \n",
       "4                                           0.469407      \n",
       "\n",
       "   train_dt_scores_soft_binary_crossentropy_data_random_median  \\\n",
       "0                                           0.516722             \n",
       "1                                           0.485136             \n",
       "2                                           0.459092             \n",
       "3                                           0.501946             \n",
       "4                                           0.470362             \n",
       "\n",
       "   train_dt_scores_binary_crossentropy  \\\n",
       "0                             0.415607   \n",
       "1                             0.380874   \n",
       "2                             0.378772   \n",
       "3                             0.377181   \n",
       "4                             0.327691   \n",
       "\n",
       "   train_dt_scores_binary_crossentropy_median  \\\n",
       "0                                    0.424973   \n",
       "1                                    0.383410   \n",
       "2                                    0.375503   \n",
       "3                                    0.381723   \n",
       "4                                    0.331907   \n",
       "\n",
       "   train_dt_scores_binary_crossentropy_data_random  \\\n",
       "0                                         0.395052   \n",
       "1                                         0.339374   \n",
       "2                                         0.286219   \n",
       "3                                         0.364557   \n",
       "4                                         0.298243   \n",
       "\n",
       "   train_dt_scores_binary_crossentropy_data_random_median  \\\n",
       "0                                           0.407307        \n",
       "1                                           0.339764        \n",
       "2                                           0.284103        \n",
       "3                                           0.373574        \n",
       "4                                           0.296453        \n",
       "\n",
       "   train_dt_scores_accuracy  train_dt_scores_accuracy_median  \\\n",
       "0                  0.812544                           0.8076   \n",
       "1                  0.833472                           0.8328   \n",
       "2                  0.852288                           0.8500   \n",
       "3                  0.831776                           0.8336   \n",
       "4                  0.857424                           0.8632   \n",
       "\n",
       "   train_dt_scores_accuracy_data_random  \\\n",
       "0                              0.823310   \n",
       "1                              0.850728   \n",
       "2                              0.878014   \n",
       "3                              0.836462   \n",
       "4                              0.866946   \n",
       "\n",
       "   train_dt_scores_accuracy_data_random_median  train_dt_scores_f1_score  \\\n",
       "0                                       0.8212                  0.804334   \n",
       "1                                       0.8509                  0.826597   \n",
       "2                                       0.8800                  0.846358   \n",
       "3                                       0.8342                  0.823214   \n",
       "4                                       0.8698                  0.850090   \n",
       "\n",
       "   train_dt_scores_f1_score_median  train_dt_scores_f1_score_data_random  \\\n",
       "0                         0.803337                              0.814989   \n",
       "1                         0.829024                              0.844733   \n",
       "2                         0.845334                              0.873359   \n",
       "3                         0.826249                              0.827982   \n",
       "4                         0.857306                              0.860619   \n",
       "\n",
       "   train_dt_scores_f1_score_data_random_median  train_dt_scores_runtime  \\\n",
       "0                                     0.819453                 0.019770   \n",
       "1                                     0.847432                 0.029923   \n",
       "2                                     0.877471                 0.038152   \n",
       "3                                     0.831399                 0.018598   \n",
       "4                                     0.870499                 0.021769   \n",
       "\n",
       "   train_dt_scores_runtime_median  train_inet_scores_soft_binary_crossentropy  \\\n",
       "0                        0.019261                                    0.616474   \n",
       "1                        0.028576                                    0.612697   \n",
       "2                        0.037787                                    0.615532   \n",
       "3                        0.017711                                    0.606874   \n",
       "4                        0.021693                                    0.600036   \n",
       "\n",
       "   train_inet_scores_soft_binary_crossentropy_median  \\\n",
       "0                                           0.615307   \n",
       "1                                           0.613249   \n",
       "2                                           0.617586   \n",
       "3                                           0.613907   \n",
       "4                                           0.600425   \n",
       "\n",
       "   train_inet_scores_binary_crossentropy  \\\n",
       "0                               0.569003   \n",
       "1                               0.564528   \n",
       "2                               0.567170   \n",
       "3                               0.561997   \n",
       "4                               0.549041   \n",
       "\n",
       "   train_inet_scores_binary_crossentropy_median  train_inet_scores_accuracy  \\\n",
       "0                                      0.567067                    0.703344   \n",
       "1                                      0.564254                    0.705632   \n",
       "2                                      0.568291                    0.708408   \n",
       "3                                      0.565275                    0.706720   \n",
       "4                                      0.550977                    0.719944   \n",
       "\n",
       "   train_inet_scores_accuracy_median  train_inet_scores_f1_score  \\\n",
       "0                             0.7024                    0.664482   \n",
       "1                             0.7028                    0.678824   \n",
       "2                             0.7088                    0.664349   \n",
       "3                             0.6992                    0.674549   \n",
       "4                             0.7132                    0.690287   \n",
       "\n",
       "   train_inet_scores_f1_score_median  train_inet_scores_runtime  \\\n",
       "0                           0.695851                   0.000763   \n",
       "1                           0.705614                   0.000880   \n",
       "2                           0.717616                   0.001042   \n",
       "3                           0.714300                   0.001107   \n",
       "4                           0.714838                   0.001228   \n",
       "\n",
       "   train_inet_scores_runtime_median  valid_dt_scores_soft_binary_crossentropy  \\\n",
       "0                          0.000763                                  0.515391   \n",
       "1                          0.000880                                  0.492361   \n",
       "2                          0.001042                                  0.473995   \n",
       "3                          0.001107                                  0.494646   \n",
       "4                          0.001228                                  0.467580   \n",
       "\n",
       "   valid_dt_scores_soft_binary_crossentropy_median  \\\n",
       "0                                         0.512567   \n",
       "1                                         0.490719   \n",
       "2                                         0.472170   \n",
       "3                                         0.498007   \n",
       "4                                         0.470944   \n",
       "\n",
       "   valid_dt_scores_soft_binary_crossentropy_data_random  \\\n",
       "0                                           0.510315      \n",
       "1                                           0.484306      \n",
       "2                                           0.460330      \n",
       "3                                           0.490680      \n",
       "4                                           0.461695      \n",
       "\n",
       "   valid_dt_scores_soft_binary_crossentropy_data_random_median  \\\n",
       "0                                           0.509515             \n",
       "1                                           0.479981             \n",
       "2                                           0.455737             \n",
       "3                                           0.494132             \n",
       "4                                           0.465275             \n",
       "\n",
       "   valid_dt_scores_binary_crossentropy  \\\n",
       "0                             0.408172   \n",
       "1                             0.367409   \n",
       "2                             0.368566   \n",
       "3                             0.365627   \n",
       "4                             0.320949   \n",
       "\n",
       "   valid_dt_scores_binary_crossentropy_median  \\\n",
       "0                                    0.409819   \n",
       "1                                    0.366267   \n",
       "2                                    0.367264   \n",
       "3                                    0.375666   \n",
       "4                                    0.327869   \n",
       "\n",
       "   valid_dt_scores_binary_crossentropy_data_random  \\\n",
       "0                                         0.389243   \n",
       "1                                         0.333593   \n",
       "2                                         0.278693   \n",
       "3                                         0.349626   \n",
       "4                                         0.286682   \n",
       "\n",
       "   valid_dt_scores_binary_crossentropy_data_random_median  \\\n",
       "0                                           0.390487        \n",
       "1                                           0.334073        \n",
       "2                                           0.274248        \n",
       "3                                           0.364636        \n",
       "4                                           0.293382        \n",
       "\n",
       "   valid_dt_scores_accuracy  valid_dt_scores_accuracy_median  \\\n",
       "0                  0.818256                           0.8260   \n",
       "1                  0.837136                           0.8392   \n",
       "2                  0.856288                           0.8572   \n",
       "3                  0.838112                           0.8364   \n",
       "4                  0.862608                           0.8588   \n",
       "\n",
       "   valid_dt_scores_accuracy_data_random  \\\n",
       "0                              0.827992   \n",
       "1                              0.852082   \n",
       "2                              0.881550   \n",
       "3                              0.844900   \n",
       "4                              0.872366   \n",
       "\n",
       "   valid_dt_scores_accuracy_data_random_median  valid_dt_scores_f1_score  \\\n",
       "0                                       0.8328                  0.814105   \n",
       "1                                       0.8547                  0.834163   \n",
       "2                                       0.8848                  0.853689   \n",
       "3                                       0.8417                  0.821327   \n",
       "4                                       0.8708                  0.849012   \n",
       "\n",
       "   valid_dt_scores_f1_score_median  valid_dt_scores_f1_score_data_random  \\\n",
       "0                         0.817887                              0.824589   \n",
       "1                         0.834104                              0.849903   \n",
       "2                         0.861308                              0.879857   \n",
       "3                         0.828912                              0.828311   \n",
       "4                         0.852081                              0.859110   \n",
       "\n",
       "   valid_dt_scores_f1_score_data_random_median  valid_dt_scores_runtime  \\\n",
       "0                                     0.829977                 0.021446   \n",
       "1                                     0.849988                 0.029598   \n",
       "2                                     0.883553                 0.036905   \n",
       "3                                     0.835338                 0.018182   \n",
       "4                                     0.859674                 0.021462   \n",
       "\n",
       "   valid_dt_scores_runtime_median  valid_inet_scores_soft_binary_crossentropy  \\\n",
       "0                        0.020898                                    0.619904   \n",
       "1                        0.027170                                    0.616877   \n",
       "2                        0.036879                                    0.617547   \n",
       "3                        0.017470                                    0.612247   \n",
       "4                        0.021297                                    0.605724   \n",
       "\n",
       "   valid_inet_scores_soft_binary_crossentropy_median  \\\n",
       "0                                           0.617766   \n",
       "1                                           0.620622   \n",
       "2                                           0.623748   \n",
       "3                                           0.613774   \n",
       "4                                           0.609405   \n",
       "\n",
       "   valid_inet_scores_binary_crossentropy  \\\n",
       "0                               0.574994   \n",
       "1                               0.570432   \n",
       "2                               0.571834   \n",
       "3                               0.571419   \n",
       "4                               0.555553   \n",
       "\n",
       "   valid_inet_scores_binary_crossentropy_median  valid_inet_scores_accuracy  \\\n",
       "0                                      0.573605                    0.702496   \n",
       "1                                      0.575686                    0.700600   \n",
       "2                                      0.579464                    0.709880   \n",
       "3                                      0.572374                    0.703800   \n",
       "4                                      0.562603                    0.714264   \n",
       "\n",
       "   valid_inet_scores_accuracy_median  valid_inet_scores_f1_score  \\\n",
       "0                             0.7152                    0.673612   \n",
       "1                             0.7136                    0.688397   \n",
       "2                             0.7084                    0.692539   \n",
       "3                             0.7056                    0.649521   \n",
       "4                             0.7112                    0.681811   \n",
       "\n",
       "   valid_inet_scores_f1_score_median  valid_inet_scores_runtime  \\\n",
       "0                           0.713898                   0.000886   \n",
       "1                           0.715229                   0.000795   \n",
       "2                           0.712673                   0.000936   \n",
       "3                           0.715474                   0.001154   \n",
       "4                           0.709280                   0.001203   \n",
       "\n",
       "   valid_inet_scores_runtime_median  test_dt_scores_soft_binary_crossentropy  \\\n",
       "0                          0.000886                                 0.523347   \n",
       "1                          0.000795                                 0.498293   \n",
       "2                          0.000936                                 0.479234   \n",
       "3                          0.001154                                 0.506019   \n",
       "4                          0.001203                                 0.480201   \n",
       "\n",
       "   test_dt_scores_soft_binary_crossentropy_median  \\\n",
       "0                                        0.524943   \n",
       "1                                        0.496365   \n",
       "2                                        0.471233   \n",
       "3                                        0.516472   \n",
       "4                                        0.490897   \n",
       "\n",
       "   test_dt_scores_soft_binary_crossentropy_data_random  \\\n",
       "0                                           0.518500     \n",
       "1                                           0.489628     \n",
       "2                                           0.464623     \n",
       "3                                           0.500962     \n",
       "4                                           0.472931     \n",
       "\n",
       "   test_dt_scores_soft_binary_crossentropy_data_random_median  \\\n",
       "0                                           0.513713            \n",
       "1                                           0.484901            \n",
       "2                                           0.459743            \n",
       "3                                           0.511956            \n",
       "4                                           0.484267            \n",
       "\n",
       "   test_dt_scores_binary_crossentropy  \\\n",
       "0                            0.421876   \n",
       "1                            0.382379   \n",
       "2                            0.376381   \n",
       "3                            0.386779   \n",
       "4                            0.343678   \n",
       "\n",
       "   test_dt_scores_binary_crossentropy_median  \\\n",
       "0                                   0.427665   \n",
       "1                                   0.378958   \n",
       "2                                   0.372756   \n",
       "3                                   0.406075   \n",
       "4                                   0.353273   \n",
       "\n",
       "   test_dt_scores_binary_crossentropy_data_random  \\\n",
       "0                                        0.403941   \n",
       "1                                        0.344514   \n",
       "2                                        0.288202   \n",
       "3                                        0.367982   \n",
       "4                                        0.307333   \n",
       "\n",
       "   test_dt_scores_binary_crossentropy_data_random_median  \\\n",
       "0                                           0.401487       \n",
       "1                                           0.342052       \n",
       "2                                           0.280073       \n",
       "3                                           0.392357       \n",
       "4                                           0.329922       \n",
       "\n",
       "   test_dt_scores_accuracy  test_dt_scores_accuracy_median  \\\n",
       "0                 0.807616                          0.8064   \n",
       "1                 0.831040                          0.8388   \n",
       "2                 0.848352                          0.8608   \n",
       "3                 0.825776                          0.8212   \n",
       "4                 0.847696                          0.8480   \n",
       "\n",
       "   test_dt_scores_accuracy_data_random  \\\n",
       "0                             0.816672   \n",
       "1                             0.847064   \n",
       "2                             0.876036   \n",
       "3                             0.834160   \n",
       "4                             0.860920   \n",
       "\n",
       "   test_dt_scores_accuracy_data_random_median  test_dt_scores_f1_score  \\\n",
       "0                                      0.8191                 0.795271   \n",
       "1                                      0.8513                 0.819418   \n",
       "2                                      0.8822                 0.840693   \n",
       "3                                      0.8281                 0.813966   \n",
       "4                                      0.8532                 0.839975   \n",
       "\n",
       "   test_dt_scores_f1_score_median  test_dt_scores_f1_score_data_random  \\\n",
       "0                        0.812604                             0.804644   \n",
       "1                        0.827358                             0.836161   \n",
       "2                        0.844355                             0.869102   \n",
       "3                        0.816357                             0.822841   \n",
       "4                        0.838571                             0.853453   \n",
       "\n",
       "   test_dt_scores_f1_score_data_random_median  test_dt_scores_runtime  \\\n",
       "0                                    0.816798                0.021521   \n",
       "1                                    0.844203                0.027813   \n",
       "2                                    0.877708                0.033396   \n",
       "3                                    0.824787                0.018056   \n",
       "4                                    0.853609                0.021356   \n",
       "\n",
       "   test_dt_scores_runtime_median  test_inet_scores_soft_binary_crossentropy  \\\n",
       "0                       0.021086                                   0.626373   \n",
       "1                       0.025573                                   0.622317   \n",
       "2                       0.031112                                   0.626444   \n",
       "3                       0.017498                                   0.611344   \n",
       "4                       0.021333                                   0.609198   \n",
       "\n",
       "   test_inet_scores_soft_binary_crossentropy_median  \\\n",
       "0                                          0.629011   \n",
       "1                                          0.622120   \n",
       "2                                          0.628567   \n",
       "3                                          0.617777   \n",
       "4                                          0.605301   \n",
       "\n",
       "   test_inet_scores_binary_crossentropy  \\\n",
       "0                              0.583992   \n",
       "1                              0.579808   \n",
       "2                              0.586809   \n",
       "3                              0.572884   \n",
       "4                              0.563761   \n",
       "\n",
       "   test_inet_scores_binary_crossentropy_median  test_inet_scores_accuracy  \\\n",
       "0                                     0.589657                   0.694880   \n",
       "1                                     0.576846                   0.698048   \n",
       "2                                     0.590938                   0.693312   \n",
       "3                                     0.580001                   0.703008   \n",
       "4                                     0.562193                   0.714608   \n",
       "\n",
       "   test_inet_scores_accuracy_median  test_inet_scores_f1_score  \\\n",
       "0                            0.6940                   0.669380   \n",
       "1                            0.6976                   0.675447   \n",
       "2                            0.6928                   0.674189   \n",
       "3                            0.6980                   0.659850   \n",
       "4                            0.7132                   0.689956   \n",
       "\n",
       "   test_inet_scores_f1_score_median  test_inet_scores_runtime  \\\n",
       "0                          0.723074                  0.001105   \n",
       "1                          0.710239                  0.001292   \n",
       "2                          0.703011                  0.001507   \n",
       "3                          0.701589                  0.002056   \n",
       "4                          0.724678                  0.002074   \n",
       "\n",
       "   test_inet_scores_runtime_median  \\\n",
       "0                         0.001105   \n",
       "1                         0.001292   \n",
       "2                         0.001507   \n",
       "3                         0.002056   \n",
       "4                         0.002074   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_adult_1000  \\\n",
       "0                                       0.676063   \n",
       "1                                       0.646430   \n",
       "2                                       0.633102   \n",
       "3                                       0.606532   \n",
       "4                                       0.646097   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_adult_1000  \\\n",
       "0                                           0.333621           \n",
       "1                                           0.332021           \n",
       "2                                           0.330991           \n",
       "3                                           0.343930           \n",
       "4                                           0.339806           \n",
       "\n",
       "   dt_scores_binary_crossentropy_adult_1000  \\\n",
       "0                                 10.156703   \n",
       "1                                 10.300082   \n",
       "2                                 10.818193   \n",
       "3                                  1.194771   \n",
       "4                                  2.083472   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_adult_1000  \\\n",
       "0                                           0.023854      \n",
       "1                                           0.011146      \n",
       "2                                           0.004090      \n",
       "3                                           0.029702      \n",
       "4                                           0.010342      \n",
       "\n",
       "   dt_scores_accuracy_adult_1000  dt_scores_accuracy_data_random_adult_1000  \\\n",
       "0                       0.592354                                      0.993   \n",
       "1                       0.686627                                      0.997   \n",
       "2                       0.686627                                      0.998   \n",
       "3                       0.722094                                      0.987   \n",
       "4                       0.698142                                      0.995   \n",
       "\n",
       "   dt_scores_f1_score_adult_1000  dt_scores_f1_score_data_random_adult_1000  \\\n",
       "0                       0.313421                                   0.996441   \n",
       "1                       0.312100                                   0.998470   \n",
       "2                       0.311636                                   0.998980   \n",
       "3                       0.473531                                   0.993296   \n",
       "4                       0.400975                                   0.997424   \n",
       "\n",
       "   dt_scores_runtime_adult_1000  \\\n",
       "0                      0.003428   \n",
       "1                      0.003833   \n",
       "2                      0.004539   \n",
       "3                      0.003951   \n",
       "4                      0.003832   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_adult_1000  \\\n",
       "0                                         0.909159   \n",
       "1                                         0.933302   \n",
       "2                                         0.836600   \n",
       "3                                         0.905225   \n",
       "4                                         0.853145   \n",
       "\n",
       "   inet_scores_binary_crossentropy_adult_1000  \\\n",
       "0                                    2.012290   \n",
       "1                                    0.000000   \n",
       "2                                    1.238022   \n",
       "3                                    1.944110   \n",
       "4                                    0.000000   \n",
       "\n",
       "   inet_scores_accuracy_adult_1000  inet_scores_f1_score_adult_1000  \\\n",
       "0                         0.382619                          0.55347   \n",
       "1                         0.382619                          0.55347   \n",
       "2                         0.382619                          0.55347   \n",
       "3                         0.382619                          0.55347   \n",
       "4                         0.382619                          0.55347   \n",
       "\n",
       "   inet_scores_runtime_adult_1000  \\\n",
       "0                        0.104282   \n",
       "1                        0.116253   \n",
       "2                        0.161816   \n",
       "3                        0.142096   \n",
       "4                        0.187451   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_titanic_1000  \\\n",
       "0                                         0.590181   \n",
       "1                                         0.628368   \n",
       "2                                         0.612476   \n",
       "3                                         0.598812   \n",
       "4                                         0.878277   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_titanic_1000  \\\n",
       "0                                           0.591582             \n",
       "1                                           0.574516             \n",
       "2                                           0.555402             \n",
       "3                                           0.507022             \n",
       "4                                           0.486462             \n",
       "\n",
       "   dt_scores_binary_crossentropy_titanic_1000  \\\n",
       "0                                    0.590905   \n",
       "1                                    0.670269   \n",
       "2                                    1.938406   \n",
       "3                                    2.277555   \n",
       "4                                   14.285952   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_titanic_1000  \\\n",
       "0                                           0.434295        \n",
       "1                                           0.364126        \n",
       "2                                           0.265439        \n",
       "3                                           0.270955        \n",
       "4                                           0.203904        \n",
       "\n",
       "   dt_scores_accuracy_titanic_1000  \\\n",
       "0                         0.765363   \n",
       "1                         0.625698   \n",
       "2                         0.743017   \n",
       "3                         0.843575   \n",
       "4                         0.357542   \n",
       "\n",
       "   dt_scores_accuracy_data_random_titanic_1000  \\\n",
       "0                                        0.802   \n",
       "1                                        0.844   \n",
       "2                                        0.888   \n",
       "3                                        0.863   \n",
       "4                                        0.921   \n",
       "\n",
       "   dt_scores_f1_score_titanic_1000  \\\n",
       "0                         0.752941   \n",
       "1                         0.496241   \n",
       "2                         0.640625   \n",
       "3                         0.820513   \n",
       "4                         0.371585   \n",
       "\n",
       "   dt_scores_f1_score_data_random_titanic_1000  \\\n",
       "0                                     0.819672   \n",
       "1                                     0.864583   \n",
       "2                                     0.900356   \n",
       "3                                     0.891871   \n",
       "4                                     0.930519   \n",
       "\n",
       "   dt_scores_runtime_titanic_1000  \\\n",
       "0                        0.003868   \n",
       "1                        0.004589   \n",
       "2                        0.005084   \n",
       "3                        0.003566   \n",
       "4                        0.004013   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_titanic_1000  \\\n",
       "0                                           0.685014   \n",
       "1                                           0.649897   \n",
       "2                                           0.713819   \n",
       "3                                           0.718173   \n",
       "4                                           0.673761   \n",
       "\n",
       "   inet_scores_binary_crossentropy_titanic_1000  \\\n",
       "0                                      0.692467   \n",
       "1                                      0.571653   \n",
       "2                                      0.766724   \n",
       "3                                      0.782591   \n",
       "4                                      0.672869   \n",
       "\n",
       "   inet_scores_accuracy_titanic_1000  inet_scores_f1_score_titanic_1000  \\\n",
       "0                           0.642458                           0.418182   \n",
       "1                           0.932961                           0.911765   \n",
       "2                           0.374302                           0.544715   \n",
       "3                           0.374302                           0.544715   \n",
       "4                           0.631285                           0.565789   \n",
       "\n",
       "   inet_scores_runtime_titanic_1000  \\\n",
       "0                          0.096975   \n",
       "1                          0.111433   \n",
       "2                          0.167118   \n",
       "3                          0.164211   \n",
       "4                          0.163254   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                           0.624773     \n",
       "1                                           0.645147     \n",
       "2                                           0.637858     \n",
       "3                                           0.617050     \n",
       "4                                           0.621212     \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000  \\\n",
       "0                                           0.435884                 \n",
       "1                                           0.430846                 \n",
       "2                                           0.425028                 \n",
       "3                                           0.426472                 \n",
       "4                                           0.424072                 \n",
       "\n",
       "   dt_scores_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                        0.472553   \n",
       "1                                        0.556817   \n",
       "2                                        2.884254   \n",
       "3                                        0.507695   \n",
       "4                                        1.211867   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_absenteeism_1000  \\\n",
       "0                                           0.155781            \n",
       "1                                           0.120251            \n",
       "2                                           0.071208            \n",
       "3                                           0.139921            \n",
       "4                                           0.122480            \n",
       "\n",
       "   dt_scores_accuracy_absenteeism_1000  \\\n",
       "0                             0.851351   \n",
       "1                             0.729730   \n",
       "2                             0.777027   \n",
       "3                             0.871622   \n",
       "4                             0.851351   \n",
       "\n",
       "   dt_scores_accuracy_data_random_absenteeism_1000  \\\n",
       "0                                            0.930   \n",
       "1                                            0.955   \n",
       "2                                            0.970   \n",
       "3                                            0.947   \n",
       "4                                            0.953   \n",
       "\n",
       "   dt_scores_f1_score_absenteeism_1000  \\\n",
       "0                             0.710526   \n",
       "1                             0.607843   \n",
       "2                             0.666667   \n",
       "3                             0.759494   \n",
       "4                             0.710526   \n",
       "\n",
       "   dt_scores_f1_score_data_random_absenteeism_1000  \\\n",
       "0                                         0.738806   \n",
       "1                                         0.820717   \n",
       "2                                         0.880952   \n",
       "3                                         0.897881   \n",
       "4                                         0.908023   \n",
       "\n",
       "   dt_scores_runtime_absenteeism_1000  \\\n",
       "0                            0.003999   \n",
       "1                            0.009758   \n",
       "2                            0.004967   \n",
       "3                            0.002132   \n",
       "4                            0.003116   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                           0.665557       \n",
       "1                                           0.668186       \n",
       "2                                           0.673644       \n",
       "3                                           0.672519       \n",
       "4                                           0.667948       \n",
       "\n",
       "   inet_scores_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                          0.624602   \n",
       "1                                          0.618528   \n",
       "2                                          0.652591   \n",
       "3                                          0.606104   \n",
       "4                                          0.592628   \n",
       "\n",
       "   inet_scores_accuracy_absenteeism_1000  \\\n",
       "0                               0.743243   \n",
       "1                               0.729730   \n",
       "2                               0.574324   \n",
       "3                               0.729730   \n",
       "4                               0.729730   \n",
       "\n",
       "   inet_scores_f1_score_absenteeism_1000  \\\n",
       "0                               0.486486   \n",
       "1                               0.000000   \n",
       "2                               0.322581   \n",
       "3                               0.000000   \n",
       "4                               0.000000   \n",
       "\n",
       "   inet_scores_runtime_absenteeism_1000  \\\n",
       "0                              0.110725   \n",
       "1                              0.125714   \n",
       "2                              0.207149   \n",
       "3                              0.135190   \n",
       "4                              0.153153   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_adult_10000  \\\n",
       "0                                        0.824417   \n",
       "1                                        0.765034   \n",
       "2                                        0.757829   \n",
       "3                                        0.773612   \n",
       "4                                        0.699088   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_adult_10000  \\\n",
       "0                                           0.339943            \n",
       "1                                           0.336949            \n",
       "2                                           0.334709            \n",
       "3                                           0.353635            \n",
       "4                                           0.349824            \n",
       "\n",
       "   dt_scores_binary_crossentropy_adult_10000  \\\n",
       "0                                   1.386253   \n",
       "1                                   0.982626   \n",
       "2                                   1.139937   \n",
       "3                                   1.059174   \n",
       "4                                   0.740372   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_adult_10000  \\\n",
       "0                                           0.045065       \n",
       "1                                           0.033669       \n",
       "2                                           0.023555       \n",
       "3                                           0.069665       \n",
       "4                                           0.057663       \n",
       "\n",
       "   dt_scores_accuracy_adult_10000  dt_scores_accuracy_data_random_adult_10000  \\\n",
       "0                        0.429295                                      0.9840   \n",
       "1                        0.466605                                      0.9877   \n",
       "2                        0.531552                                      0.9908   \n",
       "3                        0.455550                                      0.9701   \n",
       "4                        0.541379                                      0.9759   \n",
       "\n",
       "   dt_scores_f1_score_adult_10000  dt_scores_f1_score_data_random_adult_10000  \\\n",
       "0                        0.572218                                    0.991844   \n",
       "1                        0.586527                                    0.993710   \n",
       "2                        0.592058                                    0.995287   \n",
       "3                        0.582824                                    0.984545   \n",
       "4                        0.616412                                    0.987396   \n",
       "\n",
       "   dt_scores_runtime_adult_10000  \\\n",
       "0                       0.037807   \n",
       "1                       0.059062   \n",
       "2                       0.058041   \n",
       "3                       0.032153   \n",
       "4                       0.046304   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_adult_10000  \\\n",
       "0                                          0.909159   \n",
       "1                                          0.933302   \n",
       "2                                          0.836600   \n",
       "3                                          0.905225   \n",
       "4                                          0.853145   \n",
       "\n",
       "   inet_scores_binary_crossentropy_adult_10000  \\\n",
       "0                                     2.012290   \n",
       "1                                     0.000000   \n",
       "2                                     1.238022   \n",
       "3                                     1.944110   \n",
       "4                                     0.000000   \n",
       "\n",
       "   inet_scores_accuracy_adult_10000  inet_scores_f1_score_adult_10000  \\\n",
       "0                          0.382619                           0.55347   \n",
       "1                          0.382619                           0.55347   \n",
       "2                          0.382619                           0.55347   \n",
       "3                          0.382619                           0.55347   \n",
       "4                          0.382619                           0.55347   \n",
       "\n",
       "   inet_scores_runtime_adult_10000  \\\n",
       "0                         0.104282   \n",
       "1                         0.116253   \n",
       "2                         0.161816   \n",
       "3                         0.142096   \n",
       "4                         0.187451   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_titanic_10000  \\\n",
       "0                                          0.556268   \n",
       "1                                          0.558729   \n",
       "2                                          0.604942   \n",
       "3                                          0.573358   \n",
       "4                                          0.585473   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_titanic_10000  \\\n",
       "0                                           0.597483              \n",
       "1                                           0.580193              \n",
       "2                                           0.566556              \n",
       "3                                           0.512211              \n",
       "4                                           0.493887              \n",
       "\n",
       "   dt_scores_binary_crossentropy_titanic_10000  \\\n",
       "0                                     0.363571   \n",
       "1                                     0.361183   \n",
       "2                                     0.513529   \n",
       "3                                     0.453806   \n",
       "4                                     0.513521   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_titanic_10000  \\\n",
       "0                                           0.473530         \n",
       "1                                           0.421473         \n",
       "2                                           0.374040         \n",
       "3                                           0.299919         \n",
       "4                                           0.249480         \n",
       "\n",
       "   dt_scores_accuracy_titanic_10000  \\\n",
       "0                          0.882682   \n",
       "1                          0.815642   \n",
       "2                          0.882682   \n",
       "3                          0.849162   \n",
       "4                          0.731844   \n",
       "\n",
       "   dt_scores_accuracy_data_random_titanic_10000  \\\n",
       "0                                        0.7838   \n",
       "1                                        0.8057   \n",
       "2                                        0.8295   \n",
       "3                                        0.8619   \n",
       "4                                        0.8851   \n",
       "\n",
       "   dt_scores_f1_score_titanic_10000  \\\n",
       "0                          0.857143   \n",
       "1                          0.744186   \n",
       "2                          0.859060   \n",
       "3                          0.825806   \n",
       "4                          0.578947   \n",
       "\n",
       "   dt_scores_f1_score_data_random_titanic_10000  \\\n",
       "0                                      0.805050   \n",
       "1                                      0.837691   \n",
       "2                                      0.849952   \n",
       "3                                      0.884096   \n",
       "4                                      0.892987   \n",
       "\n",
       "   dt_scores_runtime_titanic_10000  \\\n",
       "0                         0.036790   \n",
       "1                         0.048559   \n",
       "2                         0.060057   \n",
       "3                         0.036592   \n",
       "4                         0.034735   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_titanic_10000  \\\n",
       "0                                           0.685014    \n",
       "1                                           0.649897    \n",
       "2                                           0.713819    \n",
       "3                                           0.718173    \n",
       "4                                           0.673761    \n",
       "\n",
       "   inet_scores_binary_crossentropy_titanic_10000  \\\n",
       "0                                       0.692467   \n",
       "1                                       0.571653   \n",
       "2                                       0.766724   \n",
       "3                                       0.782591   \n",
       "4                                       0.672869   \n",
       "\n",
       "   inet_scores_accuracy_titanic_10000  inet_scores_f1_score_titanic_10000  \\\n",
       "0                            0.642458                            0.418182   \n",
       "1                            0.932961                            0.911765   \n",
       "2                            0.374302                            0.544715   \n",
       "3                            0.374302                            0.544715   \n",
       "4                            0.631285                            0.565789   \n",
       "\n",
       "   inet_scores_runtime_titanic_10000  \\\n",
       "0                           0.096975   \n",
       "1                           0.111433   \n",
       "2                           0.167118   \n",
       "3                           0.164211   \n",
       "4                           0.163254   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                           0.636781      \n",
       "1                                           0.645694      \n",
       "2                                           0.656525      \n",
       "3                                           0.624068      \n",
       "4                                           0.634266      \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_absenteeism_10000  \\\n",
       "0                                           0.440972                  \n",
       "1                                           0.435666                  \n",
       "2                                           0.432403                  \n",
       "3                                           0.428355                  \n",
       "4                                           0.425474                  \n",
       "\n",
       "   dt_scores_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                         0.523597   \n",
       "1                                         0.624696   \n",
       "2                                         0.656106   \n",
       "3                                         0.497640   \n",
       "4                                         0.494213   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_absenteeism_10000  \\\n",
       "0                                           0.179272             \n",
       "1                                           0.157809             \n",
       "2                                           0.138408             \n",
       "3                                           0.139317             \n",
       "4                                           0.121895             \n",
       "\n",
       "   dt_scores_accuracy_absenteeism_10000  \\\n",
       "0                              0.851351   \n",
       "1                              0.716216   \n",
       "2                              0.709459   \n",
       "3                              0.851351   \n",
       "4                              0.689189   \n",
       "\n",
       "   dt_scores_accuracy_data_random_absenteeism_10000  \\\n",
       "0                                            0.9243   \n",
       "1                                            0.9349   \n",
       "2                                            0.9396   \n",
       "3                                            0.9466   \n",
       "4                                            0.9471   \n",
       "\n",
       "   dt_scores_f1_score_absenteeism_10000  \\\n",
       "0                              0.710526   \n",
       "1                              0.596154   \n",
       "2                              0.605505   \n",
       "3                              0.710526   \n",
       "4                              0.540000   \n",
       "\n",
       "   dt_scores_f1_score_data_random_absenteeism_10000  \\\n",
       "0                                          0.698286   \n",
       "1                                          0.737817   \n",
       "2                                          0.765710   \n",
       "3                                          0.899397   \n",
       "4                                          0.908746   \n",
       "\n",
       "   dt_scores_runtime_absenteeism_10000  \\\n",
       "0                             0.039880   \n",
       "1                             0.085963   \n",
       "2                             0.065024   \n",
       "3                             0.027850   \n",
       "4                             0.027859   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                           0.665557        \n",
       "1                                           0.668186        \n",
       "2                                           0.673644        \n",
       "3                                           0.672519        \n",
       "4                                           0.667948        \n",
       "\n",
       "   inet_scores_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                           0.624602   \n",
       "1                                           0.618528   \n",
       "2                                           0.652591   \n",
       "3                                           0.606104   \n",
       "4                                           0.592628   \n",
       "\n",
       "   inet_scores_accuracy_absenteeism_10000  \\\n",
       "0                                0.743243   \n",
       "1                                0.729730   \n",
       "2                                0.574324   \n",
       "3                                0.729730   \n",
       "4                                0.729730   \n",
       "\n",
       "   inet_scores_f1_score_absenteeism_10000  \\\n",
       "0                                0.486486   \n",
       "1                                0.000000   \n",
       "2                                0.322581   \n",
       "3                                0.000000   \n",
       "4                                0.000000   \n",
       "\n",
       "   inet_scores_runtime_absenteeism_10000  \\\n",
       "0                               0.110725   \n",
       "1                               0.125714   \n",
       "2                               0.207149   \n",
       "3                               0.135190   \n",
       "4                               0.153153   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_adult_100000  \\\n",
       "0                                         0.814428   \n",
       "1                                         0.745745   \n",
       "2                                         0.684232   \n",
       "3                                         0.764616   \n",
       "4                                         0.692830   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_adult_100000  \\\n",
       "0                                           0.341439             \n",
       "1                                           0.338689             \n",
       "2                                           0.336940             \n",
       "3                                           0.355650             \n",
       "4                                           0.351874             \n",
       "\n",
       "   dt_scores_binary_crossentropy_adult_100000  \\\n",
       "0                                    1.424239   \n",
       "1                                    0.958959   \n",
       "2                                    0.784067   \n",
       "3                                    1.034835   \n",
       "4                                    0.721504   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_adult_100000  \\\n",
       "0                                           0.049010        \n",
       "1                                           0.039931        \n",
       "2                                           0.033537        \n",
       "3                                           0.076282        \n",
       "4                                           0.065143        \n",
       "\n",
       "   dt_scores_accuracy_adult_100000  \\\n",
       "0                         0.477353   \n",
       "1                         0.544603   \n",
       "2                         0.541225   \n",
       "3                         0.484723   \n",
       "4                         0.550438   \n",
       "\n",
       "   dt_scores_accuracy_data_random_adult_100000  \\\n",
       "0                                      0.98186   \n",
       "1                                      0.98464   \n",
       "2                                      0.98651   \n",
       "3                                      0.96746   \n",
       "4                                      0.97234   \n",
       "\n",
       "   dt_scores_f1_score_adult_100000  \\\n",
       "0                         0.592726   \n",
       "1                         0.619646   \n",
       "2                         0.620812   \n",
       "3                         0.595273   \n",
       "4                         0.623069   \n",
       "\n",
       "   dt_scores_f1_score_data_random_adult_100000  \\\n",
       "0                                     0.990746   \n",
       "1                                     0.992122   \n",
       "2                                     0.993110   \n",
       "3                                     0.983126   \n",
       "4                                     0.985547   \n",
       "\n",
       "   dt_scores_runtime_adult_100000  \\\n",
       "0                        0.470477   \n",
       "1                        0.707219   \n",
       "2                        0.781147   \n",
       "3                        0.414240   \n",
       "4                        0.559095   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_adult_100000  \\\n",
       "0                                           0.909159   \n",
       "1                                           0.933302   \n",
       "2                                           0.836600   \n",
       "3                                           0.905225   \n",
       "4                                           0.853145   \n",
       "\n",
       "   inet_scores_binary_crossentropy_adult_100000  \\\n",
       "0                                      2.012290   \n",
       "1                                      0.000000   \n",
       "2                                      1.238022   \n",
       "3                                      1.944110   \n",
       "4                                      0.000000   \n",
       "\n",
       "   inet_scores_accuracy_adult_100000  inet_scores_f1_score_adult_100000  \\\n",
       "0                           0.382619                            0.55347   \n",
       "1                           0.382619                            0.55347   \n",
       "2                           0.382619                            0.55347   \n",
       "3                           0.382619                            0.55347   \n",
       "4                           0.382619                            0.55347   \n",
       "\n",
       "   inet_scores_runtime_adult_100000  \\\n",
       "0                          0.104282   \n",
       "1                          0.116253   \n",
       "2                          0.161816   \n",
       "3                          0.142096   \n",
       "4                          0.187451   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_titanic_100000  \\\n",
       "0                                           0.563472   \n",
       "1                                           0.570864   \n",
       "2                                           0.562121   \n",
       "3                                           0.631198   \n",
       "4                                           0.636763   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_titanic_100000  \\\n",
       "0                                           0.603581               \n",
       "1                                           0.585135               \n",
       "2                                           0.573012               \n",
       "3                                           0.513346               \n",
       "4                                           0.497750               \n",
       "\n",
       "   dt_scores_binary_crossentropy_titanic_100000  \\\n",
       "0                                      0.380740   \n",
       "1                                      0.453769   \n",
       "2                                      0.436157   \n",
       "3                                      0.627517   \n",
       "4                                      0.703895   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_titanic_100000  \\\n",
       "0                                           0.492859          \n",
       "1                                           0.440922          \n",
       "2                                           0.401912          \n",
       "3                                           0.304766          \n",
       "4                                           0.261732          \n",
       "\n",
       "   dt_scores_accuracy_titanic_100000  \\\n",
       "0                           0.877095   \n",
       "1                           0.737430   \n",
       "2                           0.871508   \n",
       "3                           0.692737   \n",
       "4                           0.743017   \n",
       "\n",
       "   dt_scores_accuracy_data_random_titanic_100000  \\\n",
       "0                                        0.76884   \n",
       "1                                        0.79334   \n",
       "2                                        0.81728   \n",
       "3                                        0.86083   \n",
       "4                                        0.88517   \n",
       "\n",
       "   dt_scores_f1_score_titanic_100000  \\\n",
       "0                           0.851351   \n",
       "1                           0.675862   \n",
       "2                           0.834532   \n",
       "3                           0.545455   \n",
       "4                           0.589286   \n",
       "\n",
       "   dt_scores_f1_score_data_random_titanic_100000  \\\n",
       "0                                       0.788945   \n",
       "1                                       0.815627   \n",
       "2                                       0.830827   \n",
       "3                                       0.882651   \n",
       "4                                       0.892607   \n",
       "\n",
       "   dt_scores_runtime_titanic_100000  \\\n",
       "0                          0.457628   \n",
       "1                          0.780087   \n",
       "2                          0.686671   \n",
       "3                          0.473240   \n",
       "4                          0.554929   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_titanic_100000  \\\n",
       "0                                           0.685014     \n",
       "1                                           0.649897     \n",
       "2                                           0.713819     \n",
       "3                                           0.718173     \n",
       "4                                           0.673761     \n",
       "\n",
       "   inet_scores_binary_crossentropy_titanic_100000  \\\n",
       "0                                        0.692467   \n",
       "1                                        0.571653   \n",
       "2                                        0.766724   \n",
       "3                                        0.782591   \n",
       "4                                        0.672869   \n",
       "\n",
       "   inet_scores_accuracy_titanic_100000  inet_scores_f1_score_titanic_100000  \\\n",
       "0                             0.642458                             0.418182   \n",
       "1                             0.932961                             0.911765   \n",
       "2                             0.374302                             0.544715   \n",
       "3                             0.374302                             0.544715   \n",
       "4                             0.631285                             0.565789   \n",
       "\n",
       "   inet_scores_runtime_titanic_100000  \\\n",
       "0                            0.096975   \n",
       "1                            0.111433   \n",
       "2                            0.167118   \n",
       "3                            0.164211   \n",
       "4                            0.163254   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                           0.621059       \n",
       "1                                           0.631943       \n",
       "2                                           0.631208       \n",
       "3                                           0.627679       \n",
       "4                                           0.640689       \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_absenteeism_100000  \\\n",
       "0                                           0.443937                   \n",
       "1                                           0.438652                   \n",
       "2                                           0.435106                   \n",
       "3                                           0.430359                   \n",
       "4                                           0.427252                   \n",
       "\n",
       "   dt_scores_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                          0.444783   \n",
       "1                                          0.517273   \n",
       "2                                          0.485480   \n",
       "3                                          0.522701   \n",
       "4                                          0.523307   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_absenteeism_100000  \\\n",
       "0                                           0.188583              \n",
       "1                                           0.167041              \n",
       "2                                           0.150556              \n",
       "3                                           0.138477              \n",
       "4                                           0.121175              \n",
       "\n",
       "   dt_scores_accuracy_absenteeism_100000  \\\n",
       "0                               0.891892   \n",
       "1                               0.641892   \n",
       "2                               0.871622   \n",
       "3                               0.851351   \n",
       "4                               0.689189   \n",
       "\n",
       "   dt_scores_accuracy_data_random_absenteeism_100000  \\\n",
       "0                                            0.92095   \n",
       "1                                            0.93158   \n",
       "2                                            0.93382   \n",
       "3                                            0.94427   \n",
       "4                                            0.94939   \n",
       "\n",
       "   dt_scores_f1_score_absenteeism_100000  \\\n",
       "0                               0.771429   \n",
       "1                               0.561983   \n",
       "2                               0.746667   \n",
       "3                               0.710526   \n",
       "4                               0.540000   \n",
       "\n",
       "   dt_scores_f1_score_data_random_absenteeism_100000  \\\n",
       "0                                           0.680516   \n",
       "1                                           0.740085   \n",
       "2                                           0.739346   \n",
       "3                                           0.896951   \n",
       "4                                           0.913842   \n",
       "\n",
       "   dt_scores_runtime_absenteeism_100000  \\\n",
       "0                              0.491968   \n",
       "1                              0.628368   \n",
       "2                              0.770864   \n",
       "3                              0.320349   \n",
       "4                              0.363112   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                           0.665557         \n",
       "1                                           0.668186         \n",
       "2                                           0.673644         \n",
       "3                                           0.672519         \n",
       "4                                           0.667948         \n",
       "\n",
       "   inet_scores_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                           0.624602    \n",
       "1                                           0.618528    \n",
       "2                                           0.652591    \n",
       "3                                           0.606104    \n",
       "4                                           0.592628    \n",
       "\n",
       "   inet_scores_accuracy_absenteeism_100000  \\\n",
       "0                                 0.743243   \n",
       "1                                 0.729730   \n",
       "2                                 0.574324   \n",
       "3                                 0.729730   \n",
       "4                                 0.729730   \n",
       "\n",
       "   inet_scores_f1_score_absenteeism_100000  \\\n",
       "0                                 0.486486   \n",
       "1                                 0.000000   \n",
       "2                                 0.322581   \n",
       "3                                 0.000000   \n",
       "4                                 0.000000   \n",
       "\n",
       "   inet_scores_runtime_absenteeism_100000  \\\n",
       "0                                0.110725   \n",
       "1                                0.125714   \n",
       "2                                0.207149   \n",
       "3                                0.135190   \n",
       "4                                0.153153   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_adult_1000000  \\\n",
       "0                                          0.831044   \n",
       "1                                          0.751342   \n",
       "2                                          0.686935   \n",
       "3                                          0.748558   \n",
       "4                                          0.675718   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_adult_1000000  \\\n",
       "0                                           0.341260              \n",
       "1                                           0.338606              \n",
       "2                                           0.337091              \n",
       "3                                           0.355464              \n",
       "4                                           0.351683              \n",
       "\n",
       "   dt_scores_binary_crossentropy_adult_1000000  \\\n",
       "0                                     1.510337   \n",
       "1                                     0.965319   \n",
       "2                                     0.807044   \n",
       "3                                     0.977142   \n",
       "4                                     0.675149   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_adult_1000000  \\\n",
       "0                                           0.048158         \n",
       "1                                           0.040026         \n",
       "2                                           0.034885         \n",
       "3                                           0.077190         \n",
       "4                                           0.066051         \n",
       "\n",
       "   dt_scores_accuracy_adult_1000000  \\\n",
       "0                          0.419929   \n",
       "1                          0.560571   \n",
       "2                          0.557807   \n",
       "3                          0.489483   \n",
       "4                          0.591279   \n",
       "\n",
       "   dt_scores_accuracy_data_random_adult_1000000  \\\n",
       "0                                      0.981878   \n",
       "1                                      0.983952   \n",
       "2                                      0.986639   \n",
       "3                                      0.966564   \n",
       "4                                      0.973342   \n",
       "\n",
       "   dt_scores_f1_score_adult_1000000  \\\n",
       "0                          0.568623   \n",
       "1                          0.625980   \n",
       "2                          0.628866   \n",
       "3                          0.597019   \n",
       "4                          0.637428   \n",
       "\n",
       "   dt_scores_f1_score_data_random_adult_1000000  \\\n",
       "0                                      0.990764   \n",
       "1                                      0.991759   \n",
       "2                                      0.993170   \n",
       "3                                      0.982700   \n",
       "4                                      0.986045   \n",
       "\n",
       "   dt_scores_runtime_adult_1000000  \\\n",
       "0                         6.078736   \n",
       "1                         8.948903   \n",
       "2                        10.499663   \n",
       "3                         6.422559   \n",
       "4                         7.925493   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_adult_1000000  \\\n",
       "0                                           0.909159    \n",
       "1                                           0.933302    \n",
       "2                                           0.836600    \n",
       "3                                           0.905225    \n",
       "4                                           0.853145    \n",
       "\n",
       "   inet_scores_binary_crossentropy_adult_1000000  \\\n",
       "0                                       2.012290   \n",
       "1                                       0.000000   \n",
       "2                                       1.238022   \n",
       "3                                       1.944110   \n",
       "4                                       0.000000   \n",
       "\n",
       "   inet_scores_accuracy_adult_1000000  inet_scores_f1_score_adult_1000000  \\\n",
       "0                            0.382619                             0.55347   \n",
       "1                            0.382619                             0.55347   \n",
       "2                            0.382619                             0.55347   \n",
       "3                            0.382619                             0.55347   \n",
       "4                            0.382619                             0.55347   \n",
       "\n",
       "   inet_scores_runtime_adult_1000000  \\\n",
       "0                           0.104282   \n",
       "1                           0.116253   \n",
       "2                           0.161816   \n",
       "3                           0.142096   \n",
       "4                           0.187451   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_titanic_1000000  \\\n",
       "0                                           0.563204    \n",
       "1                                           0.564544    \n",
       "2                                           0.562387    \n",
       "3                                           0.633388    \n",
       "4                                           0.635056    \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_titanic_1000000  \\\n",
       "0                                           0.604133                \n",
       "1                                           0.585896                \n",
       "2                                           0.573653                \n",
       "3                                           0.514233                \n",
       "4                                           0.498131                \n",
       "\n",
       "   dt_scores_binary_crossentropy_titanic_1000000  \\\n",
       "0                                       0.379736   \n",
       "1                                       0.411363   \n",
       "2                                       0.422405   \n",
       "3                                       0.638453   \n",
       "4                                       0.668259   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_titanic_1000000  \\\n",
       "0                                           0.494743           \n",
       "1                                           0.443391           \n",
       "2                                           0.405561           \n",
       "3                                           0.308172           \n",
       "4                                           0.263208           \n",
       "\n",
       "   dt_scores_accuracy_titanic_1000000  \\\n",
       "0                            0.877095   \n",
       "1                            0.737430   \n",
       "2                            0.871508   \n",
       "3                            0.692737   \n",
       "4                            0.731844   \n",
       "\n",
       "   dt_scores_accuracy_data_random_titanic_1000000  \\\n",
       "0                                        0.766630   \n",
       "1                                        0.792173   \n",
       "2                                        0.814067   \n",
       "3                                        0.860024   \n",
       "4                                        0.886782   \n",
       "\n",
       "   dt_scores_f1_score_titanic_1000000  \\\n",
       "0                            0.851351   \n",
       "1                            0.675862   \n",
       "2                            0.834532   \n",
       "3                            0.545455   \n",
       "4                            0.578947   \n",
       "\n",
       "   dt_scores_f1_score_data_random_titanic_1000000  \\\n",
       "0                                        0.785663   \n",
       "1                                        0.814393   \n",
       "2                                        0.828896   \n",
       "3                                        0.881758   \n",
       "4                                        0.894634   \n",
       "\n",
       "   dt_scores_runtime_titanic_1000000  \\\n",
       "0                           6.039980   \n",
       "1                           8.988415   \n",
       "2                           9.653604   \n",
       "3                           6.648989   \n",
       "4                           7.662928   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_titanic_1000000  \\\n",
       "0                                           0.685014      \n",
       "1                                           0.649897      \n",
       "2                                           0.713819      \n",
       "3                                           0.718173      \n",
       "4                                           0.673761      \n",
       "\n",
       "   inet_scores_binary_crossentropy_titanic_1000000  \\\n",
       "0                                         0.692467   \n",
       "1                                         0.571653   \n",
       "2                                         0.766724   \n",
       "3                                         0.782591   \n",
       "4                                         0.672869   \n",
       "\n",
       "   inet_scores_accuracy_titanic_1000000  inet_scores_f1_score_titanic_1000000  \\\n",
       "0                              0.642458                              0.418182   \n",
       "1                              0.932961                              0.911765   \n",
       "2                              0.374302                              0.544715   \n",
       "3                              0.374302                              0.544715   \n",
       "4                              0.631285                              0.565789   \n",
       "\n",
       "   inet_scores_runtime_titanic_1000000  \\\n",
       "0                             0.096975   \n",
       "1                             0.111433   \n",
       "2                             0.167118   \n",
       "3                             0.164211   \n",
       "4                             0.163254   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                           0.631333        \n",
       "1                                           0.635391        \n",
       "2                                           0.644520        \n",
       "3                                           0.627118        \n",
       "4                                           0.640127        \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_absenteeism_1000000  \\\n",
       "0                                           0.443815                    \n",
       "1                                           0.438672                    \n",
       "2                                           0.435362                    \n",
       "3                                           0.430960                    \n",
       "4                                           0.427879                    \n",
       "\n",
       "   dt_scores_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                           0.484076   \n",
       "1                                           0.511929   \n",
       "2                                           0.540236   \n",
       "3                                           0.524895   \n",
       "4                                           0.549056   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_absenteeism_1000000  \\\n",
       "0                                           0.188630               \n",
       "1                                           0.167892               \n",
       "2                                           0.152760               \n",
       "3                                           0.140045               \n",
       "4                                           0.123195               \n",
       "\n",
       "   dt_scores_accuracy_absenteeism_1000000  \\\n",
       "0                                0.851351   \n",
       "1                                0.608108   \n",
       "2                                0.702703   \n",
       "3                                0.851351   \n",
       "4                                0.689189   \n",
       "\n",
       "   dt_scores_accuracy_data_random_absenteeism_1000000  \\\n",
       "0                                           0.920433    \n",
       "1                                           0.928820    \n",
       "2                                           0.934484    \n",
       "3                                           0.944159    \n",
       "4                                           0.948687    \n",
       "\n",
       "   dt_scores_f1_score_absenteeism_1000000  \\\n",
       "0                                0.710526   \n",
       "1                                0.408163   \n",
       "2                                0.531915   \n",
       "3                                0.710526   \n",
       "4                                0.540000   \n",
       "\n",
       "   dt_scores_f1_score_data_random_absenteeism_1000000  \\\n",
       "0                                           0.682028    \n",
       "1                                           0.727265    \n",
       "2                                           0.733373    \n",
       "3                                           0.896894    \n",
       "4                                           0.912878    \n",
       "\n",
       "   dt_scores_runtime_absenteeism_1000000  \\\n",
       "0                               6.398765   \n",
       "1                               8.718359   \n",
       "2                              10.151225   \n",
       "3                               4.305763   \n",
       "4                               4.915602   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                           0.665557          \n",
       "1                                           0.668186          \n",
       "2                                           0.673644          \n",
       "3                                           0.672519          \n",
       "4                                           0.667948          \n",
       "\n",
       "   inet_scores_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                           0.624602     \n",
       "1                                           0.618528     \n",
       "2                                           0.652591     \n",
       "3                                           0.606104     \n",
       "4                                           0.592628     \n",
       "\n",
       "   inet_scores_accuracy_absenteeism_1000000  \\\n",
       "0                                  0.743243   \n",
       "1                                  0.729730   \n",
       "2                                  0.574324   \n",
       "3                                  0.729730   \n",
       "4                                  0.729730   \n",
       "\n",
       "   inet_scores_f1_score_absenteeism_1000000  \\\n",
       "0                                  0.486486   \n",
       "1                                  0.000000   \n",
       "2                                  0.322581   \n",
       "3                                  0.000000   \n",
       "4                                  0.000000   \n",
       "\n",
       "   inet_scores_runtime_absenteeism_1000000  \\\n",
       "0                                 0.110725   \n",
       "1                                 0.125714   \n",
       "2                                 0.207149   \n",
       "3                                 0.135190   \n",
       "4                                 0.153153   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                           0.509151     \n",
       "1                                           0.499393     \n",
       "2                                           0.492986     \n",
       "3                                           0.509151     \n",
       "4                                           0.499393     \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_adult_TRAIN_DATA  \\\n",
       "0                                           0.514619                 \n",
       "1                                           0.504802                 \n",
       "2                                           0.498856                 \n",
       "3                                           0.514619                 \n",
       "4                                           0.504802                 \n",
       "\n",
       "   dt_scores_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                        0.241589   \n",
       "1                                        0.185063   \n",
       "2                                        0.175076   \n",
       "3                                        0.241589   \n",
       "4                                        0.185063   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_adult_TRAIN_DATA  \\\n",
       "0                                           0.232145            \n",
       "1                                           0.176034            \n",
       "2                                           0.135784            \n",
       "3                                           0.232145            \n",
       "4                                           0.176034            \n",
       "\n",
       "   dt_scores_accuracy_adult_TRAIN_DATA  \\\n",
       "0                             0.902349   \n",
       "1                             0.934899   \n",
       "2                             0.943498   \n",
       "3                             0.902349   \n",
       "4                             0.934899   \n",
       "\n",
       "   dt_scores_accuracy_data_random_adult_TRAIN_DATA  \\\n",
       "0                                         0.913053   \n",
       "1                                         0.940567   \n",
       "2                                         0.948713   \n",
       "3                                         0.913053   \n",
       "4                                         0.940567   \n",
       "\n",
       "   dt_scores_f1_score_adult_TRAIN_DATA  \\\n",
       "0                             0.882222   \n",
       "1                             0.914859   \n",
       "2                             0.924990   \n",
       "3                             0.882222   \n",
       "4                             0.914859   \n",
       "\n",
       "   dt_scores_f1_score_data_random_adult_TRAIN_DATA  \\\n",
       "0                                         0.924979   \n",
       "1                                         0.945923   \n",
       "2                                         0.953017   \n",
       "3                                         0.924979   \n",
       "4                                         0.945923   \n",
       "\n",
       "   dt_scores_runtime_adult_TRAIN_DATA  \\\n",
       "0                            0.026223   \n",
       "1                            0.035989   \n",
       "2                            0.043957   \n",
       "3                            0.043410   \n",
       "4                            0.050534   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                           0.909159       \n",
       "1                                           0.933302       \n",
       "2                                           0.836600       \n",
       "3                                           0.905225       \n",
       "4                                           0.853145       \n",
       "\n",
       "   inet_scores_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                          2.012290   \n",
       "1                                          0.000000   \n",
       "2                                          1.238022   \n",
       "3                                          1.944110   \n",
       "4                                          0.000000   \n",
       "\n",
       "   inet_scores_accuracy_adult_TRAIN_DATA  \\\n",
       "0                               0.382619   \n",
       "1                               0.382619   \n",
       "2                               0.382619   \n",
       "3                               0.382619   \n",
       "4                               0.382619   \n",
       "\n",
       "   inet_scores_f1_score_adult_TRAIN_DATA  \\\n",
       "0                                0.55347   \n",
       "1                                0.55347   \n",
       "2                                0.55347   \n",
       "3                                0.55347   \n",
       "4                                0.55347   \n",
       "\n",
       "   inet_scores_runtime_adult_TRAIN_DATA  \\\n",
       "0                              0.104282   \n",
       "1                              0.116253   \n",
       "2                              0.161816   \n",
       "3                              0.142096   \n",
       "4                              0.187451   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                           0.501347       \n",
       "1                                           0.502917       \n",
       "2                                           0.493192       \n",
       "3                                           0.501347       \n",
       "4                                           0.502917       \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_titanic_TRAIN_DATA  \\\n",
       "0                                           0.501361                   \n",
       "1                                           0.501101                   \n",
       "2                                           0.493345                   \n",
       "3                                           0.501361                   \n",
       "4                                           0.501101                   \n",
       "\n",
       "   dt_scores_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                          0.176710   \n",
       "1                                          0.161456   \n",
       "2                                          0.444215   \n",
       "3                                          0.176710   \n",
       "4                                          0.161456   \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_titanic_TRAIN_DATA  \\\n",
       "0                                           0.114649              \n",
       "1                                           0.084286              \n",
       "2                                           0.039162              \n",
       "3                                           0.114649              \n",
       "4                                           0.084286              \n",
       "\n",
       "   dt_scores_accuracy_titanic_TRAIN_DATA  \\\n",
       "0                               0.960894   \n",
       "1                               0.960894   \n",
       "2                               0.938547   \n",
       "3                               0.960894   \n",
       "4                               0.960894   \n",
       "\n",
       "   dt_scores_accuracy_data_random_titanic_TRAIN_DATA  \\\n",
       "0                                           0.971880   \n",
       "1                                           0.971880   \n",
       "2                                           0.977153   \n",
       "3                                           0.971880   \n",
       "4                                           0.971880   \n",
       "\n",
       "   dt_scores_f1_score_titanic_TRAIN_DATA  \\\n",
       "0                               0.947368   \n",
       "1                               0.947368   \n",
       "2                               0.910569   \n",
       "3                               0.947368   \n",
       "4                               0.947368   \n",
       "\n",
       "   dt_scores_f1_score_data_random_titanic_TRAIN_DATA  \\\n",
       "0                                           0.956284   \n",
       "1                                           0.956284   \n",
       "2                                           0.962536   \n",
       "3                                           0.956284   \n",
       "4                                           0.956284   \n",
       "\n",
       "   dt_scores_runtime_titanic_TRAIN_DATA  \\\n",
       "0                              0.000762   \n",
       "1                              0.000928   \n",
       "2                              0.001271   \n",
       "3                              0.001347   \n",
       "4                              0.001466   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                           0.685014         \n",
       "1                                           0.649897         \n",
       "2                                           0.713819         \n",
       "3                                           0.718173         \n",
       "4                                           0.673761         \n",
       "\n",
       "   inet_scores_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                           0.692467    \n",
       "1                                           0.571653    \n",
       "2                                           0.766724    \n",
       "3                                           0.782591    \n",
       "4                                           0.672869    \n",
       "\n",
       "   inet_scores_accuracy_titanic_TRAIN_DATA  \\\n",
       "0                                 0.642458   \n",
       "1                                 0.932961   \n",
       "2                                 0.374302   \n",
       "3                                 0.374302   \n",
       "4                                 0.631285   \n",
       "\n",
       "   inet_scores_f1_score_titanic_TRAIN_DATA  \\\n",
       "0                                 0.418182   \n",
       "1                                 0.911765   \n",
       "2                                 0.544715   \n",
       "3                                 0.544715   \n",
       "4                                 0.565789   \n",
       "\n",
       "   inet_scores_runtime_titanic_TRAIN_DATA  \\\n",
       "0                                0.096975   \n",
       "1                                0.111433   \n",
       "2                                0.167118   \n",
       "3                                0.164211   \n",
       "4                                0.163254   \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.606344           \n",
       "1                                           0.603373           \n",
       "2                                           0.602736           \n",
       "3                                           0.606344           \n",
       "4                                           0.603373           \n",
       "\n",
       "   dt_scores_soft_binary_crossentropy_data_random_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.605437                       \n",
       "1                                           0.602921                       \n",
       "2                                           0.593692                       \n",
       "3                                           0.605437                       \n",
       "4                                           0.602921                       \n",
       "\n",
       "   dt_scores_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.258774      \n",
       "1                                           0.403781      \n",
       "2                                           0.614574      \n",
       "3                                           0.258774      \n",
       "4                                           0.403781      \n",
       "\n",
       "   dt_scores_binary_crossentropy_data_random_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.275451                  \n",
       "1                                           0.195279                  \n",
       "2                                           0.132693                  \n",
       "3                                           0.275451                  \n",
       "4                                           0.195279                  \n",
       "\n",
       "   dt_scores_accuracy_absenteeism_TRAIN_DATA  \\\n",
       "0                                   0.918919   \n",
       "1                                   0.945946   \n",
       "2                                   0.945946   \n",
       "3                                   0.918919   \n",
       "4                                   0.945946   \n",
       "\n",
       "   dt_scores_accuracy_data_random_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.915433       \n",
       "1                                           0.942918       \n",
       "2                                           0.953488       \n",
       "3                                           0.915433       \n",
       "4                                           0.942918       \n",
       "\n",
       "   dt_scores_f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                                   0.828571   \n",
       "1                                   0.891892   \n",
       "2                                   0.891892   \n",
       "3                                   0.828571   \n",
       "4                                   0.891892   \n",
       "\n",
       "   dt_scores_f1_score_data_random_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.816514       \n",
       "1                                           0.882096       \n",
       "2                                           0.902655       \n",
       "3                                           0.816514       \n",
       "4                                           0.882096       \n",
       "\n",
       "   dt_scores_runtime_absenteeism_TRAIN_DATA  \\\n",
       "0                                  0.000852   \n",
       "1                                  0.000976   \n",
       "2                                  0.001135   \n",
       "3                                  0.001130   \n",
       "4                                  0.001603   \n",
       "\n",
       "   inet_scores_soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.665557             \n",
       "1                                           0.668186             \n",
       "2                                           0.673644             \n",
       "3                                           0.672519             \n",
       "4                                           0.667948             \n",
       "\n",
       "   inet_scores_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                           0.624602        \n",
       "1                                           0.618528        \n",
       "2                                           0.652591        \n",
       "3                                           0.606104        \n",
       "4                                           0.592628        \n",
       "\n",
       "   inet_scores_accuracy_absenteeism_TRAIN_DATA  \\\n",
       "0                                     0.743243   \n",
       "1                                     0.729730   \n",
       "2                                     0.574324   \n",
       "3                                     0.729730   \n",
       "4                                     0.729730   \n",
       "\n",
       "   inet_scores_f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                                     0.486486   \n",
       "1                                     0.000000   \n",
       "2                                     0.322581   \n",
       "3                                     0.000000   \n",
       "4                                     0.000000   \n",
       "\n",
       "   inet_scores_runtime_absenteeism_TRAIN_DATA  z-score_train  z-score_valid  \\\n",
       "0                                    0.110725     890.099784     874.574718   \n",
       "1                                    0.125714     890.099784     874.574718   \n",
       "2                                    0.207149     890.099784     874.574718   \n",
       "3                                    0.135190     952.125841     938.678758   \n",
       "4                                    0.153153     952.125841     938.678758   \n",
       "\n",
       "   z-score_test  z-score_adult  z-score_titanic  z-score_absenteeism  \\\n",
       "0    870.012645    5618.356374     11957.028231          2158.561583   \n",
       "1    870.012645    5618.356374     11957.028231          2158.561583   \n",
       "2    870.012645    5618.356374     11957.028231          2158.561583   \n",
       "3    967.163455    1530.924031       815.914129           702.551322   \n",
       "4    967.163455    1530.924031       815.914129           702.551322   \n",
       "\n",
       "   dist_to_init_train  dist_to_init_valid  dist_to_init_test  \\\n",
       "0          253.216522          247.094642         237.894941   \n",
       "1          253.216522          247.094642         237.894941   \n",
       "2          253.216522          247.094642         237.894941   \n",
       "3          294.860154          288.710905         299.193479   \n",
       "4          294.860154          288.710905         299.193479   \n",
       "\n",
       "   dist_to_init_adult  dist_to_init_titanic  dist_to_init_absenteeism  \\\n",
       "0            385.6601             175.44424                 167.62558   \n",
       "1            385.6601             175.44424                 167.62558   \n",
       "2            385.6601             175.44424                 167.62558   \n",
       "3            385.6601             175.44424                 167.62558   \n",
       "4            385.6601             175.44424                 167.62558   \n",
       "\n",
       "   avg_dist_to_train_train  avg_dist_to_train_valid  avg_dist_to_train_test  \\\n",
       "0               354.663538               349.519353              344.572977   \n",
       "1               354.663538               349.519353              344.572977   \n",
       "2               354.663538               349.519353              344.572977   \n",
       "3               401.357852               397.316608              403.692552   \n",
       "4               401.357852               397.316608              403.692552   \n",
       "\n",
       "   avg_dist_to_train_adult  avg_dist_to_train_titanic  \\\n",
       "0               494.338096                 320.998343   \n",
       "1               494.338096                 320.998343   \n",
       "2               494.338096                 320.998343   \n",
       "3               506.776383                 342.340437   \n",
       "4               506.776383                 342.340437   \n",
       "\n",
       "   avg_dist_to_train_absenteeism  min_dist_to_train_sample_train  \\\n",
       "0                     304.472420                             0.0   \n",
       "1                     304.472420                             0.0   \n",
       "2                     304.472420                             0.0   \n",
       "3                     336.541181                             0.0   \n",
       "4                     336.541181                             0.0   \n",
       "\n",
       "   min_dist_to_train_sample_valid  min_dist_to_train_samplee_test  \\\n",
       "0                      216.462495                      202.812791   \n",
       "1                      216.462495                      202.812791   \n",
       "2                      216.462495                      202.812791   \n",
       "3                      247.487209                      264.935285   \n",
       "4                      247.487209                      264.935285   \n",
       "\n",
       "   min_dist_to_train_sample_adult  min_dist_to_train_sample_titanic  \\\n",
       "0                      376.585663                        202.923064   \n",
       "1                      376.585663                        202.923064   \n",
       "2                      376.585663                        202.923064   \n",
       "3                      369.448476                        174.373387   \n",
       "4                      369.448476                        174.373387   \n",
       "\n",
       "   min_dist_to_train_sample_absenteeism  \n",
       "0                            199.341257  \n",
       "1                            199.341257  \n",
       "2                            199.341257  \n",
       "3                            193.871242  \n",
       "4                            193.871242  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_complete = pd.read_csv('./results_complete.csv', delimiter=';')\n",
    "#results_complete = results_complete[results_complete['i_net_nas'] == True]\n",
    "results_complete_columns = list(results_complete.columns)\n",
    "\n",
    "results_summary = pd.read_csv('./results_summary.csv', delimiter=';')\n",
    "#results_summary = results_summary[results_summary['i_net_nas'] == True]\n",
    "results_summary_columns = list(results_summary.columns)\n",
    "\n",
    "results_summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2463e27-6257-40d3-b30f-ca8c237d59a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.098360Z",
     "iopub.status.busy": "2022-01-04T19:49:56.098133Z",
     "iopub.status.idle": "2022-01-04T19:49:56.103732Z",
     "shell.execute_reply": "2022-01-04T19:49:56.103084Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.098334Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colmuns_identifier = [\n",
    "                  'function_family_maximum_depth',\n",
    "                  'function_family_decision_sparsity', \n",
    "                  'function_family_dt_type',\n",
    "                  'data_dt_type_train',\n",
    "                  'data_maximum_depth_train',\n",
    "                  'data_number_of_variables',\n",
    "                  'data_noise_injected_level',\n",
    "                  'data_function_generation_type',\n",
    "                  'data_categorical_indices',\n",
    "                  'lambda_net_lambda_network_layers',\n",
    "                  'lambda_net_optimizer_lambda',\n",
    "                  'i_net_dense_layers',\n",
    "                  'i_net_dropout',\n",
    "                  'i_net_learning_rate',\n",
    "                  'i_net_loss',\n",
    "                  'i_net_interpretation_dataset_size',\n",
    "                  'i_net_function_representation_type',\n",
    "                  'i_net_data_reshape_version',\n",
    "                  'i_net_nas',\n",
    "                  'i_net_nas_trials',\n",
    "                  'evaluation_eval_data_description_eval_data_function_generation_type',\n",
    "                  'evaluation_eval_data_description_eval_data_noise_injected_level',\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f20b962-cba9-423e-bc2e-7c8c39648e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.104796Z",
     "iopub.status.busy": "2022-01-04T19:49:56.104628Z",
     "iopub.status.idle": "2022-01-04T19:49:56.337778Z",
     "shell.execute_reply": "2022-01-04T19:49:56.336198Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.104775Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.616474</td>\n",
       "      <td>0.615307</td>\n",
       "      <td>0.569003</td>\n",
       "      <td>0.567067</td>\n",
       "      <td>0.703344</td>\n",
       "      <td>0.7024</td>\n",
       "      <td>0.664482</td>\n",
       "      <td>0.695851</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.619904</td>\n",
       "      <td>0.617766</td>\n",
       "      <td>0.574994</td>\n",
       "      <td>0.573605</td>\n",
       "      <td>0.702496</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.673612</td>\n",
       "      <td>0.713898</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.626373</td>\n",
       "      <td>0.629011</td>\n",
       "      <td>0.583992</td>\n",
       "      <td>0.589657</td>\n",
       "      <td>0.694880</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.669380</td>\n",
       "      <td>0.723074</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612697</td>\n",
       "      <td>0.613249</td>\n",
       "      <td>0.564528</td>\n",
       "      <td>0.564254</td>\n",
       "      <td>0.705632</td>\n",
       "      <td>0.7028</td>\n",
       "      <td>0.678824</td>\n",
       "      <td>0.705614</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.616877</td>\n",
       "      <td>0.620622</td>\n",
       "      <td>0.570432</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.7136</td>\n",
       "      <td>0.688397</td>\n",
       "      <td>0.715229</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.622317</td>\n",
       "      <td>0.622120</td>\n",
       "      <td>0.579808</td>\n",
       "      <td>0.576846</td>\n",
       "      <td>0.698048</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>0.675447</td>\n",
       "      <td>0.710239</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615532</td>\n",
       "      <td>0.617586</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>0.568291</td>\n",
       "      <td>0.708408</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.717616</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.617547</td>\n",
       "      <td>0.623748</td>\n",
       "      <td>0.571834</td>\n",
       "      <td>0.579464</td>\n",
       "      <td>0.709880</td>\n",
       "      <td>0.7084</td>\n",
       "      <td>0.692539</td>\n",
       "      <td>0.712673</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.626444</td>\n",
       "      <td>0.628567</td>\n",
       "      <td>0.586809</td>\n",
       "      <td>0.590938</td>\n",
       "      <td>0.693312</td>\n",
       "      <td>0.6928</td>\n",
       "      <td>0.674189</td>\n",
       "      <td>0.703011</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606874</td>\n",
       "      <td>0.613907</td>\n",
       "      <td>0.561997</td>\n",
       "      <td>0.565275</td>\n",
       "      <td>0.706720</td>\n",
       "      <td>0.6992</td>\n",
       "      <td>0.674549</td>\n",
       "      <td>0.714300</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.612247</td>\n",
       "      <td>0.613774</td>\n",
       "      <td>0.571419</td>\n",
       "      <td>0.572374</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.7056</td>\n",
       "      <td>0.649521</td>\n",
       "      <td>0.715474</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.611344</td>\n",
       "      <td>0.617777</td>\n",
       "      <td>0.572884</td>\n",
       "      <td>0.580001</td>\n",
       "      <td>0.703008</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.659850</td>\n",
       "      <td>0.701589</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600036</td>\n",
       "      <td>0.600425</td>\n",
       "      <td>0.549041</td>\n",
       "      <td>0.550977</td>\n",
       "      <td>0.719944</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.690287</td>\n",
       "      <td>0.714838</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.605724</td>\n",
       "      <td>0.609405</td>\n",
       "      <td>0.555553</td>\n",
       "      <td>0.562603</td>\n",
       "      <td>0.714264</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.681811</td>\n",
       "      <td>0.709280</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.609198</td>\n",
       "      <td>0.605301</td>\n",
       "      <td>0.563761</td>\n",
       "      <td>0.562193</td>\n",
       "      <td>0.714608</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.724678</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603798</td>\n",
       "      <td>0.608298</td>\n",
       "      <td>0.558632</td>\n",
       "      <td>0.566768</td>\n",
       "      <td>0.714272</td>\n",
       "      <td>0.7084</td>\n",
       "      <td>0.681626</td>\n",
       "      <td>0.724234</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.600816</td>\n",
       "      <td>0.604133</td>\n",
       "      <td>0.550097</td>\n",
       "      <td>0.551280</td>\n",
       "      <td>0.719472</td>\n",
       "      <td>0.7228</td>\n",
       "      <td>0.668247</td>\n",
       "      <td>0.716966</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.612444</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>0.568646</td>\n",
       "      <td>0.564193</td>\n",
       "      <td>0.709344</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>0.689193</td>\n",
       "      <td>0.713004</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.854233</td>\n",
       "      <td>1.729415</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.595357</td>\n",
       "      <td>0.346450</td>\n",
       "      <td>0.647532</td>\n",
       "      <td>0.612850</td>\n",
       "      <td>0.659218</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.296510</td>\n",
       "      <td>0.666934</td>\n",
       "      <td>0.579294</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314534</td>\n",
       "      <td>0.854233</td>\n",
       "      <td>1.729415</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.595357</td>\n",
       "      <td>0.346450</td>\n",
       "      <td>0.647532</td>\n",
       "      <td>0.612850</td>\n",
       "      <td>0.659218</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.296510</td>\n",
       "      <td>0.666934</td>\n",
       "      <td>0.579294</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314534</td>\n",
       "      <td>0.854233</td>\n",
       "      <td>1.729415</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.595357</td>\n",
       "      <td>0.346450</td>\n",
       "      <td>0.647532</td>\n",
       "      <td>0.612850</td>\n",
       "      <td>0.659218</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.296510</td>\n",
       "      <td>0.666934</td>\n",
       "      <td>0.579294</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314534</td>\n",
       "      <td>0.854233</td>\n",
       "      <td>1.729415</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.595357</td>\n",
       "      <td>0.346450</td>\n",
       "      <td>0.647532</td>\n",
       "      <td>0.612850</td>\n",
       "      <td>0.659218</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.296510</td>\n",
       "      <td>0.666934</td>\n",
       "      <td>0.579294</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314534</td>\n",
       "      <td>0.854233</td>\n",
       "      <td>1.729415</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.595357</td>\n",
       "      <td>0.346450</td>\n",
       "      <td>0.647532</td>\n",
       "      <td>0.612850</td>\n",
       "      <td>0.659218</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.296510</td>\n",
       "      <td>0.666934</td>\n",
       "      <td>0.579294</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.593606</td>\n",
       "      <td>0.604692</td>\n",
       "      <td>0.517703</td>\n",
       "      <td>0.539913</td>\n",
       "      <td>0.734976</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>0.669948</td>\n",
       "      <td>0.775119</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.596830</td>\n",
       "      <td>0.606128</td>\n",
       "      <td>0.525489</td>\n",
       "      <td>0.540684</td>\n",
       "      <td>0.731568</td>\n",
       "      <td>0.7308</td>\n",
       "      <td>0.624560</td>\n",
       "      <td>0.731509</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.593793</td>\n",
       "      <td>0.597349</td>\n",
       "      <td>0.517497</td>\n",
       "      <td>0.520106</td>\n",
       "      <td>0.741728</td>\n",
       "      <td>0.7580</td>\n",
       "      <td>0.622474</td>\n",
       "      <td>0.777947</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.722902</td>\n",
       "      <td>0.762560</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.172872</td>\n",
       "      <td>0.588987</td>\n",
       "      <td>0.400991</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.126030</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.582609</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135682</td>\n",
       "      <td>0.722902</td>\n",
       "      <td>0.762560</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.172872</td>\n",
       "      <td>0.588987</td>\n",
       "      <td>0.400991</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.126030</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.582609</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135682</td>\n",
       "      <td>0.722902</td>\n",
       "      <td>0.762560</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.172872</td>\n",
       "      <td>0.588987</td>\n",
       "      <td>0.400991</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.126030</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.582609</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135682</td>\n",
       "      <td>0.722902</td>\n",
       "      <td>0.762560</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.172872</td>\n",
       "      <td>0.588987</td>\n",
       "      <td>0.400991</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.126030</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.582609</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135682</td>\n",
       "      <td>0.722902</td>\n",
       "      <td>0.762560</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.172872</td>\n",
       "      <td>0.588987</td>\n",
       "      <td>0.400991</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.126030</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.582609</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588485</td>\n",
       "      <td>0.598005</td>\n",
       "      <td>0.509681</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.734600</td>\n",
       "      <td>0.7264</td>\n",
       "      <td>0.662172</td>\n",
       "      <td>0.765426</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.593572</td>\n",
       "      <td>0.604143</td>\n",
       "      <td>0.518350</td>\n",
       "      <td>0.525014</td>\n",
       "      <td>0.738560</td>\n",
       "      <td>0.7368</td>\n",
       "      <td>0.618041</td>\n",
       "      <td>0.734103</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.595629</td>\n",
       "      <td>0.586489</td>\n",
       "      <td>0.521241</td>\n",
       "      <td>0.519973</td>\n",
       "      <td>0.720064</td>\n",
       "      <td>0.7408</td>\n",
       "      <td>0.601818</td>\n",
       "      <td>0.740038</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.677103</td>\n",
       "      <td>0.671711</td>\n",
       "      <td>0.629971</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.624959</td>\n",
       "      <td>0.502890</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.168827</td>\n",
       "      <td>0.668177</td>\n",
       "      <td>0.593875</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125292</td>\n",
       "      <td>0.677103</td>\n",
       "      <td>0.671711</td>\n",
       "      <td>0.629971</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.624959</td>\n",
       "      <td>0.502890</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.168827</td>\n",
       "      <td>0.668177</td>\n",
       "      <td>0.593875</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125292</td>\n",
       "      <td>0.677103</td>\n",
       "      <td>0.671711</td>\n",
       "      <td>0.629971</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.624959</td>\n",
       "      <td>0.502890</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.168827</td>\n",
       "      <td>0.668177</td>\n",
       "      <td>0.593875</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125292</td>\n",
       "      <td>0.677103</td>\n",
       "      <td>0.671711</td>\n",
       "      <td>0.629971</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.624959</td>\n",
       "      <td>0.502890</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.168827</td>\n",
       "      <td>0.668177</td>\n",
       "      <td>0.593875</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125292</td>\n",
       "      <td>0.677103</td>\n",
       "      <td>0.671711</td>\n",
       "      <td>0.629971</td>\n",
       "      <td>0.045922</td>\n",
       "      <td>0.208214</td>\n",
       "      <td>0.624959</td>\n",
       "      <td>0.502890</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.168827</td>\n",
       "      <td>0.668177</td>\n",
       "      <td>0.593875</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.584428</td>\n",
       "      <td>0.587725</td>\n",
       "      <td>0.500297</td>\n",
       "      <td>0.511078</td>\n",
       "      <td>0.741600</td>\n",
       "      <td>0.7336</td>\n",
       "      <td>0.686643</td>\n",
       "      <td>0.756975</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.589876</td>\n",
       "      <td>0.594565</td>\n",
       "      <td>0.512703</td>\n",
       "      <td>0.525379</td>\n",
       "      <td>0.731776</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.616933</td>\n",
       "      <td>0.731953</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.583267</td>\n",
       "      <td>0.588919</td>\n",
       "      <td>0.498723</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.751248</td>\n",
       "      <td>0.7520</td>\n",
       "      <td>0.679994</td>\n",
       "      <td>0.764540</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.683051</td>\n",
       "      <td>0.675042</td>\n",
       "      <td>0.620912</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.174649</td>\n",
       "      <td>0.717925</td>\n",
       "      <td>0.790258</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.173118</td>\n",
       "      <td>0.661191</td>\n",
       "      <td>0.577630</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174584</td>\n",
       "      <td>0.683051</td>\n",
       "      <td>0.675042</td>\n",
       "      <td>0.620912</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.174649</td>\n",
       "      <td>0.717925</td>\n",
       "      <td>0.790258</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.173118</td>\n",
       "      <td>0.661191</td>\n",
       "      <td>0.577630</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174584</td>\n",
       "      <td>0.683051</td>\n",
       "      <td>0.675042</td>\n",
       "      <td>0.620912</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.174649</td>\n",
       "      <td>0.717925</td>\n",
       "      <td>0.790258</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.173118</td>\n",
       "      <td>0.661191</td>\n",
       "      <td>0.577630</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174584</td>\n",
       "      <td>0.683051</td>\n",
       "      <td>0.675042</td>\n",
       "      <td>0.620912</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.174649</td>\n",
       "      <td>0.717925</td>\n",
       "      <td>0.790258</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.173118</td>\n",
       "      <td>0.661191</td>\n",
       "      <td>0.577630</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174584</td>\n",
       "      <td>0.683051</td>\n",
       "      <td>0.675042</td>\n",
       "      <td>0.620912</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.174649</td>\n",
       "      <td>0.717925</td>\n",
       "      <td>0.790258</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.173118</td>\n",
       "      <td>0.661191</td>\n",
       "      <td>0.577630</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601992</td>\n",
       "      <td>0.608825</td>\n",
       "      <td>0.535755</td>\n",
       "      <td>0.548793</td>\n",
       "      <td>0.712848</td>\n",
       "      <td>0.7140</td>\n",
       "      <td>0.667927</td>\n",
       "      <td>0.755867</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.595453</td>\n",
       "      <td>0.593468</td>\n",
       "      <td>0.526267</td>\n",
       "      <td>0.527283</td>\n",
       "      <td>0.722192</td>\n",
       "      <td>0.7228</td>\n",
       "      <td>0.683336</td>\n",
       "      <td>0.754134</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.597473</td>\n",
       "      <td>0.601863</td>\n",
       "      <td>0.532463</td>\n",
       "      <td>0.535753</td>\n",
       "      <td>0.726480</td>\n",
       "      <td>0.7196</td>\n",
       "      <td>0.615089</td>\n",
       "      <td>0.744318</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.733779</td>\n",
       "      <td>0.793555</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>48.206169</td>\n",
       "      <td>0.663955</td>\n",
       "      <td>0.676750</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.670961</td>\n",
       "      <td>0.599570</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104679</td>\n",
       "      <td>0.733779</td>\n",
       "      <td>0.793555</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>48.206169</td>\n",
       "      <td>0.663955</td>\n",
       "      <td>0.676750</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.670961</td>\n",
       "      <td>0.599570</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104679</td>\n",
       "      <td>0.733779</td>\n",
       "      <td>0.793555</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>48.206169</td>\n",
       "      <td>0.663955</td>\n",
       "      <td>0.676750</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.670961</td>\n",
       "      <td>0.599570</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104679</td>\n",
       "      <td>0.733779</td>\n",
       "      <td>0.793555</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>48.206169</td>\n",
       "      <td>0.663955</td>\n",
       "      <td>0.676750</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.670961</td>\n",
       "      <td>0.599570</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104679</td>\n",
       "      <td>0.733779</td>\n",
       "      <td>0.793555</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>48.206169</td>\n",
       "      <td>0.663955</td>\n",
       "      <td>0.676750</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>0.670961</td>\n",
       "      <td>0.599570</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.591961</td>\n",
       "      <td>0.607694</td>\n",
       "      <td>0.514091</td>\n",
       "      <td>0.551851</td>\n",
       "      <td>0.734016</td>\n",
       "      <td>0.7196</td>\n",
       "      <td>0.676836</td>\n",
       "      <td>0.757150</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.591405</td>\n",
       "      <td>0.591453</td>\n",
       "      <td>0.515608</td>\n",
       "      <td>0.513126</td>\n",
       "      <td>0.734728</td>\n",
       "      <td>0.7336</td>\n",
       "      <td>0.696861</td>\n",
       "      <td>0.770805</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.598480</td>\n",
       "      <td>0.606518</td>\n",
       "      <td>0.531945</td>\n",
       "      <td>0.551234</td>\n",
       "      <td>0.723888</td>\n",
       "      <td>0.7136</td>\n",
       "      <td>0.615156</td>\n",
       "      <td>0.751243</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.761217</td>\n",
       "      <td>0.873499</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.160855</td>\n",
       "      <td>0.702805</td>\n",
       "      <td>0.739006</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.458537</td>\n",
       "      <td>0.128241</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.593555</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129608</td>\n",
       "      <td>0.761217</td>\n",
       "      <td>0.873499</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.160855</td>\n",
       "      <td>0.702805</td>\n",
       "      <td>0.739006</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.458537</td>\n",
       "      <td>0.128241</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.593555</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129608</td>\n",
       "      <td>0.761217</td>\n",
       "      <td>0.873499</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.160855</td>\n",
       "      <td>0.702805</td>\n",
       "      <td>0.739006</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.458537</td>\n",
       "      <td>0.128241</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.593555</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129608</td>\n",
       "      <td>0.761217</td>\n",
       "      <td>0.873499</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.160855</td>\n",
       "      <td>0.702805</td>\n",
       "      <td>0.739006</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.458537</td>\n",
       "      <td>0.128241</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.593555</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129608</td>\n",
       "      <td>0.761217</td>\n",
       "      <td>0.873499</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.160855</td>\n",
       "      <td>0.702805</td>\n",
       "      <td>0.739006</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.458537</td>\n",
       "      <td>0.128241</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>0.593555</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.582421</td>\n",
       "      <td>0.592995</td>\n",
       "      <td>0.501418</td>\n",
       "      <td>0.527301</td>\n",
       "      <td>0.749272</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>0.697906</td>\n",
       "      <td>0.777340</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.586289</td>\n",
       "      <td>0.590273</td>\n",
       "      <td>0.505130</td>\n",
       "      <td>0.504487</td>\n",
       "      <td>0.749736</td>\n",
       "      <td>0.7484</td>\n",
       "      <td>0.709509</td>\n",
       "      <td>0.780531</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.594635</td>\n",
       "      <td>0.605317</td>\n",
       "      <td>0.526056</td>\n",
       "      <td>0.533470</td>\n",
       "      <td>0.730368</td>\n",
       "      <td>0.7172</td>\n",
       "      <td>0.603052</td>\n",
       "      <td>0.728388</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.706802</td>\n",
       "      <td>0.723446</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.177735</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.546939</td>\n",
       "      <td>0.181615</td>\n",
       "      <td>0.668631</td>\n",
       "      <td>0.598648</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177235</td>\n",
       "      <td>0.706802</td>\n",
       "      <td>0.723446</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.177735</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.546939</td>\n",
       "      <td>0.181615</td>\n",
       "      <td>0.668631</td>\n",
       "      <td>0.598648</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177235</td>\n",
       "      <td>0.706802</td>\n",
       "      <td>0.723446</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.177735</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.546939</td>\n",
       "      <td>0.181615</td>\n",
       "      <td>0.668631</td>\n",
       "      <td>0.598648</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177235</td>\n",
       "      <td>0.706802</td>\n",
       "      <td>0.723446</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.177735</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.546939</td>\n",
       "      <td>0.181615</td>\n",
       "      <td>0.668631</td>\n",
       "      <td>0.598648</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177235</td>\n",
       "      <td>0.706802</td>\n",
       "      <td>0.723446</td>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>0.177735</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.379888</td>\n",
       "      <td>0.546939</td>\n",
       "      <td>0.181615</td>\n",
       "      <td>0.668631</td>\n",
       "      <td>0.598648</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610132</td>\n",
       "      <td>0.612440</td>\n",
       "      <td>0.546456</td>\n",
       "      <td>0.553418</td>\n",
       "      <td>0.714528</td>\n",
       "      <td>0.7244</td>\n",
       "      <td>0.589649</td>\n",
       "      <td>0.722003</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.610781</td>\n",
       "      <td>0.612933</td>\n",
       "      <td>0.548322</td>\n",
       "      <td>0.544666</td>\n",
       "      <td>0.712248</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.631081</td>\n",
       "      <td>0.736423</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.614121</td>\n",
       "      <td>0.613544</td>\n",
       "      <td>0.552811</td>\n",
       "      <td>0.559870</td>\n",
       "      <td>0.706832</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.623076</td>\n",
       "      <td>0.712979</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.119750</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.120670</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139696</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.119750</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.120670</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139696</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.119750</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.120670</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139696</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.119750</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.120670</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139696</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.119750</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.120670</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610132</td>\n",
       "      <td>0.612440</td>\n",
       "      <td>0.546456</td>\n",
       "      <td>0.553418</td>\n",
       "      <td>0.714528</td>\n",
       "      <td>0.7244</td>\n",
       "      <td>0.589649</td>\n",
       "      <td>0.722003</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.610781</td>\n",
       "      <td>0.612933</td>\n",
       "      <td>0.548322</td>\n",
       "      <td>0.544666</td>\n",
       "      <td>0.712248</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>0.631081</td>\n",
       "      <td>0.736423</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.614121</td>\n",
       "      <td>0.613544</td>\n",
       "      <td>0.552811</td>\n",
       "      <td>0.559870</td>\n",
       "      <td>0.706832</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.623076</td>\n",
       "      <td>0.712979</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.137243</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.135688</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131851</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.137243</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.135688</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131851</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.137243</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.135688</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131851</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.137243</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.135688</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131851</td>\n",
       "      <td>0.858197</td>\n",
       "      <td>1.524665</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.137243</td>\n",
       "      <td>0.693719</td>\n",
       "      <td>0.701660</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.135688</td>\n",
       "      <td>0.670297</td>\n",
       "      <td>0.644363</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613652</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.547795</td>\n",
       "      <td>0.562111</td>\n",
       "      <td>0.711688</td>\n",
       "      <td>0.7148</td>\n",
       "      <td>0.607688</td>\n",
       "      <td>0.705994</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.609338</td>\n",
       "      <td>0.610134</td>\n",
       "      <td>0.542543</td>\n",
       "      <td>0.550926</td>\n",
       "      <td>0.715672</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>0.638185</td>\n",
       "      <td>0.741005</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.620837</td>\n",
       "      <td>0.610635</td>\n",
       "      <td>0.567701</td>\n",
       "      <td>0.553304</td>\n",
       "      <td>0.694880</td>\n",
       "      <td>0.6960</td>\n",
       "      <td>0.511830</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.094413</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.107251</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092488</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.094413</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.107251</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092488</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.094413</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.107251</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092488</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.094413</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.107251</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092488</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.094413</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.107251</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613652</td>\n",
       "      <td>0.620513</td>\n",
       "      <td>0.547795</td>\n",
       "      <td>0.562111</td>\n",
       "      <td>0.711688</td>\n",
       "      <td>0.7148</td>\n",
       "      <td>0.607688</td>\n",
       "      <td>0.705994</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.609338</td>\n",
       "      <td>0.610134</td>\n",
       "      <td>0.542543</td>\n",
       "      <td>0.550926</td>\n",
       "      <td>0.715672</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>0.638185</td>\n",
       "      <td>0.741005</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.620837</td>\n",
       "      <td>0.610635</td>\n",
       "      <td>0.567701</td>\n",
       "      <td>0.553304</td>\n",
       "      <td>0.694880</td>\n",
       "      <td>0.6960</td>\n",
       "      <td>0.511830</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.101623</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.101623</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.101623</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.101623</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.799476</td>\n",
       "      <td>1.013420</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.713038</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.101623</td>\n",
       "      <td>0.663541</td>\n",
       "      <td>0.621219</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619931</td>\n",
       "      <td>0.623571</td>\n",
       "      <td>0.562953</td>\n",
       "      <td>0.574703</td>\n",
       "      <td>0.699576</td>\n",
       "      <td>0.7040</td>\n",
       "      <td>0.567229</td>\n",
       "      <td>0.711376</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.618333</td>\n",
       "      <td>0.619782</td>\n",
       "      <td>0.561002</td>\n",
       "      <td>0.563939</td>\n",
       "      <td>0.696880</td>\n",
       "      <td>0.6964</td>\n",
       "      <td>0.614596</td>\n",
       "      <td>0.730612</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.620293</td>\n",
       "      <td>0.617894</td>\n",
       "      <td>0.570282</td>\n",
       "      <td>0.575227</td>\n",
       "      <td>0.694880</td>\n",
       "      <td>0.7068</td>\n",
       "      <td>0.594197</td>\n",
       "      <td>0.680762</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.099299</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097691</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.099299</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097691</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.099299</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097691</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.099299</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097691</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.099299</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619931</td>\n",
       "      <td>0.623571</td>\n",
       "      <td>0.562953</td>\n",
       "      <td>0.574703</td>\n",
       "      <td>0.699576</td>\n",
       "      <td>0.7040</td>\n",
       "      <td>0.567229</td>\n",
       "      <td>0.711376</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.618333</td>\n",
       "      <td>0.619782</td>\n",
       "      <td>0.561002</td>\n",
       "      <td>0.563939</td>\n",
       "      <td>0.696880</td>\n",
       "      <td>0.6964</td>\n",
       "      <td>0.614596</td>\n",
       "      <td>0.730612</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.620293</td>\n",
       "      <td>0.617894</td>\n",
       "      <td>0.570282</td>\n",
       "      <td>0.575227</td>\n",
       "      <td>0.694880</td>\n",
       "      <td>0.7068</td>\n",
       "      <td>0.594197</td>\n",
       "      <td>0.680762</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>0.900496</td>\n",
       "      <td>1.792710</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.101976</td>\n",
       "      <td>0.703797</td>\n",
       "      <td>0.731052</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.671838</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613866</td>\n",
       "      <td>0.621291</td>\n",
       "      <td>0.549314</td>\n",
       "      <td>0.561554</td>\n",
       "      <td>0.712816</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.588271</td>\n",
       "      <td>0.700661</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.609913</td>\n",
       "      <td>0.614329</td>\n",
       "      <td>0.545310</td>\n",
       "      <td>0.557027</td>\n",
       "      <td>0.711752</td>\n",
       "      <td>0.7188</td>\n",
       "      <td>0.616257</td>\n",
       "      <td>0.750342</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.623084</td>\n",
       "      <td>0.625520</td>\n",
       "      <td>0.569946</td>\n",
       "      <td>0.579273</td>\n",
       "      <td>0.694848</td>\n",
       "      <td>0.6816</td>\n",
       "      <td>0.571529</td>\n",
       "      <td>0.684033</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.104906</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.104906</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.104906</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.104906</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106533</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.104906</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613866</td>\n",
       "      <td>0.621291</td>\n",
       "      <td>0.549314</td>\n",
       "      <td>0.561554</td>\n",
       "      <td>0.712816</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.588271</td>\n",
       "      <td>0.700661</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.609913</td>\n",
       "      <td>0.614329</td>\n",
       "      <td>0.545310</td>\n",
       "      <td>0.557027</td>\n",
       "      <td>0.711752</td>\n",
       "      <td>0.7188</td>\n",
       "      <td>0.616257</td>\n",
       "      <td>0.750342</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.623084</td>\n",
       "      <td>0.625520</td>\n",
       "      <td>0.569946</td>\n",
       "      <td>0.579273</td>\n",
       "      <td>0.694848</td>\n",
       "      <td>0.6816</td>\n",
       "      <td>0.571529</td>\n",
       "      <td>0.684033</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.433442</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103857</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.433442</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103857</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.433442</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103857</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.433442</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103857</td>\n",
       "      <td>0.806887</td>\n",
       "      <td>1.056151</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.433442</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.804630</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.127601</td>\n",
       "      <td>0.666985</td>\n",
       "      <td>0.627615</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602437</td>\n",
       "      <td>0.605325</td>\n",
       "      <td>0.530152</td>\n",
       "      <td>0.541585</td>\n",
       "      <td>0.723880</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>0.624135</td>\n",
       "      <td>0.700890</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.607912</td>\n",
       "      <td>0.608541</td>\n",
       "      <td>0.539689</td>\n",
       "      <td>0.552826</td>\n",
       "      <td>0.720456</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.646326</td>\n",
       "      <td>0.727354</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.617209</td>\n",
       "      <td>0.608675</td>\n",
       "      <td>0.557960</td>\n",
       "      <td>0.545205</td>\n",
       "      <td>0.707360</td>\n",
       "      <td>0.7180</td>\n",
       "      <td>0.624082</td>\n",
       "      <td>0.676755</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.146318</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159502</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.146318</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159502</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.146318</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159502</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.146318</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159502</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.146318</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602437</td>\n",
       "      <td>0.605325</td>\n",
       "      <td>0.530152</td>\n",
       "      <td>0.541585</td>\n",
       "      <td>0.723880</td>\n",
       "      <td>0.7300</td>\n",
       "      <td>0.624135</td>\n",
       "      <td>0.700890</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.607912</td>\n",
       "      <td>0.608541</td>\n",
       "      <td>0.539689</td>\n",
       "      <td>0.552826</td>\n",
       "      <td>0.720456</td>\n",
       "      <td>0.7240</td>\n",
       "      <td>0.646326</td>\n",
       "      <td>0.727354</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.617209</td>\n",
       "      <td>0.608675</td>\n",
       "      <td>0.557960</td>\n",
       "      <td>0.545205</td>\n",
       "      <td>0.707360</td>\n",
       "      <td>0.7180</td>\n",
       "      <td>0.624082</td>\n",
       "      <td>0.676755</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.156404</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.164939</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149537</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.156404</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.164939</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149537</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.156404</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.164939</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149537</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.156404</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.164939</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149537</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>1.464826</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.156404</td>\n",
       "      <td>0.597506</td>\n",
       "      <td>0.433255</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.164939</td>\n",
       "      <td>0.671978</td>\n",
       "      <td>0.654625</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606206</td>\n",
       "      <td>0.615274</td>\n",
       "      <td>0.538088</td>\n",
       "      <td>0.556597</td>\n",
       "      <td>0.724240</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.603282</td>\n",
       "      <td>0.727100</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.612321</td>\n",
       "      <td>0.548501</td>\n",
       "      <td>0.545131</td>\n",
       "      <td>0.716688</td>\n",
       "      <td>0.7256</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>0.750342</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.616659</td>\n",
       "      <td>0.619857</td>\n",
       "      <td>0.560663</td>\n",
       "      <td>0.568172</td>\n",
       "      <td>0.714912</td>\n",
       "      <td>0.7120</td>\n",
       "      <td>0.591665</td>\n",
       "      <td>0.703552</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.478246</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.130947</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154387</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.478246</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.130947</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154387</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.478246</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.130947</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154387</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.478246</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.130947</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154387</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.478246</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.130947</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606206</td>\n",
       "      <td>0.615274</td>\n",
       "      <td>0.538088</td>\n",
       "      <td>0.556597</td>\n",
       "      <td>0.724240</td>\n",
       "      <td>0.7208</td>\n",
       "      <td>0.603282</td>\n",
       "      <td>0.727100</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.612321</td>\n",
       "      <td>0.548501</td>\n",
       "      <td>0.545131</td>\n",
       "      <td>0.716688</td>\n",
       "      <td>0.7256</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>0.750342</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.616659</td>\n",
       "      <td>0.619857</td>\n",
       "      <td>0.560663</td>\n",
       "      <td>0.568172</td>\n",
       "      <td>0.714912</td>\n",
       "      <td>0.7120</td>\n",
       "      <td>0.591665</td>\n",
       "      <td>0.703552</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193856</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193856</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193856</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193856</td>\n",
       "      <td>0.818518</td>\n",
       "      <td>1.111223</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0.685908</td>\n",
       "      <td>0.688458</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>0.657618</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.478518</td>\n",
       "      <td>0.480919</td>\n",
       "      <td>0.317343</td>\n",
       "      <td>0.342072</td>\n",
       "      <td>0.863512</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.859648</td>\n",
       "      <td>0.856860</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.475706</td>\n",
       "      <td>0.464459</td>\n",
       "      <td>0.323092</td>\n",
       "      <td>0.336398</td>\n",
       "      <td>0.864424</td>\n",
       "      <td>0.8732</td>\n",
       "      <td>0.865715</td>\n",
       "      <td>0.869372</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.464466</td>\n",
       "      <td>0.447386</td>\n",
       "      <td>0.297523</td>\n",
       "      <td>0.285332</td>\n",
       "      <td>0.872592</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>0.867161</td>\n",
       "      <td>0.882328</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.678608</td>\n",
       "      <td>1.979326</td>\n",
       "      <td>0.629203</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.187590</td>\n",
       "      <td>0.543518</td>\n",
       "      <td>0.301996</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.167211</td>\n",
       "      <td>0.743554</td>\n",
       "      <td>1.196940</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.141552</td>\n",
       "      <td>0.678608</td>\n",
       "      <td>1.979326</td>\n",
       "      <td>0.629203</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.187590</td>\n",
       "      <td>0.543518</td>\n",
       "      <td>0.301996</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.167211</td>\n",
       "      <td>0.743554</td>\n",
       "      <td>1.196940</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.141552</td>\n",
       "      <td>0.678608</td>\n",
       "      <td>1.979326</td>\n",
       "      <td>0.629203</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.187590</td>\n",
       "      <td>0.543518</td>\n",
       "      <td>0.301996</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.167211</td>\n",
       "      <td>0.743554</td>\n",
       "      <td>1.196940</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.141552</td>\n",
       "      <td>0.678608</td>\n",
       "      <td>1.979326</td>\n",
       "      <td>0.629203</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.187590</td>\n",
       "      <td>0.543518</td>\n",
       "      <td>0.301996</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.167211</td>\n",
       "      <td>0.743554</td>\n",
       "      <td>1.196940</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.141552</td>\n",
       "      <td>0.678608</td>\n",
       "      <td>1.979326</td>\n",
       "      <td>0.629203</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.187590</td>\n",
       "      <td>0.543518</td>\n",
       "      <td>0.301996</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.167211</td>\n",
       "      <td>0.743554</td>\n",
       "      <td>1.196940</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.141552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615324</td>\n",
       "      <td>0.621060</td>\n",
       "      <td>0.570071</td>\n",
       "      <td>0.582584</td>\n",
       "      <td>0.697720</td>\n",
       "      <td>0.7024</td>\n",
       "      <td>0.657838</td>\n",
       "      <td>0.713871</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.624396</td>\n",
       "      <td>0.628395</td>\n",
       "      <td>0.585812</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.685488</td>\n",
       "      <td>0.6964</td>\n",
       "      <td>0.660172</td>\n",
       "      <td>0.707446</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.629091</td>\n",
       "      <td>0.629137</td>\n",
       "      <td>0.592212</td>\n",
       "      <td>0.595413</td>\n",
       "      <td>0.675088</td>\n",
       "      <td>0.6816</td>\n",
       "      <td>0.634520</td>\n",
       "      <td>0.698263</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.932909</td>\n",
       "      <td>4.607446</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.198959</td>\n",
       "      <td>0.699143</td>\n",
       "      <td>0.724051</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.183146</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.751844</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.176855</td>\n",
       "      <td>0.932909</td>\n",
       "      <td>4.607446</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.198959</td>\n",
       "      <td>0.699143</td>\n",
       "      <td>0.724051</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.183146</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.751844</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.176855</td>\n",
       "      <td>0.932909</td>\n",
       "      <td>4.607446</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.198959</td>\n",
       "      <td>0.699143</td>\n",
       "      <td>0.724051</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.183146</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.751844</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.176855</td>\n",
       "      <td>0.932909</td>\n",
       "      <td>4.607446</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.198959</td>\n",
       "      <td>0.699143</td>\n",
       "      <td>0.724051</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.183146</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.751844</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.176855</td>\n",
       "      <td>0.932909</td>\n",
       "      <td>4.607446</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.198959</td>\n",
       "      <td>0.699143</td>\n",
       "      <td>0.724051</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.183146</td>\n",
       "      <td>0.692465</td>\n",
       "      <td>0.751844</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.176855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.558145</td>\n",
       "      <td>0.571786</td>\n",
       "      <td>0.432231</td>\n",
       "      <td>0.467675</td>\n",
       "      <td>0.801384</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.730018</td>\n",
       "      <td>0.832150</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.561438</td>\n",
       "      <td>0.570232</td>\n",
       "      <td>0.450048</td>\n",
       "      <td>0.477259</td>\n",
       "      <td>0.794384</td>\n",
       "      <td>0.8116</td>\n",
       "      <td>0.774035</td>\n",
       "      <td>0.833266</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.591708</td>\n",
       "      <td>0.605229</td>\n",
       "      <td>0.507315</td>\n",
       "      <td>0.535865</td>\n",
       "      <td>0.749232</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.657251</td>\n",
       "      <td>0.754540</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.916003</td>\n",
       "      <td>2.128734</td>\n",
       "      <td>0.386765</td>\n",
       "      <td>0.557795</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.709916</td>\n",
       "      <td>0.766176</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.111354</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.663386</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105651</td>\n",
       "      <td>0.916003</td>\n",
       "      <td>2.128734</td>\n",
       "      <td>0.386765</td>\n",
       "      <td>0.557795</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.709916</td>\n",
       "      <td>0.766176</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.111354</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.663386</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105651</td>\n",
       "      <td>0.916003</td>\n",
       "      <td>2.128734</td>\n",
       "      <td>0.386765</td>\n",
       "      <td>0.557795</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.709916</td>\n",
       "      <td>0.766176</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.111354</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.663386</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105651</td>\n",
       "      <td>0.916003</td>\n",
       "      <td>2.128734</td>\n",
       "      <td>0.386765</td>\n",
       "      <td>0.557795</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.709916</td>\n",
       "      <td>0.766176</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.111354</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.663386</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105651</td>\n",
       "      <td>0.916003</td>\n",
       "      <td>2.128734</td>\n",
       "      <td>0.386765</td>\n",
       "      <td>0.557795</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.709916</td>\n",
       "      <td>0.766176</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.111354</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.663386</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601535</td>\n",
       "      <td>0.609163</td>\n",
       "      <td>0.534350</td>\n",
       "      <td>0.554284</td>\n",
       "      <td>0.721272</td>\n",
       "      <td>0.7140</td>\n",
       "      <td>0.621994</td>\n",
       "      <td>0.743228</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.617085</td>\n",
       "      <td>0.628051</td>\n",
       "      <td>0.564542</td>\n",
       "      <td>0.587256</td>\n",
       "      <td>0.703632</td>\n",
       "      <td>0.6964</td>\n",
       "      <td>0.541300</td>\n",
       "      <td>0.687992</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.623358</td>\n",
       "      <td>0.640456</td>\n",
       "      <td>0.577334</td>\n",
       "      <td>0.618075</td>\n",
       "      <td>0.685616</td>\n",
       "      <td>0.6676</td>\n",
       "      <td>0.524440</td>\n",
       "      <td>0.633665</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.616447</td>\n",
       "      <td>0.546445</td>\n",
       "      <td>0.646860</td>\n",
       "      <td>0.466357</td>\n",
       "      <td>0.158791</td>\n",
       "      <td>0.679841</td>\n",
       "      <td>0.698207</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.172055</td>\n",
       "      <td>0.662996</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159159</td>\n",
       "      <td>0.616447</td>\n",
       "      <td>0.546445</td>\n",
       "      <td>0.646860</td>\n",
       "      <td>0.466357</td>\n",
       "      <td>0.158791</td>\n",
       "      <td>0.679841</td>\n",
       "      <td>0.698207</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.172055</td>\n",
       "      <td>0.662996</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159159</td>\n",
       "      <td>0.616447</td>\n",
       "      <td>0.546445</td>\n",
       "      <td>0.646860</td>\n",
       "      <td>0.466357</td>\n",
       "      <td>0.158791</td>\n",
       "      <td>0.679841</td>\n",
       "      <td>0.698207</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.172055</td>\n",
       "      <td>0.662996</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159159</td>\n",
       "      <td>0.616447</td>\n",
       "      <td>0.546445</td>\n",
       "      <td>0.646860</td>\n",
       "      <td>0.466357</td>\n",
       "      <td>0.158791</td>\n",
       "      <td>0.679841</td>\n",
       "      <td>0.698207</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.172055</td>\n",
       "      <td>0.662996</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159159</td>\n",
       "      <td>0.616447</td>\n",
       "      <td>0.546445</td>\n",
       "      <td>0.646860</td>\n",
       "      <td>0.466357</td>\n",
       "      <td>0.158791</td>\n",
       "      <td>0.679841</td>\n",
       "      <td>0.698207</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.172055</td>\n",
       "      <td>0.662996</td>\n",
       "      <td>0.582301</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663245</td>\n",
       "      <td>0.667209</td>\n",
       "      <td>0.646996</td>\n",
       "      <td>0.656362</td>\n",
       "      <td>0.612040</td>\n",
       "      <td>0.6048</td>\n",
       "      <td>0.500453</td>\n",
       "      <td>0.631956</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.664637</td>\n",
       "      <td>0.670496</td>\n",
       "      <td>0.651389</td>\n",
       "      <td>0.663789</td>\n",
       "      <td>0.608512</td>\n",
       "      <td>0.6164</td>\n",
       "      <td>0.530944</td>\n",
       "      <td>0.639304</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.671032</td>\n",
       "      <td>0.677291</td>\n",
       "      <td>0.661043</td>\n",
       "      <td>0.673879</td>\n",
       "      <td>0.586128</td>\n",
       "      <td>0.5924</td>\n",
       "      <td>0.456895</td>\n",
       "      <td>0.577606</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.925376</td>\n",
       "      <td>2.516976</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.160029</td>\n",
       "      <td>0.711171</td>\n",
       "      <td>0.767504</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.141309</td>\n",
       "      <td>0.675440</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.925376</td>\n",
       "      <td>2.516976</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.160029</td>\n",
       "      <td>0.711171</td>\n",
       "      <td>0.767504</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.141309</td>\n",
       "      <td>0.675440</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.925376</td>\n",
       "      <td>2.516976</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.160029</td>\n",
       "      <td>0.711171</td>\n",
       "      <td>0.767504</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.141309</td>\n",
       "      <td>0.675440</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.925376</td>\n",
       "      <td>2.516976</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.160029</td>\n",
       "      <td>0.711171</td>\n",
       "      <td>0.767504</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.141309</td>\n",
       "      <td>0.675440</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152041</td>\n",
       "      <td>0.925376</td>\n",
       "      <td>2.516976</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.160029</td>\n",
       "      <td>0.711171</td>\n",
       "      <td>0.767504</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.141309</td>\n",
       "      <td>0.675440</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.655486</td>\n",
       "      <td>0.660937</td>\n",
       "      <td>0.632567</td>\n",
       "      <td>0.642737</td>\n",
       "      <td>0.639288</td>\n",
       "      <td>0.6404</td>\n",
       "      <td>0.586414</td>\n",
       "      <td>0.665266</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.663903</td>\n",
       "      <td>0.669713</td>\n",
       "      <td>0.645446</td>\n",
       "      <td>0.655810</td>\n",
       "      <td>0.630664</td>\n",
       "      <td>0.6296</td>\n",
       "      <td>0.584749</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.665524</td>\n",
       "      <td>0.669850</td>\n",
       "      <td>0.650727</td>\n",
       "      <td>0.653842</td>\n",
       "      <td>0.625152</td>\n",
       "      <td>0.6220</td>\n",
       "      <td>0.535472</td>\n",
       "      <td>0.652609</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.829728</td>\n",
       "      <td>1.152108</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.509442</td>\n",
       "      <td>0.720780</td>\n",
       "      <td>0.795482</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.674239</td>\n",
       "      <td>0.644722</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138362</td>\n",
       "      <td>0.829728</td>\n",
       "      <td>1.152108</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.509442</td>\n",
       "      <td>0.720780</td>\n",
       "      <td>0.795482</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.674239</td>\n",
       "      <td>0.644722</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138362</td>\n",
       "      <td>0.829728</td>\n",
       "      <td>1.152108</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.509442</td>\n",
       "      <td>0.720780</td>\n",
       "      <td>0.795482</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.674239</td>\n",
       "      <td>0.644722</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138362</td>\n",
       "      <td>0.829728</td>\n",
       "      <td>1.152108</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.509442</td>\n",
       "      <td>0.720780</td>\n",
       "      <td>0.795482</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.674239</td>\n",
       "      <td>0.644722</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138362</td>\n",
       "      <td>0.829728</td>\n",
       "      <td>1.152108</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.509442</td>\n",
       "      <td>0.720780</td>\n",
       "      <td>0.795482</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.132544</td>\n",
       "      <td>0.674239</td>\n",
       "      <td>0.644722</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576694</td>\n",
       "      <td>0.595388</td>\n",
       "      <td>0.478087</td>\n",
       "      <td>0.515155</td>\n",
       "      <td>0.764944</td>\n",
       "      <td>0.7568</td>\n",
       "      <td>0.723026</td>\n",
       "      <td>0.814604</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.578661</td>\n",
       "      <td>0.592725</td>\n",
       "      <td>0.482621</td>\n",
       "      <td>0.520334</td>\n",
       "      <td>0.768768</td>\n",
       "      <td>0.7768</td>\n",
       "      <td>0.683907</td>\n",
       "      <td>0.796227</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.575104</td>\n",
       "      <td>0.573351</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.494084</td>\n",
       "      <td>0.772496</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>0.704840</td>\n",
       "      <td>0.801737</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.716565</td>\n",
       "      <td>0.745760</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>0.693429</td>\n",
       "      <td>0.738694</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112628</td>\n",
       "      <td>0.666590</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112174</td>\n",
       "      <td>0.716565</td>\n",
       "      <td>0.745760</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>0.693429</td>\n",
       "      <td>0.738694</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112628</td>\n",
       "      <td>0.666590</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112174</td>\n",
       "      <td>0.716565</td>\n",
       "      <td>0.745760</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>0.693429</td>\n",
       "      <td>0.738694</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112628</td>\n",
       "      <td>0.666590</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112174</td>\n",
       "      <td>0.716565</td>\n",
       "      <td>0.745760</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>0.693429</td>\n",
       "      <td>0.738694</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112628</td>\n",
       "      <td>0.666590</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112174</td>\n",
       "      <td>0.716565</td>\n",
       "      <td>0.745760</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>0.693429</td>\n",
       "      <td>0.738694</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112628</td>\n",
       "      <td>0.666590</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500636</td>\n",
       "      <td>0.507503</td>\n",
       "      <td>0.355413</td>\n",
       "      <td>0.381836</td>\n",
       "      <td>0.851424</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>0.849314</td>\n",
       "      <td>0.847048</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.495950</td>\n",
       "      <td>0.502726</td>\n",
       "      <td>0.352921</td>\n",
       "      <td>0.374794</td>\n",
       "      <td>0.855552</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>0.853042</td>\n",
       "      <td>0.857354</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.497636</td>\n",
       "      <td>0.481514</td>\n",
       "      <td>0.358982</td>\n",
       "      <td>0.345065</td>\n",
       "      <td>0.852752</td>\n",
       "      <td>0.8644</td>\n",
       "      <td>0.852423</td>\n",
       "      <td>0.870175</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.648073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640104</td>\n",
       "      <td>0.123411</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.546199</td>\n",
       "      <td>0.361998</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.066365</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>0.460748</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.064539</td>\n",
       "      <td>0.648073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640104</td>\n",
       "      <td>0.123411</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.546199</td>\n",
       "      <td>0.361998</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.066365</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>0.460748</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.064539</td>\n",
       "      <td>0.648073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640104</td>\n",
       "      <td>0.123411</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.546199</td>\n",
       "      <td>0.361998</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.066365</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>0.460748</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.064539</td>\n",
       "      <td>0.648073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640104</td>\n",
       "      <td>0.123411</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.546199</td>\n",
       "      <td>0.361998</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.066365</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>0.460748</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.064539</td>\n",
       "      <td>0.648073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640104</td>\n",
       "      <td>0.123411</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.546199</td>\n",
       "      <td>0.361998</td>\n",
       "      <td>0.854749</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.066365</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>0.460748</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.064539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618813</td>\n",
       "      <td>0.619496</td>\n",
       "      <td>0.566889</td>\n",
       "      <td>0.571428</td>\n",
       "      <td>0.707816</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.684986</td>\n",
       "      <td>0.725630</td>\n",
       "      <td>0.012285</td>\n",
       "      <td>0.012285</td>\n",
       "      <td>0.626683</td>\n",
       "      <td>0.632777</td>\n",
       "      <td>0.582282</td>\n",
       "      <td>0.592859</td>\n",
       "      <td>0.699248</td>\n",
       "      <td>0.6988</td>\n",
       "      <td>0.689632</td>\n",
       "      <td>0.700985</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>0.627731</td>\n",
       "      <td>0.626989</td>\n",
       "      <td>0.585449</td>\n",
       "      <td>0.587086</td>\n",
       "      <td>0.690080</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>0.673319</td>\n",
       "      <td>0.715146</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>1.368479</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.498401</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.516098</td>\n",
       "      <td>0.672035</td>\n",
       "      <td>0.623440</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.505987</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>1.368479</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.498401</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.516098</td>\n",
       "      <td>0.672035</td>\n",
       "      <td>0.623440</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.505987</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>1.368479</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.498401</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.516098</td>\n",
       "      <td>0.672035</td>\n",
       "      <td>0.623440</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.505987</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>1.368479</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.498401</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.516098</td>\n",
       "      <td>0.672035</td>\n",
       "      <td>0.623440</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.505987</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>1.368479</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.553470</td>\n",
       "      <td>0.498401</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.589957</td>\n",
       "      <td>0.921788</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.516098</td>\n",
       "      <td>0.672035</td>\n",
       "      <td>0.623440</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.505987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.534756</td>\n",
       "      <td>0.546961</td>\n",
       "      <td>0.404854</td>\n",
       "      <td>0.446135</td>\n",
       "      <td>0.811592</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>0.780269</td>\n",
       "      <td>0.810859</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.529476</td>\n",
       "      <td>0.534078</td>\n",
       "      <td>0.409803</td>\n",
       "      <td>0.434333</td>\n",
       "      <td>0.817736</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.805160</td>\n",
       "      <td>0.820632</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.546265</td>\n",
       "      <td>0.551414</td>\n",
       "      <td>0.445084</td>\n",
       "      <td>0.462184</td>\n",
       "      <td>0.797680</td>\n",
       "      <td>0.7972</td>\n",
       "      <td>0.777014</td>\n",
       "      <td>0.784528</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.663431</td>\n",
       "      <td>0.663653</td>\n",
       "      <td>0.619070</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.165916</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.351944</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.168550</td>\n",
       "      <td>0.650548</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.175453</td>\n",
       "      <td>0.663431</td>\n",
       "      <td>0.663653</td>\n",
       "      <td>0.619070</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.165916</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.351944</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.168550</td>\n",
       "      <td>0.650548</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.175453</td>\n",
       "      <td>0.663431</td>\n",
       "      <td>0.663653</td>\n",
       "      <td>0.619070</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.165916</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.351944</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.168550</td>\n",
       "      <td>0.650548</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.175453</td>\n",
       "      <td>0.663431</td>\n",
       "      <td>0.663653</td>\n",
       "      <td>0.619070</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.165916</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.351944</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.168550</td>\n",
       "      <td>0.650548</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.175453</td>\n",
       "      <td>0.663431</td>\n",
       "      <td>0.663653</td>\n",
       "      <td>0.619070</td>\n",
       "      <td>0.029722</td>\n",
       "      <td>0.165916</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.351944</td>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.168550</td>\n",
       "      <td>0.650548</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.175453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519325</td>\n",
       "      <td>0.528722</td>\n",
       "      <td>0.375076</td>\n",
       "      <td>0.410062</td>\n",
       "      <td>0.835472</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>0.836710</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.530043</td>\n",
       "      <td>0.537276</td>\n",
       "      <td>0.407801</td>\n",
       "      <td>0.428573</td>\n",
       "      <td>0.814520</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.781907</td>\n",
       "      <td>0.807335</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.532692</td>\n",
       "      <td>0.527056</td>\n",
       "      <td>0.413731</td>\n",
       "      <td>0.426233</td>\n",
       "      <td>0.817888</td>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.792837</td>\n",
       "      <td>0.806362</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.665811</td>\n",
       "      <td>0.890117</td>\n",
       "      <td>0.628896</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.057173</td>\n",
       "      <td>0.555761</td>\n",
       "      <td>0.322628</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>0.620933</td>\n",
       "      <td>0.436572</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.665811</td>\n",
       "      <td>0.890117</td>\n",
       "      <td>0.628896</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.057173</td>\n",
       "      <td>0.555761</td>\n",
       "      <td>0.322628</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>0.620933</td>\n",
       "      <td>0.436572</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.665811</td>\n",
       "      <td>0.890117</td>\n",
       "      <td>0.628896</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.057173</td>\n",
       "      <td>0.555761</td>\n",
       "      <td>0.322628</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>0.620933</td>\n",
       "      <td>0.436572</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.665811</td>\n",
       "      <td>0.890117</td>\n",
       "      <td>0.628896</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.057173</td>\n",
       "      <td>0.555761</td>\n",
       "      <td>0.322628</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>0.620933</td>\n",
       "      <td>0.436572</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.665811</td>\n",
       "      <td>0.890117</td>\n",
       "      <td>0.628896</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.057173</td>\n",
       "      <td>0.555761</td>\n",
       "      <td>0.322628</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.059792</td>\n",
       "      <td>0.620933</td>\n",
       "      <td>0.436572</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.051095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568207</td>\n",
       "      <td>0.592585</td>\n",
       "      <td>0.468251</td>\n",
       "      <td>0.521947</td>\n",
       "      <td>0.774856</td>\n",
       "      <td>0.7660</td>\n",
       "      <td>0.729292</td>\n",
       "      <td>0.796960</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.564914</td>\n",
       "      <td>0.572966</td>\n",
       "      <td>0.465107</td>\n",
       "      <td>0.479598</td>\n",
       "      <td>0.782904</td>\n",
       "      <td>0.7816</td>\n",
       "      <td>0.742303</td>\n",
       "      <td>0.813753</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.558403</td>\n",
       "      <td>0.581837</td>\n",
       "      <td>0.454458</td>\n",
       "      <td>0.510890</td>\n",
       "      <td>0.775552</td>\n",
       "      <td>0.7672</td>\n",
       "      <td>0.626310</td>\n",
       "      <td>0.782768</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.005914</td>\n",
       "      <td>0.703703</td>\n",
       "      <td>0.715697</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.479583</td>\n",
       "      <td>0.708126</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.631936</td>\n",
       "      <td>0.669737</td>\n",
       "      <td>0.603190</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.381506</td>\n",
       "      <td>0.703703</td>\n",
       "      <td>0.715697</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.479583</td>\n",
       "      <td>0.708126</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.631936</td>\n",
       "      <td>0.669737</td>\n",
       "      <td>0.603190</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.381506</td>\n",
       "      <td>0.703703</td>\n",
       "      <td>0.715697</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.479583</td>\n",
       "      <td>0.708126</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.631936</td>\n",
       "      <td>0.669737</td>\n",
       "      <td>0.603190</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.381506</td>\n",
       "      <td>0.703703</td>\n",
       "      <td>0.715697</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.479583</td>\n",
       "      <td>0.708126</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.631936</td>\n",
       "      <td>0.669737</td>\n",
       "      <td>0.603190</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.381506</td>\n",
       "      <td>0.703703</td>\n",
       "      <td>0.715697</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.479583</td>\n",
       "      <td>0.708126</td>\n",
       "      <td>0.746740</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.631936</td>\n",
       "      <td>0.669737</td>\n",
       "      <td>0.603190</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.381506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.599501</td>\n",
       "      <td>0.604363</td>\n",
       "      <td>0.544520</td>\n",
       "      <td>0.555250</td>\n",
       "      <td>0.723024</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>0.667954</td>\n",
       "      <td>0.740947</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.607360</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.557965</td>\n",
       "      <td>0.571847</td>\n",
       "      <td>0.716288</td>\n",
       "      <td>0.7144</td>\n",
       "      <td>0.635500</td>\n",
       "      <td>0.723312</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.613469</td>\n",
       "      <td>0.616810</td>\n",
       "      <td>0.569203</td>\n",
       "      <td>0.567566</td>\n",
       "      <td>0.711184</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.677552</td>\n",
       "      <td>0.736998</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.881891</td>\n",
       "      <td>1.602950</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>0.715798</td>\n",
       "      <td>0.776117</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.359247</td>\n",
       "      <td>0.666689</td>\n",
       "      <td>0.581924</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962151</td>\n",
       "      <td>0.881891</td>\n",
       "      <td>1.602950</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>0.715798</td>\n",
       "      <td>0.776117</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.359247</td>\n",
       "      <td>0.666689</td>\n",
       "      <td>0.581924</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962151</td>\n",
       "      <td>0.881891</td>\n",
       "      <td>1.602950</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>0.715798</td>\n",
       "      <td>0.776117</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.359247</td>\n",
       "      <td>0.666689</td>\n",
       "      <td>0.581924</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962151</td>\n",
       "      <td>0.881891</td>\n",
       "      <td>1.602950</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>0.715798</td>\n",
       "      <td>0.776117</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.359247</td>\n",
       "      <td>0.666689</td>\n",
       "      <td>0.581924</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962151</td>\n",
       "      <td>0.881891</td>\n",
       "      <td>1.602950</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>0.715798</td>\n",
       "      <td>0.776117</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.359247</td>\n",
       "      <td>0.666689</td>\n",
       "      <td>0.581924</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.605348</td>\n",
       "      <td>0.623445</td>\n",
       "      <td>0.530687</td>\n",
       "      <td>0.574357</td>\n",
       "      <td>0.720656</td>\n",
       "      <td>0.7296</td>\n",
       "      <td>0.565610</td>\n",
       "      <td>0.759507</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.622524</td>\n",
       "      <td>0.639224</td>\n",
       "      <td>0.564273</td>\n",
       "      <td>0.603542</td>\n",
       "      <td>0.702112</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.564056</td>\n",
       "      <td>0.725790</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.615675</td>\n",
       "      <td>0.638357</td>\n",
       "      <td>0.547178</td>\n",
       "      <td>0.604461</td>\n",
       "      <td>0.720448</td>\n",
       "      <td>0.7092</td>\n",
       "      <td>0.606219</td>\n",
       "      <td>0.683598</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.555486</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.485876</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593215</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.555486</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.485876</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593215</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.555486</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.485876</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593215</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.555486</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.485876</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593215</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.555486</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.485876</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.605348</td>\n",
       "      <td>0.623445</td>\n",
       "      <td>0.530687</td>\n",
       "      <td>0.574357</td>\n",
       "      <td>0.720656</td>\n",
       "      <td>0.7296</td>\n",
       "      <td>0.565610</td>\n",
       "      <td>0.759507</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.622524</td>\n",
       "      <td>0.639224</td>\n",
       "      <td>0.564273</td>\n",
       "      <td>0.603542</td>\n",
       "      <td>0.702112</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.564056</td>\n",
       "      <td>0.725790</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.615675</td>\n",
       "      <td>0.638357</td>\n",
       "      <td>0.547178</td>\n",
       "      <td>0.604461</td>\n",
       "      <td>0.720448</td>\n",
       "      <td>0.7092</td>\n",
       "      <td>0.606219</td>\n",
       "      <td>0.683598</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.755470</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.714134</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551471</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.755470</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.714134</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551471</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.755470</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.714134</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551471</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.755470</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.714134</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551471</td>\n",
       "      <td>0.858416</td>\n",
       "      <td>1.342356</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.755470</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.651787</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>0.714134</td>\n",
       "      <td>0.674048</td>\n",
       "      <td>0.667738</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663125</td>\n",
       "      <td>0.669832</td>\n",
       "      <td>0.643762</td>\n",
       "      <td>0.659654</td>\n",
       "      <td>0.630576</td>\n",
       "      <td>0.6328</td>\n",
       "      <td>0.514058</td>\n",
       "      <td>0.565662</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.668534</td>\n",
       "      <td>0.677660</td>\n",
       "      <td>0.651373</td>\n",
       "      <td>0.664580</td>\n",
       "      <td>0.627200</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.667368</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>0.669033</td>\n",
       "      <td>0.672005</td>\n",
       "      <td>0.653429</td>\n",
       "      <td>0.655160</td>\n",
       "      <td>0.610736</td>\n",
       "      <td>0.6352</td>\n",
       "      <td>0.520229</td>\n",
       "      <td>0.608400</td>\n",
       "      <td>0.007768</td>\n",
       "      <td>0.007768</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.323923</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141383</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.323923</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141383</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.323923</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141383</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.323923</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141383</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.245350</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.323923</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.663125</td>\n",
       "      <td>0.669832</td>\n",
       "      <td>0.643762</td>\n",
       "      <td>0.659654</td>\n",
       "      <td>0.630576</td>\n",
       "      <td>0.6328</td>\n",
       "      <td>0.514058</td>\n",
       "      <td>0.565662</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.668534</td>\n",
       "      <td>0.677660</td>\n",
       "      <td>0.651373</td>\n",
       "      <td>0.664580</td>\n",
       "      <td>0.627200</td>\n",
       "      <td>0.6264</td>\n",
       "      <td>0.495587</td>\n",
       "      <td>0.667368</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.669033</td>\n",
       "      <td>0.672005</td>\n",
       "      <td>0.653429</td>\n",
       "      <td>0.655160</td>\n",
       "      <td>0.610736</td>\n",
       "      <td>0.6352</td>\n",
       "      <td>0.520229</td>\n",
       "      <td>0.608400</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.391079</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.295167</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136782</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.391079</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.295167</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136782</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.391079</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.295167</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136782</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.391079</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.295167</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136782</td>\n",
       "      <td>0.721825</td>\n",
       "      <td>1.056322</td>\n",
       "      <td>0.626132</td>\n",
       "      <td>0.671434</td>\n",
       "      <td>0.391079</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.726948</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.489451</td>\n",
       "      <td>0.295167</td>\n",
       "      <td>0.675858</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.544599</td>\n",
       "      <td>0.537525</td>\n",
       "      <td>0.452320</td>\n",
       "      <td>0.447258</td>\n",
       "      <td>0.794728</td>\n",
       "      <td>0.8064</td>\n",
       "      <td>0.773344</td>\n",
       "      <td>0.805798</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.535699</td>\n",
       "      <td>0.534339</td>\n",
       "      <td>0.436152</td>\n",
       "      <td>0.439162</td>\n",
       "      <td>0.804048</td>\n",
       "      <td>0.8112</td>\n",
       "      <td>0.768669</td>\n",
       "      <td>0.809154</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.558478</td>\n",
       "      <td>0.555701</td>\n",
       "      <td>0.478104</td>\n",
       "      <td>0.475710</td>\n",
       "      <td>0.768512</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.758808</td>\n",
       "      <td>0.781471</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.883034</td>\n",
       "      <td>1.618120</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.295744</td>\n",
       "      <td>0.648570</td>\n",
       "      <td>0.674465</td>\n",
       "      <td>0.653631</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.149793</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.639753</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106841</td>\n",
       "      <td>0.883034</td>\n",
       "      <td>1.618120</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.295744</td>\n",
       "      <td>0.648570</td>\n",
       "      <td>0.674465</td>\n",
       "      <td>0.653631</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.149793</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.639753</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106841</td>\n",
       "      <td>0.883034</td>\n",
       "      <td>1.618120</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.295744</td>\n",
       "      <td>0.648570</td>\n",
       "      <td>0.674465</td>\n",
       "      <td>0.653631</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.149793</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.639753</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106841</td>\n",
       "      <td>0.883034</td>\n",
       "      <td>1.618120</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.295744</td>\n",
       "      <td>0.648570</td>\n",
       "      <td>0.674465</td>\n",
       "      <td>0.653631</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.149793</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.639753</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106841</td>\n",
       "      <td>0.883034</td>\n",
       "      <td>1.618120</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.295744</td>\n",
       "      <td>0.648570</td>\n",
       "      <td>0.674465</td>\n",
       "      <td>0.653631</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.149793</td>\n",
       "      <td>0.679977</td>\n",
       "      <td>0.639753</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.639205</td>\n",
       "      <td>0.644592</td>\n",
       "      <td>0.606225</td>\n",
       "      <td>0.615278</td>\n",
       "      <td>0.664568</td>\n",
       "      <td>0.6708</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.676595</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.003577</td>\n",
       "      <td>0.640570</td>\n",
       "      <td>0.645164</td>\n",
       "      <td>0.609074</td>\n",
       "      <td>0.619771</td>\n",
       "      <td>0.657920</td>\n",
       "      <td>0.6588</td>\n",
       "      <td>0.601356</td>\n",
       "      <td>0.688688</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.641461</td>\n",
       "      <td>0.648018</td>\n",
       "      <td>0.609063</td>\n",
       "      <td>0.628702</td>\n",
       "      <td>0.649952</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.603879</td>\n",
       "      <td>0.670195</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>2.129727</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.455591</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.759995</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.224715</td>\n",
       "      <td>0.667189</td>\n",
       "      <td>0.582810</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188092</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>2.129727</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.455591</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.759995</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.224715</td>\n",
       "      <td>0.667189</td>\n",
       "      <td>0.582810</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188092</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>2.129727</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.455591</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.759995</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.224715</td>\n",
       "      <td>0.667189</td>\n",
       "      <td>0.582810</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188092</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>2.129727</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.455591</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.759995</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.224715</td>\n",
       "      <td>0.667189</td>\n",
       "      <td>0.582810</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188092</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>2.129727</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.455591</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.759995</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.224715</td>\n",
       "      <td>0.667189</td>\n",
       "      <td>0.582810</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.552895</td>\n",
       "      <td>0.570207</td>\n",
       "      <td>0.441008</td>\n",
       "      <td>0.465332</td>\n",
       "      <td>0.789448</td>\n",
       "      <td>0.7940</td>\n",
       "      <td>0.753085</td>\n",
       "      <td>0.817967</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.552182</td>\n",
       "      <td>0.547658</td>\n",
       "      <td>0.439546</td>\n",
       "      <td>0.450644</td>\n",
       "      <td>0.790624</td>\n",
       "      <td>0.8016</td>\n",
       "      <td>0.782703</td>\n",
       "      <td>0.827350</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.552709</td>\n",
       "      <td>0.570224</td>\n",
       "      <td>0.442382</td>\n",
       "      <td>0.472638</td>\n",
       "      <td>0.781872</td>\n",
       "      <td>0.7940</td>\n",
       "      <td>0.707957</td>\n",
       "      <td>0.801398</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.726350</td>\n",
       "      <td>0.771724</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.203054</td>\n",
       "      <td>0.677037</td>\n",
       "      <td>0.695385</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.666326</td>\n",
       "      <td>0.586550</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090359</td>\n",
       "      <td>0.726350</td>\n",
       "      <td>0.771724</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.203054</td>\n",
       "      <td>0.677037</td>\n",
       "      <td>0.695385</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.666326</td>\n",
       "      <td>0.586550</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090359</td>\n",
       "      <td>0.726350</td>\n",
       "      <td>0.771724</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.203054</td>\n",
       "      <td>0.677037</td>\n",
       "      <td>0.695385</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.666326</td>\n",
       "      <td>0.586550</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090359</td>\n",
       "      <td>0.726350</td>\n",
       "      <td>0.771724</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.203054</td>\n",
       "      <td>0.677037</td>\n",
       "      <td>0.695385</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.666326</td>\n",
       "      <td>0.586550</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090359</td>\n",
       "      <td>0.726350</td>\n",
       "      <td>0.771724</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.203054</td>\n",
       "      <td>0.677037</td>\n",
       "      <td>0.695385</td>\n",
       "      <td>0.620112</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.666326</td>\n",
       "      <td>0.586550</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664060</td>\n",
       "      <td>0.670436</td>\n",
       "      <td>0.642971</td>\n",
       "      <td>0.658035</td>\n",
       "      <td>0.636832</td>\n",
       "      <td>0.6344</td>\n",
       "      <td>0.366949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.663652</td>\n",
       "      <td>0.666265</td>\n",
       "      <td>0.642112</td>\n",
       "      <td>0.649940</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>0.463558</td>\n",
       "      <td>0.705995</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.667859</td>\n",
       "      <td>0.672864</td>\n",
       "      <td>0.652614</td>\n",
       "      <td>0.662687</td>\n",
       "      <td>0.623616</td>\n",
       "      <td>0.6316</td>\n",
       "      <td>0.326941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.154398</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078544</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.154398</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078544</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.154398</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078544</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.154398</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078544</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.154398</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078544</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664060</td>\n",
       "      <td>0.670436</td>\n",
       "      <td>0.642971</td>\n",
       "      <td>0.658035</td>\n",
       "      <td>0.636832</td>\n",
       "      <td>0.6344</td>\n",
       "      <td>0.366949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.663652</td>\n",
       "      <td>0.666265</td>\n",
       "      <td>0.642112</td>\n",
       "      <td>0.649940</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>0.463558</td>\n",
       "      <td>0.705995</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.667859</td>\n",
       "      <td>0.672864</td>\n",
       "      <td>0.652614</td>\n",
       "      <td>0.662687</td>\n",
       "      <td>0.623616</td>\n",
       "      <td>0.6316</td>\n",
       "      <td>0.326941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.175256</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.077407</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080533</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.175256</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.077407</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080533</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.175256</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.077407</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080533</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.175256</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.077407</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080533</td>\n",
       "      <td>0.857244</td>\n",
       "      <td>2.067929</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.175256</td>\n",
       "      <td>0.701440</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.077407</td>\n",
       "      <td>0.672936</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.545972</td>\n",
       "      <td>0.558385</td>\n",
       "      <td>0.421011</td>\n",
       "      <td>0.449461</td>\n",
       "      <td>0.808248</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.750413</td>\n",
       "      <td>0.827975</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.005278</td>\n",
       "      <td>0.558702</td>\n",
       "      <td>0.568499</td>\n",
       "      <td>0.451771</td>\n",
       "      <td>0.470891</td>\n",
       "      <td>0.795168</td>\n",
       "      <td>0.8064</td>\n",
       "      <td>0.713923</td>\n",
       "      <td>0.809258</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.562588</td>\n",
       "      <td>0.562118</td>\n",
       "      <td>0.457402</td>\n",
       "      <td>0.466247</td>\n",
       "      <td>0.786240</td>\n",
       "      <td>0.8100</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>0.794008</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.682691</td>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.622754</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.226720</td>\n",
       "      <td>0.593289</td>\n",
       "      <td>0.420265</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.132813</td>\n",
       "      <td>0.659728</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122194</td>\n",
       "      <td>0.682691</td>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.622754</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.226720</td>\n",
       "      <td>0.593289</td>\n",
       "      <td>0.420265</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.132813</td>\n",
       "      <td>0.659728</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122194</td>\n",
       "      <td>0.682691</td>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.622754</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.226720</td>\n",
       "      <td>0.593289</td>\n",
       "      <td>0.420265</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.132813</td>\n",
       "      <td>0.659728</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122194</td>\n",
       "      <td>0.682691</td>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.622754</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.226720</td>\n",
       "      <td>0.593289</td>\n",
       "      <td>0.420265</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.132813</td>\n",
       "      <td>0.659728</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122194</td>\n",
       "      <td>0.682691</td>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.622754</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.226720</td>\n",
       "      <td>0.593289</td>\n",
       "      <td>0.420265</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.132813</td>\n",
       "      <td>0.659728</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508685</td>\n",
       "      <td>0.506957</td>\n",
       "      <td>0.366384</td>\n",
       "      <td>0.364273</td>\n",
       "      <td>0.847968</td>\n",
       "      <td>0.8536</td>\n",
       "      <td>0.825867</td>\n",
       "      <td>0.858705</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.513280</td>\n",
       "      <td>0.491067</td>\n",
       "      <td>0.381681</td>\n",
       "      <td>0.357566</td>\n",
       "      <td>0.836424</td>\n",
       "      <td>0.8620</td>\n",
       "      <td>0.832245</td>\n",
       "      <td>0.861390</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.500022</td>\n",
       "      <td>0.491018</td>\n",
       "      <td>0.355468</td>\n",
       "      <td>0.348079</td>\n",
       "      <td>0.853600</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>0.836008</td>\n",
       "      <td>0.867382</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.862445</td>\n",
       "      <td>1.513847</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.125940</td>\n",
       "      <td>0.609558</td>\n",
       "      <td>0.488859</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.079489</td>\n",
       "      <td>0.725109</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.079203</td>\n",
       "      <td>0.862445</td>\n",
       "      <td>1.513847</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.125940</td>\n",
       "      <td>0.609558</td>\n",
       "      <td>0.488859</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.079489</td>\n",
       "      <td>0.725109</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.079203</td>\n",
       "      <td>0.862445</td>\n",
       "      <td>1.513847</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.125940</td>\n",
       "      <td>0.609558</td>\n",
       "      <td>0.488859</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.079489</td>\n",
       "      <td>0.725109</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.079203</td>\n",
       "      <td>0.862445</td>\n",
       "      <td>1.513847</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.125940</td>\n",
       "      <td>0.609558</td>\n",
       "      <td>0.488859</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.079489</td>\n",
       "      <td>0.725109</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.079203</td>\n",
       "      <td>0.862445</td>\n",
       "      <td>1.513847</td>\n",
       "      <td>0.383080</td>\n",
       "      <td>0.553952</td>\n",
       "      <td>0.125940</td>\n",
       "      <td>0.609558</td>\n",
       "      <td>0.488859</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.079489</td>\n",
       "      <td>0.725109</td>\n",
       "      <td>0.923983</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.079203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.616086</td>\n",
       "      <td>0.520497</td>\n",
       "      <td>0.543766</td>\n",
       "      <td>0.738752</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>0.586705</td>\n",
       "      <td>0.756931</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.616344</td>\n",
       "      <td>0.632673</td>\n",
       "      <td>0.552412</td>\n",
       "      <td>0.594544</td>\n",
       "      <td>0.725144</td>\n",
       "      <td>0.7204</td>\n",
       "      <td>0.659459</td>\n",
       "      <td>0.780181</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.617557</td>\n",
       "      <td>0.648650</td>\n",
       "      <td>0.558844</td>\n",
       "      <td>0.634307</td>\n",
       "      <td>0.705968</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.573817</td>\n",
       "      <td>0.722526</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.104810</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.104810</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.104810</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.104810</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105075</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.104810</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.537093</td>\n",
       "      <td>0.549897</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>0.427786</td>\n",
       "      <td>0.819792</td>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.776094</td>\n",
       "      <td>0.840765</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.549290</td>\n",
       "      <td>0.551016</td>\n",
       "      <td>0.428711</td>\n",
       "      <td>0.438074</td>\n",
       "      <td>0.807104</td>\n",
       "      <td>0.8152</td>\n",
       "      <td>0.737478</td>\n",
       "      <td>0.819930</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.548143</td>\n",
       "      <td>0.542524</td>\n",
       "      <td>0.425723</td>\n",
       "      <td>0.425769</td>\n",
       "      <td>0.808016</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.794493</td>\n",
       "      <td>0.797792</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.754582</td>\n",
       "      <td>0.855642</td>\n",
       "      <td>0.140949</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>0.666320</td>\n",
       "      <td>0.639893</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.594847</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085431</td>\n",
       "      <td>0.754582</td>\n",
       "      <td>0.855642</td>\n",
       "      <td>0.140949</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>0.666320</td>\n",
       "      <td>0.639893</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.594847</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085431</td>\n",
       "      <td>0.754582</td>\n",
       "      <td>0.855642</td>\n",
       "      <td>0.140949</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>0.666320</td>\n",
       "      <td>0.639893</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.594847</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085431</td>\n",
       "      <td>0.754582</td>\n",
       "      <td>0.855642</td>\n",
       "      <td>0.140949</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>0.666320</td>\n",
       "      <td>0.639893</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.594847</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085431</td>\n",
       "      <td>0.754582</td>\n",
       "      <td>0.855642</td>\n",
       "      <td>0.140949</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>0.666320</td>\n",
       "      <td>0.639893</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.078860</td>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.594847</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.616086</td>\n",
       "      <td>0.520497</td>\n",
       "      <td>0.543766</td>\n",
       "      <td>0.738752</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>0.586705</td>\n",
       "      <td>0.756931</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.616344</td>\n",
       "      <td>0.632673</td>\n",
       "      <td>0.552412</td>\n",
       "      <td>0.594544</td>\n",
       "      <td>0.725144</td>\n",
       "      <td>0.7204</td>\n",
       "      <td>0.659459</td>\n",
       "      <td>0.780181</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.617557</td>\n",
       "      <td>0.648650</td>\n",
       "      <td>0.558844</td>\n",
       "      <td>0.634307</td>\n",
       "      <td>0.705968</td>\n",
       "      <td>0.6736</td>\n",
       "      <td>0.573817</td>\n",
       "      <td>0.722526</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.116319</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115530</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.116319</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115530</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.116319</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115530</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.116319</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115530</td>\n",
       "      <td>0.819148</td>\n",
       "      <td>1.105667</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.116319</td>\n",
       "      <td>0.709508</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>0.672326</td>\n",
       "      <td>0.646152</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.649108</td>\n",
       "      <td>0.651385</td>\n",
       "      <td>0.622974</td>\n",
       "      <td>0.627763</td>\n",
       "      <td>0.650400</td>\n",
       "      <td>0.6472</td>\n",
       "      <td>0.611167</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.654713</td>\n",
       "      <td>0.657239</td>\n",
       "      <td>0.632698</td>\n",
       "      <td>0.640199</td>\n",
       "      <td>0.649032</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>0.617272</td>\n",
       "      <td>0.661316</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.652783</td>\n",
       "      <td>0.655962</td>\n",
       "      <td>0.629477</td>\n",
       "      <td>0.633672</td>\n",
       "      <td>0.646176</td>\n",
       "      <td>0.6592</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.659365</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.711192</td>\n",
       "      <td>0.750445</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.497421</td>\n",
       "      <td>0.729865</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.671381</td>\n",
       "      <td>0.635908</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144323</td>\n",
       "      <td>0.711192</td>\n",
       "      <td>0.750445</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.497421</td>\n",
       "      <td>0.729865</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.671381</td>\n",
       "      <td>0.635908</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144323</td>\n",
       "      <td>0.711192</td>\n",
       "      <td>0.750445</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.497421</td>\n",
       "      <td>0.729865</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.671381</td>\n",
       "      <td>0.635908</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144323</td>\n",
       "      <td>0.711192</td>\n",
       "      <td>0.750445</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.497421</td>\n",
       "      <td>0.729865</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.671381</td>\n",
       "      <td>0.635908</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144323</td>\n",
       "      <td>0.711192</td>\n",
       "      <td>0.750445</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.497421</td>\n",
       "      <td>0.729865</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.671381</td>\n",
       "      <td>0.635908</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.662408</td>\n",
       "      <td>0.668090</td>\n",
       "      <td>0.642125</td>\n",
       "      <td>0.653104</td>\n",
       "      <td>0.626416</td>\n",
       "      <td>0.6256</td>\n",
       "      <td>0.548478</td>\n",
       "      <td>0.668284</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.666993</td>\n",
       "      <td>0.672091</td>\n",
       "      <td>0.651918</td>\n",
       "      <td>0.661287</td>\n",
       "      <td>0.610896</td>\n",
       "      <td>0.6128</td>\n",
       "      <td>0.577398</td>\n",
       "      <td>0.641970</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.662811</td>\n",
       "      <td>0.668121</td>\n",
       "      <td>0.644034</td>\n",
       "      <td>0.653131</td>\n",
       "      <td>0.620144</td>\n",
       "      <td>0.6312</td>\n",
       "      <td>0.578541</td>\n",
       "      <td>0.661276</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.839257</td>\n",
       "      <td>1.249003</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.153652</td>\n",
       "      <td>0.711331</td>\n",
       "      <td>0.761270</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.676406</td>\n",
       "      <td>0.644446</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137836</td>\n",
       "      <td>0.839257</td>\n",
       "      <td>1.249003</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.153652</td>\n",
       "      <td>0.711331</td>\n",
       "      <td>0.761270</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.676406</td>\n",
       "      <td>0.644446</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137836</td>\n",
       "      <td>0.839257</td>\n",
       "      <td>1.249003</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.153652</td>\n",
       "      <td>0.711331</td>\n",
       "      <td>0.761270</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.676406</td>\n",
       "      <td>0.644446</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137836</td>\n",
       "      <td>0.839257</td>\n",
       "      <td>1.249003</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.153652</td>\n",
       "      <td>0.711331</td>\n",
       "      <td>0.761270</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.676406</td>\n",
       "      <td>0.644446</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137836</td>\n",
       "      <td>0.839257</td>\n",
       "      <td>1.249003</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.153652</td>\n",
       "      <td>0.711331</td>\n",
       "      <td>0.761270</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.676406</td>\n",
       "      <td>0.644446</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568561</td>\n",
       "      <td>0.563319</td>\n",
       "      <td>0.472390</td>\n",
       "      <td>0.465610</td>\n",
       "      <td>0.781256</td>\n",
       "      <td>0.7976</td>\n",
       "      <td>0.702921</td>\n",
       "      <td>0.805211</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.570752</td>\n",
       "      <td>0.561846</td>\n",
       "      <td>0.481992</td>\n",
       "      <td>0.477261</td>\n",
       "      <td>0.781408</td>\n",
       "      <td>0.7968</td>\n",
       "      <td>0.734370</td>\n",
       "      <td>0.802543</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.585348</td>\n",
       "      <td>0.570143</td>\n",
       "      <td>0.509789</td>\n",
       "      <td>0.487991</td>\n",
       "      <td>0.753552</td>\n",
       "      <td>0.7872</td>\n",
       "      <td>0.680751</td>\n",
       "      <td>0.784394</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.792118</td>\n",
       "      <td>0.981287</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.074685</td>\n",
       "      <td>0.642949</td>\n",
       "      <td>0.559280</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.073720</td>\n",
       "      <td>0.676467</td>\n",
       "      <td>0.650746</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079789</td>\n",
       "      <td>0.792118</td>\n",
       "      <td>0.981287</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.074685</td>\n",
       "      <td>0.642949</td>\n",
       "      <td>0.559280</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.073720</td>\n",
       "      <td>0.676467</td>\n",
       "      <td>0.650746</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079789</td>\n",
       "      <td>0.792118</td>\n",
       "      <td>0.981287</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.074685</td>\n",
       "      <td>0.642949</td>\n",
       "      <td>0.559280</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.073720</td>\n",
       "      <td>0.676467</td>\n",
       "      <td>0.650746</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079789</td>\n",
       "      <td>0.792118</td>\n",
       "      <td>0.981287</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.074685</td>\n",
       "      <td>0.642949</td>\n",
       "      <td>0.559280</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.073720</td>\n",
       "      <td>0.676467</td>\n",
       "      <td>0.650746</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079789</td>\n",
       "      <td>0.792118</td>\n",
       "      <td>0.981287</td>\n",
       "      <td>0.382926</td>\n",
       "      <td>0.553791</td>\n",
       "      <td>0.074685</td>\n",
       "      <td>0.642949</td>\n",
       "      <td>0.559280</td>\n",
       "      <td>0.720670</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.073720</td>\n",
       "      <td>0.676467</td>\n",
       "      <td>0.650746</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores_type  function_family_maximum_depth  \\\n",
       "0   vanilla1_inet_scores                              3   \n",
       "1   vanilla1_inet_scores                              4   \n",
       "2   vanilla1_inet_scores                              5   \n",
       "3   vanilla1_inet_scores                              3   \n",
       "4   vanilla1_inet_scores                              4   \n",
       "5   vanilla1_inet_scores                              5   \n",
       "6   vanilla1_inet_scores                              3   \n",
       "7   vanilla1_inet_scores                              4   \n",
       "8   vanilla1_inet_scores                              5   \n",
       "9   vanilla1_inet_scores                              3   \n",
       "10  vanilla1_inet_scores                              4   \n",
       "11  vanilla1_inet_scores                              5   \n",
       "12  vanilla1_inet_scores                              4   \n",
       "13  vanilla1_inet_scores                              4   \n",
       "14  vanilla1_inet_scores                              3   \n",
       "15  vanilla1_inet_scores                              3   \n",
       "16  vanilla1_inet_scores                              3   \n",
       "17  vanilla1_inet_scores                              3   \n",
       "18  vanilla1_inet_scores                              4   \n",
       "19  vanilla1_inet_scores                              4   \n",
       "20  vanilla1_inet_scores                              5   \n",
       "21  vanilla1_inet_scores                              5   \n",
       "22  vanilla1_inet_scores                              5   \n",
       "23  vanilla1_inet_scores                              5   \n",
       "24      SDT9_inet_scores                              4   \n",
       "25      SDT1_inet_scores                              4   \n",
       "26      SDT1_inet_scores                              4   \n",
       "27  vanilla1_inet_scores                              4   \n",
       "28      SDT1_inet_scores                              4   \n",
       "29  vanilla1_inet_scores                              4   \n",
       "30      SDT1_inet_scores                              4   \n",
       "31     SDT15_inet_scores                              4   \n",
       "32  vanilla1_inet_scores                              4   \n",
       "33     SDT15_inet_scores                              4   \n",
       "34      SDT9_inet_scores                              4   \n",
       "35      SDT1_inet_scores                              4   \n",
       "36      SDT1_inet_scores                              4   \n",
       "37      SDT1_inet_scores                              4   \n",
       "38      SDT1_inet_scores                              4   \n",
       "39     SDT15_inet_scores                              4   \n",
       "40     SDT15_inet_scores                              4   \n",
       "41      SDT9_inet_scores                              4   \n",
       "42      SDT1_inet_scores                              4   \n",
       "43      SDT9_inet_scores                              4   \n",
       "44     SDT15_inet_scores                              4   \n",
       "45     SDT15_inet_scores                              4   \n",
       "46      SDT1_inet_scores                              4   \n",
       "47      SDT9_inet_scores                              4   \n",
       "48      SDT1_inet_scores                              4   \n",
       "49      SDT9_inet_scores                              4   \n",
       "50      SDT1_inet_scores                              4   \n",
       "51  vanilla1_inet_scores                              4   \n",
       "52      SDT1_inet_scores                              4   \n",
       "53     SDT15_inet_scores                              4   \n",
       "\n",
       "    function_family_decision_sparsity function_family_dt_type  \\\n",
       "0                                   1                 vanilla   \n",
       "1                                   1                 vanilla   \n",
       "2                                   1                 vanilla   \n",
       "3                                   1                 vanilla   \n",
       "4                                   1                 vanilla   \n",
       "5                                   1                 vanilla   \n",
       "6                                   1                 vanilla   \n",
       "7                                   1                 vanilla   \n",
       "8                                   1                 vanilla   \n",
       "9                                   1                 vanilla   \n",
       "10                                  1                 vanilla   \n",
       "11                                  1                 vanilla   \n",
       "12                                  1                 vanilla   \n",
       "13                                  1                 vanilla   \n",
       "14                                  1                 vanilla   \n",
       "15                                  1                 vanilla   \n",
       "16                                  1                 vanilla   \n",
       "17                                  1                 vanilla   \n",
       "18                                  1                 vanilla   \n",
       "19                                  1                 vanilla   \n",
       "20                                  1                 vanilla   \n",
       "21                                  1                 vanilla   \n",
       "22                                  1                 vanilla   \n",
       "23                                  1                 vanilla   \n",
       "24                                  9                     SDT   \n",
       "25                                  1                     SDT   \n",
       "26                                  1                     SDT   \n",
       "27                                  1                 vanilla   \n",
       "28                                  1                     SDT   \n",
       "29                                  1                 vanilla   \n",
       "30                                  1                     SDT   \n",
       "31                                 15                     SDT   \n",
       "32                                  1                 vanilla   \n",
       "33                                 15                     SDT   \n",
       "34                                  9                     SDT   \n",
       "35                                  1                     SDT   \n",
       "36                                  1                     SDT   \n",
       "37                                  1                     SDT   \n",
       "38                                  1                     SDT   \n",
       "39                                 15                     SDT   \n",
       "40                                 15                     SDT   \n",
       "41                                  9                     SDT   \n",
       "42                                  1                     SDT   \n",
       "43                                  9                     SDT   \n",
       "44                                 15                     SDT   \n",
       "45                                 15                     SDT   \n",
       "46                                  1                     SDT   \n",
       "47                                  9                     SDT   \n",
       "48                                  1                     SDT   \n",
       "49                                  9                     SDT   \n",
       "50                                  1                     SDT   \n",
       "51                                  1                 vanilla   \n",
       "52                                  1                     SDT   \n",
       "53                                 15                     SDT   \n",
       "\n",
       "   data_dt_type_train  data_maximum_depth_train  data_number_of_variables  \\\n",
       "0             vanilla                         5                         9   \n",
       "1             vanilla                         5                         9   \n",
       "2             vanilla                         5                         9   \n",
       "3             vanilla                         5                         9   \n",
       "4             vanilla                         5                         9   \n",
       "5             vanilla                         5                         9   \n",
       "6             vanilla                         5                         9   \n",
       "7             vanilla                         5                         9   \n",
       "8             vanilla                         5                         9   \n",
       "9             vanilla                         5                         9   \n",
       "10            vanilla                         5                         9   \n",
       "11            vanilla                         5                         9   \n",
       "12            vanilla                         5                        15   \n",
       "13            vanilla                         5                        15   \n",
       "14            vanilla                         5                        15   \n",
       "15            vanilla                         5                        15   \n",
       "16            vanilla                         5                        15   \n",
       "17            vanilla                         5                        15   \n",
       "18            vanilla                         5                        15   \n",
       "19            vanilla                         5                        15   \n",
       "20            vanilla                         5                        15   \n",
       "21            vanilla                         5                        15   \n",
       "22            vanilla                         5                        15   \n",
       "23            vanilla                         5                        15   \n",
       "24            vanilla                         5                         9   \n",
       "25            vanilla                         5                         9   \n",
       "26            vanilla                         5                        15   \n",
       "27            vanilla                         5                         9   \n",
       "28            vanilla                         5                        15   \n",
       "29            vanilla                         5                        15   \n",
       "30            vanilla                         5                         9   \n",
       "31            vanilla                         5                        15   \n",
       "32            vanilla                         5                         9   \n",
       "33            vanilla                         5                        15   \n",
       "34            vanilla                         5                         9   \n",
       "35            vanilla                         5                         9   \n",
       "36            vanilla                         5                         9   \n",
       "37            vanilla                         5                        15   \n",
       "38            vanilla                         5                        15   \n",
       "39            vanilla                         5                        15   \n",
       "40            vanilla                         5                        15   \n",
       "41            vanilla                         5                         9   \n",
       "42            vanilla                         5                         9   \n",
       "43            vanilla                         5                         9   \n",
       "44            vanilla                         5                        15   \n",
       "45            vanilla                         5                        15   \n",
       "46            vanilla                         5                         9   \n",
       "47            vanilla                         5                         9   \n",
       "48            vanilla                         5                        15   \n",
       "49            vanilla                         5                         9   \n",
       "50            vanilla                         5                        15   \n",
       "51            vanilla                         5                        15   \n",
       "52            vanilla                         5                        15   \n",
       "53            vanilla                         5                        15   \n",
       "\n",
       "    data_noise_injected_level data_function_generation_type  \\\n",
       "0                           0   make_classification_trained   \n",
       "1                           0   make_classification_trained   \n",
       "2                           0   make_classification_trained   \n",
       "3                           0   make_classification_trained   \n",
       "4                           0   make_classification_trained   \n",
       "5                           0   make_classification_trained   \n",
       "6                           0  random_decision_tree_trained   \n",
       "7                           0  random_decision_tree_trained   \n",
       "8                           0  random_decision_tree_trained   \n",
       "9                           0  random_decision_tree_trained   \n",
       "10                          0  random_decision_tree_trained   \n",
       "11                          0  random_decision_tree_trained   \n",
       "12                          0  random_decision_tree_trained   \n",
       "13                          0  random_decision_tree_trained   \n",
       "14                          0  random_decision_tree_trained   \n",
       "15                          0  random_decision_tree_trained   \n",
       "16                          0  random_decision_tree_trained   \n",
       "17                          0  random_decision_tree_trained   \n",
       "18                          0  random_decision_tree_trained   \n",
       "19                          0  random_decision_tree_trained   \n",
       "20                          0  random_decision_tree_trained   \n",
       "21                          0  random_decision_tree_trained   \n",
       "22                          0  random_decision_tree_trained   \n",
       "23                          0  random_decision_tree_trained   \n",
       "24                          0   make_classification_trained   \n",
       "25                          0   make_classification_trained   \n",
       "26                          0  random_decision_tree_trained   \n",
       "27                          0  random_decision_tree_trained   \n",
       "28                          0   make_classification_trained   \n",
       "29                          0   make_classification_trained   \n",
       "30                          0  random_decision_tree_trained   \n",
       "31                          0   make_classification_trained   \n",
       "32                          0   make_classification_trained   \n",
       "33                          0  random_decision_tree_trained   \n",
       "34                          0  random_decision_tree_trained   \n",
       "35                          0  random_decision_tree_trained   \n",
       "36                          0   make_classification_trained   \n",
       "37                          0  random_decision_tree_trained   \n",
       "38                          0  random_decision_tree_trained   \n",
       "39                          0  random_decision_tree_trained   \n",
       "40                          0  random_decision_tree_trained   \n",
       "41                          0   make_classification_trained   \n",
       "42                          0   make_classification_trained   \n",
       "43                          0  random_decision_tree_trained   \n",
       "44                          0  random_decision_tree_trained   \n",
       "45                          0  random_decision_tree_trained   \n",
       "46                          0  random_decision_tree_trained   \n",
       "47                          0   make_classification_trained   \n",
       "48                          0  random_decision_tree_trained   \n",
       "49                          0  random_decision_tree_trained   \n",
       "50                          0  random_decision_tree_trained   \n",
       "51                          0   make_classification_trained   \n",
       "52                          0   make_classification_trained   \n",
       "53                          0   make_classification_trained   \n",
       "\n",
       "   data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "0                        []                            [128]   \n",
       "1                        []                            [128]   \n",
       "2                        []                            [128]   \n",
       "3              [0, 1, 2, 3]                            [128]   \n",
       "4              [0, 1, 2, 3]                            [128]   \n",
       "5              [0, 1, 2, 3]                            [128]   \n",
       "6                        []                            [128]   \n",
       "7                        []                            [128]   \n",
       "8                        []                            [128]   \n",
       "9              [0, 1, 2, 3]                            [128]   \n",
       "10             [0, 1, 2, 3]                            [128]   \n",
       "11             [0, 1, 2, 3]                            [128]   \n",
       "12             [0, 1, 2, 9]                            [128]   \n",
       "13             [0, 1, 2, 9]                            [128]   \n",
       "14                       []                            [128]   \n",
       "15                       []                            [128]   \n",
       "16             [0, 1, 2, 9]                            [128]   \n",
       "17             [0, 1, 2, 9]                            [128]   \n",
       "18                       []                            [128]   \n",
       "19                       []                            [128]   \n",
       "20             [0, 1, 2, 9]                            [128]   \n",
       "21             [0, 1, 2, 9]                            [128]   \n",
       "22                       []                            [128]   \n",
       "23                       []                            [128]   \n",
       "24                       []                            [128]   \n",
       "25                       []                            [128]   \n",
       "26                       []                            [128]   \n",
       "27                       []                            [128]   \n",
       "28                       []                            [128]   \n",
       "29                       []                            [128]   \n",
       "30                       []                            [128]   \n",
       "31                       []                            [128]   \n",
       "32                       []                            [128]   \n",
       "33                       []                            [128]   \n",
       "34                       []                            [128]   \n",
       "35             [0, 1, 2, 3]                            [128]   \n",
       "36             [0, 1, 2, 3]                            [128]   \n",
       "37             [0, 1, 2, 9]                            [128]   \n",
       "38             [0, 1, 2, 9]                            [128]   \n",
       "39             [0, 1, 2, 9]                            [128]   \n",
       "40             [0, 1, 2, 9]                            [128]   \n",
       "41             [0, 1, 2, 3]                            [128]   \n",
       "42                       []                            [128]   \n",
       "43             [0, 1, 2, 3]                            [128]   \n",
       "44                       []                            [128]   \n",
       "45                       []                            [128]   \n",
       "46                       []                            [128]   \n",
       "47                       []                            [128]   \n",
       "48                       []                            [128]   \n",
       "49                       []                            [128]   \n",
       "50                       []                            [128]   \n",
       "51                       []                            [128]   \n",
       "52                       []                            [128]   \n",
       "53                       []                            [128]   \n",
       "\n",
       "   lambda_net_optimizer_lambda             i_net_dense_layers  \\\n",
       "0                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "1                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "2                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "3                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "4                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "5                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "6                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "7                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "8                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "9                         adam  [1024, 1024, 256, 2048, 2048]   \n",
       "10                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "11                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "12                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "13                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "14                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "15                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "16                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "17                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "18                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "19                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "20                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "21                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "22                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "23                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "24                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "25                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "26                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "27                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "28                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "29                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "30                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "31                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "32                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "33                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "34                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "35                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "36                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "37                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "38                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "39                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "40                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "41                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "42                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "43                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "44                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "45                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "46                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "47                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "48                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "49                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "50                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "51                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "52                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "53                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "\n",
       "                i_net_dropout  i_net_learning_rate           i_net_loss  \\\n",
       "0   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "1   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "2   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "3   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "4   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "5   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "6   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "7   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "8   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "9   [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "10  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "11  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "12  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "13  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "14  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "15  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "16  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "17  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "18  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "19  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "20  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "21  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "22  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "23  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "24  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "25  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "26  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "27  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "28  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "29  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "30  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "31  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "32  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "33  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "34  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "35  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "36  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "37  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "38  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "39  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "40  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "41  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "42  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "43  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "44  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "45  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "46  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "47  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "48  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "49  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "50  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "51  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "52  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "53  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "0                               10000                                   3   \n",
       "1                               10000                                   3   \n",
       "2                               10000                                   3   \n",
       "3                               10000                                   3   \n",
       "4                               10000                                   3   \n",
       "5                               10000                                   3   \n",
       "6                               10000                                   3   \n",
       "7                               10000                                   3   \n",
       "8                               10000                                   3   \n",
       "9                               10000                                   3   \n",
       "10                              10000                                   3   \n",
       "11                              10000                                   3   \n",
       "12                              10000                                   3   \n",
       "13                              10000                                   3   \n",
       "14                              10000                                   3   \n",
       "15                              10000                                   3   \n",
       "16                              10000                                   3   \n",
       "17                              10000                                   3   \n",
       "18                              10000                                   3   \n",
       "19                              10000                                   3   \n",
       "20                              10000                                   3   \n",
       "21                              10000                                   3   \n",
       "22                              10000                                   3   \n",
       "23                              10000                                   3   \n",
       "24                              10000                                   1   \n",
       "25                              10000                                   3   \n",
       "26                              10000                                   3   \n",
       "27                              10000                                   3   \n",
       "28                              10000                                   3   \n",
       "29                              10000                                   3   \n",
       "30                              10000                                   3   \n",
       "31                              10000                                   1   \n",
       "32                              10000                                   3   \n",
       "33                              10000                                   1   \n",
       "34                              10000                                   1   \n",
       "35                              10000                                   3   \n",
       "36                              10000                                   3   \n",
       "37                              10000                                   3   \n",
       "38                              10000                                   3   \n",
       "39                              10000                                   1   \n",
       "40                              10000                                   1   \n",
       "41                              10000                                   1   \n",
       "42                              10000                                   3   \n",
       "43                              10000                                   1   \n",
       "44                              10000                                   1   \n",
       "45                              10000                                   1   \n",
       "46                              10000                                   3   \n",
       "47                              10000                                   1   \n",
       "48                              10000                                   3   \n",
       "49                              10000                                   1   \n",
       "50                              10000                                   3   \n",
       "51                              10000                                   3   \n",
       "52                              10000                                   3   \n",
       "53                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "0                        None      False               100   \n",
       "1                        None      False               100   \n",
       "2                        None      False               100   \n",
       "3                        None      False               100   \n",
       "4                        None      False               100   \n",
       "5                        None      False               100   \n",
       "6                        None      False               100   \n",
       "7                        None      False               100   \n",
       "8                        None      False               100   \n",
       "9                        None      False               100   \n",
       "10                       None      False               100   \n",
       "11                       None      False               100   \n",
       "12                       None      False               100   \n",
       "13                       None      False               100   \n",
       "14                       None      False               100   \n",
       "15                       None      False               100   \n",
       "16                       None      False               100   \n",
       "17                       None      False               100   \n",
       "18                       None      False               100   \n",
       "19                       None      False               100   \n",
       "20                       None      False               100   \n",
       "21                       None      False               100   \n",
       "22                       None      False               100   \n",
       "23                       None      False               100   \n",
       "24                       None       True                60   \n",
       "25                       None       True                60   \n",
       "26                       None       True                60   \n",
       "27                       None       True                20   \n",
       "28                       None       True                20   \n",
       "29                       None       True                20   \n",
       "30                       None       True                20   \n",
       "31                       None       True                20   \n",
       "32                       None       True                60   \n",
       "33                       None       True                60   \n",
       "34                       None       True                60   \n",
       "35                       None      False               100   \n",
       "36                       None      False               100   \n",
       "37                       None      False               100   \n",
       "38                       None      False               100   \n",
       "39                       None      False               100   \n",
       "40                       None      False               100   \n",
       "41                       None      False               100   \n",
       "42                       None      False               100   \n",
       "43                       None      False               100   \n",
       "44                       None      False               100   \n",
       "45                       None      False               100   \n",
       "46                       None      False               100   \n",
       "47                       None      False               100   \n",
       "48                       None      False               100   \n",
       "49                       None      False               100   \n",
       "50                       None      False               100   \n",
       "51                       None      False               100   \n",
       "52                       None      False               100   \n",
       "53                       None      False               100   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                 make_classification                    \n",
       "1                                 make_classification                    \n",
       "2                                 make_classification                    \n",
       "3                                 make_classification                    \n",
       "4                                 make_classification                    \n",
       "5                                 make_classification                    \n",
       "6                                 make_classification                    \n",
       "7                                 make_classification                    \n",
       "8                                 make_classification                    \n",
       "9                                 make_classification                    \n",
       "10                                make_classification                    \n",
       "11                                make_classification                    \n",
       "12                                make_classification                    \n",
       "13                                make_classification                    \n",
       "14                                make_classification                    \n",
       "15                                make_classification                    \n",
       "16                                make_classification                    \n",
       "17                                make_classification                    \n",
       "18                                make_classification                    \n",
       "19                                make_classification                    \n",
       "20                                make_classification                    \n",
       "21                                make_classification                    \n",
       "22                                make_classification                    \n",
       "23                                make_classification                    \n",
       "24                                make_classification                    \n",
       "25                                make_classification                    \n",
       "26                                make_classification                    \n",
       "27                                make_classification                    \n",
       "28                                make_classification                    \n",
       "29                                make_classification                    \n",
       "30                                make_classification                    \n",
       "31                                make_classification                    \n",
       "32                                make_classification                    \n",
       "33                                make_classification                    \n",
       "34                                make_classification                    \n",
       "35                                make_classification                    \n",
       "36                                make_classification                    \n",
       "37                                make_classification                    \n",
       "38                                make_classification                    \n",
       "39                                make_classification                    \n",
       "40                                make_classification                    \n",
       "41                                make_classification                    \n",
       "42                                make_classification                    \n",
       "43                                make_classification                    \n",
       "44                                make_classification                    \n",
       "45                                make_classification                    \n",
       "46                                make_classification                    \n",
       "47                                make_classification                    \n",
       "48                                make_classification                    \n",
       "49                                make_classification                    \n",
       "50                                make_classification                    \n",
       "51                                make_classification                    \n",
       "52                                make_classification                    \n",
       "53                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                   0                 \n",
       "1                                                   0                 \n",
       "2                                                   0                 \n",
       "3                                                   0                 \n",
       "4                                                   0                 \n",
       "5                                                   0                 \n",
       "6                                                   0                 \n",
       "7                                                   0                 \n",
       "8                                                   0                 \n",
       "9                                                   0                 \n",
       "10                                                  0                 \n",
       "11                                                  0                 \n",
       "12                                                  0                 \n",
       "13                                                  0                 \n",
       "14                                                  0                 \n",
       "15                                                  0                 \n",
       "16                                                  0                 \n",
       "17                                                  0                 \n",
       "18                                                  0                 \n",
       "19                                                  0                 \n",
       "20                                                  0                 \n",
       "21                                                  0                 \n",
       "22                                                  0                 \n",
       "23                                                  0                 \n",
       "24                                                  0                 \n",
       "25                                                  0                 \n",
       "26                                                  0                 \n",
       "27                                                  0                 \n",
       "28                                                  0                 \n",
       "29                                                  0                 \n",
       "30                                                  0                 \n",
       "31                                                  0                 \n",
       "32                                                  0                 \n",
       "33                                                  0                 \n",
       "34                                                  0                 \n",
       "35                                                  0                 \n",
       "36                                                  0                 \n",
       "37                                                  0                 \n",
       "38                                                  0                 \n",
       "39                                                  0                 \n",
       "40                                                  0                 \n",
       "41                                                  0                 \n",
       "42                                                  0                 \n",
       "43                                                  0                 \n",
       "44                                                  0                 \n",
       "45                                                  0                 \n",
       "46                                                  0                 \n",
       "47                                                  0                 \n",
       "48                                                  0                 \n",
       "49                                                  0                 \n",
       "50                                                  0                 \n",
       "51                                                  0                 \n",
       "52                                                  0                 \n",
       "53                                                  0                 \n",
       "\n",
       "    train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "0                         0.616474                               0.615307   \n",
       "1                         0.612697                               0.613249   \n",
       "2                         0.615532                               0.617586   \n",
       "3                         0.606874                               0.613907   \n",
       "4                         0.600036                               0.600425   \n",
       "5                         0.603798                               0.608298   \n",
       "6                         0.593606                               0.604692   \n",
       "7                         0.588485                               0.598005   \n",
       "8                         0.584428                               0.587725   \n",
       "9                         0.601992                               0.608825   \n",
       "10                        0.591961                               0.607694   \n",
       "11                        0.582421                               0.592995   \n",
       "12                        0.610132                               0.612440   \n",
       "13                        0.610132                               0.612440   \n",
       "14                        0.613652                               0.620513   \n",
       "15                        0.613652                               0.620513   \n",
       "16                        0.619931                               0.623571   \n",
       "17                        0.619931                               0.623571   \n",
       "18                        0.613866                               0.621291   \n",
       "19                        0.613866                               0.621291   \n",
       "20                        0.602437                               0.605325   \n",
       "21                        0.602437                               0.605325   \n",
       "22                        0.606206                               0.615274   \n",
       "23                        0.606206                               0.615274   \n",
       "24                        0.478518                               0.480919   \n",
       "25                        0.615324                               0.621060   \n",
       "26                        0.558145                               0.571786   \n",
       "27                        0.601535                               0.609163   \n",
       "28                        0.663245                               0.667209   \n",
       "29                        0.655486                               0.660937   \n",
       "30                        0.576694                               0.595388   \n",
       "31                        0.500636                               0.507503   \n",
       "32                        0.618813                               0.619496   \n",
       "33                        0.534756                               0.546961   \n",
       "34                        0.519325                               0.528722   \n",
       "35                        0.568207                               0.592585   \n",
       "36                        0.599501                               0.604363   \n",
       "37                        0.605348                               0.623445   \n",
       "38                        0.605348                               0.623445   \n",
       "39                        0.663125                               0.669832   \n",
       "40                        0.663125                               0.669832   \n",
       "41                        0.544599                               0.537525   \n",
       "42                        0.639205                               0.644592   \n",
       "43                        0.552895                               0.570207   \n",
       "44                        0.664060                               0.670436   \n",
       "45                        0.664060                               0.670436   \n",
       "46                        0.545972                               0.558385   \n",
       "47                        0.508685                               0.506957   \n",
       "48                        0.601382                               0.616086   \n",
       "49                        0.537093                               0.549897   \n",
       "50                        0.601382                               0.616086   \n",
       "51                        0.649108                               0.651385   \n",
       "52                        0.662408                               0.668090   \n",
       "53                        0.568561                               0.563319   \n",
       "\n",
       "    train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "0                    0.569003                          0.567067   \n",
       "1                    0.564528                          0.564254   \n",
       "2                    0.567170                          0.568291   \n",
       "3                    0.561997                          0.565275   \n",
       "4                    0.549041                          0.550977   \n",
       "5                    0.558632                          0.566768   \n",
       "6                    0.517703                          0.539913   \n",
       "7                    0.509681                          0.527241   \n",
       "8                    0.500297                          0.511078   \n",
       "9                    0.535755                          0.548793   \n",
       "10                   0.514091                          0.551851   \n",
       "11                   0.501418                          0.527301   \n",
       "12                   0.546456                          0.553418   \n",
       "13                   0.546456                          0.553418   \n",
       "14                   0.547795                          0.562111   \n",
       "15                   0.547795                          0.562111   \n",
       "16                   0.562953                          0.574703   \n",
       "17                   0.562953                          0.574703   \n",
       "18                   0.549314                          0.561554   \n",
       "19                   0.549314                          0.561554   \n",
       "20                   0.530152                          0.541585   \n",
       "21                   0.530152                          0.541585   \n",
       "22                   0.538088                          0.556597   \n",
       "23                   0.538088                          0.556597   \n",
       "24                   0.317343                          0.342072   \n",
       "25                   0.570071                          0.582584   \n",
       "26                   0.432231                          0.467675   \n",
       "27                   0.534350                          0.554284   \n",
       "28                   0.646996                          0.656362   \n",
       "29                   0.632567                          0.642737   \n",
       "30                   0.478087                          0.515155   \n",
       "31                   0.355413                          0.381836   \n",
       "32                   0.566889                          0.571428   \n",
       "33                   0.404854                          0.446135   \n",
       "34                   0.375076                          0.410062   \n",
       "35                   0.468251                          0.521947   \n",
       "36                   0.544520                          0.555250   \n",
       "37                   0.530687                          0.574357   \n",
       "38                   0.530687                          0.574357   \n",
       "39                   0.643762                          0.659654   \n",
       "40                   0.643762                          0.659654   \n",
       "41                   0.452320                          0.447258   \n",
       "42                   0.606225                          0.615278   \n",
       "43                   0.441008                          0.465332   \n",
       "44                   0.642971                          0.658035   \n",
       "45                   0.642971                          0.658035   \n",
       "46                   0.421011                          0.449461   \n",
       "47                   0.366384                          0.364273   \n",
       "48                   0.520497                          0.543766   \n",
       "49                   0.400990                          0.427786   \n",
       "50                   0.520497                          0.543766   \n",
       "51                   0.622974                          0.627763   \n",
       "52                   0.642125                          0.653104   \n",
       "53                   0.472390                          0.465610   \n",
       "\n",
       "    train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "0         0.703344                 0.7024        0.664482   \n",
       "1         0.705632                 0.7028        0.678824   \n",
       "2         0.708408                 0.7088        0.664349   \n",
       "3         0.706720                 0.6992        0.674549   \n",
       "4         0.719944                 0.7132        0.690287   \n",
       "5         0.714272                 0.7084        0.681626   \n",
       "6         0.734976                 0.7268        0.669948   \n",
       "7         0.734600                 0.7264        0.662172   \n",
       "8         0.741600                 0.7336        0.686643   \n",
       "9         0.712848                 0.7140        0.667927   \n",
       "10        0.734016                 0.7196        0.676836   \n",
       "11        0.749272                 0.7396        0.697906   \n",
       "12        0.714528                 0.7244        0.589649   \n",
       "13        0.714528                 0.7244        0.589649   \n",
       "14        0.711688                 0.7148        0.607688   \n",
       "15        0.711688                 0.7148        0.607688   \n",
       "16        0.699576                 0.7040        0.567229   \n",
       "17        0.699576                 0.7040        0.567229   \n",
       "18        0.712816                 0.7152        0.588271   \n",
       "19        0.712816                 0.7152        0.588271   \n",
       "20        0.723880                 0.7300        0.624135   \n",
       "21        0.723880                 0.7300        0.624135   \n",
       "22        0.724240                 0.7208        0.603282   \n",
       "23        0.724240                 0.7208        0.603282   \n",
       "24        0.863512                 0.8572        0.859648   \n",
       "25        0.697720                 0.7024        0.657838   \n",
       "26        0.801384                 0.7900        0.730018   \n",
       "27        0.721272                 0.7140        0.621994   \n",
       "28        0.612040                 0.6048        0.500453   \n",
       "29        0.639288                 0.6404        0.586414   \n",
       "30        0.764944                 0.7568        0.723026   \n",
       "31        0.851424                 0.8472        0.849314   \n",
       "32        0.707816                 0.7132        0.684986   \n",
       "33        0.811592                 0.8008        0.780269   \n",
       "34        0.835472                 0.8316        0.819074   \n",
       "35        0.774856                 0.7660        0.729292   \n",
       "36        0.723024                 0.7372        0.667954   \n",
       "37        0.720656                 0.7296        0.565610   \n",
       "38        0.720656                 0.7296        0.565610   \n",
       "39        0.630576                 0.6328        0.514058   \n",
       "40        0.630576                 0.6328        0.514058   \n",
       "41        0.794728                 0.8064        0.773344   \n",
       "42        0.664568                 0.6708        0.567087   \n",
       "43        0.789448                 0.7940        0.753085   \n",
       "44        0.636832                 0.6344        0.366949   \n",
       "45        0.636832                 0.6344        0.366949   \n",
       "46        0.808248                 0.8132        0.750413   \n",
       "47        0.847968                 0.8536        0.825867   \n",
       "48        0.738752                 0.7372        0.586705   \n",
       "49        0.819792                 0.8272        0.776094   \n",
       "50        0.738752                 0.7372        0.586705   \n",
       "51        0.650400                 0.6472        0.611167   \n",
       "52        0.626416                 0.6256        0.548478   \n",
       "53        0.781256                 0.7976        0.702921   \n",
       "\n",
       "    train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "0                0.695851       0.000763              0.000763   \n",
       "1                0.705614       0.000880              0.000880   \n",
       "2                0.717616       0.001042              0.001042   \n",
       "3                0.714300       0.001107              0.001107   \n",
       "4                0.714838       0.001228              0.001228   \n",
       "5                0.724234       0.001385              0.001385   \n",
       "6                0.775119       0.001062              0.001062   \n",
       "7                0.765426       0.000971              0.000971   \n",
       "8                0.756975       0.000920              0.000920   \n",
       "9                0.755867       0.001198              0.001198   \n",
       "10               0.757150       0.001080              0.001080   \n",
       "11               0.777340       0.000928              0.000928   \n",
       "12               0.722003       0.001008              0.001008   \n",
       "13               0.722003       0.001147              0.001147   \n",
       "14               0.705994       0.000817              0.000817   \n",
       "15               0.705994       0.000798              0.000798   \n",
       "16               0.711376       0.000869              0.000869   \n",
       "17               0.711376       0.001009              0.001009   \n",
       "18               0.700661       0.001181              0.001181   \n",
       "19               0.700661       0.001033              0.001033   \n",
       "20               0.700890       0.001251              0.001251   \n",
       "21               0.700890       0.001036              0.001036   \n",
       "22               0.727100       0.000892              0.000892   \n",
       "23               0.727100       0.001046              0.001046   \n",
       "24               0.856860       0.004424              0.004424   \n",
       "25               0.713871       0.000850              0.000850   \n",
       "26               0.832150       0.000727              0.000727   \n",
       "27               0.743228       0.000905              0.000905   \n",
       "28               0.631956       0.000937              0.000937   \n",
       "29               0.665266       0.000919              0.000919   \n",
       "30               0.814604       0.000883              0.000883   \n",
       "31               0.847048       0.000759              0.000759   \n",
       "32               0.725630       0.012285              0.012285   \n",
       "33               0.810859       0.001617              0.001617   \n",
       "34               0.836710       0.000734              0.000734   \n",
       "35               0.796960       0.001440              0.001440   \n",
       "36               0.740947       0.000924              0.000924   \n",
       "37               0.759507       0.001637              0.001637   \n",
       "38               0.759507       0.001685              0.001685   \n",
       "39               0.565662       0.003697              0.003697   \n",
       "40               0.565662       0.004002              0.004002   \n",
       "41               0.805798       0.002398              0.002398   \n",
       "42               0.676595       0.003577              0.003577   \n",
       "43               0.817967       0.004092              0.004092   \n",
       "44               0.000000       0.004746              0.004746   \n",
       "45               0.000000       0.004689              0.004689   \n",
       "46               0.827975       0.005278              0.005278   \n",
       "47               0.858705       0.004920              0.004920   \n",
       "48               0.756931       0.003976              0.003976   \n",
       "49               0.840765       0.003789              0.003789   \n",
       "50               0.756931       0.004419              0.004419   \n",
       "51               0.656100       0.001350              0.001350   \n",
       "52               0.668284       0.000972              0.000972   \n",
       "53               0.805211       0.000825              0.000825   \n",
       "\n",
       "    valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "0                         0.619904                               0.617766   \n",
       "1                         0.616877                               0.620622   \n",
       "2                         0.617547                               0.623748   \n",
       "3                         0.612247                               0.613774   \n",
       "4                         0.605724                               0.609405   \n",
       "5                         0.600816                               0.604133   \n",
       "6                         0.596830                               0.606128   \n",
       "7                         0.593572                               0.604143   \n",
       "8                         0.589876                               0.594565   \n",
       "9                         0.595453                               0.593468   \n",
       "10                        0.591405                               0.591453   \n",
       "11                        0.586289                               0.590273   \n",
       "12                        0.610781                               0.612933   \n",
       "13                        0.610781                               0.612933   \n",
       "14                        0.609338                               0.610134   \n",
       "15                        0.609338                               0.610134   \n",
       "16                        0.618333                               0.619782   \n",
       "17                        0.618333                               0.619782   \n",
       "18                        0.609913                               0.614329   \n",
       "19                        0.609913                               0.614329   \n",
       "20                        0.607912                               0.608541   \n",
       "21                        0.607912                               0.608541   \n",
       "22                        0.611500                               0.612321   \n",
       "23                        0.611500                               0.612321   \n",
       "24                        0.475706                               0.464459   \n",
       "25                        0.624396                               0.628395   \n",
       "26                        0.561438                               0.570232   \n",
       "27                        0.617085                               0.628051   \n",
       "28                        0.664637                               0.670496   \n",
       "29                        0.663903                               0.669713   \n",
       "30                        0.578661                               0.592725   \n",
       "31                        0.495950                               0.502726   \n",
       "32                        0.626683                               0.632777   \n",
       "33                        0.529476                               0.534078   \n",
       "34                        0.530043                               0.537276   \n",
       "35                        0.564914                               0.572966   \n",
       "36                        0.607360                               0.612159   \n",
       "37                        0.622524                               0.639224   \n",
       "38                        0.622524                               0.639224   \n",
       "39                        0.668534                               0.677660   \n",
       "40                        0.668534                               0.677660   \n",
       "41                        0.535699                               0.534339   \n",
       "42                        0.640570                               0.645164   \n",
       "43                        0.552182                               0.547658   \n",
       "44                        0.663652                               0.666265   \n",
       "45                        0.663652                               0.666265   \n",
       "46                        0.558702                               0.568499   \n",
       "47                        0.513280                               0.491067   \n",
       "48                        0.616344                               0.632673   \n",
       "49                        0.549290                               0.551016   \n",
       "50                        0.616344                               0.632673   \n",
       "51                        0.654713                               0.657239   \n",
       "52                        0.666993                               0.672091   \n",
       "53                        0.570752                               0.561846   \n",
       "\n",
       "    valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "0                    0.574994                          0.573605   \n",
       "1                    0.570432                          0.575686   \n",
       "2                    0.571834                          0.579464   \n",
       "3                    0.571419                          0.572374   \n",
       "4                    0.555553                          0.562603   \n",
       "5                    0.550097                          0.551280   \n",
       "6                    0.525489                          0.540684   \n",
       "7                    0.518350                          0.525014   \n",
       "8                    0.512703                          0.525379   \n",
       "9                    0.526267                          0.527283   \n",
       "10                   0.515608                          0.513126   \n",
       "11                   0.505130                          0.504487   \n",
       "12                   0.548322                          0.544666   \n",
       "13                   0.548322                          0.544666   \n",
       "14                   0.542543                          0.550926   \n",
       "15                   0.542543                          0.550926   \n",
       "16                   0.561002                          0.563939   \n",
       "17                   0.561002                          0.563939   \n",
       "18                   0.545310                          0.557027   \n",
       "19                   0.545310                          0.557027   \n",
       "20                   0.539689                          0.552826   \n",
       "21                   0.539689                          0.552826   \n",
       "22                   0.548501                          0.545131   \n",
       "23                   0.548501                          0.545131   \n",
       "24                   0.323092                          0.336398   \n",
       "25                   0.585812                          0.589463   \n",
       "26                   0.450048                          0.477259   \n",
       "27                   0.564542                          0.587256   \n",
       "28                   0.651389                          0.663789   \n",
       "29                   0.645446                          0.655810   \n",
       "30                   0.482621                          0.520334   \n",
       "31                   0.352921                          0.374794   \n",
       "32                   0.582282                          0.592859   \n",
       "33                   0.409803                          0.434333   \n",
       "34                   0.407801                          0.428573   \n",
       "35                   0.465107                          0.479598   \n",
       "36                   0.557965                          0.571847   \n",
       "37                   0.564273                          0.603542   \n",
       "38                   0.564273                          0.603542   \n",
       "39                   0.651373                          0.664580   \n",
       "40                   0.651373                          0.664580   \n",
       "41                   0.436152                          0.439162   \n",
       "42                   0.609074                          0.619771   \n",
       "43                   0.439546                          0.450644   \n",
       "44                   0.642112                          0.649940   \n",
       "45                   0.642112                          0.649940   \n",
       "46                   0.451771                          0.470891   \n",
       "47                   0.381681                          0.357566   \n",
       "48                   0.552412                          0.594544   \n",
       "49                   0.428711                          0.438074   \n",
       "50                   0.552412                          0.594544   \n",
       "51                   0.632698                          0.640199   \n",
       "52                   0.651918                          0.661287   \n",
       "53                   0.481992                          0.477261   \n",
       "\n",
       "    valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "0         0.702496                 0.7152        0.673612   \n",
       "1         0.700600                 0.7136        0.688397   \n",
       "2         0.709880                 0.7084        0.692539   \n",
       "3         0.703800                 0.7056        0.649521   \n",
       "4         0.714264                 0.7112        0.681811   \n",
       "5         0.719472                 0.7228        0.668247   \n",
       "6         0.731568                 0.7308        0.624560   \n",
       "7         0.738560                 0.7368        0.618041   \n",
       "8         0.731776                 0.7360        0.616933   \n",
       "9         0.722192                 0.7228        0.683336   \n",
       "10        0.734728                 0.7336        0.696861   \n",
       "11        0.749736                 0.7484        0.709509   \n",
       "12        0.712248                 0.7156        0.631081   \n",
       "13        0.712248                 0.7156        0.631081   \n",
       "14        0.715672                 0.7200        0.638185   \n",
       "15        0.715672                 0.7200        0.638185   \n",
       "16        0.696880                 0.6964        0.614596   \n",
       "17        0.696880                 0.6964        0.614596   \n",
       "18        0.711752                 0.7188        0.616257   \n",
       "19        0.711752                 0.7188        0.616257   \n",
       "20        0.720456                 0.7240        0.646326   \n",
       "21        0.720456                 0.7240        0.646326   \n",
       "22        0.716688                 0.7256        0.649500   \n",
       "23        0.716688                 0.7256        0.649500   \n",
       "24        0.864424                 0.8732        0.865715   \n",
       "25        0.685488                 0.6964        0.660172   \n",
       "26        0.794384                 0.8116        0.774035   \n",
       "27        0.703632                 0.6964        0.541300   \n",
       "28        0.608512                 0.6164        0.530944   \n",
       "29        0.630664                 0.6296        0.584749   \n",
       "30        0.768768                 0.7768        0.683907   \n",
       "31        0.855552                 0.8520        0.853042   \n",
       "32        0.699248                 0.6988        0.689632   \n",
       "33        0.817736                 0.8200        0.805160   \n",
       "34        0.814520                 0.8108        0.781907   \n",
       "35        0.782904                 0.7816        0.742303   \n",
       "36        0.716288                 0.7144        0.635500   \n",
       "37        0.702112                 0.7060        0.564056   \n",
       "38        0.702112                 0.7060        0.564056   \n",
       "39        0.627200                 0.6264        0.495587   \n",
       "40        0.627200                 0.6264        0.495587   \n",
       "41        0.804048                 0.8112        0.768669   \n",
       "42        0.657920                 0.6588        0.601356   \n",
       "43        0.790624                 0.8016        0.782703   \n",
       "44        0.646200                 0.6612        0.463558   \n",
       "45        0.646200                 0.6612        0.463558   \n",
       "46        0.795168                 0.8064        0.713923   \n",
       "47        0.836424                 0.8620        0.832245   \n",
       "48        0.725144                 0.7204        0.659459   \n",
       "49        0.807104                 0.8152        0.737478   \n",
       "50        0.725144                 0.7204        0.659459   \n",
       "51        0.649032                 0.6576        0.617272   \n",
       "52        0.610896                 0.6128        0.577398   \n",
       "53        0.781408                 0.7968        0.734370   \n",
       "\n",
       "    valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "0                0.713898       0.000886              0.000886   \n",
       "1                0.715229       0.000795              0.000795   \n",
       "2                0.712673       0.000936              0.000936   \n",
       "3                0.715474       0.001154              0.001154   \n",
       "4                0.709280       0.001203              0.001203   \n",
       "5                0.716966       0.001114              0.001114   \n",
       "6                0.731509       0.001316              0.001316   \n",
       "7                0.734103       0.001069              0.001069   \n",
       "8                0.731953       0.000920              0.000920   \n",
       "9                0.754134       0.001441              0.001441   \n",
       "10               0.770805       0.000980              0.000980   \n",
       "11               0.780531       0.000948              0.000948   \n",
       "12               0.736423       0.000900              0.000900   \n",
       "13               0.736423       0.001261              0.001261   \n",
       "14               0.741005       0.000834              0.000834   \n",
       "15               0.741005       0.000780              0.000780   \n",
       "16               0.730612       0.001006              0.001006   \n",
       "17               0.730612       0.000917              0.000917   \n",
       "18               0.750342       0.000924              0.000924   \n",
       "19               0.750342       0.000867              0.000867   \n",
       "20               0.727354       0.000877              0.000877   \n",
       "21               0.727354       0.001133              0.001133   \n",
       "22               0.750342       0.001016              0.001016   \n",
       "23               0.750342       0.000939              0.000939   \n",
       "24               0.869372       0.000729              0.000729   \n",
       "25               0.707446       0.001109              0.001109   \n",
       "26               0.833266       0.000823              0.000823   \n",
       "27               0.687992       0.000766              0.000766   \n",
       "28               0.639304       0.001234              0.001234   \n",
       "29               0.635252       0.000859              0.000859   \n",
       "30               0.796227       0.000757              0.000757   \n",
       "31               0.857354       0.000742              0.000742   \n",
       "32               0.700985       0.009874              0.009874   \n",
       "33               0.820632       0.000707              0.000707   \n",
       "34               0.807335       0.000546              0.000546   \n",
       "35               0.813753       0.002010              0.002010   \n",
       "36               0.723312       0.003039              0.003039   \n",
       "37               0.725790       0.002325              0.002325   \n",
       "38               0.725790       0.002939              0.002939   \n",
       "39               0.667368       0.003547              0.003547   \n",
       "40               0.667368       0.004136              0.004136   \n",
       "41               0.809154       0.004622              0.004622   \n",
       "42               0.688688       0.005023              0.005023   \n",
       "43               0.827350       0.002976              0.002976   \n",
       "44               0.705995       0.003866              0.003866   \n",
       "45               0.705995       0.004488              0.004488   \n",
       "46               0.809258       0.003450              0.003450   \n",
       "47               0.861390       0.002548              0.002548   \n",
       "48               0.780181       0.002353              0.002353   \n",
       "49               0.819930       0.003010              0.003010   \n",
       "50               0.780181       0.002529              0.002529   \n",
       "51               0.661316       0.001376              0.001376   \n",
       "52               0.641970       0.000881              0.000881   \n",
       "53               0.802543       0.000980              0.000980   \n",
       "\n",
       "    test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "0                        0.626373                              0.629011   \n",
       "1                        0.622317                              0.622120   \n",
       "2                        0.626444                              0.628567   \n",
       "3                        0.611344                              0.617777   \n",
       "4                        0.609198                              0.605301   \n",
       "5                        0.612444                              0.612346   \n",
       "6                        0.593793                              0.597349   \n",
       "7                        0.595629                              0.586489   \n",
       "8                        0.583267                              0.588919   \n",
       "9                        0.597473                              0.601863   \n",
       "10                       0.598480                              0.606518   \n",
       "11                       0.594635                              0.605317   \n",
       "12                       0.614121                              0.613544   \n",
       "13                       0.614121                              0.613544   \n",
       "14                       0.620837                              0.610635   \n",
       "15                       0.620837                              0.610635   \n",
       "16                       0.620293                              0.617894   \n",
       "17                       0.620293                              0.617894   \n",
       "18                       0.623084                              0.625520   \n",
       "19                       0.623084                              0.625520   \n",
       "20                       0.617209                              0.608675   \n",
       "21                       0.617209                              0.608675   \n",
       "22                       0.616659                              0.619857   \n",
       "23                       0.616659                              0.619857   \n",
       "24                       0.464466                              0.447386   \n",
       "25                       0.629091                              0.629137   \n",
       "26                       0.591708                              0.605229   \n",
       "27                       0.623358                              0.640456   \n",
       "28                       0.671032                              0.677291   \n",
       "29                       0.665524                              0.669850   \n",
       "30                       0.575104                              0.573351   \n",
       "31                       0.497636                              0.481514   \n",
       "32                       0.627731                              0.626989   \n",
       "33                       0.546265                              0.551414   \n",
       "34                       0.532692                              0.527056   \n",
       "35                       0.558403                              0.581837   \n",
       "36                       0.613469                              0.616810   \n",
       "37                       0.615675                              0.638357   \n",
       "38                       0.615675                              0.638357   \n",
       "39                       0.669033                              0.672005   \n",
       "40                       0.669033                              0.672005   \n",
       "41                       0.558478                              0.555701   \n",
       "42                       0.641461                              0.648018   \n",
       "43                       0.552709                              0.570224   \n",
       "44                       0.667859                              0.672864   \n",
       "45                       0.667859                              0.672864   \n",
       "46                       0.562588                              0.562118   \n",
       "47                       0.500022                              0.491018   \n",
       "48                       0.617557                              0.648650   \n",
       "49                       0.548143                              0.542524   \n",
       "50                       0.617557                              0.648650   \n",
       "51                       0.652783                              0.655962   \n",
       "52                       0.662811                              0.668121   \n",
       "53                       0.585348                              0.570143   \n",
       "\n",
       "    test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "0                   0.583992                         0.589657       0.694880   \n",
       "1                   0.579808                         0.576846       0.698048   \n",
       "2                   0.586809                         0.590938       0.693312   \n",
       "3                   0.572884                         0.580001       0.703008   \n",
       "4                   0.563761                         0.562193       0.714608   \n",
       "5                   0.568646                         0.564193       0.709344   \n",
       "6                   0.517497                         0.520106       0.741728   \n",
       "7                   0.521241                         0.519973       0.720064   \n",
       "8                   0.498723                         0.507614       0.751248   \n",
       "9                   0.532463                         0.535753       0.726480   \n",
       "10                  0.531945                         0.551234       0.723888   \n",
       "11                  0.526056                         0.533470       0.730368   \n",
       "12                  0.552811                         0.559870       0.706832   \n",
       "13                  0.552811                         0.559870       0.706832   \n",
       "14                  0.567701                         0.553304       0.694880   \n",
       "15                  0.567701                         0.553304       0.694880   \n",
       "16                  0.570282                         0.575227       0.694880   \n",
       "17                  0.570282                         0.575227       0.694880   \n",
       "18                  0.569946                         0.579273       0.694848   \n",
       "19                  0.569946                         0.579273       0.694848   \n",
       "20                  0.557960                         0.545205       0.707360   \n",
       "21                  0.557960                         0.545205       0.707360   \n",
       "22                  0.560663                         0.568172       0.714912   \n",
       "23                  0.560663                         0.568172       0.714912   \n",
       "24                  0.297523                         0.285332       0.872592   \n",
       "25                  0.592212                         0.595413       0.675088   \n",
       "26                  0.507315                         0.535865       0.749232   \n",
       "27                  0.577334                         0.618075       0.685616   \n",
       "28                  0.661043                         0.673879       0.586128   \n",
       "29                  0.650727                         0.653842       0.625152   \n",
       "30                  0.481818                         0.494084       0.772496   \n",
       "31                  0.358982                         0.345065       0.852752   \n",
       "32                  0.585449                         0.587086       0.690080   \n",
       "33                  0.445084                         0.462184       0.797680   \n",
       "34                  0.413731                         0.426233       0.817888   \n",
       "35                  0.454458                         0.510890       0.775552   \n",
       "36                  0.569203                         0.567566       0.711184   \n",
       "37                  0.547178                         0.604461       0.720448   \n",
       "38                  0.547178                         0.604461       0.720448   \n",
       "39                  0.653429                         0.655160       0.610736   \n",
       "40                  0.653429                         0.655160       0.610736   \n",
       "41                  0.478104                         0.475710       0.768512   \n",
       "42                  0.609063                         0.628702       0.649952   \n",
       "43                  0.442382                         0.472638       0.781872   \n",
       "44                  0.652614                         0.662687       0.623616   \n",
       "45                  0.652614                         0.662687       0.623616   \n",
       "46                  0.457402                         0.466247       0.786240   \n",
       "47                  0.355468                         0.348079       0.853600   \n",
       "48                  0.558844                         0.634307       0.705968   \n",
       "49                  0.425723                         0.425769       0.808016   \n",
       "50                  0.558844                         0.634307       0.705968   \n",
       "51                  0.629477                         0.633672       0.646176   \n",
       "52                  0.644034                         0.653131       0.620144   \n",
       "53                  0.509789                         0.487991       0.753552   \n",
       "\n",
       "    test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "0                 0.6940       0.669380              0.723074      0.001105   \n",
       "1                 0.6976       0.675447              0.710239      0.001292   \n",
       "2                 0.6928       0.674189              0.703011      0.001507   \n",
       "3                 0.6980       0.659850              0.701589      0.002056   \n",
       "4                 0.7132       0.689956              0.724678      0.002074   \n",
       "5                 0.7016       0.689193              0.713004      0.002903   \n",
       "6                 0.7580       0.622474              0.777947      0.001922   \n",
       "7                 0.7408       0.601818              0.740038      0.001354   \n",
       "8                 0.7520       0.679994              0.764540      0.001282   \n",
       "9                 0.7196       0.615089              0.744318      0.002577   \n",
       "10                0.7136       0.615156              0.751243      0.001729   \n",
       "11                0.7172       0.603052              0.728388      0.001561   \n",
       "12                0.7088       0.623076              0.712979      0.001878   \n",
       "13                0.7088       0.623076              0.712979      0.001451   \n",
       "14                0.6960       0.511830              0.711640      0.001224   \n",
       "15                0.6960       0.511830              0.711640      0.001201   \n",
       "16                0.7068       0.594197              0.680762      0.001493   \n",
       "17                0.7068       0.594197              0.680762      0.001438   \n",
       "18                0.6816       0.571529              0.684033      0.001049   \n",
       "19                0.6816       0.571529              0.684033      0.001185   \n",
       "20                0.7180       0.624082              0.676755      0.001629   \n",
       "21                0.7180       0.624082              0.676755      0.001579   \n",
       "22                0.7120       0.591665              0.703552      0.001427   \n",
       "23                0.7120       0.591665              0.703552      0.001413   \n",
       "24                0.8936       0.867161              0.882328      0.001165   \n",
       "25                0.6816       0.634520              0.698263      0.001833   \n",
       "26                0.7456       0.657251              0.754540      0.001208   \n",
       "27                0.6676       0.524440              0.633665      0.001252   \n",
       "28                0.5924       0.456895              0.577606      0.002021   \n",
       "29                0.6220       0.535472              0.652609      0.001324   \n",
       "30                0.7764       0.704840              0.801737      0.001454   \n",
       "31                0.8644       0.852423              0.870175      0.001634   \n",
       "32                0.6944       0.673319              0.715146      0.001407   \n",
       "33                0.7972       0.777014              0.784528      0.000935   \n",
       "34                0.8312       0.792837              0.806362      0.000874   \n",
       "35                0.7672       0.626310              0.782768      0.005914   \n",
       "36                0.7220       0.677552              0.736998      0.006727   \n",
       "37                0.7092       0.606219              0.683598      0.007537   \n",
       "38                0.7092       0.606219              0.683598      0.004844   \n",
       "39                0.6352       0.520229              0.608400      0.007768   \n",
       "40                0.6352       0.520229              0.608400      0.008309   \n",
       "41                0.7900       0.758808              0.781471      0.006085   \n",
       "42                0.6500       0.603879              0.670195      0.005938   \n",
       "43                0.7940       0.707957              0.801398      0.004092   \n",
       "44                0.6316       0.326941              0.000000      0.003885   \n",
       "45                0.6316       0.326941              0.000000      0.002814   \n",
       "46                0.8100       0.743057              0.794008      0.003140   \n",
       "47                0.8656       0.836008              0.867382      0.002031   \n",
       "48                0.6736       0.573817              0.722526      0.001368   \n",
       "49                0.8276       0.794493              0.797792      0.001759   \n",
       "50                0.6736       0.573817              0.722526      0.001587   \n",
       "51                0.6592       0.608252              0.659365      0.001829   \n",
       "52                0.6312       0.578541              0.661276      0.001565   \n",
       "53                0.7872       0.680751              0.784394      0.001350   \n",
       "\n",
       "    test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "0              0.001105                             0.909159   \n",
       "1              0.001292                             0.933302   \n",
       "2              0.001507                             0.836600   \n",
       "3              0.002056                             0.905225   \n",
       "4              0.002074                             0.853145   \n",
       "5              0.002903                             0.854233   \n",
       "6              0.001922                             0.722902   \n",
       "7              0.001354                             0.677103   \n",
       "8              0.001282                             0.683051   \n",
       "9              0.002577                             0.733779   \n",
       "10             0.001729                             0.761217   \n",
       "11             0.001561                             0.706802   \n",
       "12             0.001878                             0.858197   \n",
       "13             0.001451                             0.858197   \n",
       "14             0.001224                             0.799476   \n",
       "15             0.001201                             0.799476   \n",
       "16             0.001493                             0.900496   \n",
       "17             0.001438                             0.900496   \n",
       "18             0.001049                             0.806887   \n",
       "19             0.001185                             0.806887   \n",
       "20             0.001629                             0.866803   \n",
       "21             0.001579                             0.866803   \n",
       "22             0.001427                             0.818518   \n",
       "23             0.001413                             0.818518   \n",
       "24             0.001165                             0.678608   \n",
       "25             0.001833                             0.932909   \n",
       "26             0.001208                             0.916003   \n",
       "27             0.001252                             0.616447   \n",
       "28             0.002021                             0.925376   \n",
       "29             0.001324                             0.829728   \n",
       "30             0.001454                             0.716565   \n",
       "31             0.001634                             0.648073   \n",
       "32             0.001407                             0.853060   \n",
       "33             0.000935                             0.663431   \n",
       "34             0.000874                             0.665811   \n",
       "35             0.005914                             0.703703   \n",
       "36             0.006727                             0.881891   \n",
       "37             0.007537                             0.858416   \n",
       "38             0.004844                             0.858416   \n",
       "39             0.007768                             0.721825   \n",
       "40             0.008309                             0.721825   \n",
       "41             0.006085                             0.883034   \n",
       "42             0.005938                             0.913043   \n",
       "43             0.004092                             0.726350   \n",
       "44             0.003885                             0.857244   \n",
       "45             0.002814                             0.857244   \n",
       "46             0.003140                             0.682691   \n",
       "47             0.002031                             0.862445   \n",
       "48             0.001368                             0.819148   \n",
       "49             0.001759                             0.754582   \n",
       "50             0.001587                             0.819148   \n",
       "51             0.001829                             0.711192   \n",
       "52             0.001565                             0.839257   \n",
       "53             0.001350                             0.792118   \n",
       "\n",
       "    binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "0                         2.012290             0.382619             0.553470   \n",
       "1                         0.000000             0.382619             0.553470   \n",
       "2                         1.238022             0.382619             0.553470   \n",
       "3                         1.944110             0.382619             0.553470   \n",
       "4                         0.000000             0.382619             0.553470   \n",
       "5                         1.729415             0.488868             0.595357   \n",
       "6                         0.762560             0.378934             0.549605   \n",
       "7                         0.671711             0.629971             0.045922   \n",
       "8                         0.675042             0.620912             0.004034   \n",
       "9                         0.793555             0.378934             0.549605   \n",
       "10                        0.873499             0.378934             0.549605   \n",
       "11                        0.723446             0.378934             0.549605   \n",
       "12                        1.524665             0.382926             0.553791   \n",
       "13                        1.524665             0.382926             0.553791   \n",
       "14                        1.013420             0.382926             0.553791   \n",
       "15                        1.013420             0.382926             0.553791   \n",
       "16                        1.792710             0.382926             0.553791   \n",
       "17                        1.792710             0.382926             0.553791   \n",
       "18                        1.056151             0.382926             0.553791   \n",
       "19                        1.056151             0.382926             0.553791   \n",
       "20                        1.464826             0.382926             0.553791   \n",
       "21                        1.464826             0.382926             0.553791   \n",
       "22                        1.111223             0.382926             0.553791   \n",
       "23                        1.111223             0.382926             0.553791   \n",
       "24                        1.979326             0.629203             0.059946   \n",
       "25                        4.607446             0.382619             0.553470   \n",
       "26                        2.128734             0.386765             0.557795   \n",
       "27                        0.546445             0.646860             0.466357   \n",
       "28                        2.516976             0.382926             0.553791   \n",
       "29                        1.152108             0.382926             0.553791   \n",
       "30                        0.745760             0.383080             0.553952   \n",
       "31                        0.000000             0.640104             0.123411   \n",
       "32                        1.368479             0.382619             0.553470   \n",
       "33                        0.663653             0.619070             0.029722   \n",
       "34                        0.890117             0.628896             0.040492   \n",
       "35                        0.715697             0.383080             0.553952   \n",
       "36                        1.602950             0.383080             0.553952   \n",
       "37                        1.342356             0.382926             0.553791   \n",
       "38                        1.342356             0.382926             0.553791   \n",
       "39                        1.056322             0.626132             0.671434   \n",
       "40                        1.056322             0.626132             0.671434   \n",
       "41                        1.618120             0.383080             0.553952   \n",
       "42                        2.129727             0.383080             0.553952   \n",
       "43                        0.771724             0.383080             0.553952   \n",
       "44                        2.067929             0.382926             0.553791   \n",
       "45                        2.067929             0.382926             0.553791   \n",
       "46                        0.675333             0.622754             0.030004   \n",
       "47                        1.513847             0.383080             0.553952   \n",
       "48                        1.105667             0.382926             0.553791   \n",
       "49                        0.855642             0.140949             0.079618   \n",
       "50                        1.105667             0.382926             0.553791   \n",
       "51                        0.750445             0.382926             0.553791   \n",
       "52                        1.249003             0.382926             0.553791   \n",
       "53                        0.981287             0.382926             0.553791   \n",
       "\n",
       "    runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "0             0.104282                               0.685014   \n",
       "1             0.116253                               0.649897   \n",
       "2             0.161816                               0.713819   \n",
       "3             0.142096                               0.718173   \n",
       "4             0.187451                               0.673761   \n",
       "5             0.346450                               0.647532   \n",
       "6             0.172872                               0.588987   \n",
       "7             0.208214                               0.624959   \n",
       "8             0.174649                               0.717925   \n",
       "9            48.206169                               0.663955   \n",
       "10            0.160855                               0.702805   \n",
       "11            0.177735                               0.691413   \n",
       "12            0.119750                               0.693719   \n",
       "13            0.137243                               0.693719   \n",
       "14            0.094413                               0.697205   \n",
       "15            0.098506                               0.697205   \n",
       "16            0.099299                               0.703797   \n",
       "17            0.101976                               0.703797   \n",
       "18            0.091847                               0.718736   \n",
       "19            0.433442                               0.718736   \n",
       "20            0.136998                               0.597506   \n",
       "21            0.156404                               0.597506   \n",
       "22            0.478246                               0.685908   \n",
       "23            0.148448                               0.685908   \n",
       "24            0.187590                               0.543518   \n",
       "25            0.198959                               0.699143   \n",
       "26            0.127063                               0.709916   \n",
       "27            0.158791                               0.679841   \n",
       "28            0.160029                               0.711171   \n",
       "29            0.509442                               0.720780   \n",
       "30            0.110954                               0.693429   \n",
       "31            0.063903                               0.546199   \n",
       "32            0.498401                               0.654627   \n",
       "33            0.165916                               0.555836   \n",
       "34            0.057173                               0.555761   \n",
       "35            0.479583                               0.708126   \n",
       "36            0.816631                               0.715798   \n",
       "37            0.555486                               0.664220   \n",
       "38            0.755470                               0.664220   \n",
       "39            0.245350                               0.705104   \n",
       "40            0.391079                               0.705104   \n",
       "41            0.295744                               0.648570   \n",
       "42            0.455591                               0.711864   \n",
       "43            0.203054                               0.677037   \n",
       "44            0.154398                               0.701440   \n",
       "45            0.175256                               0.701440   \n",
       "46            0.226720                               0.593289   \n",
       "47            0.125940                               0.609558   \n",
       "48            0.104810                               0.709508   \n",
       "49            0.081968                               0.666320   \n",
       "50            0.116319                               0.709508   \n",
       "51            0.497421                               0.729865   \n",
       "52            0.153652                               0.711331   \n",
       "53            0.074685                               0.642949   \n",
       "\n",
       "    binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "0                           0.692467               0.642458   \n",
       "1                           0.571653               0.932961   \n",
       "2                           0.766724               0.374302   \n",
       "3                           0.782591               0.374302   \n",
       "4                           0.672869               0.631285   \n",
       "5                           0.612850               0.659218   \n",
       "6                           0.400991               0.932961   \n",
       "7                           0.502890               0.932961   \n",
       "8                           0.790258               0.374302   \n",
       "9                           0.676750               0.620112   \n",
       "10                          0.739006               0.379888   \n",
       "11                          0.698940               0.379888   \n",
       "12                          0.701660               0.374302   \n",
       "13                          0.701660               0.374302   \n",
       "14                          0.713038               0.374302   \n",
       "15                          0.713038               0.374302   \n",
       "16                          0.731052               0.374302   \n",
       "17                          0.731052               0.374302   \n",
       "18                          0.804630               0.374302   \n",
       "19                          0.804630               0.374302   \n",
       "20                          0.433255               0.905028   \n",
       "21                          0.433255               0.905028   \n",
       "22                          0.688458               0.374302   \n",
       "23                          0.688458               0.374302   \n",
       "24                          0.301996               0.888268   \n",
       "25                          0.724051               0.575419   \n",
       "26                          0.766176               0.374302   \n",
       "27                          0.698207               0.648045   \n",
       "28                          0.767504               0.374302   \n",
       "29                          0.795482               0.374302   \n",
       "30                          0.738694               0.374302   \n",
       "31                          0.361998               0.854749   \n",
       "32                          0.589957               0.921788   \n",
       "33                          0.351944               0.899441   \n",
       "34                          0.322628               0.938547   \n",
       "35                          0.746740               0.374302   \n",
       "36                          0.776117               0.385475   \n",
       "37                          0.651787               0.614525   \n",
       "38                          0.651787               0.614525   \n",
       "39                          0.726948               0.324022   \n",
       "40                          0.726948               0.324022   \n",
       "41                          0.674465               0.653631   \n",
       "42                          0.759995               0.374302   \n",
       "43                          0.695385               0.620112   \n",
       "44                          0.719665               0.374302   \n",
       "45                          0.719665               0.374302   \n",
       "46                          0.420265               0.932961   \n",
       "47                          0.488859               0.787709   \n",
       "48                          0.751050               0.374302   \n",
       "49                          0.639893               0.374302   \n",
       "50                          0.751050               0.374302   \n",
       "51                          0.803884               0.374302   \n",
       "52                          0.761270               0.374302   \n",
       "53                          0.559280               0.720670   \n",
       "\n",
       "    f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "0                0.418182              0.096975   \n",
       "1                0.911765              0.111433   \n",
       "2                0.544715              0.167118   \n",
       "3                0.544715              0.164211   \n",
       "4                0.565789              0.163254   \n",
       "5                0.519685              0.296510   \n",
       "6                0.911765              0.126030   \n",
       "7                0.911765              0.168827   \n",
       "8                0.544715              0.173118   \n",
       "9                0.433333              0.135714   \n",
       "10               0.458537              0.128241   \n",
       "11               0.546939              0.181615   \n",
       "12               0.544715              0.120670   \n",
       "13               0.544715              0.135688   \n",
       "14               0.544715              0.107251   \n",
       "15               0.544715              0.101623   \n",
       "16               0.544715              0.095954   \n",
       "17               0.544715              0.097345   \n",
       "18               0.544715              0.104906   \n",
       "19               0.544715              0.127601   \n",
       "20               0.870229              0.146318   \n",
       "21               0.870229              0.164939   \n",
       "22               0.544715              0.130947   \n",
       "23               0.544715              0.153490   \n",
       "24               0.838710              0.167211   \n",
       "25               0.464789              0.183146   \n",
       "26               0.544715              0.111354   \n",
       "27               0.651934              0.172055   \n",
       "28               0.544715              0.141309   \n",
       "29               0.544715              0.132544   \n",
       "30               0.544715              0.112628   \n",
       "31               0.783333              0.066365   \n",
       "32               0.898551              0.516098   \n",
       "33               0.875000              0.168550   \n",
       "34               0.918519              0.059792   \n",
       "35               0.544715              0.631936   \n",
       "36               0.545455              0.359247   \n",
       "37               0.429752              0.485876   \n",
       "38               0.429752              0.714134   \n",
       "39               0.489451              0.323923   \n",
       "40               0.489451              0.295167   \n",
       "41               0.507937              0.149793   \n",
       "42               0.544715              0.224715   \n",
       "43               0.521127              0.131490   \n",
       "44               0.544715              0.078544   \n",
       "45               0.544715              0.077407   \n",
       "46               0.911765              0.132813   \n",
       "47               0.612245              0.079489   \n",
       "48               0.544715              0.099175   \n",
       "49               0.544715              0.078860   \n",
       "50               0.544715              0.112988   \n",
       "51               0.544715              0.142056   \n",
       "52               0.544715              0.113839   \n",
       "53               0.431818              0.073720   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                    0.665557   \n",
       "1                                    0.668186   \n",
       "2                                    0.673644   \n",
       "3                                    0.672519   \n",
       "4                                    0.667948   \n",
       "5                                    0.666934   \n",
       "6                                    0.667058   \n",
       "7                                    0.668177   \n",
       "8                                    0.661191   \n",
       "9                                    0.670961   \n",
       "10                                   0.668651   \n",
       "11                                   0.668631   \n",
       "12                                   0.670297   \n",
       "13                                   0.670297   \n",
       "14                                   0.663541   \n",
       "15                                   0.663541   \n",
       "16                                   0.671838   \n",
       "17                                   0.671838   \n",
       "18                                   0.666985   \n",
       "19                                   0.666985   \n",
       "20                                   0.671978   \n",
       "21                                   0.671978   \n",
       "22                                   0.673639   \n",
       "23                                   0.673639   \n",
       "24                                   0.743554   \n",
       "25                                   0.692465   \n",
       "26                                   0.676349   \n",
       "27                                   0.662996   \n",
       "28                                   0.675440   \n",
       "29                                   0.674239   \n",
       "30                                   0.666590   \n",
       "31                                   0.626692   \n",
       "32                                   0.672035   \n",
       "33                                   0.650548   \n",
       "34                                   0.620933   \n",
       "35                                   0.669737   \n",
       "36                                   0.666689   \n",
       "37                                   0.674048   \n",
       "38                                   0.674048   \n",
       "39                                   0.675858   \n",
       "40                                   0.675858   \n",
       "41                                   0.679977   \n",
       "42                                   0.667189   \n",
       "43                                   0.666326   \n",
       "44                                   0.672936   \n",
       "45                                   0.672936   \n",
       "46                                   0.659728   \n",
       "47                                   0.725109   \n",
       "48                                   0.672326   \n",
       "49                                   0.669753   \n",
       "50                                   0.672326   \n",
       "51                                   0.671381   \n",
       "52                                   0.676406   \n",
       "53                                   0.676467   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "0                               0.624602                   0.743243   \n",
       "1                               0.618528                   0.729730   \n",
       "2                               0.652591                   0.574324   \n",
       "3                               0.606104                   0.729730   \n",
       "4                               0.592628                   0.729730   \n",
       "5                               0.579294                   0.729730   \n",
       "6                               0.582609                   0.729730   \n",
       "7                               0.593875                   0.729730   \n",
       "8                               0.577630                   0.729730   \n",
       "9                               0.599570                   0.729730   \n",
       "10                              0.593555                   0.729730   \n",
       "11                              0.598648                   0.729730   \n",
       "12                              0.644363                   0.648649   \n",
       "13                              0.644363                   0.648649   \n",
       "14                              0.621219                   0.648649   \n",
       "15                              0.621219                   0.648649   \n",
       "16                              0.650384                   0.648649   \n",
       "17                              0.650384                   0.648649   \n",
       "18                              0.627615                   0.648649   \n",
       "19                              0.627615                   0.648649   \n",
       "20                              0.654625                   0.648649   \n",
       "21                              0.654625                   0.648649   \n",
       "22                              0.657618                   0.648649   \n",
       "23                              0.657618                   0.648649   \n",
       "24                              1.196940                   0.472973   \n",
       "25                              0.751844                   0.337838   \n",
       "26                              0.663386                   0.648649   \n",
       "27                              0.582301                   0.729730   \n",
       "28                              0.655696                   0.648649   \n",
       "29                              0.644722                   0.648649   \n",
       "30                              0.581631                   0.729730   \n",
       "31                              0.460748                   0.777027   \n",
       "32                              0.623440                   0.675676   \n",
       "33                              0.595131                   0.608108   \n",
       "34                              0.436572                   0.743243   \n",
       "35                              0.603190                   0.729730   \n",
       "36                              0.581924                   0.729730   \n",
       "37                              0.667738                   0.648649   \n",
       "38                              0.667738                   0.648649   \n",
       "39                              0.656414                   0.648649   \n",
       "40                              0.656414                   0.648649   \n",
       "41                              0.639753                   0.729730   \n",
       "42                              0.582810                   0.729730   \n",
       "43                              0.586550                   0.729730   \n",
       "44                              0.651634                   0.648649   \n",
       "45                              0.651634                   0.648649   \n",
       "46                              0.548108                   0.729730   \n",
       "47                              0.923983                   0.337838   \n",
       "48                              0.646152                   0.648649   \n",
       "49                              0.594847                   0.729730   \n",
       "50                              0.646152                   0.648649   \n",
       "51                              0.635908                   0.648649   \n",
       "52                              0.644446                   0.648649   \n",
       "53                              0.650746                   0.648649   \n",
       "\n",
       "    f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "0                    0.486486                  0.110725   \n",
       "1                    0.000000                  0.125714   \n",
       "2                    0.322581                  0.207149   \n",
       "3                    0.000000                  0.135190   \n",
       "4                    0.000000                  0.153153   \n",
       "5                    0.000000                  0.314534   \n",
       "6                    0.000000                  0.135682   \n",
       "7                    0.000000                  0.125292   \n",
       "8                    0.000000                  0.174584   \n",
       "9                    0.000000                  0.104679   \n",
       "10                   0.000000                  0.129608   \n",
       "11                   0.000000                  0.177235   \n",
       "12                   0.000000                  0.139696   \n",
       "13                   0.000000                  0.131851   \n",
       "14                   0.000000                  0.092488   \n",
       "15                   0.000000                  0.097656   \n",
       "16                   0.000000                  0.097691   \n",
       "17                   0.000000                  0.086548   \n",
       "18                   0.000000                  0.106533   \n",
       "19                   0.000000                  0.103857   \n",
       "20                   0.000000                  0.159502   \n",
       "21                   0.000000                  0.149537   \n",
       "22                   0.000000                  0.154387   \n",
       "23                   0.000000                  0.193856   \n",
       "24                   0.506329                  0.141552   \n",
       "25                   0.449438                  0.176855   \n",
       "26                   0.000000                  0.105651   \n",
       "27                   0.000000                  0.159159   \n",
       "28                   0.000000                  0.152041   \n",
       "29                   0.000000                  0.138362   \n",
       "30                   0.000000                  0.112174   \n",
       "31                   0.755556                  0.064539   \n",
       "32                   0.351351                  0.505987   \n",
       "33                   0.632911                  0.175453   \n",
       "34                   0.677966                  0.051095   \n",
       "35                   0.000000                  5.381506   \n",
       "36                   0.000000                  0.962151   \n",
       "37                   0.000000                  0.593215   \n",
       "38                   0.000000                  0.551471   \n",
       "39                   0.000000                  0.141383   \n",
       "40                   0.000000                  0.136782   \n",
       "41                   0.000000                  0.106841   \n",
       "42                   0.000000                  0.188092   \n",
       "43                   0.000000                  0.090359   \n",
       "44                   0.000000                  0.079618   \n",
       "45                   0.000000                  0.080533   \n",
       "46                   0.000000                  0.122194   \n",
       "47                   0.449438                  0.079203   \n",
       "48                   0.000000                  0.105075   \n",
       "49                   0.000000                  0.085431   \n",
       "50                   0.000000                  0.115530   \n",
       "51                   0.000000                  0.144323   \n",
       "52                   0.000000                  0.137836   \n",
       "53                   0.000000                  0.079789   \n",
       "\n",
       "    soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "0                               0.909159                         2.012290   \n",
       "1                               0.933302                         0.000000   \n",
       "2                               0.836600                         1.238022   \n",
       "3                               0.905225                         1.944110   \n",
       "4                               0.853145                         0.000000   \n",
       "5                               0.854233                         1.729415   \n",
       "6                               0.722902                         0.762560   \n",
       "7                               0.677103                         0.671711   \n",
       "8                               0.683051                         0.675042   \n",
       "9                               0.733779                         0.793555   \n",
       "10                              0.761217                         0.873499   \n",
       "11                              0.706802                         0.723446   \n",
       "12                              0.858197                         1.524665   \n",
       "13                              0.858197                         1.524665   \n",
       "14                              0.799476                         1.013420   \n",
       "15                              0.799476                         1.013420   \n",
       "16                              0.900496                         1.792710   \n",
       "17                              0.900496                         1.792710   \n",
       "18                              0.806887                         1.056151   \n",
       "19                              0.806887                         1.056151   \n",
       "20                              0.866803                         1.464826   \n",
       "21                              0.866803                         1.464826   \n",
       "22                              0.818518                         1.111223   \n",
       "23                              0.818518                         1.111223   \n",
       "24                              0.678608                         1.979326   \n",
       "25                              0.932909                         4.607446   \n",
       "26                              0.916003                         2.128734   \n",
       "27                              0.616447                         0.546445   \n",
       "28                              0.925376                         2.516976   \n",
       "29                              0.829728                         1.152108   \n",
       "30                              0.716565                         0.745760   \n",
       "31                              0.648073                         0.000000   \n",
       "32                              0.853060                         1.368479   \n",
       "33                              0.663431                         0.663653   \n",
       "34                              0.665811                         0.890117   \n",
       "35                              0.703703                         0.715697   \n",
       "36                              0.881891                         1.602950   \n",
       "37                              0.858416                         1.342356   \n",
       "38                              0.858416                         1.342356   \n",
       "39                              0.721825                         1.056322   \n",
       "40                              0.721825                         1.056322   \n",
       "41                              0.883034                         1.618120   \n",
       "42                              0.913043                         2.129727   \n",
       "43                              0.726350                         0.771724   \n",
       "44                              0.857244                         2.067929   \n",
       "45                              0.857244                         2.067929   \n",
       "46                              0.682691                         0.675333   \n",
       "47                              0.862445                         1.513847   \n",
       "48                              0.819148                         1.105667   \n",
       "49                              0.754582                         0.855642   \n",
       "50                              0.819148                         1.105667   \n",
       "51                              0.711192                         0.750445   \n",
       "52                              0.839257                         1.249003   \n",
       "53                              0.792118                         0.981287   \n",
       "\n",
       "    accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "0               0.382619              0.553470             0.104282   \n",
       "1               0.382619              0.553470             0.116253   \n",
       "2               0.382619              0.553470             0.161816   \n",
       "3               0.382619              0.553470             0.142096   \n",
       "4               0.382619              0.553470             0.187451   \n",
       "5               0.488868              0.595357             0.346450   \n",
       "6               0.378934              0.549605             0.172872   \n",
       "7               0.629971              0.045922             0.208214   \n",
       "8               0.620912              0.004034             0.174649   \n",
       "9               0.378934              0.549605            48.206169   \n",
       "10              0.378934              0.549605             0.160855   \n",
       "11              0.378934              0.549605             0.177735   \n",
       "12              0.382926              0.553791             0.119750   \n",
       "13              0.382926              0.553791             0.137243   \n",
       "14              0.382926              0.553791             0.094413   \n",
       "15              0.382926              0.553791             0.098506   \n",
       "16              0.382926              0.553791             0.099299   \n",
       "17              0.382926              0.553791             0.101976   \n",
       "18              0.382926              0.553791             0.091847   \n",
       "19              0.382926              0.553791             0.433442   \n",
       "20              0.382926              0.553791             0.136998   \n",
       "21              0.382926              0.553791             0.156404   \n",
       "22              0.382926              0.553791             0.478246   \n",
       "23              0.382926              0.553791             0.148448   \n",
       "24              0.629203              0.059946             0.187590   \n",
       "25              0.382619              0.553470             0.198959   \n",
       "26              0.386765              0.557795             0.127063   \n",
       "27              0.646860              0.466357             0.158791   \n",
       "28              0.382926              0.553791             0.160029   \n",
       "29              0.382926              0.553791             0.509442   \n",
       "30              0.383080              0.553952             0.110954   \n",
       "31              0.640104              0.123411             0.063903   \n",
       "32              0.382619              0.553470             0.498401   \n",
       "33              0.619070              0.029722             0.165916   \n",
       "34              0.628896              0.040492             0.057173   \n",
       "35              0.383080              0.553952             0.479583   \n",
       "36              0.383080              0.553952             0.816631   \n",
       "37              0.382926              0.553791             0.555486   \n",
       "38              0.382926              0.553791             0.755470   \n",
       "39              0.626132              0.671434             0.245350   \n",
       "40              0.626132              0.671434             0.391079   \n",
       "41              0.383080              0.553952             0.295744   \n",
       "42              0.383080              0.553952             0.455591   \n",
       "43              0.383080              0.553952             0.203054   \n",
       "44              0.382926              0.553791             0.154398   \n",
       "45              0.382926              0.553791             0.175256   \n",
       "46              0.622754              0.030004             0.226720   \n",
       "47              0.383080              0.553952             0.125940   \n",
       "48              0.382926              0.553791             0.104810   \n",
       "49              0.140949              0.079618             0.081968   \n",
       "50              0.382926              0.553791             0.116319   \n",
       "51              0.382926              0.553791             0.497421   \n",
       "52              0.382926              0.553791             0.153652   \n",
       "53              0.382926              0.553791             0.074685   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "0                                 0.685014                           0.692467   \n",
       "1                                 0.649897                           0.571653   \n",
       "2                                 0.713819                           0.766724   \n",
       "3                                 0.718173                           0.782591   \n",
       "4                                 0.673761                           0.672869   \n",
       "5                                 0.647532                           0.612850   \n",
       "6                                 0.588987                           0.400991   \n",
       "7                                 0.624959                           0.502890   \n",
       "8                                 0.717925                           0.790258   \n",
       "9                                 0.663955                           0.676750   \n",
       "10                                0.702805                           0.739006   \n",
       "11                                0.691413                           0.698940   \n",
       "12                                0.693719                           0.701660   \n",
       "13                                0.693719                           0.701660   \n",
       "14                                0.697205                           0.713038   \n",
       "15                                0.697205                           0.713038   \n",
       "16                                0.703797                           0.731052   \n",
       "17                                0.703797                           0.731052   \n",
       "18                                0.718736                           0.804630   \n",
       "19                                0.718736                           0.804630   \n",
       "20                                0.597506                           0.433255   \n",
       "21                                0.597506                           0.433255   \n",
       "22                                0.685908                           0.688458   \n",
       "23                                0.685908                           0.688458   \n",
       "24                                0.543518                           0.301996   \n",
       "25                                0.699143                           0.724051   \n",
       "26                                0.709916                           0.766176   \n",
       "27                                0.679841                           0.698207   \n",
       "28                                0.711171                           0.767504   \n",
       "29                                0.720780                           0.795482   \n",
       "30                                0.693429                           0.738694   \n",
       "31                                0.546199                           0.361998   \n",
       "32                                0.654627                           0.589957   \n",
       "33                                0.555836                           0.351944   \n",
       "34                                0.555761                           0.322628   \n",
       "35                                0.708126                           0.746740   \n",
       "36                                0.715798                           0.776117   \n",
       "37                                0.664220                           0.651787   \n",
       "38                                0.664220                           0.651787   \n",
       "39                                0.705104                           0.726948   \n",
       "40                                0.705104                           0.726948   \n",
       "41                                0.648570                           0.674465   \n",
       "42                                0.711864                           0.759995   \n",
       "43                                0.677037                           0.695385   \n",
       "44                                0.701440                           0.719665   \n",
       "45                                0.701440                           0.719665   \n",
       "46                                0.593289                           0.420265   \n",
       "47                                0.609558                           0.488859   \n",
       "48                                0.709508                           0.751050   \n",
       "49                                0.666320                           0.639893   \n",
       "50                                0.709508                           0.751050   \n",
       "51                                0.729865                           0.803884   \n",
       "52                                0.711331                           0.761270   \n",
       "53                                0.642949                           0.559280   \n",
       "\n",
       "    accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "0                 0.642458                0.418182               0.096975   \n",
       "1                 0.932961                0.911765               0.111433   \n",
       "2                 0.374302                0.544715               0.167118   \n",
       "3                 0.374302                0.544715               0.164211   \n",
       "4                 0.631285                0.565789               0.163254   \n",
       "5                 0.659218                0.519685               0.296510   \n",
       "6                 0.932961                0.911765               0.126030   \n",
       "7                 0.932961                0.911765               0.168827   \n",
       "8                 0.374302                0.544715               0.173118   \n",
       "9                 0.620112                0.433333               0.135714   \n",
       "10                0.379888                0.458537               0.128241   \n",
       "11                0.379888                0.546939               0.181615   \n",
       "12                0.374302                0.544715               0.120670   \n",
       "13                0.374302                0.544715               0.135688   \n",
       "14                0.374302                0.544715               0.107251   \n",
       "15                0.374302                0.544715               0.101623   \n",
       "16                0.374302                0.544715               0.095954   \n",
       "17                0.374302                0.544715               0.097345   \n",
       "18                0.374302                0.544715               0.104906   \n",
       "19                0.374302                0.544715               0.127601   \n",
       "20                0.905028                0.870229               0.146318   \n",
       "21                0.905028                0.870229               0.164939   \n",
       "22                0.374302                0.544715               0.130947   \n",
       "23                0.374302                0.544715               0.153490   \n",
       "24                0.888268                0.838710               0.167211   \n",
       "25                0.575419                0.464789               0.183146   \n",
       "26                0.374302                0.544715               0.111354   \n",
       "27                0.648045                0.651934               0.172055   \n",
       "28                0.374302                0.544715               0.141309   \n",
       "29                0.374302                0.544715               0.132544   \n",
       "30                0.374302                0.544715               0.112628   \n",
       "31                0.854749                0.783333               0.066365   \n",
       "32                0.921788                0.898551               0.516098   \n",
       "33                0.899441                0.875000               0.168550   \n",
       "34                0.938547                0.918519               0.059792   \n",
       "35                0.374302                0.544715               0.631936   \n",
       "36                0.385475                0.545455               0.359247   \n",
       "37                0.614525                0.429752               0.485876   \n",
       "38                0.614525                0.429752               0.714134   \n",
       "39                0.324022                0.489451               0.323923   \n",
       "40                0.324022                0.489451               0.295167   \n",
       "41                0.653631                0.507937               0.149793   \n",
       "42                0.374302                0.544715               0.224715   \n",
       "43                0.620112                0.521127               0.131490   \n",
       "44                0.374302                0.544715               0.078544   \n",
       "45                0.374302                0.544715               0.077407   \n",
       "46                0.932961                0.911765               0.132813   \n",
       "47                0.787709                0.612245               0.079489   \n",
       "48                0.374302                0.544715               0.099175   \n",
       "49                0.374302                0.544715               0.078860   \n",
       "50                0.374302                0.544715               0.112988   \n",
       "51                0.374302                0.544715               0.142056   \n",
       "52                0.374302                0.544715               0.113839   \n",
       "53                0.720670                0.431818               0.073720   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                     0.665557   \n",
       "1                                     0.668186   \n",
       "2                                     0.673644   \n",
       "3                                     0.672519   \n",
       "4                                     0.667948   \n",
       "5                                     0.666934   \n",
       "6                                     0.667058   \n",
       "7                                     0.668177   \n",
       "8                                     0.661191   \n",
       "9                                     0.670961   \n",
       "10                                    0.668651   \n",
       "11                                    0.668631   \n",
       "12                                    0.670297   \n",
       "13                                    0.670297   \n",
       "14                                    0.663541   \n",
       "15                                    0.663541   \n",
       "16                                    0.671838   \n",
       "17                                    0.671838   \n",
       "18                                    0.666985   \n",
       "19                                    0.666985   \n",
       "20                                    0.671978   \n",
       "21                                    0.671978   \n",
       "22                                    0.673639   \n",
       "23                                    0.673639   \n",
       "24                                    0.743554   \n",
       "25                                    0.692465   \n",
       "26                                    0.676349   \n",
       "27                                    0.662996   \n",
       "28                                    0.675440   \n",
       "29                                    0.674239   \n",
       "30                                    0.666590   \n",
       "31                                    0.626692   \n",
       "32                                    0.672035   \n",
       "33                                    0.650548   \n",
       "34                                    0.620933   \n",
       "35                                    0.669737   \n",
       "36                                    0.666689   \n",
       "37                                    0.674048   \n",
       "38                                    0.674048   \n",
       "39                                    0.675858   \n",
       "40                                    0.675858   \n",
       "41                                    0.679977   \n",
       "42                                    0.667189   \n",
       "43                                    0.666326   \n",
       "44                                    0.672936   \n",
       "45                                    0.672936   \n",
       "46                                    0.659728   \n",
       "47                                    0.725109   \n",
       "48                                    0.672326   \n",
       "49                                    0.669753   \n",
       "50                                    0.672326   \n",
       "51                                    0.671381   \n",
       "52                                    0.676406   \n",
       "53                                    0.676467   \n",
       "\n",
       "    binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "0                                0.624602                    0.743243   \n",
       "1                                0.618528                    0.729730   \n",
       "2                                0.652591                    0.574324   \n",
       "3                                0.606104                    0.729730   \n",
       "4                                0.592628                    0.729730   \n",
       "5                                0.579294                    0.729730   \n",
       "6                                0.582609                    0.729730   \n",
       "7                                0.593875                    0.729730   \n",
       "8                                0.577630                    0.729730   \n",
       "9                                0.599570                    0.729730   \n",
       "10                               0.593555                    0.729730   \n",
       "11                               0.598648                    0.729730   \n",
       "12                               0.644363                    0.648649   \n",
       "13                               0.644363                    0.648649   \n",
       "14                               0.621219                    0.648649   \n",
       "15                               0.621219                    0.648649   \n",
       "16                               0.650384                    0.648649   \n",
       "17                               0.650384                    0.648649   \n",
       "18                               0.627615                    0.648649   \n",
       "19                               0.627615                    0.648649   \n",
       "20                               0.654625                    0.648649   \n",
       "21                               0.654625                    0.648649   \n",
       "22                               0.657618                    0.648649   \n",
       "23                               0.657618                    0.648649   \n",
       "24                               1.196940                    0.472973   \n",
       "25                               0.751844                    0.337838   \n",
       "26                               0.663386                    0.648649   \n",
       "27                               0.582301                    0.729730   \n",
       "28                               0.655696                    0.648649   \n",
       "29                               0.644722                    0.648649   \n",
       "30                               0.581631                    0.729730   \n",
       "31                               0.460748                    0.777027   \n",
       "32                               0.623440                    0.675676   \n",
       "33                               0.595131                    0.608108   \n",
       "34                               0.436572                    0.743243   \n",
       "35                               0.603190                    0.729730   \n",
       "36                               0.581924                    0.729730   \n",
       "37                               0.667738                    0.648649   \n",
       "38                               0.667738                    0.648649   \n",
       "39                               0.656414                    0.648649   \n",
       "40                               0.656414                    0.648649   \n",
       "41                               0.639753                    0.729730   \n",
       "42                               0.582810                    0.729730   \n",
       "43                               0.586550                    0.729730   \n",
       "44                               0.651634                    0.648649   \n",
       "45                               0.651634                    0.648649   \n",
       "46                               0.548108                    0.729730   \n",
       "47                               0.923983                    0.337838   \n",
       "48                               0.646152                    0.648649   \n",
       "49                               0.594847                    0.729730   \n",
       "50                               0.646152                    0.648649   \n",
       "51                               0.635908                    0.648649   \n",
       "52                               0.644446                    0.648649   \n",
       "53                               0.650746                    0.648649   \n",
       "\n",
       "    f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "0                     0.486486                   0.110725   \n",
       "1                     0.000000                   0.125714   \n",
       "2                     0.322581                   0.207149   \n",
       "3                     0.000000                   0.135190   \n",
       "4                     0.000000                   0.153153   \n",
       "5                     0.000000                   0.314534   \n",
       "6                     0.000000                   0.135682   \n",
       "7                     0.000000                   0.125292   \n",
       "8                     0.000000                   0.174584   \n",
       "9                     0.000000                   0.104679   \n",
       "10                    0.000000                   0.129608   \n",
       "11                    0.000000                   0.177235   \n",
       "12                    0.000000                   0.139696   \n",
       "13                    0.000000                   0.131851   \n",
       "14                    0.000000                   0.092488   \n",
       "15                    0.000000                   0.097656   \n",
       "16                    0.000000                   0.097691   \n",
       "17                    0.000000                   0.086548   \n",
       "18                    0.000000                   0.106533   \n",
       "19                    0.000000                   0.103857   \n",
       "20                    0.000000                   0.159502   \n",
       "21                    0.000000                   0.149537   \n",
       "22                    0.000000                   0.154387   \n",
       "23                    0.000000                   0.193856   \n",
       "24                    0.506329                   0.141552   \n",
       "25                    0.449438                   0.176855   \n",
       "26                    0.000000                   0.105651   \n",
       "27                    0.000000                   0.159159   \n",
       "28                    0.000000                   0.152041   \n",
       "29                    0.000000                   0.138362   \n",
       "30                    0.000000                   0.112174   \n",
       "31                    0.755556                   0.064539   \n",
       "32                    0.351351                   0.505987   \n",
       "33                    0.632911                   0.175453   \n",
       "34                    0.677966                   0.051095   \n",
       "35                    0.000000                   5.381506   \n",
       "36                    0.000000                   0.962151   \n",
       "37                    0.000000                   0.593215   \n",
       "38                    0.000000                   0.551471   \n",
       "39                    0.000000                   0.141383   \n",
       "40                    0.000000                   0.136782   \n",
       "41                    0.000000                   0.106841   \n",
       "42                    0.000000                   0.188092   \n",
       "43                    0.000000                   0.090359   \n",
       "44                    0.000000                   0.079618   \n",
       "45                    0.000000                   0.080533   \n",
       "46                    0.000000                   0.122194   \n",
       "47                    0.449438                   0.079203   \n",
       "48                    0.000000                   0.105075   \n",
       "49                    0.000000                   0.085431   \n",
       "50                    0.000000                   0.115530   \n",
       "51                    0.000000                   0.144323   \n",
       "52                    0.000000                   0.137836   \n",
       "53                    0.000000                   0.079789   \n",
       "\n",
       "    soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "0                                0.909159                          2.012290   \n",
       "1                                0.933302                          0.000000   \n",
       "2                                0.836600                          1.238022   \n",
       "3                                0.905225                          1.944110   \n",
       "4                                0.853145                          0.000000   \n",
       "5                                0.854233                          1.729415   \n",
       "6                                0.722902                          0.762560   \n",
       "7                                0.677103                          0.671711   \n",
       "8                                0.683051                          0.675042   \n",
       "9                                0.733779                          0.793555   \n",
       "10                               0.761217                          0.873499   \n",
       "11                               0.706802                          0.723446   \n",
       "12                               0.858197                          1.524665   \n",
       "13                               0.858197                          1.524665   \n",
       "14                               0.799476                          1.013420   \n",
       "15                               0.799476                          1.013420   \n",
       "16                               0.900496                          1.792710   \n",
       "17                               0.900496                          1.792710   \n",
       "18                               0.806887                          1.056151   \n",
       "19                               0.806887                          1.056151   \n",
       "20                               0.866803                          1.464826   \n",
       "21                               0.866803                          1.464826   \n",
       "22                               0.818518                          1.111223   \n",
       "23                               0.818518                          1.111223   \n",
       "24                               0.678608                          1.979326   \n",
       "25                               0.932909                          4.607446   \n",
       "26                               0.916003                          2.128734   \n",
       "27                               0.616447                          0.546445   \n",
       "28                               0.925376                          2.516976   \n",
       "29                               0.829728                          1.152108   \n",
       "30                               0.716565                          0.745760   \n",
       "31                               0.648073                          0.000000   \n",
       "32                               0.853060                          1.368479   \n",
       "33                               0.663431                          0.663653   \n",
       "34                               0.665811                          0.890117   \n",
       "35                               0.703703                          0.715697   \n",
       "36                               0.881891                          1.602950   \n",
       "37                               0.858416                          1.342356   \n",
       "38                               0.858416                          1.342356   \n",
       "39                               0.721825                          1.056322   \n",
       "40                               0.721825                          1.056322   \n",
       "41                               0.883034                          1.618120   \n",
       "42                               0.913043                          2.129727   \n",
       "43                               0.726350                          0.771724   \n",
       "44                               0.857244                          2.067929   \n",
       "45                               0.857244                          2.067929   \n",
       "46                               0.682691                          0.675333   \n",
       "47                               0.862445                          1.513847   \n",
       "48                               0.819148                          1.105667   \n",
       "49                               0.754582                          0.855642   \n",
       "50                               0.819148                          1.105667   \n",
       "51                               0.711192                          0.750445   \n",
       "52                               0.839257                          1.249003   \n",
       "53                               0.792118                          0.981287   \n",
       "\n",
       "    accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "0                0.382619               0.553470              0.104282   \n",
       "1                0.382619               0.553470              0.116253   \n",
       "2                0.382619               0.553470              0.161816   \n",
       "3                0.382619               0.553470              0.142096   \n",
       "4                0.382619               0.553470              0.187451   \n",
       "5                0.488868               0.595357              0.346450   \n",
       "6                0.378934               0.549605              0.172872   \n",
       "7                0.629971               0.045922              0.208214   \n",
       "8                0.620912               0.004034              0.174649   \n",
       "9                0.378934               0.549605             48.206169   \n",
       "10               0.378934               0.549605              0.160855   \n",
       "11               0.378934               0.549605              0.177735   \n",
       "12               0.382926               0.553791              0.119750   \n",
       "13               0.382926               0.553791              0.137243   \n",
       "14               0.382926               0.553791              0.094413   \n",
       "15               0.382926               0.553791              0.098506   \n",
       "16               0.382926               0.553791              0.099299   \n",
       "17               0.382926               0.553791              0.101976   \n",
       "18               0.382926               0.553791              0.091847   \n",
       "19               0.382926               0.553791              0.433442   \n",
       "20               0.382926               0.553791              0.136998   \n",
       "21               0.382926               0.553791              0.156404   \n",
       "22               0.382926               0.553791              0.478246   \n",
       "23               0.382926               0.553791              0.148448   \n",
       "24               0.629203               0.059946              0.187590   \n",
       "25               0.382619               0.553470              0.198959   \n",
       "26               0.386765               0.557795              0.127063   \n",
       "27               0.646860               0.466357              0.158791   \n",
       "28               0.382926               0.553791              0.160029   \n",
       "29               0.382926               0.553791              0.509442   \n",
       "30               0.383080               0.553952              0.110954   \n",
       "31               0.640104               0.123411              0.063903   \n",
       "32               0.382619               0.553470              0.498401   \n",
       "33               0.619070               0.029722              0.165916   \n",
       "34               0.628896               0.040492              0.057173   \n",
       "35               0.383080               0.553952              0.479583   \n",
       "36               0.383080               0.553952              0.816631   \n",
       "37               0.382926               0.553791              0.555486   \n",
       "38               0.382926               0.553791              0.755470   \n",
       "39               0.626132               0.671434              0.245350   \n",
       "40               0.626132               0.671434              0.391079   \n",
       "41               0.383080               0.553952              0.295744   \n",
       "42               0.383080               0.553952              0.455591   \n",
       "43               0.383080               0.553952              0.203054   \n",
       "44               0.382926               0.553791              0.154398   \n",
       "45               0.382926               0.553791              0.175256   \n",
       "46               0.622754               0.030004              0.226720   \n",
       "47               0.383080               0.553952              0.125940   \n",
       "48               0.382926               0.553791              0.104810   \n",
       "49               0.140949               0.079618              0.081968   \n",
       "50               0.382926               0.553791              0.116319   \n",
       "51               0.382926               0.553791              0.497421   \n",
       "52               0.382926               0.553791              0.153652   \n",
       "53               0.382926               0.553791              0.074685   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_100000  \\\n",
       "0                                  0.685014   \n",
       "1                                  0.649897   \n",
       "2                                  0.713819   \n",
       "3                                  0.718173   \n",
       "4                                  0.673761   \n",
       "5                                  0.647532   \n",
       "6                                  0.588987   \n",
       "7                                  0.624959   \n",
       "8                                  0.717925   \n",
       "9                                  0.663955   \n",
       "10                                 0.702805   \n",
       "11                                 0.691413   \n",
       "12                                 0.693719   \n",
       "13                                 0.693719   \n",
       "14                                 0.697205   \n",
       "15                                 0.697205   \n",
       "16                                 0.703797   \n",
       "17                                 0.703797   \n",
       "18                                 0.718736   \n",
       "19                                 0.718736   \n",
       "20                                 0.597506   \n",
       "21                                 0.597506   \n",
       "22                                 0.685908   \n",
       "23                                 0.685908   \n",
       "24                                 0.543518   \n",
       "25                                 0.699143   \n",
       "26                                 0.709916   \n",
       "27                                 0.679841   \n",
       "28                                 0.711171   \n",
       "29                                 0.720780   \n",
       "30                                 0.693429   \n",
       "31                                 0.546199   \n",
       "32                                 0.654627   \n",
       "33                                 0.555836   \n",
       "34                                 0.555761   \n",
       "35                                 0.708126   \n",
       "36                                 0.715798   \n",
       "37                                 0.664220   \n",
       "38                                 0.664220   \n",
       "39                                 0.705104   \n",
       "40                                 0.705104   \n",
       "41                                 0.648570   \n",
       "42                                 0.711864   \n",
       "43                                 0.677037   \n",
       "44                                 0.701440   \n",
       "45                                 0.701440   \n",
       "46                                 0.593289   \n",
       "47                                 0.609558   \n",
       "48                                 0.709508   \n",
       "49                                 0.666320   \n",
       "50                                 0.709508   \n",
       "51                                 0.729865   \n",
       "52                                 0.711331   \n",
       "53                                 0.642949   \n",
       "\n",
       "    binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "0                             0.692467                 0.642458   \n",
       "1                             0.571653                 0.932961   \n",
       "2                             0.766724                 0.374302   \n",
       "3                             0.782591                 0.374302   \n",
       "4                             0.672869                 0.631285   \n",
       "5                             0.612850                 0.659218   \n",
       "6                             0.400991                 0.932961   \n",
       "7                             0.502890                 0.932961   \n",
       "8                             0.790258                 0.374302   \n",
       "9                             0.676750                 0.620112   \n",
       "10                            0.739006                 0.379888   \n",
       "11                            0.698940                 0.379888   \n",
       "12                            0.701660                 0.374302   \n",
       "13                            0.701660                 0.374302   \n",
       "14                            0.713038                 0.374302   \n",
       "15                            0.713038                 0.374302   \n",
       "16                            0.731052                 0.374302   \n",
       "17                            0.731052                 0.374302   \n",
       "18                            0.804630                 0.374302   \n",
       "19                            0.804630                 0.374302   \n",
       "20                            0.433255                 0.905028   \n",
       "21                            0.433255                 0.905028   \n",
       "22                            0.688458                 0.374302   \n",
       "23                            0.688458                 0.374302   \n",
       "24                            0.301996                 0.888268   \n",
       "25                            0.724051                 0.575419   \n",
       "26                            0.766176                 0.374302   \n",
       "27                            0.698207                 0.648045   \n",
       "28                            0.767504                 0.374302   \n",
       "29                            0.795482                 0.374302   \n",
       "30                            0.738694                 0.374302   \n",
       "31                            0.361998                 0.854749   \n",
       "32                            0.589957                 0.921788   \n",
       "33                            0.351944                 0.899441   \n",
       "34                            0.322628                 0.938547   \n",
       "35                            0.746740                 0.374302   \n",
       "36                            0.776117                 0.385475   \n",
       "37                            0.651787                 0.614525   \n",
       "38                            0.651787                 0.614525   \n",
       "39                            0.726948                 0.324022   \n",
       "40                            0.726948                 0.324022   \n",
       "41                            0.674465                 0.653631   \n",
       "42                            0.759995                 0.374302   \n",
       "43                            0.695385                 0.620112   \n",
       "44                            0.719665                 0.374302   \n",
       "45                            0.719665                 0.374302   \n",
       "46                            0.420265                 0.932961   \n",
       "47                            0.488859                 0.787709   \n",
       "48                            0.751050                 0.374302   \n",
       "49                            0.639893                 0.374302   \n",
       "50                            0.751050                 0.374302   \n",
       "51                            0.803884                 0.374302   \n",
       "52                            0.761270                 0.374302   \n",
       "53                            0.559280                 0.720670   \n",
       "\n",
       "    f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "0                  0.418182                0.096975   \n",
       "1                  0.911765                0.111433   \n",
       "2                  0.544715                0.167118   \n",
       "3                  0.544715                0.164211   \n",
       "4                  0.565789                0.163254   \n",
       "5                  0.519685                0.296510   \n",
       "6                  0.911765                0.126030   \n",
       "7                  0.911765                0.168827   \n",
       "8                  0.544715                0.173118   \n",
       "9                  0.433333                0.135714   \n",
       "10                 0.458537                0.128241   \n",
       "11                 0.546939                0.181615   \n",
       "12                 0.544715                0.120670   \n",
       "13                 0.544715                0.135688   \n",
       "14                 0.544715                0.107251   \n",
       "15                 0.544715                0.101623   \n",
       "16                 0.544715                0.095954   \n",
       "17                 0.544715                0.097345   \n",
       "18                 0.544715                0.104906   \n",
       "19                 0.544715                0.127601   \n",
       "20                 0.870229                0.146318   \n",
       "21                 0.870229                0.164939   \n",
       "22                 0.544715                0.130947   \n",
       "23                 0.544715                0.153490   \n",
       "24                 0.838710                0.167211   \n",
       "25                 0.464789                0.183146   \n",
       "26                 0.544715                0.111354   \n",
       "27                 0.651934                0.172055   \n",
       "28                 0.544715                0.141309   \n",
       "29                 0.544715                0.132544   \n",
       "30                 0.544715                0.112628   \n",
       "31                 0.783333                0.066365   \n",
       "32                 0.898551                0.516098   \n",
       "33                 0.875000                0.168550   \n",
       "34                 0.918519                0.059792   \n",
       "35                 0.544715                0.631936   \n",
       "36                 0.545455                0.359247   \n",
       "37                 0.429752                0.485876   \n",
       "38                 0.429752                0.714134   \n",
       "39                 0.489451                0.323923   \n",
       "40                 0.489451                0.295167   \n",
       "41                 0.507937                0.149793   \n",
       "42                 0.544715                0.224715   \n",
       "43                 0.521127                0.131490   \n",
       "44                 0.544715                0.078544   \n",
       "45                 0.544715                0.077407   \n",
       "46                 0.911765                0.132813   \n",
       "47                 0.612245                0.079489   \n",
       "48                 0.544715                0.099175   \n",
       "49                 0.544715                0.078860   \n",
       "50                 0.544715                0.112988   \n",
       "51                 0.544715                0.142056   \n",
       "52                 0.544715                0.113839   \n",
       "53                 0.431818                0.073720   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                      0.665557   \n",
       "1                                      0.668186   \n",
       "2                                      0.673644   \n",
       "3                                      0.672519   \n",
       "4                                      0.667948   \n",
       "5                                      0.666934   \n",
       "6                                      0.667058   \n",
       "7                                      0.668177   \n",
       "8                                      0.661191   \n",
       "9                                      0.670961   \n",
       "10                                     0.668651   \n",
       "11                                     0.668631   \n",
       "12                                     0.670297   \n",
       "13                                     0.670297   \n",
       "14                                     0.663541   \n",
       "15                                     0.663541   \n",
       "16                                     0.671838   \n",
       "17                                     0.671838   \n",
       "18                                     0.666985   \n",
       "19                                     0.666985   \n",
       "20                                     0.671978   \n",
       "21                                     0.671978   \n",
       "22                                     0.673639   \n",
       "23                                     0.673639   \n",
       "24                                     0.743554   \n",
       "25                                     0.692465   \n",
       "26                                     0.676349   \n",
       "27                                     0.662996   \n",
       "28                                     0.675440   \n",
       "29                                     0.674239   \n",
       "30                                     0.666590   \n",
       "31                                     0.626692   \n",
       "32                                     0.672035   \n",
       "33                                     0.650548   \n",
       "34                                     0.620933   \n",
       "35                                     0.669737   \n",
       "36                                     0.666689   \n",
       "37                                     0.674048   \n",
       "38                                     0.674048   \n",
       "39                                     0.675858   \n",
       "40                                     0.675858   \n",
       "41                                     0.679977   \n",
       "42                                     0.667189   \n",
       "43                                     0.666326   \n",
       "44                                     0.672936   \n",
       "45                                     0.672936   \n",
       "46                                     0.659728   \n",
       "47                                     0.725109   \n",
       "48                                     0.672326   \n",
       "49                                     0.669753   \n",
       "50                                     0.672326   \n",
       "51                                     0.671381   \n",
       "52                                     0.676406   \n",
       "53                                     0.676467   \n",
       "\n",
       "    binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "0                                 0.624602                     0.743243   \n",
       "1                                 0.618528                     0.729730   \n",
       "2                                 0.652591                     0.574324   \n",
       "3                                 0.606104                     0.729730   \n",
       "4                                 0.592628                     0.729730   \n",
       "5                                 0.579294                     0.729730   \n",
       "6                                 0.582609                     0.729730   \n",
       "7                                 0.593875                     0.729730   \n",
       "8                                 0.577630                     0.729730   \n",
       "9                                 0.599570                     0.729730   \n",
       "10                                0.593555                     0.729730   \n",
       "11                                0.598648                     0.729730   \n",
       "12                                0.644363                     0.648649   \n",
       "13                                0.644363                     0.648649   \n",
       "14                                0.621219                     0.648649   \n",
       "15                                0.621219                     0.648649   \n",
       "16                                0.650384                     0.648649   \n",
       "17                                0.650384                     0.648649   \n",
       "18                                0.627615                     0.648649   \n",
       "19                                0.627615                     0.648649   \n",
       "20                                0.654625                     0.648649   \n",
       "21                                0.654625                     0.648649   \n",
       "22                                0.657618                     0.648649   \n",
       "23                                0.657618                     0.648649   \n",
       "24                                1.196940                     0.472973   \n",
       "25                                0.751844                     0.337838   \n",
       "26                                0.663386                     0.648649   \n",
       "27                                0.582301                     0.729730   \n",
       "28                                0.655696                     0.648649   \n",
       "29                                0.644722                     0.648649   \n",
       "30                                0.581631                     0.729730   \n",
       "31                                0.460748                     0.777027   \n",
       "32                                0.623440                     0.675676   \n",
       "33                                0.595131                     0.608108   \n",
       "34                                0.436572                     0.743243   \n",
       "35                                0.603190                     0.729730   \n",
       "36                                0.581924                     0.729730   \n",
       "37                                0.667738                     0.648649   \n",
       "38                                0.667738                     0.648649   \n",
       "39                                0.656414                     0.648649   \n",
       "40                                0.656414                     0.648649   \n",
       "41                                0.639753                     0.729730   \n",
       "42                                0.582810                     0.729730   \n",
       "43                                0.586550                     0.729730   \n",
       "44                                0.651634                     0.648649   \n",
       "45                                0.651634                     0.648649   \n",
       "46                                0.548108                     0.729730   \n",
       "47                                0.923983                     0.337838   \n",
       "48                                0.646152                     0.648649   \n",
       "49                                0.594847                     0.729730   \n",
       "50                                0.646152                     0.648649   \n",
       "51                                0.635908                     0.648649   \n",
       "52                                0.644446                     0.648649   \n",
       "53                                0.650746                     0.648649   \n",
       "\n",
       "    f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "0                      0.486486                    0.110725   \n",
       "1                      0.000000                    0.125714   \n",
       "2                      0.322581                    0.207149   \n",
       "3                      0.000000                    0.135190   \n",
       "4                      0.000000                    0.153153   \n",
       "5                      0.000000                    0.314534   \n",
       "6                      0.000000                    0.135682   \n",
       "7                      0.000000                    0.125292   \n",
       "8                      0.000000                    0.174584   \n",
       "9                      0.000000                    0.104679   \n",
       "10                     0.000000                    0.129608   \n",
       "11                     0.000000                    0.177235   \n",
       "12                     0.000000                    0.139696   \n",
       "13                     0.000000                    0.131851   \n",
       "14                     0.000000                    0.092488   \n",
       "15                     0.000000                    0.097656   \n",
       "16                     0.000000                    0.097691   \n",
       "17                     0.000000                    0.086548   \n",
       "18                     0.000000                    0.106533   \n",
       "19                     0.000000                    0.103857   \n",
       "20                     0.000000                    0.159502   \n",
       "21                     0.000000                    0.149537   \n",
       "22                     0.000000                    0.154387   \n",
       "23                     0.000000                    0.193856   \n",
       "24                     0.506329                    0.141552   \n",
       "25                     0.449438                    0.176855   \n",
       "26                     0.000000                    0.105651   \n",
       "27                     0.000000                    0.159159   \n",
       "28                     0.000000                    0.152041   \n",
       "29                     0.000000                    0.138362   \n",
       "30                     0.000000                    0.112174   \n",
       "31                     0.755556                    0.064539   \n",
       "32                     0.351351                    0.505987   \n",
       "33                     0.632911                    0.175453   \n",
       "34                     0.677966                    0.051095   \n",
       "35                     0.000000                    5.381506   \n",
       "36                     0.000000                    0.962151   \n",
       "37                     0.000000                    0.593215   \n",
       "38                     0.000000                    0.551471   \n",
       "39                     0.000000                    0.141383   \n",
       "40                     0.000000                    0.136782   \n",
       "41                     0.000000                    0.106841   \n",
       "42                     0.000000                    0.188092   \n",
       "43                     0.000000                    0.090359   \n",
       "44                     0.000000                    0.079618   \n",
       "45                     0.000000                    0.080533   \n",
       "46                     0.000000                    0.122194   \n",
       "47                     0.449438                    0.079203   \n",
       "48                     0.000000                    0.105075   \n",
       "49                     0.000000                    0.085431   \n",
       "50                     0.000000                    0.115530   \n",
       "51                     0.000000                    0.144323   \n",
       "52                     0.000000                    0.137836   \n",
       "53                     0.000000                    0.079789   \n",
       "\n",
       "    soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "0                                 0.909159                           2.012290   \n",
       "1                                 0.933302                           0.000000   \n",
       "2                                 0.836600                           1.238022   \n",
       "3                                 0.905225                           1.944110   \n",
       "4                                 0.853145                           0.000000   \n",
       "5                                 0.854233                           1.729415   \n",
       "6                                 0.722902                           0.762560   \n",
       "7                                 0.677103                           0.671711   \n",
       "8                                 0.683051                           0.675042   \n",
       "9                                 0.733779                           0.793555   \n",
       "10                                0.761217                           0.873499   \n",
       "11                                0.706802                           0.723446   \n",
       "12                                0.858197                           1.524665   \n",
       "13                                0.858197                           1.524665   \n",
       "14                                0.799476                           1.013420   \n",
       "15                                0.799476                           1.013420   \n",
       "16                                0.900496                           1.792710   \n",
       "17                                0.900496                           1.792710   \n",
       "18                                0.806887                           1.056151   \n",
       "19                                0.806887                           1.056151   \n",
       "20                                0.866803                           1.464826   \n",
       "21                                0.866803                           1.464826   \n",
       "22                                0.818518                           1.111223   \n",
       "23                                0.818518                           1.111223   \n",
       "24                                0.678608                           1.979326   \n",
       "25                                0.932909                           4.607446   \n",
       "26                                0.916003                           2.128734   \n",
       "27                                0.616447                           0.546445   \n",
       "28                                0.925376                           2.516976   \n",
       "29                                0.829728                           1.152108   \n",
       "30                                0.716565                           0.745760   \n",
       "31                                0.648073                           0.000000   \n",
       "32                                0.853060                           1.368479   \n",
       "33                                0.663431                           0.663653   \n",
       "34                                0.665811                           0.890117   \n",
       "35                                0.703703                           0.715697   \n",
       "36                                0.881891                           1.602950   \n",
       "37                                0.858416                           1.342356   \n",
       "38                                0.858416                           1.342356   \n",
       "39                                0.721825                           1.056322   \n",
       "40                                0.721825                           1.056322   \n",
       "41                                0.883034                           1.618120   \n",
       "42                                0.913043                           2.129727   \n",
       "43                                0.726350                           0.771724   \n",
       "44                                0.857244                           2.067929   \n",
       "45                                0.857244                           2.067929   \n",
       "46                                0.682691                           0.675333   \n",
       "47                                0.862445                           1.513847   \n",
       "48                                0.819148                           1.105667   \n",
       "49                                0.754582                           0.855642   \n",
       "50                                0.819148                           1.105667   \n",
       "51                                0.711192                           0.750445   \n",
       "52                                0.839257                           1.249003   \n",
       "53                                0.792118                           0.981287   \n",
       "\n",
       "    accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "0                 0.382619                0.553470               0.104282   \n",
       "1                 0.382619                0.553470               0.116253   \n",
       "2                 0.382619                0.553470               0.161816   \n",
       "3                 0.382619                0.553470               0.142096   \n",
       "4                 0.382619                0.553470               0.187451   \n",
       "5                 0.488868                0.595357               0.346450   \n",
       "6                 0.378934                0.549605               0.172872   \n",
       "7                 0.629971                0.045922               0.208214   \n",
       "8                 0.620912                0.004034               0.174649   \n",
       "9                 0.378934                0.549605              48.206169   \n",
       "10                0.378934                0.549605               0.160855   \n",
       "11                0.378934                0.549605               0.177735   \n",
       "12                0.382926                0.553791               0.119750   \n",
       "13                0.382926                0.553791               0.137243   \n",
       "14                0.382926                0.553791               0.094413   \n",
       "15                0.382926                0.553791               0.098506   \n",
       "16                0.382926                0.553791               0.099299   \n",
       "17                0.382926                0.553791               0.101976   \n",
       "18                0.382926                0.553791               0.091847   \n",
       "19                0.382926                0.553791               0.433442   \n",
       "20                0.382926                0.553791               0.136998   \n",
       "21                0.382926                0.553791               0.156404   \n",
       "22                0.382926                0.553791               0.478246   \n",
       "23                0.382926                0.553791               0.148448   \n",
       "24                0.629203                0.059946               0.187590   \n",
       "25                0.382619                0.553470               0.198959   \n",
       "26                0.386765                0.557795               0.127063   \n",
       "27                0.646860                0.466357               0.158791   \n",
       "28                0.382926                0.553791               0.160029   \n",
       "29                0.382926                0.553791               0.509442   \n",
       "30                0.383080                0.553952               0.110954   \n",
       "31                0.640104                0.123411               0.063903   \n",
       "32                0.382619                0.553470               0.498401   \n",
       "33                0.619070                0.029722               0.165916   \n",
       "34                0.628896                0.040492               0.057173   \n",
       "35                0.383080                0.553952               0.479583   \n",
       "36                0.383080                0.553952               0.816631   \n",
       "37                0.382926                0.553791               0.555486   \n",
       "38                0.382926                0.553791               0.755470   \n",
       "39                0.626132                0.671434               0.245350   \n",
       "40                0.626132                0.671434               0.391079   \n",
       "41                0.383080                0.553952               0.295744   \n",
       "42                0.383080                0.553952               0.455591   \n",
       "43                0.383080                0.553952               0.203054   \n",
       "44                0.382926                0.553791               0.154398   \n",
       "45                0.382926                0.553791               0.175256   \n",
       "46                0.622754                0.030004               0.226720   \n",
       "47                0.383080                0.553952               0.125940   \n",
       "48                0.382926                0.553791               0.104810   \n",
       "49                0.140949                0.079618               0.081968   \n",
       "50                0.382926                0.553791               0.116319   \n",
       "51                0.382926                0.553791               0.497421   \n",
       "52                0.382926                0.553791               0.153652   \n",
       "53                0.382926                0.553791               0.074685   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_1000000  \\\n",
       "0                                   0.685014   \n",
       "1                                   0.649897   \n",
       "2                                   0.713819   \n",
       "3                                   0.718173   \n",
       "4                                   0.673761   \n",
       "5                                   0.647532   \n",
       "6                                   0.588987   \n",
       "7                                   0.624959   \n",
       "8                                   0.717925   \n",
       "9                                   0.663955   \n",
       "10                                  0.702805   \n",
       "11                                  0.691413   \n",
       "12                                  0.693719   \n",
       "13                                  0.693719   \n",
       "14                                  0.697205   \n",
       "15                                  0.697205   \n",
       "16                                  0.703797   \n",
       "17                                  0.703797   \n",
       "18                                  0.718736   \n",
       "19                                  0.718736   \n",
       "20                                  0.597506   \n",
       "21                                  0.597506   \n",
       "22                                  0.685908   \n",
       "23                                  0.685908   \n",
       "24                                  0.543518   \n",
       "25                                  0.699143   \n",
       "26                                  0.709916   \n",
       "27                                  0.679841   \n",
       "28                                  0.711171   \n",
       "29                                  0.720780   \n",
       "30                                  0.693429   \n",
       "31                                  0.546199   \n",
       "32                                  0.654627   \n",
       "33                                  0.555836   \n",
       "34                                  0.555761   \n",
       "35                                  0.708126   \n",
       "36                                  0.715798   \n",
       "37                                  0.664220   \n",
       "38                                  0.664220   \n",
       "39                                  0.705104   \n",
       "40                                  0.705104   \n",
       "41                                  0.648570   \n",
       "42                                  0.711864   \n",
       "43                                  0.677037   \n",
       "44                                  0.701440   \n",
       "45                                  0.701440   \n",
       "46                                  0.593289   \n",
       "47                                  0.609558   \n",
       "48                                  0.709508   \n",
       "49                                  0.666320   \n",
       "50                                  0.709508   \n",
       "51                                  0.729865   \n",
       "52                                  0.711331   \n",
       "53                                  0.642949   \n",
       "\n",
       "    binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "0                              0.692467                  0.642458   \n",
       "1                              0.571653                  0.932961   \n",
       "2                              0.766724                  0.374302   \n",
       "3                              0.782591                  0.374302   \n",
       "4                              0.672869                  0.631285   \n",
       "5                              0.612850                  0.659218   \n",
       "6                              0.400991                  0.932961   \n",
       "7                              0.502890                  0.932961   \n",
       "8                              0.790258                  0.374302   \n",
       "9                              0.676750                  0.620112   \n",
       "10                             0.739006                  0.379888   \n",
       "11                             0.698940                  0.379888   \n",
       "12                             0.701660                  0.374302   \n",
       "13                             0.701660                  0.374302   \n",
       "14                             0.713038                  0.374302   \n",
       "15                             0.713038                  0.374302   \n",
       "16                             0.731052                  0.374302   \n",
       "17                             0.731052                  0.374302   \n",
       "18                             0.804630                  0.374302   \n",
       "19                             0.804630                  0.374302   \n",
       "20                             0.433255                  0.905028   \n",
       "21                             0.433255                  0.905028   \n",
       "22                             0.688458                  0.374302   \n",
       "23                             0.688458                  0.374302   \n",
       "24                             0.301996                  0.888268   \n",
       "25                             0.724051                  0.575419   \n",
       "26                             0.766176                  0.374302   \n",
       "27                             0.698207                  0.648045   \n",
       "28                             0.767504                  0.374302   \n",
       "29                             0.795482                  0.374302   \n",
       "30                             0.738694                  0.374302   \n",
       "31                             0.361998                  0.854749   \n",
       "32                             0.589957                  0.921788   \n",
       "33                             0.351944                  0.899441   \n",
       "34                             0.322628                  0.938547   \n",
       "35                             0.746740                  0.374302   \n",
       "36                             0.776117                  0.385475   \n",
       "37                             0.651787                  0.614525   \n",
       "38                             0.651787                  0.614525   \n",
       "39                             0.726948                  0.324022   \n",
       "40                             0.726948                  0.324022   \n",
       "41                             0.674465                  0.653631   \n",
       "42                             0.759995                  0.374302   \n",
       "43                             0.695385                  0.620112   \n",
       "44                             0.719665                  0.374302   \n",
       "45                             0.719665                  0.374302   \n",
       "46                             0.420265                  0.932961   \n",
       "47                             0.488859                  0.787709   \n",
       "48                             0.751050                  0.374302   \n",
       "49                             0.639893                  0.374302   \n",
       "50                             0.751050                  0.374302   \n",
       "51                             0.803884                  0.374302   \n",
       "52                             0.761270                  0.374302   \n",
       "53                             0.559280                  0.720670   \n",
       "\n",
       "    f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "0                   0.418182                 0.096975   \n",
       "1                   0.911765                 0.111433   \n",
       "2                   0.544715                 0.167118   \n",
       "3                   0.544715                 0.164211   \n",
       "4                   0.565789                 0.163254   \n",
       "5                   0.519685                 0.296510   \n",
       "6                   0.911765                 0.126030   \n",
       "7                   0.911765                 0.168827   \n",
       "8                   0.544715                 0.173118   \n",
       "9                   0.433333                 0.135714   \n",
       "10                  0.458537                 0.128241   \n",
       "11                  0.546939                 0.181615   \n",
       "12                  0.544715                 0.120670   \n",
       "13                  0.544715                 0.135688   \n",
       "14                  0.544715                 0.107251   \n",
       "15                  0.544715                 0.101623   \n",
       "16                  0.544715                 0.095954   \n",
       "17                  0.544715                 0.097345   \n",
       "18                  0.544715                 0.104906   \n",
       "19                  0.544715                 0.127601   \n",
       "20                  0.870229                 0.146318   \n",
       "21                  0.870229                 0.164939   \n",
       "22                  0.544715                 0.130947   \n",
       "23                  0.544715                 0.153490   \n",
       "24                  0.838710                 0.167211   \n",
       "25                  0.464789                 0.183146   \n",
       "26                  0.544715                 0.111354   \n",
       "27                  0.651934                 0.172055   \n",
       "28                  0.544715                 0.141309   \n",
       "29                  0.544715                 0.132544   \n",
       "30                  0.544715                 0.112628   \n",
       "31                  0.783333                 0.066365   \n",
       "32                  0.898551                 0.516098   \n",
       "33                  0.875000                 0.168550   \n",
       "34                  0.918519                 0.059792   \n",
       "35                  0.544715                 0.631936   \n",
       "36                  0.545455                 0.359247   \n",
       "37                  0.429752                 0.485876   \n",
       "38                  0.429752                 0.714134   \n",
       "39                  0.489451                 0.323923   \n",
       "40                  0.489451                 0.295167   \n",
       "41                  0.507937                 0.149793   \n",
       "42                  0.544715                 0.224715   \n",
       "43                  0.521127                 0.131490   \n",
       "44                  0.544715                 0.078544   \n",
       "45                  0.544715                 0.077407   \n",
       "46                  0.911765                 0.132813   \n",
       "47                  0.612245                 0.079489   \n",
       "48                  0.544715                 0.099175   \n",
       "49                  0.544715                 0.078860   \n",
       "50                  0.544715                 0.112988   \n",
       "51                  0.544715                 0.142056   \n",
       "52                  0.544715                 0.113839   \n",
       "53                  0.431818                 0.073720   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                       0.665557   \n",
       "1                                       0.668186   \n",
       "2                                       0.673644   \n",
       "3                                       0.672519   \n",
       "4                                       0.667948   \n",
       "5                                       0.666934   \n",
       "6                                       0.667058   \n",
       "7                                       0.668177   \n",
       "8                                       0.661191   \n",
       "9                                       0.670961   \n",
       "10                                      0.668651   \n",
       "11                                      0.668631   \n",
       "12                                      0.670297   \n",
       "13                                      0.670297   \n",
       "14                                      0.663541   \n",
       "15                                      0.663541   \n",
       "16                                      0.671838   \n",
       "17                                      0.671838   \n",
       "18                                      0.666985   \n",
       "19                                      0.666985   \n",
       "20                                      0.671978   \n",
       "21                                      0.671978   \n",
       "22                                      0.673639   \n",
       "23                                      0.673639   \n",
       "24                                      0.743554   \n",
       "25                                      0.692465   \n",
       "26                                      0.676349   \n",
       "27                                      0.662996   \n",
       "28                                      0.675440   \n",
       "29                                      0.674239   \n",
       "30                                      0.666590   \n",
       "31                                      0.626692   \n",
       "32                                      0.672035   \n",
       "33                                      0.650548   \n",
       "34                                      0.620933   \n",
       "35                                      0.669737   \n",
       "36                                      0.666689   \n",
       "37                                      0.674048   \n",
       "38                                      0.674048   \n",
       "39                                      0.675858   \n",
       "40                                      0.675858   \n",
       "41                                      0.679977   \n",
       "42                                      0.667189   \n",
       "43                                      0.666326   \n",
       "44                                      0.672936   \n",
       "45                                      0.672936   \n",
       "46                                      0.659728   \n",
       "47                                      0.725109   \n",
       "48                                      0.672326   \n",
       "49                                      0.669753   \n",
       "50                                      0.672326   \n",
       "51                                      0.671381   \n",
       "52                                      0.676406   \n",
       "53                                      0.676467   \n",
       "\n",
       "    binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "0                                  0.624602                      0.743243   \n",
       "1                                  0.618528                      0.729730   \n",
       "2                                  0.652591                      0.574324   \n",
       "3                                  0.606104                      0.729730   \n",
       "4                                  0.592628                      0.729730   \n",
       "5                                  0.579294                      0.729730   \n",
       "6                                  0.582609                      0.729730   \n",
       "7                                  0.593875                      0.729730   \n",
       "8                                  0.577630                      0.729730   \n",
       "9                                  0.599570                      0.729730   \n",
       "10                                 0.593555                      0.729730   \n",
       "11                                 0.598648                      0.729730   \n",
       "12                                 0.644363                      0.648649   \n",
       "13                                 0.644363                      0.648649   \n",
       "14                                 0.621219                      0.648649   \n",
       "15                                 0.621219                      0.648649   \n",
       "16                                 0.650384                      0.648649   \n",
       "17                                 0.650384                      0.648649   \n",
       "18                                 0.627615                      0.648649   \n",
       "19                                 0.627615                      0.648649   \n",
       "20                                 0.654625                      0.648649   \n",
       "21                                 0.654625                      0.648649   \n",
       "22                                 0.657618                      0.648649   \n",
       "23                                 0.657618                      0.648649   \n",
       "24                                 1.196940                      0.472973   \n",
       "25                                 0.751844                      0.337838   \n",
       "26                                 0.663386                      0.648649   \n",
       "27                                 0.582301                      0.729730   \n",
       "28                                 0.655696                      0.648649   \n",
       "29                                 0.644722                      0.648649   \n",
       "30                                 0.581631                      0.729730   \n",
       "31                                 0.460748                      0.777027   \n",
       "32                                 0.623440                      0.675676   \n",
       "33                                 0.595131                      0.608108   \n",
       "34                                 0.436572                      0.743243   \n",
       "35                                 0.603190                      0.729730   \n",
       "36                                 0.581924                      0.729730   \n",
       "37                                 0.667738                      0.648649   \n",
       "38                                 0.667738                      0.648649   \n",
       "39                                 0.656414                      0.648649   \n",
       "40                                 0.656414                      0.648649   \n",
       "41                                 0.639753                      0.729730   \n",
       "42                                 0.582810                      0.729730   \n",
       "43                                 0.586550                      0.729730   \n",
       "44                                 0.651634                      0.648649   \n",
       "45                                 0.651634                      0.648649   \n",
       "46                                 0.548108                      0.729730   \n",
       "47                                 0.923983                      0.337838   \n",
       "48                                 0.646152                      0.648649   \n",
       "49                                 0.594847                      0.729730   \n",
       "50                                 0.646152                      0.648649   \n",
       "51                                 0.635908                      0.648649   \n",
       "52                                 0.644446                      0.648649   \n",
       "53                                 0.650746                      0.648649   \n",
       "\n",
       "    f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "0                       0.486486                     0.110725   \n",
       "1                       0.000000                     0.125714   \n",
       "2                       0.322581                     0.207149   \n",
       "3                       0.000000                     0.135190   \n",
       "4                       0.000000                     0.153153   \n",
       "5                       0.000000                     0.314534   \n",
       "6                       0.000000                     0.135682   \n",
       "7                       0.000000                     0.125292   \n",
       "8                       0.000000                     0.174584   \n",
       "9                       0.000000                     0.104679   \n",
       "10                      0.000000                     0.129608   \n",
       "11                      0.000000                     0.177235   \n",
       "12                      0.000000                     0.139696   \n",
       "13                      0.000000                     0.131851   \n",
       "14                      0.000000                     0.092488   \n",
       "15                      0.000000                     0.097656   \n",
       "16                      0.000000                     0.097691   \n",
       "17                      0.000000                     0.086548   \n",
       "18                      0.000000                     0.106533   \n",
       "19                      0.000000                     0.103857   \n",
       "20                      0.000000                     0.159502   \n",
       "21                      0.000000                     0.149537   \n",
       "22                      0.000000                     0.154387   \n",
       "23                      0.000000                     0.193856   \n",
       "24                      0.506329                     0.141552   \n",
       "25                      0.449438                     0.176855   \n",
       "26                      0.000000                     0.105651   \n",
       "27                      0.000000                     0.159159   \n",
       "28                      0.000000                     0.152041   \n",
       "29                      0.000000                     0.138362   \n",
       "30                      0.000000                     0.112174   \n",
       "31                      0.755556                     0.064539   \n",
       "32                      0.351351                     0.505987   \n",
       "33                      0.632911                     0.175453   \n",
       "34                      0.677966                     0.051095   \n",
       "35                      0.000000                     5.381506   \n",
       "36                      0.000000                     0.962151   \n",
       "37                      0.000000                     0.593215   \n",
       "38                      0.000000                     0.551471   \n",
       "39                      0.000000                     0.141383   \n",
       "40                      0.000000                     0.136782   \n",
       "41                      0.000000                     0.106841   \n",
       "42                      0.000000                     0.188092   \n",
       "43                      0.000000                     0.090359   \n",
       "44                      0.000000                     0.079618   \n",
       "45                      0.000000                     0.080533   \n",
       "46                      0.000000                     0.122194   \n",
       "47                      0.449438                     0.079203   \n",
       "48                      0.000000                     0.105075   \n",
       "49                      0.000000                     0.085431   \n",
       "50                      0.000000                     0.115530   \n",
       "51                      0.000000                     0.144323   \n",
       "52                      0.000000                     0.137836   \n",
       "53                      0.000000                     0.079789   \n",
       "\n",
       "    soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                    0.909159   \n",
       "1                                    0.933302   \n",
       "2                                    0.836600   \n",
       "3                                    0.905225   \n",
       "4                                    0.853145   \n",
       "5                                    0.854233   \n",
       "6                                    0.722902   \n",
       "7                                    0.677103   \n",
       "8                                    0.683051   \n",
       "9                                    0.733779   \n",
       "10                                   0.761217   \n",
       "11                                   0.706802   \n",
       "12                                   0.858197   \n",
       "13                                   0.858197   \n",
       "14                                   0.799476   \n",
       "15                                   0.799476   \n",
       "16                                   0.900496   \n",
       "17                                   0.900496   \n",
       "18                                   0.806887   \n",
       "19                                   0.806887   \n",
       "20                                   0.866803   \n",
       "21                                   0.866803   \n",
       "22                                   0.818518   \n",
       "23                                   0.818518   \n",
       "24                                   0.678608   \n",
       "25                                   0.932909   \n",
       "26                                   0.916003   \n",
       "27                                   0.616447   \n",
       "28                                   0.925376   \n",
       "29                                   0.829728   \n",
       "30                                   0.716565   \n",
       "31                                   0.648073   \n",
       "32                                   0.853060   \n",
       "33                                   0.663431   \n",
       "34                                   0.665811   \n",
       "35                                   0.703703   \n",
       "36                                   0.881891   \n",
       "37                                   0.858416   \n",
       "38                                   0.858416   \n",
       "39                                   0.721825   \n",
       "40                                   0.721825   \n",
       "41                                   0.883034   \n",
       "42                                   0.913043   \n",
       "43                                   0.726350   \n",
       "44                                   0.857244   \n",
       "45                                   0.857244   \n",
       "46                                   0.682691   \n",
       "47                                   0.862445   \n",
       "48                                   0.819148   \n",
       "49                                   0.754582   \n",
       "50                                   0.819148   \n",
       "51                                   0.711192   \n",
       "52                                   0.839257   \n",
       "53                                   0.792118   \n",
       "\n",
       "    binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "0                               2.012290                   0.382619   \n",
       "1                               0.000000                   0.382619   \n",
       "2                               1.238022                   0.382619   \n",
       "3                               1.944110                   0.382619   \n",
       "4                               0.000000                   0.382619   \n",
       "5                               1.729415                   0.488868   \n",
       "6                               0.762560                   0.378934   \n",
       "7                               0.671711                   0.629971   \n",
       "8                               0.675042                   0.620912   \n",
       "9                               0.793555                   0.378934   \n",
       "10                              0.873499                   0.378934   \n",
       "11                              0.723446                   0.378934   \n",
       "12                              1.524665                   0.382926   \n",
       "13                              1.524665                   0.382926   \n",
       "14                              1.013420                   0.382926   \n",
       "15                              1.013420                   0.382926   \n",
       "16                              1.792710                   0.382926   \n",
       "17                              1.792710                   0.382926   \n",
       "18                              1.056151                   0.382926   \n",
       "19                              1.056151                   0.382926   \n",
       "20                              1.464826                   0.382926   \n",
       "21                              1.464826                   0.382926   \n",
       "22                              1.111223                   0.382926   \n",
       "23                              1.111223                   0.382926   \n",
       "24                              1.979326                   0.629203   \n",
       "25                              4.607446                   0.382619   \n",
       "26                              2.128734                   0.386765   \n",
       "27                              0.546445                   0.646860   \n",
       "28                              2.516976                   0.382926   \n",
       "29                              1.152108                   0.382926   \n",
       "30                              0.745760                   0.383080   \n",
       "31                              0.000000                   0.640104   \n",
       "32                              1.368479                   0.382619   \n",
       "33                              0.663653                   0.619070   \n",
       "34                              0.890117                   0.628896   \n",
       "35                              0.715697                   0.383080   \n",
       "36                              1.602950                   0.383080   \n",
       "37                              1.342356                   0.382926   \n",
       "38                              1.342356                   0.382926   \n",
       "39                              1.056322                   0.626132   \n",
       "40                              1.056322                   0.626132   \n",
       "41                              1.618120                   0.383080   \n",
       "42                              2.129727                   0.383080   \n",
       "43                              0.771724                   0.383080   \n",
       "44                              2.067929                   0.382926   \n",
       "45                              2.067929                   0.382926   \n",
       "46                              0.675333                   0.622754   \n",
       "47                              1.513847                   0.383080   \n",
       "48                              1.105667                   0.382926   \n",
       "49                              0.855642                   0.140949   \n",
       "50                              1.105667                   0.382926   \n",
       "51                              0.750445                   0.382926   \n",
       "52                              1.249003                   0.382926   \n",
       "53                              0.981287                   0.382926   \n",
       "\n",
       "    f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "0                    0.553470                  0.104282   \n",
       "1                    0.553470                  0.116253   \n",
       "2                    0.553470                  0.161816   \n",
       "3                    0.553470                  0.142096   \n",
       "4                    0.553470                  0.187451   \n",
       "5                    0.595357                  0.346450   \n",
       "6                    0.549605                  0.172872   \n",
       "7                    0.045922                  0.208214   \n",
       "8                    0.004034                  0.174649   \n",
       "9                    0.549605                 48.206169   \n",
       "10                   0.549605                  0.160855   \n",
       "11                   0.549605                  0.177735   \n",
       "12                   0.553791                  0.119750   \n",
       "13                   0.553791                  0.137243   \n",
       "14                   0.553791                  0.094413   \n",
       "15                   0.553791                  0.098506   \n",
       "16                   0.553791                  0.099299   \n",
       "17                   0.553791                  0.101976   \n",
       "18                   0.553791                  0.091847   \n",
       "19                   0.553791                  0.433442   \n",
       "20                   0.553791                  0.136998   \n",
       "21                   0.553791                  0.156404   \n",
       "22                   0.553791                  0.478246   \n",
       "23                   0.553791                  0.148448   \n",
       "24                   0.059946                  0.187590   \n",
       "25                   0.553470                  0.198959   \n",
       "26                   0.557795                  0.127063   \n",
       "27                   0.466357                  0.158791   \n",
       "28                   0.553791                  0.160029   \n",
       "29                   0.553791                  0.509442   \n",
       "30                   0.553952                  0.110954   \n",
       "31                   0.123411                  0.063903   \n",
       "32                   0.553470                  0.498401   \n",
       "33                   0.029722                  0.165916   \n",
       "34                   0.040492                  0.057173   \n",
       "35                   0.553952                  0.479583   \n",
       "36                   0.553952                  0.816631   \n",
       "37                   0.553791                  0.555486   \n",
       "38                   0.553791                  0.755470   \n",
       "39                   0.671434                  0.245350   \n",
       "40                   0.671434                  0.391079   \n",
       "41                   0.553952                  0.295744   \n",
       "42                   0.553952                  0.455591   \n",
       "43                   0.553952                  0.203054   \n",
       "44                   0.553791                  0.154398   \n",
       "45                   0.553791                  0.175256   \n",
       "46                   0.030004                  0.226720   \n",
       "47                   0.553952                  0.125940   \n",
       "48                   0.553791                  0.104810   \n",
       "49                   0.079618                  0.081968   \n",
       "50                   0.553791                  0.116319   \n",
       "51                   0.553791                  0.497421   \n",
       "52                   0.553791                  0.153652   \n",
       "53                   0.553791                  0.074685   \n",
       "\n",
       "    soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                      0.685014   \n",
       "1                                      0.649897   \n",
       "2                                      0.713819   \n",
       "3                                      0.718173   \n",
       "4                                      0.673761   \n",
       "5                                      0.647532   \n",
       "6                                      0.588987   \n",
       "7                                      0.624959   \n",
       "8                                      0.717925   \n",
       "9                                      0.663955   \n",
       "10                                     0.702805   \n",
       "11                                     0.691413   \n",
       "12                                     0.693719   \n",
       "13                                     0.693719   \n",
       "14                                     0.697205   \n",
       "15                                     0.697205   \n",
       "16                                     0.703797   \n",
       "17                                     0.703797   \n",
       "18                                     0.718736   \n",
       "19                                     0.718736   \n",
       "20                                     0.597506   \n",
       "21                                     0.597506   \n",
       "22                                     0.685908   \n",
       "23                                     0.685908   \n",
       "24                                     0.543518   \n",
       "25                                     0.699143   \n",
       "26                                     0.709916   \n",
       "27                                     0.679841   \n",
       "28                                     0.711171   \n",
       "29                                     0.720780   \n",
       "30                                     0.693429   \n",
       "31                                     0.546199   \n",
       "32                                     0.654627   \n",
       "33                                     0.555836   \n",
       "34                                     0.555761   \n",
       "35                                     0.708126   \n",
       "36                                     0.715798   \n",
       "37                                     0.664220   \n",
       "38                                     0.664220   \n",
       "39                                     0.705104   \n",
       "40                                     0.705104   \n",
       "41                                     0.648570   \n",
       "42                                     0.711864   \n",
       "43                                     0.677037   \n",
       "44                                     0.701440   \n",
       "45                                     0.701440   \n",
       "46                                     0.593289   \n",
       "47                                     0.609558   \n",
       "48                                     0.709508   \n",
       "49                                     0.666320   \n",
       "50                                     0.709508   \n",
       "51                                     0.729865   \n",
       "52                                     0.711331   \n",
       "53                                     0.642949   \n",
       "\n",
       "    binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "0                                 0.692467                     0.642458   \n",
       "1                                 0.571653                     0.932961   \n",
       "2                                 0.766724                     0.374302   \n",
       "3                                 0.782591                     0.374302   \n",
       "4                                 0.672869                     0.631285   \n",
       "5                                 0.612850                     0.659218   \n",
       "6                                 0.400991                     0.932961   \n",
       "7                                 0.502890                     0.932961   \n",
       "8                                 0.790258                     0.374302   \n",
       "9                                 0.676750                     0.620112   \n",
       "10                                0.739006                     0.379888   \n",
       "11                                0.698940                     0.379888   \n",
       "12                                0.701660                     0.374302   \n",
       "13                                0.701660                     0.374302   \n",
       "14                                0.713038                     0.374302   \n",
       "15                                0.713038                     0.374302   \n",
       "16                                0.731052                     0.374302   \n",
       "17                                0.731052                     0.374302   \n",
       "18                                0.804630                     0.374302   \n",
       "19                                0.804630                     0.374302   \n",
       "20                                0.433255                     0.905028   \n",
       "21                                0.433255                     0.905028   \n",
       "22                                0.688458                     0.374302   \n",
       "23                                0.688458                     0.374302   \n",
       "24                                0.301996                     0.888268   \n",
       "25                                0.724051                     0.575419   \n",
       "26                                0.766176                     0.374302   \n",
       "27                                0.698207                     0.648045   \n",
       "28                                0.767504                     0.374302   \n",
       "29                                0.795482                     0.374302   \n",
       "30                                0.738694                     0.374302   \n",
       "31                                0.361998                     0.854749   \n",
       "32                                0.589957                     0.921788   \n",
       "33                                0.351944                     0.899441   \n",
       "34                                0.322628                     0.938547   \n",
       "35                                0.746740                     0.374302   \n",
       "36                                0.776117                     0.385475   \n",
       "37                                0.651787                     0.614525   \n",
       "38                                0.651787                     0.614525   \n",
       "39                                0.726948                     0.324022   \n",
       "40                                0.726948                     0.324022   \n",
       "41                                0.674465                     0.653631   \n",
       "42                                0.759995                     0.374302   \n",
       "43                                0.695385                     0.620112   \n",
       "44                                0.719665                     0.374302   \n",
       "45                                0.719665                     0.374302   \n",
       "46                                0.420265                     0.932961   \n",
       "47                                0.488859                     0.787709   \n",
       "48                                0.751050                     0.374302   \n",
       "49                                0.639893                     0.374302   \n",
       "50                                0.751050                     0.374302   \n",
       "51                                0.803884                     0.374302   \n",
       "52                                0.761270                     0.374302   \n",
       "53                                0.559280                     0.720670   \n",
       "\n",
       "    f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "0                      0.418182                    0.096975   \n",
       "1                      0.911765                    0.111433   \n",
       "2                      0.544715                    0.167118   \n",
       "3                      0.544715                    0.164211   \n",
       "4                      0.565789                    0.163254   \n",
       "5                      0.519685                    0.296510   \n",
       "6                      0.911765                    0.126030   \n",
       "7                      0.911765                    0.168827   \n",
       "8                      0.544715                    0.173118   \n",
       "9                      0.433333                    0.135714   \n",
       "10                     0.458537                    0.128241   \n",
       "11                     0.546939                    0.181615   \n",
       "12                     0.544715                    0.120670   \n",
       "13                     0.544715                    0.135688   \n",
       "14                     0.544715                    0.107251   \n",
       "15                     0.544715                    0.101623   \n",
       "16                     0.544715                    0.095954   \n",
       "17                     0.544715                    0.097345   \n",
       "18                     0.544715                    0.104906   \n",
       "19                     0.544715                    0.127601   \n",
       "20                     0.870229                    0.146318   \n",
       "21                     0.870229                    0.164939   \n",
       "22                     0.544715                    0.130947   \n",
       "23                     0.544715                    0.153490   \n",
       "24                     0.838710                    0.167211   \n",
       "25                     0.464789                    0.183146   \n",
       "26                     0.544715                    0.111354   \n",
       "27                     0.651934                    0.172055   \n",
       "28                     0.544715                    0.141309   \n",
       "29                     0.544715                    0.132544   \n",
       "30                     0.544715                    0.112628   \n",
       "31                     0.783333                    0.066365   \n",
       "32                     0.898551                    0.516098   \n",
       "33                     0.875000                    0.168550   \n",
       "34                     0.918519                    0.059792   \n",
       "35                     0.544715                    0.631936   \n",
       "36                     0.545455                    0.359247   \n",
       "37                     0.429752                    0.485876   \n",
       "38                     0.429752                    0.714134   \n",
       "39                     0.489451                    0.323923   \n",
       "40                     0.489451                    0.295167   \n",
       "41                     0.507937                    0.149793   \n",
       "42                     0.544715                    0.224715   \n",
       "43                     0.521127                    0.131490   \n",
       "44                     0.544715                    0.078544   \n",
       "45                     0.544715                    0.077407   \n",
       "46                     0.911765                    0.132813   \n",
       "47                     0.612245                    0.079489   \n",
       "48                     0.544715                    0.099175   \n",
       "49                     0.544715                    0.078860   \n",
       "50                     0.544715                    0.112988   \n",
       "51                     0.544715                    0.142056   \n",
       "52                     0.544715                    0.113839   \n",
       "53                     0.431818                    0.073720   \n",
       "\n",
       "    soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                          0.665557   \n",
       "1                                          0.668186   \n",
       "2                                          0.673644   \n",
       "3                                          0.672519   \n",
       "4                                          0.667948   \n",
       "5                                          0.666934   \n",
       "6                                          0.667058   \n",
       "7                                          0.668177   \n",
       "8                                          0.661191   \n",
       "9                                          0.670961   \n",
       "10                                         0.668651   \n",
       "11                                         0.668631   \n",
       "12                                         0.670297   \n",
       "13                                         0.670297   \n",
       "14                                         0.663541   \n",
       "15                                         0.663541   \n",
       "16                                         0.671838   \n",
       "17                                         0.671838   \n",
       "18                                         0.666985   \n",
       "19                                         0.666985   \n",
       "20                                         0.671978   \n",
       "21                                         0.671978   \n",
       "22                                         0.673639   \n",
       "23                                         0.673639   \n",
       "24                                         0.743554   \n",
       "25                                         0.692465   \n",
       "26                                         0.676349   \n",
       "27                                         0.662996   \n",
       "28                                         0.675440   \n",
       "29                                         0.674239   \n",
       "30                                         0.666590   \n",
       "31                                         0.626692   \n",
       "32                                         0.672035   \n",
       "33                                         0.650548   \n",
       "34                                         0.620933   \n",
       "35                                         0.669737   \n",
       "36                                         0.666689   \n",
       "37                                         0.674048   \n",
       "38                                         0.674048   \n",
       "39                                         0.675858   \n",
       "40                                         0.675858   \n",
       "41                                         0.679977   \n",
       "42                                         0.667189   \n",
       "43                                         0.666326   \n",
       "44                                         0.672936   \n",
       "45                                         0.672936   \n",
       "46                                         0.659728   \n",
       "47                                         0.725109   \n",
       "48                                         0.672326   \n",
       "49                                         0.669753   \n",
       "50                                         0.672326   \n",
       "51                                         0.671381   \n",
       "52                                         0.676406   \n",
       "53                                         0.676467   \n",
       "\n",
       "    binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                     0.624602   \n",
       "1                                     0.618528   \n",
       "2                                     0.652591   \n",
       "3                                     0.606104   \n",
       "4                                     0.592628   \n",
       "5                                     0.579294   \n",
       "6                                     0.582609   \n",
       "7                                     0.593875   \n",
       "8                                     0.577630   \n",
       "9                                     0.599570   \n",
       "10                                    0.593555   \n",
       "11                                    0.598648   \n",
       "12                                    0.644363   \n",
       "13                                    0.644363   \n",
       "14                                    0.621219   \n",
       "15                                    0.621219   \n",
       "16                                    0.650384   \n",
       "17                                    0.650384   \n",
       "18                                    0.627615   \n",
       "19                                    0.627615   \n",
       "20                                    0.654625   \n",
       "21                                    0.654625   \n",
       "22                                    0.657618   \n",
       "23                                    0.657618   \n",
       "24                                    1.196940   \n",
       "25                                    0.751844   \n",
       "26                                    0.663386   \n",
       "27                                    0.582301   \n",
       "28                                    0.655696   \n",
       "29                                    0.644722   \n",
       "30                                    0.581631   \n",
       "31                                    0.460748   \n",
       "32                                    0.623440   \n",
       "33                                    0.595131   \n",
       "34                                    0.436572   \n",
       "35                                    0.603190   \n",
       "36                                    0.581924   \n",
       "37                                    0.667738   \n",
       "38                                    0.667738   \n",
       "39                                    0.656414   \n",
       "40                                    0.656414   \n",
       "41                                    0.639753   \n",
       "42                                    0.582810   \n",
       "43                                    0.586550   \n",
       "44                                    0.651634   \n",
       "45                                    0.651634   \n",
       "46                                    0.548108   \n",
       "47                                    0.923983   \n",
       "48                                    0.646152   \n",
       "49                                    0.594847   \n",
       "50                                    0.646152   \n",
       "51                                    0.635908   \n",
       "52                                    0.644446   \n",
       "53                                    0.650746   \n",
       "\n",
       "    accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                          0.743243                         0.486486   \n",
       "1                          0.729730                         0.000000   \n",
       "2                          0.574324                         0.322581   \n",
       "3                          0.729730                         0.000000   \n",
       "4                          0.729730                         0.000000   \n",
       "5                          0.729730                         0.000000   \n",
       "6                          0.729730                         0.000000   \n",
       "7                          0.729730                         0.000000   \n",
       "8                          0.729730                         0.000000   \n",
       "9                          0.729730                         0.000000   \n",
       "10                         0.729730                         0.000000   \n",
       "11                         0.729730                         0.000000   \n",
       "12                         0.648649                         0.000000   \n",
       "13                         0.648649                         0.000000   \n",
       "14                         0.648649                         0.000000   \n",
       "15                         0.648649                         0.000000   \n",
       "16                         0.648649                         0.000000   \n",
       "17                         0.648649                         0.000000   \n",
       "18                         0.648649                         0.000000   \n",
       "19                         0.648649                         0.000000   \n",
       "20                         0.648649                         0.000000   \n",
       "21                         0.648649                         0.000000   \n",
       "22                         0.648649                         0.000000   \n",
       "23                         0.648649                         0.000000   \n",
       "24                         0.472973                         0.506329   \n",
       "25                         0.337838                         0.449438   \n",
       "26                         0.648649                         0.000000   \n",
       "27                         0.729730                         0.000000   \n",
       "28                         0.648649                         0.000000   \n",
       "29                         0.648649                         0.000000   \n",
       "30                         0.729730                         0.000000   \n",
       "31                         0.777027                         0.755556   \n",
       "32                         0.675676                         0.351351   \n",
       "33                         0.608108                         0.632911   \n",
       "34                         0.743243                         0.677966   \n",
       "35                         0.729730                         0.000000   \n",
       "36                         0.729730                         0.000000   \n",
       "37                         0.648649                         0.000000   \n",
       "38                         0.648649                         0.000000   \n",
       "39                         0.648649                         0.000000   \n",
       "40                         0.648649                         0.000000   \n",
       "41                         0.729730                         0.000000   \n",
       "42                         0.729730                         0.000000   \n",
       "43                         0.729730                         0.000000   \n",
       "44                         0.648649                         0.000000   \n",
       "45                         0.648649                         0.000000   \n",
       "46                         0.729730                         0.000000   \n",
       "47                         0.337838                         0.449438   \n",
       "48                         0.648649                         0.000000   \n",
       "49                         0.729730                         0.000000   \n",
       "50                         0.648649                         0.000000   \n",
       "51                         0.648649                         0.000000   \n",
       "52                         0.648649                         0.000000   \n",
       "53                         0.648649                         0.000000   \n",
       "\n",
       "    runtime_absenteeism_TRAIN_DATA  \n",
       "0                         0.110725  \n",
       "1                         0.125714  \n",
       "2                         0.207149  \n",
       "3                         0.135190  \n",
       "4                         0.153153  \n",
       "5                         0.314534  \n",
       "6                         0.135682  \n",
       "7                         0.125292  \n",
       "8                         0.174584  \n",
       "9                         0.104679  \n",
       "10                        0.129608  \n",
       "11                        0.177235  \n",
       "12                        0.139696  \n",
       "13                        0.131851  \n",
       "14                        0.092488  \n",
       "15                        0.097656  \n",
       "16                        0.097691  \n",
       "17                        0.086548  \n",
       "18                        0.106533  \n",
       "19                        0.103857  \n",
       "20                        0.159502  \n",
       "21                        0.149537  \n",
       "22                        0.154387  \n",
       "23                        0.193856  \n",
       "24                        0.141552  \n",
       "25                        0.176855  \n",
       "26                        0.105651  \n",
       "27                        0.159159  \n",
       "28                        0.152041  \n",
       "29                        0.138362  \n",
       "30                        0.112174  \n",
       "31                        0.064539  \n",
       "32                        0.505987  \n",
       "33                        0.175453  \n",
       "34                        0.051095  \n",
       "35                        5.381506  \n",
       "36                        0.962151  \n",
       "37                        0.593215  \n",
       "38                        0.551471  \n",
       "39                        0.141383  \n",
       "40                        0.136782  \n",
       "41                        0.106841  \n",
       "42                        0.188092  \n",
       "43                        0.090359  \n",
       "44                        0.079618  \n",
       "45                        0.080533  \n",
       "46                        0.122194  \n",
       "47                        0.079203  \n",
       "48                        0.105075  \n",
       "49                        0.085431  \n",
       "50                        0.115530  \n",
       "51                        0.144323  \n",
       "52                        0.137836  \n",
       "53                        0.079789  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_inet = []\n",
    "for column in results_summary_columns:\n",
    "    if 'inet_scores' in column:\n",
    "        columns_inet.append(column)\n",
    "results_summary_inet = results_summary[flatten([colmuns_identifier, columns_inet])]\n",
    "\n",
    "columns_inet_rename = []\n",
    "for column in columns_inet:\n",
    "    column = column.replace('inet_scores_', '')\n",
    "    columns_inet_rename.append(column)\n",
    "\n",
    "results_summary_inet.columns = flatten([colmuns_identifier, columns_inet_rename])\n",
    "\n",
    "#results_summary_inet.insert(0, 'scores_type', 'inet_scores')\n",
    "results_summary_inet.insert(0, 'scores_type', [dt_type + str(decision_sparsity) + '_inet_scores' for dt_type, decision_sparsity in zip(results_summary_inet['function_family_dt_type'].values, results_summary_inet['function_family_decision_sparsity'].values)])\n",
    "\n",
    "    \n",
    "print(results_summary_inet.shape)\n",
    "results_summary_inet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5671878-22c3-4a2f-9d46-a516fd0fc5d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.339477Z",
     "iopub.status.busy": "2022-01-04T19:49:56.339185Z",
     "iopub.status.idle": "2022-01-04T19:49:56.421701Z",
     "shell.execute_reply": "2022-01-04T19:49:56.420366Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.339459Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519338</td>\n",
       "      <td>0.521510</td>\n",
       "      <td>0.415607</td>\n",
       "      <td>0.424973</td>\n",
       "      <td>0.812544</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.804334</td>\n",
       "      <td>0.803337</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.019261</td>\n",
       "      <td>0.515391</td>\n",
       "      <td>0.512567</td>\n",
       "      <td>0.408172</td>\n",
       "      <td>0.409819</td>\n",
       "      <td>0.818256</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>0.814105</td>\n",
       "      <td>0.817887</td>\n",
       "      <td>0.021446</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.523347</td>\n",
       "      <td>0.524943</td>\n",
       "      <td>0.421876</td>\n",
       "      <td>0.427665</td>\n",
       "      <td>0.807616</td>\n",
       "      <td>0.8064</td>\n",
       "      <td>0.795271</td>\n",
       "      <td>0.812604</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>0.021086</td>\n",
       "      <td>0.676063</td>\n",
       "      <td>10.156703</td>\n",
       "      <td>0.592354</td>\n",
       "      <td>0.313421</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.590181</td>\n",
       "      <td>0.590905</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.624773</td>\n",
       "      <td>0.472553</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.824417</td>\n",
       "      <td>1.386253</td>\n",
       "      <td>0.429295</td>\n",
       "      <td>0.572218</td>\n",
       "      <td>0.037807</td>\n",
       "      <td>0.556268</td>\n",
       "      <td>0.363571</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>0.636781</td>\n",
       "      <td>0.523597</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>0.814428</td>\n",
       "      <td>1.424239</td>\n",
       "      <td>0.477353</td>\n",
       "      <td>0.592726</td>\n",
       "      <td>0.470477</td>\n",
       "      <td>0.563472</td>\n",
       "      <td>0.380740</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.457628</td>\n",
       "      <td>0.621059</td>\n",
       "      <td>0.444783</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.491968</td>\n",
       "      <td>0.831044</td>\n",
       "      <td>1.510337</td>\n",
       "      <td>0.419929</td>\n",
       "      <td>0.568623</td>\n",
       "      <td>6.078736</td>\n",
       "      <td>0.563204</td>\n",
       "      <td>0.379736</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>6.039980</td>\n",
       "      <td>0.631333</td>\n",
       "      <td>0.484076</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>6.398765</td>\n",
       "      <td>0.509151</td>\n",
       "      <td>0.241589</td>\n",
       "      <td>0.902349</td>\n",
       "      <td>0.882222</td>\n",
       "      <td>0.026223</td>\n",
       "      <td>0.501347</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.606344</td>\n",
       "      <td>0.258774</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495972</td>\n",
       "      <td>0.495769</td>\n",
       "      <td>0.380874</td>\n",
       "      <td>0.383410</td>\n",
       "      <td>0.833472</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.826597</td>\n",
       "      <td>0.829024</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>0.028576</td>\n",
       "      <td>0.492361</td>\n",
       "      <td>0.490719</td>\n",
       "      <td>0.367409</td>\n",
       "      <td>0.366267</td>\n",
       "      <td>0.837136</td>\n",
       "      <td>0.8392</td>\n",
       "      <td>0.834163</td>\n",
       "      <td>0.834104</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.027170</td>\n",
       "      <td>0.498293</td>\n",
       "      <td>0.496365</td>\n",
       "      <td>0.382379</td>\n",
       "      <td>0.378958</td>\n",
       "      <td>0.831040</td>\n",
       "      <td>0.8388</td>\n",
       "      <td>0.819418</td>\n",
       "      <td>0.827358</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.646430</td>\n",
       "      <td>10.300082</td>\n",
       "      <td>0.686627</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.628368</td>\n",
       "      <td>0.670269</td>\n",
       "      <td>0.625698</td>\n",
       "      <td>0.496241</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.645147</td>\n",
       "      <td>0.556817</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.765034</td>\n",
       "      <td>0.982626</td>\n",
       "      <td>0.466605</td>\n",
       "      <td>0.586527</td>\n",
       "      <td>0.059062</td>\n",
       "      <td>0.558729</td>\n",
       "      <td>0.361183</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.048559</td>\n",
       "      <td>0.645694</td>\n",
       "      <td>0.624696</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.085963</td>\n",
       "      <td>0.745745</td>\n",
       "      <td>0.958959</td>\n",
       "      <td>0.544603</td>\n",
       "      <td>0.619646</td>\n",
       "      <td>0.707219</td>\n",
       "      <td>0.570864</td>\n",
       "      <td>0.453769</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.675862</td>\n",
       "      <td>0.780087</td>\n",
       "      <td>0.631943</td>\n",
       "      <td>0.517273</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.561983</td>\n",
       "      <td>0.628368</td>\n",
       "      <td>0.751342</td>\n",
       "      <td>0.965319</td>\n",
       "      <td>0.560571</td>\n",
       "      <td>0.625980</td>\n",
       "      <td>8.948903</td>\n",
       "      <td>0.564544</td>\n",
       "      <td>0.411363</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.675862</td>\n",
       "      <td>8.988415</td>\n",
       "      <td>0.635391</td>\n",
       "      <td>0.511929</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>8.718359</td>\n",
       "      <td>0.499393</td>\n",
       "      <td>0.185063</td>\n",
       "      <td>0.934899</td>\n",
       "      <td>0.914859</td>\n",
       "      <td>0.035989</td>\n",
       "      <td>0.502917</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.603373</td>\n",
       "      <td>0.403781</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.477639</td>\n",
       "      <td>0.476955</td>\n",
       "      <td>0.378772</td>\n",
       "      <td>0.375503</td>\n",
       "      <td>0.852288</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.846358</td>\n",
       "      <td>0.845334</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>0.472170</td>\n",
       "      <td>0.368566</td>\n",
       "      <td>0.367264</td>\n",
       "      <td>0.856288</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>0.853689</td>\n",
       "      <td>0.861308</td>\n",
       "      <td>0.036905</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>0.479234</td>\n",
       "      <td>0.471233</td>\n",
       "      <td>0.376381</td>\n",
       "      <td>0.372756</td>\n",
       "      <td>0.848352</td>\n",
       "      <td>0.8608</td>\n",
       "      <td>0.840693</td>\n",
       "      <td>0.844355</td>\n",
       "      <td>0.033396</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>0.633102</td>\n",
       "      <td>10.818193</td>\n",
       "      <td>0.686627</td>\n",
       "      <td>0.311636</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.612476</td>\n",
       "      <td>1.938406</td>\n",
       "      <td>0.743017</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.637858</td>\n",
       "      <td>2.884254</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.757829</td>\n",
       "      <td>1.139937</td>\n",
       "      <td>0.531552</td>\n",
       "      <td>0.592058</td>\n",
       "      <td>0.058041</td>\n",
       "      <td>0.604942</td>\n",
       "      <td>0.513529</td>\n",
       "      <td>0.882682</td>\n",
       "      <td>0.859060</td>\n",
       "      <td>0.060057</td>\n",
       "      <td>0.656525</td>\n",
       "      <td>0.656106</td>\n",
       "      <td>0.709459</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.065024</td>\n",
       "      <td>0.684232</td>\n",
       "      <td>0.784067</td>\n",
       "      <td>0.541225</td>\n",
       "      <td>0.620812</td>\n",
       "      <td>0.781147</td>\n",
       "      <td>0.562121</td>\n",
       "      <td>0.436157</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>0.686671</td>\n",
       "      <td>0.631208</td>\n",
       "      <td>0.485480</td>\n",
       "      <td>0.871622</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.770864</td>\n",
       "      <td>0.686935</td>\n",
       "      <td>0.807044</td>\n",
       "      <td>0.557807</td>\n",
       "      <td>0.628866</td>\n",
       "      <td>10.499663</td>\n",
       "      <td>0.562387</td>\n",
       "      <td>0.422405</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>9.653604</td>\n",
       "      <td>0.644520</td>\n",
       "      <td>0.540236</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>10.151225</td>\n",
       "      <td>0.492986</td>\n",
       "      <td>0.175076</td>\n",
       "      <td>0.943498</td>\n",
       "      <td>0.924990</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>0.493192</td>\n",
       "      <td>0.444215</td>\n",
       "      <td>0.938547</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.602736</td>\n",
       "      <td>0.614574</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502905</td>\n",
       "      <td>0.502870</td>\n",
       "      <td>0.377181</td>\n",
       "      <td>0.381723</td>\n",
       "      <td>0.831776</td>\n",
       "      <td>0.8336</td>\n",
       "      <td>0.823214</td>\n",
       "      <td>0.826249</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.494646</td>\n",
       "      <td>0.498007</td>\n",
       "      <td>0.365627</td>\n",
       "      <td>0.375666</td>\n",
       "      <td>0.838112</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.821327</td>\n",
       "      <td>0.828912</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.506019</td>\n",
       "      <td>0.516472</td>\n",
       "      <td>0.386779</td>\n",
       "      <td>0.406075</td>\n",
       "      <td>0.825776</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>0.813966</td>\n",
       "      <td>0.816357</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>0.017498</td>\n",
       "      <td>0.606532</td>\n",
       "      <td>1.194771</td>\n",
       "      <td>0.722094</td>\n",
       "      <td>0.473531</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.598812</td>\n",
       "      <td>2.277555</td>\n",
       "      <td>0.843575</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.617050</td>\n",
       "      <td>0.507695</td>\n",
       "      <td>0.871622</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.773612</td>\n",
       "      <td>1.059174</td>\n",
       "      <td>0.455550</td>\n",
       "      <td>0.582824</td>\n",
       "      <td>0.032153</td>\n",
       "      <td>0.573358</td>\n",
       "      <td>0.453806</td>\n",
       "      <td>0.849162</td>\n",
       "      <td>0.825806</td>\n",
       "      <td>0.036592</td>\n",
       "      <td>0.624068</td>\n",
       "      <td>0.497640</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.027850</td>\n",
       "      <td>0.764616</td>\n",
       "      <td>1.034835</td>\n",
       "      <td>0.484723</td>\n",
       "      <td>0.595273</td>\n",
       "      <td>0.414240</td>\n",
       "      <td>0.631198</td>\n",
       "      <td>0.627517</td>\n",
       "      <td>0.692737</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.473240</td>\n",
       "      <td>0.627679</td>\n",
       "      <td>0.522701</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.320349</td>\n",
       "      <td>0.748558</td>\n",
       "      <td>0.977142</td>\n",
       "      <td>0.489483</td>\n",
       "      <td>0.597019</td>\n",
       "      <td>6.422559</td>\n",
       "      <td>0.633388</td>\n",
       "      <td>0.638453</td>\n",
       "      <td>0.692737</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>6.648989</td>\n",
       "      <td>0.627118</td>\n",
       "      <td>0.524895</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>4.305763</td>\n",
       "      <td>0.509151</td>\n",
       "      <td>0.241589</td>\n",
       "      <td>0.902349</td>\n",
       "      <td>0.882222</td>\n",
       "      <td>0.043410</td>\n",
       "      <td>0.501347</td>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.606344</td>\n",
       "      <td>0.258774</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.001130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vanilla1_dt_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.474580</td>\n",
       "      <td>0.473859</td>\n",
       "      <td>0.327691</td>\n",
       "      <td>0.331907</td>\n",
       "      <td>0.857424</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.850090</td>\n",
       "      <td>0.857306</td>\n",
       "      <td>0.021769</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.467580</td>\n",
       "      <td>0.470944</td>\n",
       "      <td>0.320949</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.862608</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>0.849012</td>\n",
       "      <td>0.852081</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.480201</td>\n",
       "      <td>0.490897</td>\n",
       "      <td>0.343678</td>\n",
       "      <td>0.353273</td>\n",
       "      <td>0.847696</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>0.839975</td>\n",
       "      <td>0.838571</td>\n",
       "      <td>0.021356</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.646097</td>\n",
       "      <td>2.083472</td>\n",
       "      <td>0.698142</td>\n",
       "      <td>0.400975</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.878277</td>\n",
       "      <td>14.285952</td>\n",
       "      <td>0.357542</td>\n",
       "      <td>0.371585</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>1.211867</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.699088</td>\n",
       "      <td>0.740372</td>\n",
       "      <td>0.541379</td>\n",
       "      <td>0.616412</td>\n",
       "      <td>0.046304</td>\n",
       "      <td>0.585473</td>\n",
       "      <td>0.513521</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.634266</td>\n",
       "      <td>0.494213</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.027859</td>\n",
       "      <td>0.692830</td>\n",
       "      <td>0.721504</td>\n",
       "      <td>0.550438</td>\n",
       "      <td>0.623069</td>\n",
       "      <td>0.559095</td>\n",
       "      <td>0.636763</td>\n",
       "      <td>0.703895</td>\n",
       "      <td>0.743017</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.554929</td>\n",
       "      <td>0.640689</td>\n",
       "      <td>0.523307</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.363112</td>\n",
       "      <td>0.675718</td>\n",
       "      <td>0.675149</td>\n",
       "      <td>0.591279</td>\n",
       "      <td>0.637428</td>\n",
       "      <td>7.925493</td>\n",
       "      <td>0.635056</td>\n",
       "      <td>0.668259</td>\n",
       "      <td>0.731844</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>7.662928</td>\n",
       "      <td>0.640127</td>\n",
       "      <td>0.549056</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>4.915602</td>\n",
       "      <td>0.499393</td>\n",
       "      <td>0.185063</td>\n",
       "      <td>0.934899</td>\n",
       "      <td>0.914859</td>\n",
       "      <td>0.050534</td>\n",
       "      <td>0.502917</td>\n",
       "      <td>0.161456</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.603373</td>\n",
       "      <td>0.403781</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.001603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          scores_type  function_family_maximum_depth  \\\n",
       "0  vanilla1_dt_scores                              3   \n",
       "1  vanilla1_dt_scores                              4   \n",
       "2  vanilla1_dt_scores                              5   \n",
       "3  vanilla1_dt_scores                              3   \n",
       "4  vanilla1_dt_scores                              4   \n",
       "\n",
       "   function_family_decision_sparsity function_family_dt_type  \\\n",
       "0                                  1                 vanilla   \n",
       "1                                  1                 vanilla   \n",
       "2                                  1                 vanilla   \n",
       "3                                  1                 vanilla   \n",
       "4                                  1                 vanilla   \n",
       "\n",
       "  data_dt_type_train  data_maximum_depth_train  data_number_of_variables  \\\n",
       "0            vanilla                         5                         9   \n",
       "1            vanilla                         5                         9   \n",
       "2            vanilla                         5                         9   \n",
       "3            vanilla                         5                         9   \n",
       "4            vanilla                         5                         9   \n",
       "\n",
       "   data_noise_injected_level data_function_generation_type  \\\n",
       "0                          0   make_classification_trained   \n",
       "1                          0   make_classification_trained   \n",
       "2                          0   make_classification_trained   \n",
       "3                          0   make_classification_trained   \n",
       "4                          0   make_classification_trained   \n",
       "\n",
       "  data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "0                       []                            [128]   \n",
       "1                       []                            [128]   \n",
       "2                       []                            [128]   \n",
       "3             [0, 1, 2, 3]                            [128]   \n",
       "4             [0, 1, 2, 3]                            [128]   \n",
       "\n",
       "  lambda_net_optimizer_lambda             i_net_dense_layers  \\\n",
       "0                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "1                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "2                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "3                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "4                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "\n",
       "               i_net_dropout  i_net_learning_rate           i_net_loss  \\\n",
       "0  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "1  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "2  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "3  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "4  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "\n",
       "   i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "0                              10000                                   3   \n",
       "1                              10000                                   3   \n",
       "2                              10000                                   3   \n",
       "3                              10000                                   3   \n",
       "4                              10000                                   3   \n",
       "\n",
       "  i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "0                       None      False               100   \n",
       "1                       None      False               100   \n",
       "2                       None      False               100   \n",
       "3                       None      False               100   \n",
       "4                       None      False               100   \n",
       "\n",
       "  evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                make_classification                    \n",
       "1                                make_classification                    \n",
       "2                                make_classification                    \n",
       "3                                make_classification                    \n",
       "4                                make_classification                    \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                  0                 \n",
       "1                                                  0                 \n",
       "2                                                  0                 \n",
       "3                                                  0                 \n",
       "4                                                  0                 \n",
       "\n",
       "   train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "0                        0.519338                               0.521510   \n",
       "1                        0.495972                               0.495769   \n",
       "2                        0.477639                               0.476955   \n",
       "3                        0.502905                               0.502870   \n",
       "4                        0.474580                               0.473859   \n",
       "\n",
       "   train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "0                   0.415607                          0.424973   \n",
       "1                   0.380874                          0.383410   \n",
       "2                   0.378772                          0.375503   \n",
       "3                   0.377181                          0.381723   \n",
       "4                   0.327691                          0.331907   \n",
       "\n",
       "   train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "0        0.812544                 0.8076        0.804334   \n",
       "1        0.833472                 0.8328        0.826597   \n",
       "2        0.852288                 0.8500        0.846358   \n",
       "3        0.831776                 0.8336        0.823214   \n",
       "4        0.857424                 0.8632        0.850090   \n",
       "\n",
       "   train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "0               0.803337       0.019770              0.019261   \n",
       "1               0.829024       0.029923              0.028576   \n",
       "2               0.845334       0.038152              0.037787   \n",
       "3               0.826249       0.018598              0.017711   \n",
       "4               0.857306       0.021769              0.021693   \n",
       "\n",
       "   valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "0                        0.515391                               0.512567   \n",
       "1                        0.492361                               0.490719   \n",
       "2                        0.473995                               0.472170   \n",
       "3                        0.494646                               0.498007   \n",
       "4                        0.467580                               0.470944   \n",
       "\n",
       "   valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "0                   0.408172                          0.409819   \n",
       "1                   0.367409                          0.366267   \n",
       "2                   0.368566                          0.367264   \n",
       "3                   0.365627                          0.375666   \n",
       "4                   0.320949                          0.327869   \n",
       "\n",
       "   valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "0        0.818256                 0.8260        0.814105   \n",
       "1        0.837136                 0.8392        0.834163   \n",
       "2        0.856288                 0.8572        0.853689   \n",
       "3        0.838112                 0.8364        0.821327   \n",
       "4        0.862608                 0.8588        0.849012   \n",
       "\n",
       "   valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "0               0.817887       0.021446              0.020898   \n",
       "1               0.834104       0.029598              0.027170   \n",
       "2               0.861308       0.036905              0.036879   \n",
       "3               0.828912       0.018182              0.017470   \n",
       "4               0.852081       0.021462              0.021297   \n",
       "\n",
       "   test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "0                       0.523347                              0.524943   \n",
       "1                       0.498293                              0.496365   \n",
       "2                       0.479234                              0.471233   \n",
       "3                       0.506019                              0.516472   \n",
       "4                       0.480201                              0.490897   \n",
       "\n",
       "   test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "0                  0.421876                         0.427665       0.807616   \n",
       "1                  0.382379                         0.378958       0.831040   \n",
       "2                  0.376381                         0.372756       0.848352   \n",
       "3                  0.386779                         0.406075       0.825776   \n",
       "4                  0.343678                         0.353273       0.847696   \n",
       "\n",
       "   test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "0                0.8064       0.795271              0.812604      0.021521   \n",
       "1                0.8388       0.819418              0.827358      0.027813   \n",
       "2                0.8608       0.840693              0.844355      0.033396   \n",
       "3                0.8212       0.813966              0.816357      0.018056   \n",
       "4                0.8480       0.839975              0.838571      0.021356   \n",
       "\n",
       "   test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "0             0.021086                             0.676063   \n",
       "1             0.025573                             0.646430   \n",
       "2             0.031112                             0.633102   \n",
       "3             0.017498                             0.606532   \n",
       "4             0.021333                             0.646097   \n",
       "\n",
       "   binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "0                       10.156703             0.592354             0.313421   \n",
       "1                       10.300082             0.686627             0.312100   \n",
       "2                       10.818193             0.686627             0.311636   \n",
       "3                        1.194771             0.722094             0.473531   \n",
       "4                        2.083472             0.698142             0.400975   \n",
       "\n",
       "   runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "0            0.003428                               0.590181   \n",
       "1            0.003833                               0.628368   \n",
       "2            0.004539                               0.612476   \n",
       "3            0.003951                               0.598812   \n",
       "4            0.003832                               0.878277   \n",
       "\n",
       "   binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "0                          0.590905               0.765363   \n",
       "1                          0.670269               0.625698   \n",
       "2                          1.938406               0.743017   \n",
       "3                          2.277555               0.843575   \n",
       "4                         14.285952               0.357542   \n",
       "\n",
       "   f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "0               0.752941              0.003868   \n",
       "1               0.496241              0.004589   \n",
       "2               0.640625              0.005084   \n",
       "3               0.820513              0.003566   \n",
       "4               0.371585              0.004013   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                   0.624773   \n",
       "1                                   0.645147   \n",
       "2                                   0.637858   \n",
       "3                                   0.617050   \n",
       "4                                   0.621212   \n",
       "\n",
       "   binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "0                              0.472553                   0.851351   \n",
       "1                              0.556817                   0.729730   \n",
       "2                              2.884254                   0.777027   \n",
       "3                              0.507695                   0.871622   \n",
       "4                              1.211867                   0.851351   \n",
       "\n",
       "   f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "0                   0.710526                  0.003999   \n",
       "1                   0.607843                  0.009758   \n",
       "2                   0.666667                  0.004967   \n",
       "3                   0.759494                  0.002132   \n",
       "4                   0.710526                  0.003116   \n",
       "\n",
       "   soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "0                              0.824417                         1.386253   \n",
       "1                              0.765034                         0.982626   \n",
       "2                              0.757829                         1.139937   \n",
       "3                              0.773612                         1.059174   \n",
       "4                              0.699088                         0.740372   \n",
       "\n",
       "   accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "0              0.429295              0.572218             0.037807   \n",
       "1              0.466605              0.586527             0.059062   \n",
       "2              0.531552              0.592058             0.058041   \n",
       "3              0.455550              0.582824             0.032153   \n",
       "4              0.541379              0.616412             0.046304   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "0                                0.556268                           0.363571   \n",
       "1                                0.558729                           0.361183   \n",
       "2                                0.604942                           0.513529   \n",
       "3                                0.573358                           0.453806   \n",
       "4                                0.585473                           0.513521   \n",
       "\n",
       "   accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "0                0.882682                0.857143               0.036790   \n",
       "1                0.815642                0.744186               0.048559   \n",
       "2                0.882682                0.859060               0.060057   \n",
       "3                0.849162                0.825806               0.036592   \n",
       "4                0.731844                0.578947               0.034735   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                    0.636781   \n",
       "1                                    0.645694   \n",
       "2                                    0.656525   \n",
       "3                                    0.624068   \n",
       "4                                    0.634266   \n",
       "\n",
       "   binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "0                               0.523597                    0.851351   \n",
       "1                               0.624696                    0.716216   \n",
       "2                               0.656106                    0.709459   \n",
       "3                               0.497640                    0.851351   \n",
       "4                               0.494213                    0.689189   \n",
       "\n",
       "   f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "0                    0.710526                   0.039880   \n",
       "1                    0.596154                   0.085963   \n",
       "2                    0.605505                   0.065024   \n",
       "3                    0.710526                   0.027850   \n",
       "4                    0.540000                   0.027859   \n",
       "\n",
       "   soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "0                               0.814428                          1.424239   \n",
       "1                               0.745745                          0.958959   \n",
       "2                               0.684232                          0.784067   \n",
       "3                               0.764616                          1.034835   \n",
       "4                               0.692830                          0.721504   \n",
       "\n",
       "   accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "0               0.477353               0.592726              0.470477   \n",
       "1               0.544603               0.619646              0.707219   \n",
       "2               0.541225               0.620812              0.781147   \n",
       "3               0.484723               0.595273              0.414240   \n",
       "4               0.550438               0.623069              0.559095   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_100000  \\\n",
       "0                                 0.563472   \n",
       "1                                 0.570864   \n",
       "2                                 0.562121   \n",
       "3                                 0.631198   \n",
       "4                                 0.636763   \n",
       "\n",
       "   binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "0                            0.380740                 0.877095   \n",
       "1                            0.453769                 0.737430   \n",
       "2                            0.436157                 0.871508   \n",
       "3                            0.627517                 0.692737   \n",
       "4                            0.703895                 0.743017   \n",
       "\n",
       "   f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "0                 0.851351                0.457628   \n",
       "1                 0.675862                0.780087   \n",
       "2                 0.834532                0.686671   \n",
       "3                 0.545455                0.473240   \n",
       "4                 0.589286                0.554929   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                     0.621059   \n",
       "1                                     0.631943   \n",
       "2                                     0.631208   \n",
       "3                                     0.627679   \n",
       "4                                     0.640689   \n",
       "\n",
       "   binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "0                                0.444783                     0.891892   \n",
       "1                                0.517273                     0.641892   \n",
       "2                                0.485480                     0.871622   \n",
       "3                                0.522701                     0.851351   \n",
       "4                                0.523307                     0.689189   \n",
       "\n",
       "   f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "0                     0.771429                    0.491968   \n",
       "1                     0.561983                    0.628368   \n",
       "2                     0.746667                    0.770864   \n",
       "3                     0.710526                    0.320349   \n",
       "4                     0.540000                    0.363112   \n",
       "\n",
       "   soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "0                                0.831044                           1.510337   \n",
       "1                                0.751342                           0.965319   \n",
       "2                                0.686935                           0.807044   \n",
       "3                                0.748558                           0.977142   \n",
       "4                                0.675718                           0.675149   \n",
       "\n",
       "   accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "0                0.419929                0.568623               6.078736   \n",
       "1                0.560571                0.625980               8.948903   \n",
       "2                0.557807                0.628866              10.499663   \n",
       "3                0.489483                0.597019               6.422559   \n",
       "4                0.591279                0.637428               7.925493   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_1000000  \\\n",
       "0                                  0.563204   \n",
       "1                                  0.564544   \n",
       "2                                  0.562387   \n",
       "3                                  0.633388   \n",
       "4                                  0.635056   \n",
       "\n",
       "   binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "0                             0.379736                  0.877095   \n",
       "1                             0.411363                  0.737430   \n",
       "2                             0.422405                  0.871508   \n",
       "3                             0.638453                  0.692737   \n",
       "4                             0.668259                  0.731844   \n",
       "\n",
       "   f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "0                  0.851351                 6.039980   \n",
       "1                  0.675862                 8.988415   \n",
       "2                  0.834532                 9.653604   \n",
       "3                  0.545455                 6.648989   \n",
       "4                  0.578947                 7.662928   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                      0.631333   \n",
       "1                                      0.635391   \n",
       "2                                      0.644520   \n",
       "3                                      0.627118   \n",
       "4                                      0.640127   \n",
       "\n",
       "   binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "0                                 0.484076                      0.851351   \n",
       "1                                 0.511929                      0.608108   \n",
       "2                                 0.540236                      0.702703   \n",
       "3                                 0.524895                      0.851351   \n",
       "4                                 0.549056                      0.689189   \n",
       "\n",
       "   f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "0                      0.710526                     6.398765   \n",
       "1                      0.408163                     8.718359   \n",
       "2                      0.531915                    10.151225   \n",
       "3                      0.710526                     4.305763   \n",
       "4                      0.540000                     4.915602   \n",
       "\n",
       "   soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                   0.509151   \n",
       "1                                   0.499393   \n",
       "2                                   0.492986   \n",
       "3                                   0.509151   \n",
       "4                                   0.499393   \n",
       "\n",
       "   binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "0                              0.241589                   0.902349   \n",
       "1                              0.185063                   0.934899   \n",
       "2                              0.175076                   0.943498   \n",
       "3                              0.241589                   0.902349   \n",
       "4                              0.185063                   0.934899   \n",
       "\n",
       "   f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "0                   0.882222                  0.026223   \n",
       "1                   0.914859                  0.035989   \n",
       "2                   0.924990                  0.043957   \n",
       "3                   0.882222                  0.043410   \n",
       "4                   0.914859                  0.050534   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                     0.501347   \n",
       "1                                     0.502917   \n",
       "2                                     0.493192   \n",
       "3                                     0.501347   \n",
       "4                                     0.502917   \n",
       "\n",
       "   binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "0                                0.176710                     0.960894   \n",
       "1                                0.161456                     0.960894   \n",
       "2                                0.444215                     0.938547   \n",
       "3                                0.176710                     0.960894   \n",
       "4                                0.161456                     0.960894   \n",
       "\n",
       "   f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "0                     0.947368                    0.000762   \n",
       "1                     0.947368                    0.000928   \n",
       "2                     0.910569                    0.001271   \n",
       "3                     0.947368                    0.001347   \n",
       "4                     0.947368                    0.001466   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                         0.606344   \n",
       "1                                         0.603373   \n",
       "2                                         0.602736   \n",
       "3                                         0.606344   \n",
       "4                                         0.603373   \n",
       "\n",
       "   binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                    0.258774   \n",
       "1                                    0.403781   \n",
       "2                                    0.614574   \n",
       "3                                    0.258774   \n",
       "4                                    0.403781   \n",
       "\n",
       "   accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                         0.918919                         0.828571   \n",
       "1                         0.945946                         0.891892   \n",
       "2                         0.945946                         0.891892   \n",
       "3                         0.918919                         0.828571   \n",
       "4                         0.945946                         0.891892   \n",
       "\n",
       "   runtime_absenteeism_TRAIN_DATA  \n",
       "0                        0.000852  \n",
       "1                        0.000976  \n",
       "2                        0.001135  \n",
       "3                        0.001130  \n",
       "4                        0.001603  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_dt_distilled = []\n",
    "for column in results_summary_columns:\n",
    "    if 'dt_scores' in column:\n",
    "        if 'data_random' not in column:\n",
    "            columns_dt_distilled.append(column)\n",
    "results_summary_dt_distilled = results_summary[flatten([colmuns_identifier, columns_dt_distilled])]\n",
    "\n",
    "columns_dt_distilled_rename = []\n",
    "for column in columns_dt_distilled:\n",
    "    column = column.replace('dt_scores_','')\n",
    "    columns_dt_distilled_rename.append(column)\n",
    "\n",
    "results_summary_dt_distilled.columns = flatten([colmuns_identifier, columns_dt_distilled_rename])\n",
    "    \n",
    "#results_summary_dt_distilled.insert(0, 'scores_type', 'dt_scores')\n",
    "results_summary_dt_distilled.insert(0, 'scores_type', [dt_type + str(decision_sparsity) + '_dt_scores' for dt_type, decision_sparsity in zip(results_summary_dt_distilled['function_family_dt_type'].values, results_summary_dt_distilled['function_family_decision_sparsity'].values)])\n",
    "\n",
    "    \n",
    "print(results_summary_dt_distilled.shape)\n",
    "results_summary_dt_distilled.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9f51f3-f754-4dc2-9ceb-0b7113b51911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.422643Z",
     "iopub.status.busy": "2022-01-04T19:49:56.422503Z",
     "iopub.status.idle": "2022-01-04T19:49:56.506829Z",
     "shell.execute_reply": "2022-01-04T19:49:56.505618Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.422626Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.513757</td>\n",
       "      <td>0.516722</td>\n",
       "      <td>0.395052</td>\n",
       "      <td>0.407307</td>\n",
       "      <td>0.823310</td>\n",
       "      <td>0.8212</td>\n",
       "      <td>0.814989</td>\n",
       "      <td>0.819453</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.019261</td>\n",
       "      <td>0.510315</td>\n",
       "      <td>0.509515</td>\n",
       "      <td>0.389243</td>\n",
       "      <td>0.390487</td>\n",
       "      <td>0.827992</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.824589</td>\n",
       "      <td>0.829977</td>\n",
       "      <td>0.021446</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.513713</td>\n",
       "      <td>0.403941</td>\n",
       "      <td>0.401487</td>\n",
       "      <td>0.816672</td>\n",
       "      <td>0.8191</td>\n",
       "      <td>0.804644</td>\n",
       "      <td>0.816798</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>0.021086</td>\n",
       "      <td>0.333621</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.996441</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.591582</td>\n",
       "      <td>0.434295</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.435884</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.339943</td>\n",
       "      <td>0.045065</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.991844</td>\n",
       "      <td>0.037807</td>\n",
       "      <td>0.597483</td>\n",
       "      <td>0.473530</td>\n",
       "      <td>0.7838</td>\n",
       "      <td>0.805050</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>0.440972</td>\n",
       "      <td>0.179272</td>\n",
       "      <td>0.9243</td>\n",
       "      <td>0.698286</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>0.341439</td>\n",
       "      <td>0.049010</td>\n",
       "      <td>0.98186</td>\n",
       "      <td>0.990746</td>\n",
       "      <td>0.470477</td>\n",
       "      <td>0.603581</td>\n",
       "      <td>0.492859</td>\n",
       "      <td>0.76884</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.457628</td>\n",
       "      <td>0.443937</td>\n",
       "      <td>0.188583</td>\n",
       "      <td>0.92095</td>\n",
       "      <td>0.680516</td>\n",
       "      <td>0.491968</td>\n",
       "      <td>0.341260</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.981878</td>\n",
       "      <td>0.990764</td>\n",
       "      <td>6.078736</td>\n",
       "      <td>0.604133</td>\n",
       "      <td>0.494743</td>\n",
       "      <td>0.766630</td>\n",
       "      <td>0.785663</td>\n",
       "      <td>6.039980</td>\n",
       "      <td>0.443815</td>\n",
       "      <td>0.188630</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.682028</td>\n",
       "      <td>6.398765</td>\n",
       "      <td>0.514619</td>\n",
       "      <td>0.232145</td>\n",
       "      <td>0.913053</td>\n",
       "      <td>0.924979</td>\n",
       "      <td>0.026223</td>\n",
       "      <td>0.501361</td>\n",
       "      <td>0.114649</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.605437</td>\n",
       "      <td>0.275451</td>\n",
       "      <td>0.915433</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.486865</td>\n",
       "      <td>0.485136</td>\n",
       "      <td>0.339374</td>\n",
       "      <td>0.339764</td>\n",
       "      <td>0.850728</td>\n",
       "      <td>0.8509</td>\n",
       "      <td>0.844733</td>\n",
       "      <td>0.847432</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>0.028576</td>\n",
       "      <td>0.484306</td>\n",
       "      <td>0.479981</td>\n",
       "      <td>0.333593</td>\n",
       "      <td>0.334073</td>\n",
       "      <td>0.852082</td>\n",
       "      <td>0.8547</td>\n",
       "      <td>0.849903</td>\n",
       "      <td>0.849988</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.027170</td>\n",
       "      <td>0.489628</td>\n",
       "      <td>0.484901</td>\n",
       "      <td>0.344514</td>\n",
       "      <td>0.342052</td>\n",
       "      <td>0.847064</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.836161</td>\n",
       "      <td>0.844203</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>0.332021</td>\n",
       "      <td>0.011146</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.998470</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.574516</td>\n",
       "      <td>0.364126</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.430846</td>\n",
       "      <td>0.120251</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.820717</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.336949</td>\n",
       "      <td>0.033669</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.993710</td>\n",
       "      <td>0.059062</td>\n",
       "      <td>0.580193</td>\n",
       "      <td>0.421473</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.837691</td>\n",
       "      <td>0.048559</td>\n",
       "      <td>0.435666</td>\n",
       "      <td>0.157809</td>\n",
       "      <td>0.9349</td>\n",
       "      <td>0.737817</td>\n",
       "      <td>0.085963</td>\n",
       "      <td>0.338689</td>\n",
       "      <td>0.039931</td>\n",
       "      <td>0.98464</td>\n",
       "      <td>0.992122</td>\n",
       "      <td>0.707219</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.440922</td>\n",
       "      <td>0.79334</td>\n",
       "      <td>0.815627</td>\n",
       "      <td>0.780087</td>\n",
       "      <td>0.438652</td>\n",
       "      <td>0.167041</td>\n",
       "      <td>0.93158</td>\n",
       "      <td>0.740085</td>\n",
       "      <td>0.628368</td>\n",
       "      <td>0.338606</td>\n",
       "      <td>0.040026</td>\n",
       "      <td>0.983952</td>\n",
       "      <td>0.991759</td>\n",
       "      <td>8.948903</td>\n",
       "      <td>0.585896</td>\n",
       "      <td>0.443391</td>\n",
       "      <td>0.792173</td>\n",
       "      <td>0.814393</td>\n",
       "      <td>8.988415</td>\n",
       "      <td>0.438672</td>\n",
       "      <td>0.167892</td>\n",
       "      <td>0.928820</td>\n",
       "      <td>0.727265</td>\n",
       "      <td>8.718359</td>\n",
       "      <td>0.504802</td>\n",
       "      <td>0.176034</td>\n",
       "      <td>0.940567</td>\n",
       "      <td>0.945923</td>\n",
       "      <td>0.035989</td>\n",
       "      <td>0.501101</td>\n",
       "      <td>0.084286</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.602921</td>\n",
       "      <td>0.195279</td>\n",
       "      <td>0.942918</td>\n",
       "      <td>0.882096</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.463469</td>\n",
       "      <td>0.459092</td>\n",
       "      <td>0.286219</td>\n",
       "      <td>0.284103</td>\n",
       "      <td>0.878014</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>0.873359</td>\n",
       "      <td>0.877471</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>0.037787</td>\n",
       "      <td>0.460330</td>\n",
       "      <td>0.455737</td>\n",
       "      <td>0.278693</td>\n",
       "      <td>0.274248</td>\n",
       "      <td>0.881550</td>\n",
       "      <td>0.8848</td>\n",
       "      <td>0.879857</td>\n",
       "      <td>0.883553</td>\n",
       "      <td>0.036905</td>\n",
       "      <td>0.036879</td>\n",
       "      <td>0.464623</td>\n",
       "      <td>0.459743</td>\n",
       "      <td>0.288202</td>\n",
       "      <td>0.280073</td>\n",
       "      <td>0.876036</td>\n",
       "      <td>0.8822</td>\n",
       "      <td>0.869102</td>\n",
       "      <td>0.877708</td>\n",
       "      <td>0.033396</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>0.330991</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.998980</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.555402</td>\n",
       "      <td>0.265439</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.900356</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.425028</td>\n",
       "      <td>0.071208</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.334709</td>\n",
       "      <td>0.023555</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>0.995287</td>\n",
       "      <td>0.058041</td>\n",
       "      <td>0.566556</td>\n",
       "      <td>0.374040</td>\n",
       "      <td>0.8295</td>\n",
       "      <td>0.849952</td>\n",
       "      <td>0.060057</td>\n",
       "      <td>0.432403</td>\n",
       "      <td>0.138408</td>\n",
       "      <td>0.9396</td>\n",
       "      <td>0.765710</td>\n",
       "      <td>0.065024</td>\n",
       "      <td>0.336940</td>\n",
       "      <td>0.033537</td>\n",
       "      <td>0.98651</td>\n",
       "      <td>0.993110</td>\n",
       "      <td>0.781147</td>\n",
       "      <td>0.573012</td>\n",
       "      <td>0.401912</td>\n",
       "      <td>0.81728</td>\n",
       "      <td>0.830827</td>\n",
       "      <td>0.686671</td>\n",
       "      <td>0.435106</td>\n",
       "      <td>0.150556</td>\n",
       "      <td>0.93382</td>\n",
       "      <td>0.739346</td>\n",
       "      <td>0.770864</td>\n",
       "      <td>0.337091</td>\n",
       "      <td>0.034885</td>\n",
       "      <td>0.986639</td>\n",
       "      <td>0.993170</td>\n",
       "      <td>10.499663</td>\n",
       "      <td>0.573653</td>\n",
       "      <td>0.405561</td>\n",
       "      <td>0.814067</td>\n",
       "      <td>0.828896</td>\n",
       "      <td>9.653604</td>\n",
       "      <td>0.435362</td>\n",
       "      <td>0.152760</td>\n",
       "      <td>0.934484</td>\n",
       "      <td>0.733373</td>\n",
       "      <td>10.151225</td>\n",
       "      <td>0.498856</td>\n",
       "      <td>0.135784</td>\n",
       "      <td>0.948713</td>\n",
       "      <td>0.953017</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>0.493345</td>\n",
       "      <td>0.039162</td>\n",
       "      <td>0.977153</td>\n",
       "      <td>0.962536</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.593692</td>\n",
       "      <td>0.132693</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500508</td>\n",
       "      <td>0.501946</td>\n",
       "      <td>0.364557</td>\n",
       "      <td>0.373574</td>\n",
       "      <td>0.836462</td>\n",
       "      <td>0.8342</td>\n",
       "      <td>0.827982</td>\n",
       "      <td>0.831399</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.017711</td>\n",
       "      <td>0.490680</td>\n",
       "      <td>0.494132</td>\n",
       "      <td>0.349626</td>\n",
       "      <td>0.364636</td>\n",
       "      <td>0.844900</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>0.828311</td>\n",
       "      <td>0.835338</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.500962</td>\n",
       "      <td>0.511956</td>\n",
       "      <td>0.367982</td>\n",
       "      <td>0.392357</td>\n",
       "      <td>0.834160</td>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.822841</td>\n",
       "      <td>0.824787</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>0.017498</td>\n",
       "      <td>0.343930</td>\n",
       "      <td>0.029702</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.993296</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.507022</td>\n",
       "      <td>0.270955</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.891871</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.426472</td>\n",
       "      <td>0.139921</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.897881</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.353635</td>\n",
       "      <td>0.069665</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.984545</td>\n",
       "      <td>0.032153</td>\n",
       "      <td>0.512211</td>\n",
       "      <td>0.299919</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.884096</td>\n",
       "      <td>0.036592</td>\n",
       "      <td>0.428355</td>\n",
       "      <td>0.139317</td>\n",
       "      <td>0.9466</td>\n",
       "      <td>0.899397</td>\n",
       "      <td>0.027850</td>\n",
       "      <td>0.355650</td>\n",
       "      <td>0.076282</td>\n",
       "      <td>0.96746</td>\n",
       "      <td>0.983126</td>\n",
       "      <td>0.414240</td>\n",
       "      <td>0.513346</td>\n",
       "      <td>0.304766</td>\n",
       "      <td>0.86083</td>\n",
       "      <td>0.882651</td>\n",
       "      <td>0.473240</td>\n",
       "      <td>0.430359</td>\n",
       "      <td>0.138477</td>\n",
       "      <td>0.94427</td>\n",
       "      <td>0.896951</td>\n",
       "      <td>0.320349</td>\n",
       "      <td>0.355464</td>\n",
       "      <td>0.077190</td>\n",
       "      <td>0.966564</td>\n",
       "      <td>0.982700</td>\n",
       "      <td>6.422559</td>\n",
       "      <td>0.514233</td>\n",
       "      <td>0.308172</td>\n",
       "      <td>0.860024</td>\n",
       "      <td>0.881758</td>\n",
       "      <td>6.648989</td>\n",
       "      <td>0.430960</td>\n",
       "      <td>0.140045</td>\n",
       "      <td>0.944159</td>\n",
       "      <td>0.896894</td>\n",
       "      <td>4.305763</td>\n",
       "      <td>0.514619</td>\n",
       "      <td>0.232145</td>\n",
       "      <td>0.913053</td>\n",
       "      <td>0.924979</td>\n",
       "      <td>0.043410</td>\n",
       "      <td>0.501361</td>\n",
       "      <td>0.114649</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.605437</td>\n",
       "      <td>0.275451</td>\n",
       "      <td>0.915433</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.001130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vanilla1_dt_scores_data_random</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469407</td>\n",
       "      <td>0.470362</td>\n",
       "      <td>0.298243</td>\n",
       "      <td>0.296453</td>\n",
       "      <td>0.866946</td>\n",
       "      <td>0.8698</td>\n",
       "      <td>0.860619</td>\n",
       "      <td>0.870499</td>\n",
       "      <td>0.021769</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.461695</td>\n",
       "      <td>0.465275</td>\n",
       "      <td>0.286682</td>\n",
       "      <td>0.293382</td>\n",
       "      <td>0.872366</td>\n",
       "      <td>0.8708</td>\n",
       "      <td>0.859110</td>\n",
       "      <td>0.859674</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>0.472931</td>\n",
       "      <td>0.484267</td>\n",
       "      <td>0.307333</td>\n",
       "      <td>0.329922</td>\n",
       "      <td>0.860920</td>\n",
       "      <td>0.8532</td>\n",
       "      <td>0.853453</td>\n",
       "      <td>0.853609</td>\n",
       "      <td>0.021356</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.339806</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.997424</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>0.486462</td>\n",
       "      <td>0.203904</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.930519</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.424072</td>\n",
       "      <td>0.122480</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.908023</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.349824</td>\n",
       "      <td>0.057663</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.987396</td>\n",
       "      <td>0.046304</td>\n",
       "      <td>0.493887</td>\n",
       "      <td>0.249480</td>\n",
       "      <td>0.8851</td>\n",
       "      <td>0.892987</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.425474</td>\n",
       "      <td>0.121895</td>\n",
       "      <td>0.9471</td>\n",
       "      <td>0.908746</td>\n",
       "      <td>0.027859</td>\n",
       "      <td>0.351874</td>\n",
       "      <td>0.065143</td>\n",
       "      <td>0.97234</td>\n",
       "      <td>0.985547</td>\n",
       "      <td>0.559095</td>\n",
       "      <td>0.497750</td>\n",
       "      <td>0.261732</td>\n",
       "      <td>0.88517</td>\n",
       "      <td>0.892607</td>\n",
       "      <td>0.554929</td>\n",
       "      <td>0.427252</td>\n",
       "      <td>0.121175</td>\n",
       "      <td>0.94939</td>\n",
       "      <td>0.913842</td>\n",
       "      <td>0.363112</td>\n",
       "      <td>0.351683</td>\n",
       "      <td>0.066051</td>\n",
       "      <td>0.973342</td>\n",
       "      <td>0.986045</td>\n",
       "      <td>7.925493</td>\n",
       "      <td>0.498131</td>\n",
       "      <td>0.263208</td>\n",
       "      <td>0.886782</td>\n",
       "      <td>0.894634</td>\n",
       "      <td>7.662928</td>\n",
       "      <td>0.427879</td>\n",
       "      <td>0.123195</td>\n",
       "      <td>0.948687</td>\n",
       "      <td>0.912878</td>\n",
       "      <td>4.915602</td>\n",
       "      <td>0.504802</td>\n",
       "      <td>0.176034</td>\n",
       "      <td>0.940567</td>\n",
       "      <td>0.945923</td>\n",
       "      <td>0.050534</td>\n",
       "      <td>0.501101</td>\n",
       "      <td>0.084286</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.956284</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.602921</td>\n",
       "      <td>0.195279</td>\n",
       "      <td>0.942918</td>\n",
       "      <td>0.882096</td>\n",
       "      <td>0.001603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      scores_type  function_family_maximum_depth  \\\n",
       "0  vanilla1_dt_scores_data_random                              3   \n",
       "1  vanilla1_dt_scores_data_random                              4   \n",
       "2  vanilla1_dt_scores_data_random                              5   \n",
       "3  vanilla1_dt_scores_data_random                              3   \n",
       "4  vanilla1_dt_scores_data_random                              4   \n",
       "\n",
       "   function_family_decision_sparsity function_family_dt_type  \\\n",
       "0                                  1                 vanilla   \n",
       "1                                  1                 vanilla   \n",
       "2                                  1                 vanilla   \n",
       "3                                  1                 vanilla   \n",
       "4                                  1                 vanilla   \n",
       "\n",
       "  data_dt_type_train  data_maximum_depth_train  data_number_of_variables  \\\n",
       "0            vanilla                         5                         9   \n",
       "1            vanilla                         5                         9   \n",
       "2            vanilla                         5                         9   \n",
       "3            vanilla                         5                         9   \n",
       "4            vanilla                         5                         9   \n",
       "\n",
       "   data_noise_injected_level data_function_generation_type  \\\n",
       "0                          0   make_classification_trained   \n",
       "1                          0   make_classification_trained   \n",
       "2                          0   make_classification_trained   \n",
       "3                          0   make_classification_trained   \n",
       "4                          0   make_classification_trained   \n",
       "\n",
       "  data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "0                       []                            [128]   \n",
       "1                       []                            [128]   \n",
       "2                       []                            [128]   \n",
       "3             [0, 1, 2, 3]                            [128]   \n",
       "4             [0, 1, 2, 3]                            [128]   \n",
       "\n",
       "  lambda_net_optimizer_lambda             i_net_dense_layers  \\\n",
       "0                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "1                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "2                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "3                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "4                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "\n",
       "               i_net_dropout  i_net_learning_rate           i_net_loss  \\\n",
       "0  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "1  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "2  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "3  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "4  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "\n",
       "   i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "0                              10000                                   3   \n",
       "1                              10000                                   3   \n",
       "2                              10000                                   3   \n",
       "3                              10000                                   3   \n",
       "4                              10000                                   3   \n",
       "\n",
       "  i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "0                       None      False               100   \n",
       "1                       None      False               100   \n",
       "2                       None      False               100   \n",
       "3                       None      False               100   \n",
       "4                       None      False               100   \n",
       "\n",
       "  evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                make_classification                    \n",
       "1                                make_classification                    \n",
       "2                                make_classification                    \n",
       "3                                make_classification                    \n",
       "4                                make_classification                    \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                  0                 \n",
       "1                                                  0                 \n",
       "2                                                  0                 \n",
       "3                                                  0                 \n",
       "4                                                  0                 \n",
       "\n",
       "   train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "0                        0.513757                               0.516722   \n",
       "1                        0.486865                               0.485136   \n",
       "2                        0.463469                               0.459092   \n",
       "3                        0.500508                               0.501946   \n",
       "4                        0.469407                               0.470362   \n",
       "\n",
       "   train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "0                   0.395052                          0.407307   \n",
       "1                   0.339374                          0.339764   \n",
       "2                   0.286219                          0.284103   \n",
       "3                   0.364557                          0.373574   \n",
       "4                   0.298243                          0.296453   \n",
       "\n",
       "   train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "0        0.823310                 0.8212        0.814989   \n",
       "1        0.850728                 0.8509        0.844733   \n",
       "2        0.878014                 0.8800        0.873359   \n",
       "3        0.836462                 0.8342        0.827982   \n",
       "4        0.866946                 0.8698        0.860619   \n",
       "\n",
       "   train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "0               0.819453       0.019770              0.019261   \n",
       "1               0.847432       0.029923              0.028576   \n",
       "2               0.877471       0.038152              0.037787   \n",
       "3               0.831399       0.018598              0.017711   \n",
       "4               0.870499       0.021769              0.021693   \n",
       "\n",
       "   valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "0                        0.510315                               0.509515   \n",
       "1                        0.484306                               0.479981   \n",
       "2                        0.460330                               0.455737   \n",
       "3                        0.490680                               0.494132   \n",
       "4                        0.461695                               0.465275   \n",
       "\n",
       "   valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "0                   0.389243                          0.390487   \n",
       "1                   0.333593                          0.334073   \n",
       "2                   0.278693                          0.274248   \n",
       "3                   0.349626                          0.364636   \n",
       "4                   0.286682                          0.293382   \n",
       "\n",
       "   valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "0        0.827992                 0.8328        0.824589   \n",
       "1        0.852082                 0.8547        0.849903   \n",
       "2        0.881550                 0.8848        0.879857   \n",
       "3        0.844900                 0.8417        0.828311   \n",
       "4        0.872366                 0.8708        0.859110   \n",
       "\n",
       "   valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "0               0.829977       0.021446              0.020898   \n",
       "1               0.849988       0.029598              0.027170   \n",
       "2               0.883553       0.036905              0.036879   \n",
       "3               0.835338       0.018182              0.017470   \n",
       "4               0.859674       0.021462              0.021297   \n",
       "\n",
       "   test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "0                       0.518500                              0.513713   \n",
       "1                       0.489628                              0.484901   \n",
       "2                       0.464623                              0.459743   \n",
       "3                       0.500962                              0.511956   \n",
       "4                       0.472931                              0.484267   \n",
       "\n",
       "   test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "0                  0.403941                         0.401487       0.816672   \n",
       "1                  0.344514                         0.342052       0.847064   \n",
       "2                  0.288202                         0.280073       0.876036   \n",
       "3                  0.367982                         0.392357       0.834160   \n",
       "4                  0.307333                         0.329922       0.860920   \n",
       "\n",
       "   test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "0                0.8191       0.804644              0.816798      0.021521   \n",
       "1                0.8513       0.836161              0.844203      0.027813   \n",
       "2                0.8822       0.869102              0.877708      0.033396   \n",
       "3                0.8281       0.822841              0.824787      0.018056   \n",
       "4                0.8532       0.853453              0.853609      0.021356   \n",
       "\n",
       "   test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "0             0.021086                             0.333621   \n",
       "1             0.025573                             0.332021   \n",
       "2             0.031112                             0.330991   \n",
       "3             0.017498                             0.343930   \n",
       "4             0.021333                             0.339806   \n",
       "\n",
       "   binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "0                        0.023854                0.993             0.996441   \n",
       "1                        0.011146                0.997             0.998470   \n",
       "2                        0.004090                0.998             0.998980   \n",
       "3                        0.029702                0.987             0.993296   \n",
       "4                        0.010342                0.995             0.997424   \n",
       "\n",
       "   runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "0            0.003428                               0.591582   \n",
       "1            0.003833                               0.574516   \n",
       "2            0.004539                               0.555402   \n",
       "3            0.003951                               0.507022   \n",
       "4            0.003832                               0.486462   \n",
       "\n",
       "   binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "0                          0.434295                  0.802   \n",
       "1                          0.364126                  0.844   \n",
       "2                          0.265439                  0.888   \n",
       "3                          0.270955                  0.863   \n",
       "4                          0.203904                  0.921   \n",
       "\n",
       "   f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "0               0.819672              0.003868   \n",
       "1               0.864583              0.004589   \n",
       "2               0.900356              0.005084   \n",
       "3               0.891871              0.003566   \n",
       "4               0.930519              0.004013   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                   0.435884   \n",
       "1                                   0.430846   \n",
       "2                                   0.425028   \n",
       "3                                   0.426472   \n",
       "4                                   0.424072   \n",
       "\n",
       "   binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "0                              0.155781                      0.930   \n",
       "1                              0.120251                      0.955   \n",
       "2                              0.071208                      0.970   \n",
       "3                              0.139921                      0.947   \n",
       "4                              0.122480                      0.953   \n",
       "\n",
       "   f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "0                   0.738806                  0.003999   \n",
       "1                   0.820717                  0.009758   \n",
       "2                   0.880952                  0.004967   \n",
       "3                   0.897881                  0.002132   \n",
       "4                   0.908023                  0.003116   \n",
       "\n",
       "   soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "0                              0.339943                         0.045065   \n",
       "1                              0.336949                         0.033669   \n",
       "2                              0.334709                         0.023555   \n",
       "3                              0.353635                         0.069665   \n",
       "4                              0.349824                         0.057663   \n",
       "\n",
       "   accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "0                0.9840              0.991844             0.037807   \n",
       "1                0.9877              0.993710             0.059062   \n",
       "2                0.9908              0.995287             0.058041   \n",
       "3                0.9701              0.984545             0.032153   \n",
       "4                0.9759              0.987396             0.046304   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "0                                0.597483                           0.473530   \n",
       "1                                0.580193                           0.421473   \n",
       "2                                0.566556                           0.374040   \n",
       "3                                0.512211                           0.299919   \n",
       "4                                0.493887                           0.249480   \n",
       "\n",
       "   accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "0                  0.7838                0.805050               0.036790   \n",
       "1                  0.8057                0.837691               0.048559   \n",
       "2                  0.8295                0.849952               0.060057   \n",
       "3                  0.8619                0.884096               0.036592   \n",
       "4                  0.8851                0.892987               0.034735   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                    0.440972   \n",
       "1                                    0.435666   \n",
       "2                                    0.432403   \n",
       "3                                    0.428355   \n",
       "4                                    0.425474   \n",
       "\n",
       "   binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "0                               0.179272                      0.9243   \n",
       "1                               0.157809                      0.9349   \n",
       "2                               0.138408                      0.9396   \n",
       "3                               0.139317                      0.9466   \n",
       "4                               0.121895                      0.9471   \n",
       "\n",
       "   f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "0                    0.698286                   0.039880   \n",
       "1                    0.737817                   0.085963   \n",
       "2                    0.765710                   0.065024   \n",
       "3                    0.899397                   0.027850   \n",
       "4                    0.908746                   0.027859   \n",
       "\n",
       "   soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "0                               0.341439                          0.049010   \n",
       "1                               0.338689                          0.039931   \n",
       "2                               0.336940                          0.033537   \n",
       "3                               0.355650                          0.076282   \n",
       "4                               0.351874                          0.065143   \n",
       "\n",
       "   accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "0                0.98186               0.990746              0.470477   \n",
       "1                0.98464               0.992122              0.707219   \n",
       "2                0.98651               0.993110              0.781147   \n",
       "3                0.96746               0.983126              0.414240   \n",
       "4                0.97234               0.985547              0.559095   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_100000  \\\n",
       "0                                 0.603581   \n",
       "1                                 0.585135   \n",
       "2                                 0.573012   \n",
       "3                                 0.513346   \n",
       "4                                 0.497750   \n",
       "\n",
       "   binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "0                            0.492859                  0.76884   \n",
       "1                            0.440922                  0.79334   \n",
       "2                            0.401912                  0.81728   \n",
       "3                            0.304766                  0.86083   \n",
       "4                            0.261732                  0.88517   \n",
       "\n",
       "   f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "0                 0.788945                0.457628   \n",
       "1                 0.815627                0.780087   \n",
       "2                 0.830827                0.686671   \n",
       "3                 0.882651                0.473240   \n",
       "4                 0.892607                0.554929   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                     0.443937   \n",
       "1                                     0.438652   \n",
       "2                                     0.435106   \n",
       "3                                     0.430359   \n",
       "4                                     0.427252   \n",
       "\n",
       "   binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "0                                0.188583                      0.92095   \n",
       "1                                0.167041                      0.93158   \n",
       "2                                0.150556                      0.93382   \n",
       "3                                0.138477                      0.94427   \n",
       "4                                0.121175                      0.94939   \n",
       "\n",
       "   f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "0                     0.680516                    0.491968   \n",
       "1                     0.740085                    0.628368   \n",
       "2                     0.739346                    0.770864   \n",
       "3                     0.896951                    0.320349   \n",
       "4                     0.913842                    0.363112   \n",
       "\n",
       "   soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "0                                0.341260                           0.048158   \n",
       "1                                0.338606                           0.040026   \n",
       "2                                0.337091                           0.034885   \n",
       "3                                0.355464                           0.077190   \n",
       "4                                0.351683                           0.066051   \n",
       "\n",
       "   accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "0                0.981878                0.990764               6.078736   \n",
       "1                0.983952                0.991759               8.948903   \n",
       "2                0.986639                0.993170              10.499663   \n",
       "3                0.966564                0.982700               6.422559   \n",
       "4                0.973342                0.986045               7.925493   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_1000000  \\\n",
       "0                                  0.604133   \n",
       "1                                  0.585896   \n",
       "2                                  0.573653   \n",
       "3                                  0.514233   \n",
       "4                                  0.498131   \n",
       "\n",
       "   binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "0                             0.494743                  0.766630   \n",
       "1                             0.443391                  0.792173   \n",
       "2                             0.405561                  0.814067   \n",
       "3                             0.308172                  0.860024   \n",
       "4                             0.263208                  0.886782   \n",
       "\n",
       "   f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "0                  0.785663                 6.039980   \n",
       "1                  0.814393                 8.988415   \n",
       "2                  0.828896                 9.653604   \n",
       "3                  0.881758                 6.648989   \n",
       "4                  0.894634                 7.662928   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                      0.443815   \n",
       "1                                      0.438672   \n",
       "2                                      0.435362   \n",
       "3                                      0.430960   \n",
       "4                                      0.427879   \n",
       "\n",
       "   binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "0                                 0.188630                      0.920433   \n",
       "1                                 0.167892                      0.928820   \n",
       "2                                 0.152760                      0.934484   \n",
       "3                                 0.140045                      0.944159   \n",
       "4                                 0.123195                      0.948687   \n",
       "\n",
       "   f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "0                      0.682028                     6.398765   \n",
       "1                      0.727265                     8.718359   \n",
       "2                      0.733373                    10.151225   \n",
       "3                      0.896894                     4.305763   \n",
       "4                      0.912878                     4.915602   \n",
       "\n",
       "   soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                   0.514619   \n",
       "1                                   0.504802   \n",
       "2                                   0.498856   \n",
       "3                                   0.514619   \n",
       "4                                   0.504802   \n",
       "\n",
       "   binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "0                              0.232145                   0.913053   \n",
       "1                              0.176034                   0.940567   \n",
       "2                              0.135784                   0.948713   \n",
       "3                              0.232145                   0.913053   \n",
       "4                              0.176034                   0.940567   \n",
       "\n",
       "   f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "0                   0.924979                  0.026223   \n",
       "1                   0.945923                  0.035989   \n",
       "2                   0.953017                  0.043957   \n",
       "3                   0.924979                  0.043410   \n",
       "4                   0.945923                  0.050534   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                     0.501361   \n",
       "1                                     0.501101   \n",
       "2                                     0.493345   \n",
       "3                                     0.501361   \n",
       "4                                     0.501101   \n",
       "\n",
       "   binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "0                                0.114649                     0.971880   \n",
       "1                                0.084286                     0.971880   \n",
       "2                                0.039162                     0.977153   \n",
       "3                                0.114649                     0.971880   \n",
       "4                                0.084286                     0.971880   \n",
       "\n",
       "   f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "0                     0.956284                    0.000762   \n",
       "1                     0.956284                    0.000928   \n",
       "2                     0.962536                    0.001271   \n",
       "3                     0.956284                    0.001347   \n",
       "4                     0.956284                    0.001466   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                         0.605437   \n",
       "1                                         0.602921   \n",
       "2                                         0.593692   \n",
       "3                                         0.605437   \n",
       "4                                         0.602921   \n",
       "\n",
       "   binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                    0.275451   \n",
       "1                                    0.195279   \n",
       "2                                    0.132693   \n",
       "3                                    0.275451   \n",
       "4                                    0.195279   \n",
       "\n",
       "   accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                         0.915433                         0.816514   \n",
       "1                         0.942918                         0.882096   \n",
       "2                         0.953488                         0.902655   \n",
       "3                         0.915433                         0.816514   \n",
       "4                         0.942918                         0.882096   \n",
       "\n",
       "   runtime_absenteeism_TRAIN_DATA  \n",
       "0                        0.000852  \n",
       "1                        0.000976  \n",
       "2                        0.001135  \n",
       "3                        0.001130  \n",
       "4                        0.001603  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_dt_distilled_random_data = []\n",
    "for column in results_summary_columns:\n",
    "    if 'dt_scores' in column:\n",
    "        if 'runtime' not in column:\n",
    "            if 'data_random' in column:\n",
    "                columns_dt_distilled_random_data.append(column)\n",
    "        else: \n",
    "            columns_dt_distilled_random_data.append(column)\n",
    "results_summary_dt_distilled_random_data = results_summary[flatten([colmuns_identifier, columns_dt_distilled_random_data])]\n",
    "\n",
    "columns_dt_distilled_random_data_rename = []\n",
    "for column in columns_dt_distilled_random_data:\n",
    "    column = column.replace('dt_scores_','')\n",
    "    column = column.replace('_data_random','')\n",
    "    columns_dt_distilled_random_data_rename.append(column)\n",
    "\n",
    "results_summary_dt_distilled_random_data.columns = flatten([colmuns_identifier, columns_dt_distilled_random_data_rename])\n",
    "\n",
    "#results_summary_dt_distilled_random_data.insert(0, 'scores_type', 'dt_scores_data_random')\n",
    "results_summary_dt_distilled_random_data.insert(0, 'scores_type', [dt_type + str(decision_sparsity) + '_dt_scores_data_random' for dt_type, decision_sparsity in zip(results_summary_dt_distilled_random_data['function_family_dt_type'].values, results_summary_dt_distilled_random_data['function_family_decision_sparsity'].values)])\n",
    "\n",
    "\n",
    "print(results_summary_dt_distilled_random_data.shape)\n",
    "results_summary_dt_distilled_random_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e348a43e-1352-4078-9ebd-715deed82ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.508810Z",
     "iopub.status.busy": "2022-01-04T19:49:56.508425Z",
     "iopub.status.idle": "2022-01-04T19:49:56.591183Z",
     "shell.execute_reply": "2022-01-04T19:49:56.589886Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.508778Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_type</th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>train_soft_binary_crossentropy</th>\n",
       "      <th>train_soft_binary_crossentropy_median</th>\n",
       "      <th>train_binary_crossentropy</th>\n",
       "      <th>train_binary_crossentropy_median</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_accuracy_median</th>\n",
       "      <th>train_f1_score</th>\n",
       "      <th>train_f1_score_median</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_runtime_median</th>\n",
       "      <th>valid_soft_binary_crossentropy</th>\n",
       "      <th>valid_soft_binary_crossentropy_median</th>\n",
       "      <th>valid_binary_crossentropy</th>\n",
       "      <th>valid_binary_crossentropy_median</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>valid_accuracy_median</th>\n",
       "      <th>valid_f1_score</th>\n",
       "      <th>valid_f1_score_median</th>\n",
       "      <th>valid_runtime</th>\n",
       "      <th>valid_runtime_median</th>\n",
       "      <th>test_soft_binary_crossentropy</th>\n",
       "      <th>test_soft_binary_crossentropy_median</th>\n",
       "      <th>test_binary_crossentropy</th>\n",
       "      <th>test_binary_crossentropy_median</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_accuracy_median</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>test_f1_score_median</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_runtime_median</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000</th>\n",
       "      <th>binary_crossentropy_adult_1000</th>\n",
       "      <th>accuracy_adult_1000</th>\n",
       "      <th>f1_score_adult_1000</th>\n",
       "      <th>runtime_adult_1000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000</th>\n",
       "      <th>binary_crossentropy_titanic_1000</th>\n",
       "      <th>accuracy_titanic_1000</th>\n",
       "      <th>f1_score_titanic_1000</th>\n",
       "      <th>runtime_titanic_1000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000</th>\n",
       "      <th>accuracy_absenteeism_1000</th>\n",
       "      <th>f1_score_absenteeism_1000</th>\n",
       "      <th>runtime_absenteeism_1000</th>\n",
       "      <th>soft_binary_crossentropy_adult_10000</th>\n",
       "      <th>binary_crossentropy_adult_10000</th>\n",
       "      <th>accuracy_adult_10000</th>\n",
       "      <th>f1_score_adult_10000</th>\n",
       "      <th>runtime_adult_10000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_10000</th>\n",
       "      <th>binary_crossentropy_titanic_10000</th>\n",
       "      <th>accuracy_titanic_10000</th>\n",
       "      <th>f1_score_titanic_10000</th>\n",
       "      <th>runtime_titanic_10000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>binary_crossentropy_absenteeism_10000</th>\n",
       "      <th>accuracy_absenteeism_10000</th>\n",
       "      <th>f1_score_absenteeism_10000</th>\n",
       "      <th>runtime_absenteeism_10000</th>\n",
       "      <th>soft_binary_crossentropy_adult_100000</th>\n",
       "      <th>binary_crossentropy_adult_100000</th>\n",
       "      <th>accuracy_adult_100000</th>\n",
       "      <th>f1_score_adult_100000</th>\n",
       "      <th>runtime_adult_100000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_100000</th>\n",
       "      <th>binary_crossentropy_titanic_100000</th>\n",
       "      <th>accuracy_titanic_100000</th>\n",
       "      <th>f1_score_titanic_100000</th>\n",
       "      <th>runtime_titanic_100000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>binary_crossentropy_absenteeism_100000</th>\n",
       "      <th>accuracy_absenteeism_100000</th>\n",
       "      <th>f1_score_absenteeism_100000</th>\n",
       "      <th>runtime_absenteeism_100000</th>\n",
       "      <th>soft_binary_crossentropy_adult_1000000</th>\n",
       "      <th>binary_crossentropy_adult_1000000</th>\n",
       "      <th>accuracy_adult_1000000</th>\n",
       "      <th>f1_score_adult_1000000</th>\n",
       "      <th>runtime_adult_1000000</th>\n",
       "      <th>soft_binary_crossentropy_titanic_1000000</th>\n",
       "      <th>binary_crossentropy_titanic_1000000</th>\n",
       "      <th>accuracy_titanic_1000000</th>\n",
       "      <th>f1_score_titanic_1000000</th>\n",
       "      <th>runtime_titanic_1000000</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>binary_crossentropy_absenteeism_1000000</th>\n",
       "      <th>accuracy_absenteeism_1000000</th>\n",
       "      <th>f1_score_absenteeism_1000000</th>\n",
       "      <th>runtime_absenteeism_1000000</th>\n",
       "      <th>soft_binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_adult_TRAIN_DATA</th>\n",
       "      <th>accuracy_adult_TRAIN_DATA</th>\n",
       "      <th>f1_score_adult_TRAIN_DATA</th>\n",
       "      <th>runtime_adult_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_titanic_TRAIN_DATA</th>\n",
       "      <th>accuracy_titanic_TRAIN_DATA</th>\n",
       "      <th>f1_score_titanic_TRAIN_DATA</th>\n",
       "      <th>runtime_titanic_TRAIN_DATA</th>\n",
       "      <th>soft_binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>binary_crossentropy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>accuracy_absenteeism_TRAIN_DATA</th>\n",
       "      <th>f1_score_absenteeism_TRAIN_DATA</th>\n",
       "      <th>runtime_absenteeism_TRAIN_DATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.616474</td>\n",
       "      <td>0.615307</td>\n",
       "      <td>0.569003</td>\n",
       "      <td>0.567067</td>\n",
       "      <td>0.703344</td>\n",
       "      <td>0.7024</td>\n",
       "      <td>0.664482</td>\n",
       "      <td>0.695851</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.619904</td>\n",
       "      <td>0.617766</td>\n",
       "      <td>0.574994</td>\n",
       "      <td>0.573605</td>\n",
       "      <td>0.702496</td>\n",
       "      <td>0.7152</td>\n",
       "      <td>0.673612</td>\n",
       "      <td>0.713898</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.626373</td>\n",
       "      <td>0.629011</td>\n",
       "      <td>0.583992</td>\n",
       "      <td>0.589657</td>\n",
       "      <td>0.694880</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.669380</td>\n",
       "      <td>0.723074</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>2.012290</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.692467</td>\n",
       "      <td>0.642458</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.096975</td>\n",
       "      <td>0.665557</td>\n",
       "      <td>0.624602</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.110725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612697</td>\n",
       "      <td>0.613249</td>\n",
       "      <td>0.564528</td>\n",
       "      <td>0.564254</td>\n",
       "      <td>0.705632</td>\n",
       "      <td>0.7028</td>\n",
       "      <td>0.678824</td>\n",
       "      <td>0.705614</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.616877</td>\n",
       "      <td>0.620622</td>\n",
       "      <td>0.570432</td>\n",
       "      <td>0.575686</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.7136</td>\n",
       "      <td>0.688397</td>\n",
       "      <td>0.715229</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.622317</td>\n",
       "      <td>0.622120</td>\n",
       "      <td>0.579808</td>\n",
       "      <td>0.576846</td>\n",
       "      <td>0.698048</td>\n",
       "      <td>0.6976</td>\n",
       "      <td>0.675447</td>\n",
       "      <td>0.710239</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "      <td>0.933302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.116253</td>\n",
       "      <td>0.649897</td>\n",
       "      <td>0.571653</td>\n",
       "      <td>0.932961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.111433</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>0.618528</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615532</td>\n",
       "      <td>0.617586</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>0.568291</td>\n",
       "      <td>0.708408</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.664349</td>\n",
       "      <td>0.717616</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.617547</td>\n",
       "      <td>0.623748</td>\n",
       "      <td>0.571834</td>\n",
       "      <td>0.579464</td>\n",
       "      <td>0.709880</td>\n",
       "      <td>0.7084</td>\n",
       "      <td>0.692539</td>\n",
       "      <td>0.712673</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.626444</td>\n",
       "      <td>0.628567</td>\n",
       "      <td>0.586809</td>\n",
       "      <td>0.590938</td>\n",
       "      <td>0.693312</td>\n",
       "      <td>0.6928</td>\n",
       "      <td>0.674189</td>\n",
       "      <td>0.703011</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "      <td>0.836600</td>\n",
       "      <td>1.238022</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.161816</td>\n",
       "      <td>0.713819</td>\n",
       "      <td>0.766724</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.167118</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.652591</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.207149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606874</td>\n",
       "      <td>0.613907</td>\n",
       "      <td>0.561997</td>\n",
       "      <td>0.565275</td>\n",
       "      <td>0.706720</td>\n",
       "      <td>0.6992</td>\n",
       "      <td>0.674549</td>\n",
       "      <td>0.714300</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.612247</td>\n",
       "      <td>0.613774</td>\n",
       "      <td>0.571419</td>\n",
       "      <td>0.572374</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.7056</td>\n",
       "      <td>0.649521</td>\n",
       "      <td>0.715474</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.611344</td>\n",
       "      <td>0.617777</td>\n",
       "      <td>0.572884</td>\n",
       "      <td>0.580001</td>\n",
       "      <td>0.703008</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.659850</td>\n",
       "      <td>0.701589</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "      <td>0.905225</td>\n",
       "      <td>1.944110</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.718173</td>\n",
       "      <td>0.782591</td>\n",
       "      <td>0.374302</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.164211</td>\n",
       "      <td>0.672519</td>\n",
       "      <td>0.606104</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600036</td>\n",
       "      <td>0.600425</td>\n",
       "      <td>0.549041</td>\n",
       "      <td>0.550977</td>\n",
       "      <td>0.719944</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.690287</td>\n",
       "      <td>0.714838</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.605724</td>\n",
       "      <td>0.609405</td>\n",
       "      <td>0.555553</td>\n",
       "      <td>0.562603</td>\n",
       "      <td>0.714264</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>0.681811</td>\n",
       "      <td>0.709280</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.609198</td>\n",
       "      <td>0.605301</td>\n",
       "      <td>0.563761</td>\n",
       "      <td>0.562193</td>\n",
       "      <td>0.714608</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>0.724678</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.853145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382619</td>\n",
       "      <td>0.55347</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.672869</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.163254</td>\n",
       "      <td>0.667948</td>\n",
       "      <td>0.592628</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            scores_type  function_family_maximum_depth  \\\n",
       "0  vanilla1_inet_scores                              3   \n",
       "1  vanilla1_inet_scores                              4   \n",
       "2  vanilla1_inet_scores                              5   \n",
       "3  vanilla1_inet_scores                              3   \n",
       "4  vanilla1_inet_scores                              4   \n",
       "\n",
       "   function_family_decision_sparsity function_family_dt_type  \\\n",
       "0                                  1                 vanilla   \n",
       "1                                  1                 vanilla   \n",
       "2                                  1                 vanilla   \n",
       "3                                  1                 vanilla   \n",
       "4                                  1                 vanilla   \n",
       "\n",
       "  data_dt_type_train  data_maximum_depth_train  data_number_of_variables  \\\n",
       "0            vanilla                         5                         9   \n",
       "1            vanilla                         5                         9   \n",
       "2            vanilla                         5                         9   \n",
       "3            vanilla                         5                         9   \n",
       "4            vanilla                         5                         9   \n",
       "\n",
       "   data_noise_injected_level data_function_generation_type  \\\n",
       "0                          0   make_classification_trained   \n",
       "1                          0   make_classification_trained   \n",
       "2                          0   make_classification_trained   \n",
       "3                          0   make_classification_trained   \n",
       "4                          0   make_classification_trained   \n",
       "\n",
       "  data_categorical_indices lambda_net_lambda_network_layers  \\\n",
       "0                       []                            [128]   \n",
       "1                       []                            [128]   \n",
       "2                       []                            [128]   \n",
       "3             [0, 1, 2, 3]                            [128]   \n",
       "4             [0, 1, 2, 3]                            [128]   \n",
       "\n",
       "  lambda_net_optimizer_lambda             i_net_dense_layers  \\\n",
       "0                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "1                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "2                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "3                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "4                        adam  [1024, 1024, 256, 2048, 2048]   \n",
       "\n",
       "               i_net_dropout  i_net_learning_rate           i_net_loss  \\\n",
       "0  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "1  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "2  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "3  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "4  [0.3, 0.3, 0.3, 0.3, 0.3]               0.0001  binary_crossentropy   \n",
       "\n",
       "   i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "0                              10000                                   3   \n",
       "1                              10000                                   3   \n",
       "2                              10000                                   3   \n",
       "3                              10000                                   3   \n",
       "4                              10000                                   3   \n",
       "\n",
       "  i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "0                       None      False               100   \n",
       "1                       None      False               100   \n",
       "2                       None      False               100   \n",
       "3                       None      False               100   \n",
       "4                       None      False               100   \n",
       "\n",
       "  evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "0                                make_classification                    \n",
       "1                                make_classification                    \n",
       "2                                make_classification                    \n",
       "3                                make_classification                    \n",
       "4                                make_classification                    \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "0                                                  0                 \n",
       "1                                                  0                 \n",
       "2                                                  0                 \n",
       "3                                                  0                 \n",
       "4                                                  0                 \n",
       "\n",
       "   train_soft_binary_crossentropy  train_soft_binary_crossentropy_median  \\\n",
       "0                        0.616474                               0.615307   \n",
       "1                        0.612697                               0.613249   \n",
       "2                        0.615532                               0.617586   \n",
       "3                        0.606874                               0.613907   \n",
       "4                        0.600036                               0.600425   \n",
       "\n",
       "   train_binary_crossentropy  train_binary_crossentropy_median  \\\n",
       "0                   0.569003                          0.567067   \n",
       "1                   0.564528                          0.564254   \n",
       "2                   0.567170                          0.568291   \n",
       "3                   0.561997                          0.565275   \n",
       "4                   0.549041                          0.550977   \n",
       "\n",
       "   train_accuracy  train_accuracy_median  train_f1_score  \\\n",
       "0        0.703344                 0.7024        0.664482   \n",
       "1        0.705632                 0.7028        0.678824   \n",
       "2        0.708408                 0.7088        0.664349   \n",
       "3        0.706720                 0.6992        0.674549   \n",
       "4        0.719944                 0.7132        0.690287   \n",
       "\n",
       "   train_f1_score_median  train_runtime  train_runtime_median  \\\n",
       "0               0.695851       0.000763              0.000763   \n",
       "1               0.705614       0.000880              0.000880   \n",
       "2               0.717616       0.001042              0.001042   \n",
       "3               0.714300       0.001107              0.001107   \n",
       "4               0.714838       0.001228              0.001228   \n",
       "\n",
       "   valid_soft_binary_crossentropy  valid_soft_binary_crossentropy_median  \\\n",
       "0                        0.619904                               0.617766   \n",
       "1                        0.616877                               0.620622   \n",
       "2                        0.617547                               0.623748   \n",
       "3                        0.612247                               0.613774   \n",
       "4                        0.605724                               0.609405   \n",
       "\n",
       "   valid_binary_crossentropy  valid_binary_crossentropy_median  \\\n",
       "0                   0.574994                          0.573605   \n",
       "1                   0.570432                          0.575686   \n",
       "2                   0.571834                          0.579464   \n",
       "3                   0.571419                          0.572374   \n",
       "4                   0.555553                          0.562603   \n",
       "\n",
       "   valid_accuracy  valid_accuracy_median  valid_f1_score  \\\n",
       "0        0.702496                 0.7152        0.673612   \n",
       "1        0.700600                 0.7136        0.688397   \n",
       "2        0.709880                 0.7084        0.692539   \n",
       "3        0.703800                 0.7056        0.649521   \n",
       "4        0.714264                 0.7112        0.681811   \n",
       "\n",
       "   valid_f1_score_median  valid_runtime  valid_runtime_median  \\\n",
       "0               0.713898       0.000886              0.000886   \n",
       "1               0.715229       0.000795              0.000795   \n",
       "2               0.712673       0.000936              0.000936   \n",
       "3               0.715474       0.001154              0.001154   \n",
       "4               0.709280       0.001203              0.001203   \n",
       "\n",
       "   test_soft_binary_crossentropy  test_soft_binary_crossentropy_median  \\\n",
       "0                       0.626373                              0.629011   \n",
       "1                       0.622317                              0.622120   \n",
       "2                       0.626444                              0.628567   \n",
       "3                       0.611344                              0.617777   \n",
       "4                       0.609198                              0.605301   \n",
       "\n",
       "   test_binary_crossentropy  test_binary_crossentropy_median  test_accuracy  \\\n",
       "0                  0.583992                         0.589657       0.694880   \n",
       "1                  0.579808                         0.576846       0.698048   \n",
       "2                  0.586809                         0.590938       0.693312   \n",
       "3                  0.572884                         0.580001       0.703008   \n",
       "4                  0.563761                         0.562193       0.714608   \n",
       "\n",
       "   test_accuracy_median  test_f1_score  test_f1_score_median  test_runtime  \\\n",
       "0                0.6940       0.669380              0.723074      0.001105   \n",
       "1                0.6976       0.675447              0.710239      0.001292   \n",
       "2                0.6928       0.674189              0.703011      0.001507   \n",
       "3                0.6980       0.659850              0.701589      0.002056   \n",
       "4                0.7132       0.689956              0.724678      0.002074   \n",
       "\n",
       "   test_runtime_median  soft_binary_crossentropy_adult_1000  \\\n",
       "0             0.001105                             0.909159   \n",
       "1             0.001292                             0.933302   \n",
       "2             0.001507                             0.836600   \n",
       "3             0.002056                             0.905225   \n",
       "4             0.002074                             0.853145   \n",
       "\n",
       "   binary_crossentropy_adult_1000  accuracy_adult_1000  f1_score_adult_1000  \\\n",
       "0                        2.012290             0.382619              0.55347   \n",
       "1                        0.000000             0.382619              0.55347   \n",
       "2                        1.238022             0.382619              0.55347   \n",
       "3                        1.944110             0.382619              0.55347   \n",
       "4                        0.000000             0.382619              0.55347   \n",
       "\n",
       "   runtime_adult_1000  soft_binary_crossentropy_titanic_1000  \\\n",
       "0            0.104282                               0.685014   \n",
       "1            0.116253                               0.649897   \n",
       "2            0.161816                               0.713819   \n",
       "3            0.142096                               0.718173   \n",
       "4            0.187451                               0.673761   \n",
       "\n",
       "   binary_crossentropy_titanic_1000  accuracy_titanic_1000  \\\n",
       "0                          0.692467               0.642458   \n",
       "1                          0.571653               0.932961   \n",
       "2                          0.766724               0.374302   \n",
       "3                          0.782591               0.374302   \n",
       "4                          0.672869               0.631285   \n",
       "\n",
       "   f1_score_titanic_1000  runtime_titanic_1000  \\\n",
       "0               0.418182              0.096975   \n",
       "1               0.911765              0.111433   \n",
       "2               0.544715              0.167118   \n",
       "3               0.544715              0.164211   \n",
       "4               0.565789              0.163254   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_1000  \\\n",
       "0                                   0.665557   \n",
       "1                                   0.668186   \n",
       "2                                   0.673644   \n",
       "3                                   0.672519   \n",
       "4                                   0.667948   \n",
       "\n",
       "   binary_crossentropy_absenteeism_1000  accuracy_absenteeism_1000  \\\n",
       "0                              0.624602                   0.743243   \n",
       "1                              0.618528                   0.729730   \n",
       "2                              0.652591                   0.574324   \n",
       "3                              0.606104                   0.729730   \n",
       "4                              0.592628                   0.729730   \n",
       "\n",
       "   f1_score_absenteeism_1000  runtime_absenteeism_1000  \\\n",
       "0                   0.486486                  0.110725   \n",
       "1                   0.000000                  0.125714   \n",
       "2                   0.322581                  0.207149   \n",
       "3                   0.000000                  0.135190   \n",
       "4                   0.000000                  0.153153   \n",
       "\n",
       "   soft_binary_crossentropy_adult_10000  binary_crossentropy_adult_10000  \\\n",
       "0                              0.909159                         2.012290   \n",
       "1                              0.933302                         0.000000   \n",
       "2                              0.836600                         1.238022   \n",
       "3                              0.905225                         1.944110   \n",
       "4                              0.853145                         0.000000   \n",
       "\n",
       "   accuracy_adult_10000  f1_score_adult_10000  runtime_adult_10000  \\\n",
       "0              0.382619               0.55347             0.104282   \n",
       "1              0.382619               0.55347             0.116253   \n",
       "2              0.382619               0.55347             0.161816   \n",
       "3              0.382619               0.55347             0.142096   \n",
       "4              0.382619               0.55347             0.187451   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_10000  binary_crossentropy_titanic_10000  \\\n",
       "0                                0.685014                           0.692467   \n",
       "1                                0.649897                           0.571653   \n",
       "2                                0.713819                           0.766724   \n",
       "3                                0.718173                           0.782591   \n",
       "4                                0.673761                           0.672869   \n",
       "\n",
       "   accuracy_titanic_10000  f1_score_titanic_10000  runtime_titanic_10000  \\\n",
       "0                0.642458                0.418182               0.096975   \n",
       "1                0.932961                0.911765               0.111433   \n",
       "2                0.374302                0.544715               0.167118   \n",
       "3                0.374302                0.544715               0.164211   \n",
       "4                0.631285                0.565789               0.163254   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_10000  \\\n",
       "0                                    0.665557   \n",
       "1                                    0.668186   \n",
       "2                                    0.673644   \n",
       "3                                    0.672519   \n",
       "4                                    0.667948   \n",
       "\n",
       "   binary_crossentropy_absenteeism_10000  accuracy_absenteeism_10000  \\\n",
       "0                               0.624602                    0.743243   \n",
       "1                               0.618528                    0.729730   \n",
       "2                               0.652591                    0.574324   \n",
       "3                               0.606104                    0.729730   \n",
       "4                               0.592628                    0.729730   \n",
       "\n",
       "   f1_score_absenteeism_10000  runtime_absenteeism_10000  \\\n",
       "0                    0.486486                   0.110725   \n",
       "1                    0.000000                   0.125714   \n",
       "2                    0.322581                   0.207149   \n",
       "3                    0.000000                   0.135190   \n",
       "4                    0.000000                   0.153153   \n",
       "\n",
       "   soft_binary_crossentropy_adult_100000  binary_crossentropy_adult_100000  \\\n",
       "0                               0.909159                          2.012290   \n",
       "1                               0.933302                          0.000000   \n",
       "2                               0.836600                          1.238022   \n",
       "3                               0.905225                          1.944110   \n",
       "4                               0.853145                          0.000000   \n",
       "\n",
       "   accuracy_adult_100000  f1_score_adult_100000  runtime_adult_100000  \\\n",
       "0               0.382619                0.55347              0.104282   \n",
       "1               0.382619                0.55347              0.116253   \n",
       "2               0.382619                0.55347              0.161816   \n",
       "3               0.382619                0.55347              0.142096   \n",
       "4               0.382619                0.55347              0.187451   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_100000  \\\n",
       "0                                 0.685014   \n",
       "1                                 0.649897   \n",
       "2                                 0.713819   \n",
       "3                                 0.718173   \n",
       "4                                 0.673761   \n",
       "\n",
       "   binary_crossentropy_titanic_100000  accuracy_titanic_100000  \\\n",
       "0                            0.692467                 0.642458   \n",
       "1                            0.571653                 0.932961   \n",
       "2                            0.766724                 0.374302   \n",
       "3                            0.782591                 0.374302   \n",
       "4                            0.672869                 0.631285   \n",
       "\n",
       "   f1_score_titanic_100000  runtime_titanic_100000  \\\n",
       "0                 0.418182                0.096975   \n",
       "1                 0.911765                0.111433   \n",
       "2                 0.544715                0.167118   \n",
       "3                 0.544715                0.164211   \n",
       "4                 0.565789                0.163254   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_100000  \\\n",
       "0                                     0.665557   \n",
       "1                                     0.668186   \n",
       "2                                     0.673644   \n",
       "3                                     0.672519   \n",
       "4                                     0.667948   \n",
       "\n",
       "   binary_crossentropy_absenteeism_100000  accuracy_absenteeism_100000  \\\n",
       "0                                0.624602                     0.743243   \n",
       "1                                0.618528                     0.729730   \n",
       "2                                0.652591                     0.574324   \n",
       "3                                0.606104                     0.729730   \n",
       "4                                0.592628                     0.729730   \n",
       "\n",
       "   f1_score_absenteeism_100000  runtime_absenteeism_100000  \\\n",
       "0                     0.486486                    0.110725   \n",
       "1                     0.000000                    0.125714   \n",
       "2                     0.322581                    0.207149   \n",
       "3                     0.000000                    0.135190   \n",
       "4                     0.000000                    0.153153   \n",
       "\n",
       "   soft_binary_crossentropy_adult_1000000  binary_crossentropy_adult_1000000  \\\n",
       "0                                0.909159                           2.012290   \n",
       "1                                0.933302                           0.000000   \n",
       "2                                0.836600                           1.238022   \n",
       "3                                0.905225                           1.944110   \n",
       "4                                0.853145                           0.000000   \n",
       "\n",
       "   accuracy_adult_1000000  f1_score_adult_1000000  runtime_adult_1000000  \\\n",
       "0                0.382619                 0.55347               0.104282   \n",
       "1                0.382619                 0.55347               0.116253   \n",
       "2                0.382619                 0.55347               0.161816   \n",
       "3                0.382619                 0.55347               0.142096   \n",
       "4                0.382619                 0.55347               0.187451   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_1000000  \\\n",
       "0                                  0.685014   \n",
       "1                                  0.649897   \n",
       "2                                  0.713819   \n",
       "3                                  0.718173   \n",
       "4                                  0.673761   \n",
       "\n",
       "   binary_crossentropy_titanic_1000000  accuracy_titanic_1000000  \\\n",
       "0                             0.692467                  0.642458   \n",
       "1                             0.571653                  0.932961   \n",
       "2                             0.766724                  0.374302   \n",
       "3                             0.782591                  0.374302   \n",
       "4                             0.672869                  0.631285   \n",
       "\n",
       "   f1_score_titanic_1000000  runtime_titanic_1000000  \\\n",
       "0                  0.418182                 0.096975   \n",
       "1                  0.911765                 0.111433   \n",
       "2                  0.544715                 0.167118   \n",
       "3                  0.544715                 0.164211   \n",
       "4                  0.565789                 0.163254   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_1000000  \\\n",
       "0                                      0.665557   \n",
       "1                                      0.668186   \n",
       "2                                      0.673644   \n",
       "3                                      0.672519   \n",
       "4                                      0.667948   \n",
       "\n",
       "   binary_crossentropy_absenteeism_1000000  accuracy_absenteeism_1000000  \\\n",
       "0                                 0.624602                      0.743243   \n",
       "1                                 0.618528                      0.729730   \n",
       "2                                 0.652591                      0.574324   \n",
       "3                                 0.606104                      0.729730   \n",
       "4                                 0.592628                      0.729730   \n",
       "\n",
       "   f1_score_absenteeism_1000000  runtime_absenteeism_1000000  \\\n",
       "0                      0.486486                     0.110725   \n",
       "1                      0.000000                     0.125714   \n",
       "2                      0.322581                     0.207149   \n",
       "3                      0.000000                     0.135190   \n",
       "4                      0.000000                     0.153153   \n",
       "\n",
       "   soft_binary_crossentropy_adult_TRAIN_DATA  \\\n",
       "0                                   0.909159   \n",
       "1                                   0.933302   \n",
       "2                                   0.836600   \n",
       "3                                   0.905225   \n",
       "4                                   0.853145   \n",
       "\n",
       "   binary_crossentropy_adult_TRAIN_DATA  accuracy_adult_TRAIN_DATA  \\\n",
       "0                              2.012290                   0.382619   \n",
       "1                              0.000000                   0.382619   \n",
       "2                              1.238022                   0.382619   \n",
       "3                              1.944110                   0.382619   \n",
       "4                              0.000000                   0.382619   \n",
       "\n",
       "   f1_score_adult_TRAIN_DATA  runtime_adult_TRAIN_DATA  \\\n",
       "0                    0.55347                  0.104282   \n",
       "1                    0.55347                  0.116253   \n",
       "2                    0.55347                  0.161816   \n",
       "3                    0.55347                  0.142096   \n",
       "4                    0.55347                  0.187451   \n",
       "\n",
       "   soft_binary_crossentropy_titanic_TRAIN_DATA  \\\n",
       "0                                     0.685014   \n",
       "1                                     0.649897   \n",
       "2                                     0.713819   \n",
       "3                                     0.718173   \n",
       "4                                     0.673761   \n",
       "\n",
       "   binary_crossentropy_titanic_TRAIN_DATA  accuracy_titanic_TRAIN_DATA  \\\n",
       "0                                0.692467                     0.642458   \n",
       "1                                0.571653                     0.932961   \n",
       "2                                0.766724                     0.374302   \n",
       "3                                0.782591                     0.374302   \n",
       "4                                0.672869                     0.631285   \n",
       "\n",
       "   f1_score_titanic_TRAIN_DATA  runtime_titanic_TRAIN_DATA  \\\n",
       "0                     0.418182                    0.096975   \n",
       "1                     0.911765                    0.111433   \n",
       "2                     0.544715                    0.167118   \n",
       "3                     0.544715                    0.164211   \n",
       "4                     0.565789                    0.163254   \n",
       "\n",
       "   soft_binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                         0.665557   \n",
       "1                                         0.668186   \n",
       "2                                         0.673644   \n",
       "3                                         0.672519   \n",
       "4                                         0.667948   \n",
       "\n",
       "   binary_crossentropy_absenteeism_TRAIN_DATA  \\\n",
       "0                                    0.624602   \n",
       "1                                    0.618528   \n",
       "2                                    0.652591   \n",
       "3                                    0.606104   \n",
       "4                                    0.592628   \n",
       "\n",
       "   accuracy_absenteeism_TRAIN_DATA  f1_score_absenteeism_TRAIN_DATA  \\\n",
       "0                         0.743243                         0.486486   \n",
       "1                         0.729730                         0.000000   \n",
       "2                         0.574324                         0.322581   \n",
       "3                         0.729730                         0.000000   \n",
       "4                         0.729730                         0.000000   \n",
       "\n",
       "   runtime_absenteeism_TRAIN_DATA  \n",
       "0                        0.110725  \n",
       "1                        0.125714  \n",
       "2                        0.207149  \n",
       "3                        0.135190  \n",
       "4                        0.153153  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced = pd.concat([\n",
    "                                     results_summary_inet, \n",
    "                                     results_summary_dt_distilled, \n",
    "                                     #results_summary_dt_distilled_random_data\n",
    "                                    ]).reset_index(drop=True)\n",
    "results_summary_reduced_columns = results_summary_reduced.columns\n",
    "results_summary_reduced.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5dd9d45-4def-41d0-be00-eb037f12ee6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.592095Z",
     "iopub.status.busy": "2022-01-04T19:49:56.591955Z",
     "iopub.status.idle": "2022-01-04T19:49:56.595051Z",
     "shell.execute_reply": "2022-01-04T19:49:56.594577Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.592079Z"
    }
   },
   "outputs": [],
   "source": [
    "colmuns_identifier.append('scores_type')\n",
    "not_considered_random_dataset_sizes = ['1000', '100000', '1000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e45d52-591b-4f1d-9d7f-c67ff5d4e7dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.596287Z",
     "iopub.status.busy": "2022-01-04T19:49:56.595854Z",
     "iopub.status.idle": "2022-01-04T19:49:56.615232Z",
     "shell.execute_reply": "2022-01-04T19:49:56.614775Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.596267Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_results_summary_reduced_for_metric(config, metric='accuracy', soft=False):\n",
    "    \n",
    "    results_summary_reduced_metric_columns = []\n",
    "    for column in results_summary_reduced_columns:  \n",
    "        if metric in column:\n",
    "            if 'soft' not in column and not soft:           \n",
    "                if 'median' not in column:\n",
    "                    tokens = column.split('_')\n",
    "                    integer = [token for token in tokens if token.isdigit()]\n",
    "                    if len(integer) > 0:\n",
    "                        integer = integer[0]\n",
    "                        if integer not in not_considered_random_dataset_sizes:                    \n",
    "                            results_summary_reduced_metric_columns.append(column)\n",
    "                    else:\n",
    "                        if 'TRAIN_DATA' not in column:\n",
    "                            results_summary_reduced_metric_columns.append(column)\n",
    "\n",
    "    results_summary_reduced_metric_with_identifier_columns = flatten([colmuns_identifier, results_summary_reduced_metric_columns])\n",
    "    results_summary_reduced_metric_with_identifier = results_summary_reduced[results_summary_reduced_metric_with_identifier_columns]\n",
    "    results_summary_reduced_metric_with_identifier.head(5)    \n",
    "    \n",
    "    \n",
    "    results_summary_reduced_metric_plot = results_summary_reduced_metric_with_identifier\n",
    "\n",
    "    try:\n",
    "        results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['i_net_nas'] == config['nas']]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if config['nas'] == True:\n",
    "        try:\n",
    "            results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['i_net_nas_trials'] == config['nas_trials']]\n",
    "        except:\n",
    "            pass    \n",
    "    else:\n",
    "        try:\n",
    "            results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['i_net_dense_layers'] == config['inet_structure']]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['i_net_loss'] == config['loss']]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['data_noise_injected_level'] == config['noise_injected_level']]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['data_categorical_indices'] == config['categorical_indices']]\n",
    "    except:\n",
    "        pass    \n",
    "    try:\n",
    "        results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['i_net_data_reshape_version'] == config['data_reshape_version']]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['data_function_generation_type'] == config['function_generation_type']]\n",
    "    except:\n",
    "        pass        \n",
    "        \n",
    "    results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['data_number_of_variables'].isin(config['number_of_variables'])]\n",
    "    results_summary_reduced_metric_plot = results_summary_reduced_metric_plot[results_summary_reduced_metric_plot['function_family_maximum_depth'].isin(config['maximum_depth'])]\n",
    "\n",
    "    results_summary_reduced_metric_plot_columns = list(results_summary_reduced_metric_plot.columns)\n",
    "    result_columns = []\n",
    "    identifier_columns = []\n",
    "    for column in results_summary_reduced_metric_plot_columns:\n",
    "        if metric in column:\n",
    "            result_columns.append(column)\n",
    "        else:\n",
    "            identifier_columns.append(column)\n",
    "    number_of_results = results_summary_reduced_metric_plot.shape[0]\n",
    "\n",
    "    results_summary_reduced_metric_plot_single_column_identifier = pd.concat([results_summary_reduced_metric_plot[identifier_columns] for _ in range(len(result_columns))], axis=0)\n",
    "    results_summary_reduced_metric_plot_single_column_identifier['result_identifier'] = flatten([[result_column]*number_of_results for result_column in result_columns])\n",
    "    #results_summary_reduced_metric_plot_single_column_identifier['result_identifier'] = flatten([[i]*number_of_results for i in range(len(result_columns))])\n",
    "\n",
    "    results_summary_reduced_metric_plot_single_column_results = pd.concat([results_summary_reduced_metric_plot[result_column] for result_column in result_columns], axis=0)\n",
    "    results_summary_reduced_metric_plot_single_column_results.name = 'score'\n",
    "\n",
    "    results_summary_reduced_metric_plot = pd.concat([results_summary_reduced_metric_plot_single_column_identifier, results_summary_reduced_metric_plot_single_column_results], axis=1)\n",
    "\n",
    "    results_summary_reduced_metric_plot = results_summary_reduced_metric_plot.sort_values(by=['function_family_dt_type', 'function_family_decision_sparsity'], ascending=(False, True))\n",
    "    \n",
    "    \n",
    "    return results_summary_reduced_metric_plot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e0ce79-1096-4a18-946a-eea59f41d93d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.616046Z",
     "iopub.status.busy": "2022-01-04T19:49:56.615891Z",
     "iopub.status.idle": "2022-01-04T19:49:56.628565Z",
     "shell.execute_reply": "2022-01-04T19:49:56.627952Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.616027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_results(data_reduced, col, x, y, hue, plot_type=sns.barplot):\n",
    "    \n",
    "    #sns.set(rc={'figure.figsize':(20,10)})\n",
    "    \n",
    "    g = sns.FacetGrid(data_reduced, \n",
    "                      col=col,\n",
    "                      ##hue='scores_type', \n",
    "                      #height=5, \n",
    "                      col_wrap=2,\n",
    "                      aspect=1.5,\n",
    "                      ##legend_out=False,\n",
    "                     )\n",
    "    g.map(plot_type, \n",
    "          x, \n",
    "          y, \n",
    "          hue,\n",
    "          hue_order=np.unique(data_reduced[hue]),\n",
    "          ##figsize=(20,10),\n",
    "          palette=sns.color_palette(),#'colorblind'\n",
    "          ##order=np.unique(results_summary_reduced_accuracy_plot[\"scores_type\"]),\n",
    "         )\n",
    "    g.add_legend(fontsize=12,\n",
    "               ncol=3,\n",
    "               bbox_to_anchor=(0.5, -0.025),\n",
    "               borderaxespad=0)    \n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba60c75-a5d2-41a2-aa82-363561902504",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.629875Z",
     "iopub.status.busy": "2022-01-04T19:49:56.629433Z",
     "iopub.status.idle": "2022-01-04T19:49:56.647116Z",
     "shell.execute_reply": "2022-01-04T19:49:56.645575Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.629847Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "figsize = (20, 10)\n",
    "font_scale = 2.5\n",
    "\n",
    "legend_fontsize = 25\n",
    "legend_loc = 2\n",
    "\n",
    "color_1 = '#84b7e9'#'#c0d6ff'\n",
    "color_2 = '#0a6fd3'#'#96bcff'\n",
    "color_3 = '#06427e'#'#6ca1ff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f1aaef-6056-477c-bc18-168be35fd7fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:56.653155Z",
     "iopub.status.busy": "2022-01-04T19:49:56.652168Z",
     "iopub.status.idle": "2022-01-04T19:49:59.804808Z",
     "shell.execute_reply": "2022-01-04T19:49:59.804295Z",
     "shell.execute_reply.started": "2022-01-04T19:49:56.653088Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/seaborn/axisgrid.py:670: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1138.65x648 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'inet_structure': '[1024, 1024, 256, 2048, 2048]',\n",
    "    'loss': 'binary_crossentropy', # 'binary_crossentropy', 'soft_binary_crossentropy'\n",
    "\n",
    "    'noise_injected_level': 0, #0, 0.2\n",
    "    #'categorical_indices': '[]',#'[0, 1, 2, 3]',\n",
    "    'data_reshape_version': 'None', #'None', '3' =autoencode\n",
    "    #'function_generation_type': 'random_decision_tree_trained', #make_classification_trained, random_decision_tree_trained\n",
    "\n",
    "    'nas': False, # 'True', 'False'\n",
    "    'nas_trials': 20, #20, 100\n",
    "\n",
    "    'number_of_variables': [9], # [10]\n",
    "    'maximum_depth': [3, 4, 5], # [3, 4, 5]\n",
    "}\n",
    "\n",
    "results_summary_reduced_accuracy_plot = get_results_summary_reduced_for_metric(config, metric='accuracy', soft=False)\n",
    "\n",
    "plot = plot_results(\n",
    "                     data_reduced=results_summary_reduced_accuracy_plot,\n",
    "                     col='result_identifier',\n",
    "                     x='function_family_maximum_depth',\n",
    "                     y='score',\n",
    "                     hue='scores_type',\n",
    "                     plot_type=sns.barplot\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6983c151-d4e1-46c0-980e-34e08163d057",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:59.806078Z",
     "iopub.status.busy": "2022-01-04T19:49:59.805908Z",
     "iopub.status.idle": "2022-01-04T19:49:59.858448Z",
     "shell.execute_reply": "2022-01-04T19:49:59.857898Z",
     "shell.execute_reply.started": "2022-01-04T19:49:59.806058Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>result_identifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.932961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.631285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.932961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.379888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.385475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.932961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.653631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.620112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.787709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_decision_sparsity  \\\n",
       "1                               4                                  1   \n",
       "4                               4                                  1   \n",
       "7                               4                                  1   \n",
       "10                              4                                  1   \n",
       "35                              4                                  1   \n",
       "36                              4                                  1   \n",
       "42                              4                                  1   \n",
       "46                              4                                  1   \n",
       "41                              4                                  9   \n",
       "43                              4                                  9   \n",
       "47                              4                                  9   \n",
       "49                              4                                  9   \n",
       "\n",
       "   function_family_dt_type data_dt_type_train  data_maximum_depth_train  \\\n",
       "1                  vanilla            vanilla                         5   \n",
       "4                  vanilla            vanilla                         5   \n",
       "7                  vanilla            vanilla                         5   \n",
       "10                 vanilla            vanilla                         5   \n",
       "35                     SDT            vanilla                         5   \n",
       "36                     SDT            vanilla                         5   \n",
       "42                     SDT            vanilla                         5   \n",
       "46                     SDT            vanilla                         5   \n",
       "41                     SDT            vanilla                         5   \n",
       "43                     SDT            vanilla                         5   \n",
       "47                     SDT            vanilla                         5   \n",
       "49                     SDT            vanilla                         5   \n",
       "\n",
       "    data_number_of_variables  data_noise_injected_level  \\\n",
       "1                          9                          0   \n",
       "4                          9                          0   \n",
       "7                          9                          0   \n",
       "10                         9                          0   \n",
       "35                         9                          0   \n",
       "36                         9                          0   \n",
       "42                         9                          0   \n",
       "46                         9                          0   \n",
       "41                         9                          0   \n",
       "43                         9                          0   \n",
       "47                         9                          0   \n",
       "49                         9                          0   \n",
       "\n",
       "   data_function_generation_type data_categorical_indices  \\\n",
       "1    make_classification_trained                       []   \n",
       "4    make_classification_trained             [0, 1, 2, 3]   \n",
       "7   random_decision_tree_trained                       []   \n",
       "10  random_decision_tree_trained             [0, 1, 2, 3]   \n",
       "35  random_decision_tree_trained             [0, 1, 2, 3]   \n",
       "36   make_classification_trained             [0, 1, 2, 3]   \n",
       "42   make_classification_trained                       []   \n",
       "46  random_decision_tree_trained                       []   \n",
       "41   make_classification_trained             [0, 1, 2, 3]   \n",
       "43  random_decision_tree_trained             [0, 1, 2, 3]   \n",
       "47   make_classification_trained                       []   \n",
       "49  random_decision_tree_trained                       []   \n",
       "\n",
       "   lambda_net_lambda_network_layers lambda_net_optimizer_lambda  \\\n",
       "1                             [128]                        adam   \n",
       "4                             [128]                        adam   \n",
       "7                             [128]                        adam   \n",
       "10                            [128]                        adam   \n",
       "35                            [128]                        adam   \n",
       "36                            [128]                        adam   \n",
       "42                            [128]                        adam   \n",
       "46                            [128]                        adam   \n",
       "41                            [128]                        adam   \n",
       "43                            [128]                        adam   \n",
       "47                            [128]                        adam   \n",
       "49                            [128]                        adam   \n",
       "\n",
       "               i_net_dense_layers              i_net_dropout  \\\n",
       "1   [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "4   [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "7   [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "10  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "35  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "36  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "42  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "46  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "41  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "43  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "47  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "49  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "\n",
       "    i_net_learning_rate           i_net_loss  \\\n",
       "1                0.0001  binary_crossentropy   \n",
       "4                0.0001  binary_crossentropy   \n",
       "7                0.0001  binary_crossentropy   \n",
       "10               0.0001  binary_crossentropy   \n",
       "35               0.0001  binary_crossentropy   \n",
       "36               0.0001  binary_crossentropy   \n",
       "42               0.0001  binary_crossentropy   \n",
       "46               0.0001  binary_crossentropy   \n",
       "41               0.0001  binary_crossentropy   \n",
       "43               0.0001  binary_crossentropy   \n",
       "47               0.0001  binary_crossentropy   \n",
       "49               0.0001  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "1                               10000                                   3   \n",
       "4                               10000                                   3   \n",
       "7                               10000                                   3   \n",
       "10                              10000                                   3   \n",
       "35                              10000                                   3   \n",
       "36                              10000                                   3   \n",
       "42                              10000                                   3   \n",
       "46                              10000                                   3   \n",
       "41                              10000                                   1   \n",
       "43                              10000                                   1   \n",
       "47                              10000                                   1   \n",
       "49                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "1                        None      False               100   \n",
       "4                        None      False               100   \n",
       "7                        None      False               100   \n",
       "10                       None      False               100   \n",
       "35                       None      False               100   \n",
       "36                       None      False               100   \n",
       "42                       None      False               100   \n",
       "46                       None      False               100   \n",
       "41                       None      False               100   \n",
       "43                       None      False               100   \n",
       "47                       None      False               100   \n",
       "49                       None      False               100   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "1                                 make_classification                    \n",
       "4                                 make_classification                    \n",
       "7                                 make_classification                    \n",
       "10                                make_classification                    \n",
       "35                                make_classification                    \n",
       "36                                make_classification                    \n",
       "42                                make_classification                    \n",
       "46                                make_classification                    \n",
       "41                                make_classification                    \n",
       "43                                make_classification                    \n",
       "47                                make_classification                    \n",
       "49                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "1                                                   0                 \n",
       "4                                                   0                 \n",
       "7                                                   0                 \n",
       "10                                                  0                 \n",
       "35                                                  0                 \n",
       "36                                                  0                 \n",
       "42                                                  0                 \n",
       "46                                                  0                 \n",
       "41                                                  0                 \n",
       "43                                                  0                 \n",
       "47                                                  0                 \n",
       "49                                                  0                 \n",
       "\n",
       "             scores_type       result_identifier     score  \n",
       "1   vanilla1_inet_scores  accuracy_titanic_10000  0.932961  \n",
       "4   vanilla1_inet_scores  accuracy_titanic_10000  0.631285  \n",
       "7   vanilla1_inet_scores  accuracy_titanic_10000  0.932961  \n",
       "10  vanilla1_inet_scores  accuracy_titanic_10000  0.379888  \n",
       "35      SDT1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "36      SDT1_inet_scores  accuracy_titanic_10000  0.385475  \n",
       "42      SDT1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "46      SDT1_inet_scores  accuracy_titanic_10000  0.932961  \n",
       "41      SDT9_inet_scores  accuracy_titanic_10000  0.653631  \n",
       "43      SDT9_inet_scores  accuracy_titanic_10000  0.620112  \n",
       "47      SDT9_inet_scores  accuracy_titanic_10000  0.787709  \n",
       "49      SDT9_inet_scores  accuracy_titanic_10000  0.374302  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_accuracy_plot[(results_summary_reduced_accuracy_plot['result_identifier'] == 'accuracy_titanic_10000') &\n",
    "                                      #(results_summary_reduced_accuracy_plot['scores_type'] == 'vanilla1_inet_scores') & \n",
    "                                      (results_summary_reduced_accuracy_plot['scores_type'].str.contains('inet')) &\n",
    "                                      (results_summary_reduced_accuracy_plot['function_family_maximum_depth'] == 4)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cafe83b6-53b5-4c8a-8c1c-51e32dcf903d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:49:59.859712Z",
     "iopub.status.busy": "2022-01-04T19:49:59.859312Z",
     "iopub.status.idle": "2022-01-04T19:50:02.654812Z",
     "shell.execute_reply": "2022-01-04T19:50:02.653200Z",
     "shell.execute_reply.started": "2022-01-04T19:49:59.859690Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/seaborn/axisgrid.py:670: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1146.15x648 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'inet_structure': '[1024, 1024, 256, 2048, 2048]',\n",
    "    'loss': 'binary_crossentropy', # 'binary_crossentropy', 'soft_binary_crossentropy'\n",
    "\n",
    "    'noise_injected_level': 0, #0, 0.2\n",
    "    #'categorical_indices': '[]',\n",
    "    'data_reshape_version': 'None', #'None', '3' =autoencode\n",
    "    #'function_generation_type': 'random_decision_tree_trained', #make_classification_trained, random_decision_tree_trained\n",
    "\n",
    "    'nas': False, # 'True', 'False'\n",
    "    'nas_trials': 20, #20, 100\n",
    "\n",
    "    'number_of_variables': [15], # [10]\n",
    "    'maximum_depth': [3, 4, 5], # [3, 4, 5]\n",
    "}\n",
    "\n",
    "results_summary_reduced_accuracy_plot = get_results_summary_reduced_for_metric(config, metric='accuracy', soft=False)\n",
    "\n",
    "plot = plot_results(\n",
    "                     data_reduced=results_summary_reduced_accuracy_plot,\n",
    "                     col='result_identifier',\n",
    "                     x='function_family_maximum_depth',\n",
    "                     y='score',\n",
    "                     hue='scores_type',\n",
    "                     plot_type=sns.barplot\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8458132-3518-455c-89c2-6a73ab86cfa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:50:02.657459Z",
     "iopub.status.busy": "2022-01-04T19:50:02.657106Z",
     "iopub.status.idle": "2022-01-04T19:50:02.688226Z",
     "shell.execute_reply": "2022-01-04T19:50:02.686152Z",
     "shell.execute_reply.started": "2022-01-04T19:50:02.657413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>result_identifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.614525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.614525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.324022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[0, 1, 2, 9]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.324022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.720670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_decision_sparsity  \\\n",
       "12                              4                                  1   \n",
       "13                              4                                  1   \n",
       "18                              4                                  1   \n",
       "19                              4                                  1   \n",
       "51                              4                                  1   \n",
       "37                              4                                  1   \n",
       "38                              4                                  1   \n",
       "48                              4                                  1   \n",
       "50                              4                                  1   \n",
       "52                              4                                  1   \n",
       "39                              4                                 15   \n",
       "40                              4                                 15   \n",
       "44                              4                                 15   \n",
       "45                              4                                 15   \n",
       "53                              4                                 15   \n",
       "\n",
       "   function_family_dt_type data_dt_type_train  data_maximum_depth_train  \\\n",
       "12                 vanilla            vanilla                         5   \n",
       "13                 vanilla            vanilla                         5   \n",
       "18                 vanilla            vanilla                         5   \n",
       "19                 vanilla            vanilla                         5   \n",
       "51                 vanilla            vanilla                         5   \n",
       "37                     SDT            vanilla                         5   \n",
       "38                     SDT            vanilla                         5   \n",
       "48                     SDT            vanilla                         5   \n",
       "50                     SDT            vanilla                         5   \n",
       "52                     SDT            vanilla                         5   \n",
       "39                     SDT            vanilla                         5   \n",
       "40                     SDT            vanilla                         5   \n",
       "44                     SDT            vanilla                         5   \n",
       "45                     SDT            vanilla                         5   \n",
       "53                     SDT            vanilla                         5   \n",
       "\n",
       "    data_number_of_variables  data_noise_injected_level  \\\n",
       "12                        15                          0   \n",
       "13                        15                          0   \n",
       "18                        15                          0   \n",
       "19                        15                          0   \n",
       "51                        15                          0   \n",
       "37                        15                          0   \n",
       "38                        15                          0   \n",
       "48                        15                          0   \n",
       "50                        15                          0   \n",
       "52                        15                          0   \n",
       "39                        15                          0   \n",
       "40                        15                          0   \n",
       "44                        15                          0   \n",
       "45                        15                          0   \n",
       "53                        15                          0   \n",
       "\n",
       "   data_function_generation_type data_categorical_indices  \\\n",
       "12  random_decision_tree_trained             [0, 1, 2, 9]   \n",
       "13  random_decision_tree_trained             [0, 1, 2, 9]   \n",
       "18  random_decision_tree_trained                       []   \n",
       "19  random_decision_tree_trained                       []   \n",
       "51   make_classification_trained                       []   \n",
       "37  random_decision_tree_trained             [0, 1, 2, 9]   \n",
       "38  random_decision_tree_trained             [0, 1, 2, 9]   \n",
       "48  random_decision_tree_trained                       []   \n",
       "50  random_decision_tree_trained                       []   \n",
       "52   make_classification_trained                       []   \n",
       "39  random_decision_tree_trained             [0, 1, 2, 9]   \n",
       "40  random_decision_tree_trained             [0, 1, 2, 9]   \n",
       "44  random_decision_tree_trained                       []   \n",
       "45  random_decision_tree_trained                       []   \n",
       "53   make_classification_trained                       []   \n",
       "\n",
       "   lambda_net_lambda_network_layers lambda_net_optimizer_lambda  \\\n",
       "12                            [128]                        adam   \n",
       "13                            [128]                        adam   \n",
       "18                            [128]                        adam   \n",
       "19                            [128]                        adam   \n",
       "51                            [128]                        adam   \n",
       "37                            [128]                        adam   \n",
       "38                            [128]                        adam   \n",
       "48                            [128]                        adam   \n",
       "50                            [128]                        adam   \n",
       "52                            [128]                        adam   \n",
       "39                            [128]                        adam   \n",
       "40                            [128]                        adam   \n",
       "44                            [128]                        adam   \n",
       "45                            [128]                        adam   \n",
       "53                            [128]                        adam   \n",
       "\n",
       "               i_net_dense_layers              i_net_dropout  \\\n",
       "12  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "13  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "18  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "19  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "51  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "37  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "38  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "48  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "50  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "52  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "39  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "40  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "44  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "45  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "53  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "\n",
       "    i_net_learning_rate           i_net_loss  \\\n",
       "12               0.0001  binary_crossentropy   \n",
       "13               0.0001  binary_crossentropy   \n",
       "18               0.0001  binary_crossentropy   \n",
       "19               0.0001  binary_crossentropy   \n",
       "51               0.0001  binary_crossentropy   \n",
       "37               0.0001  binary_crossentropy   \n",
       "38               0.0001  binary_crossentropy   \n",
       "48               0.0001  binary_crossentropy   \n",
       "50               0.0001  binary_crossentropy   \n",
       "52               0.0001  binary_crossentropy   \n",
       "39               0.0001  binary_crossentropy   \n",
       "40               0.0001  binary_crossentropy   \n",
       "44               0.0001  binary_crossentropy   \n",
       "45               0.0001  binary_crossentropy   \n",
       "53               0.0001  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "12                              10000                                   3   \n",
       "13                              10000                                   3   \n",
       "18                              10000                                   3   \n",
       "19                              10000                                   3   \n",
       "51                              10000                                   3   \n",
       "37                              10000                                   3   \n",
       "38                              10000                                   3   \n",
       "48                              10000                                   3   \n",
       "50                              10000                                   3   \n",
       "52                              10000                                   3   \n",
       "39                              10000                                   1   \n",
       "40                              10000                                   1   \n",
       "44                              10000                                   1   \n",
       "45                              10000                                   1   \n",
       "53                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "12                       None      False               100   \n",
       "13                       None      False               100   \n",
       "18                       None      False               100   \n",
       "19                       None      False               100   \n",
       "51                       None      False               100   \n",
       "37                       None      False               100   \n",
       "38                       None      False               100   \n",
       "48                       None      False               100   \n",
       "50                       None      False               100   \n",
       "52                       None      False               100   \n",
       "39                       None      False               100   \n",
       "40                       None      False               100   \n",
       "44                       None      False               100   \n",
       "45                       None      False               100   \n",
       "53                       None      False               100   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "12                                make_classification                    \n",
       "13                                make_classification                    \n",
       "18                                make_classification                    \n",
       "19                                make_classification                    \n",
       "51                                make_classification                    \n",
       "37                                make_classification                    \n",
       "38                                make_classification                    \n",
       "48                                make_classification                    \n",
       "50                                make_classification                    \n",
       "52                                make_classification                    \n",
       "39                                make_classification                    \n",
       "40                                make_classification                    \n",
       "44                                make_classification                    \n",
       "45                                make_classification                    \n",
       "53                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "12                                                  0                 \n",
       "13                                                  0                 \n",
       "18                                                  0                 \n",
       "19                                                  0                 \n",
       "51                                                  0                 \n",
       "37                                                  0                 \n",
       "38                                                  0                 \n",
       "48                                                  0                 \n",
       "50                                                  0                 \n",
       "52                                                  0                 \n",
       "39                                                  0                 \n",
       "40                                                  0                 \n",
       "44                                                  0                 \n",
       "45                                                  0                 \n",
       "53                                                  0                 \n",
       "\n",
       "             scores_type       result_identifier     score  \n",
       "12  vanilla1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "13  vanilla1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "18  vanilla1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "19  vanilla1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "51  vanilla1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "37      SDT1_inet_scores  accuracy_titanic_10000  0.614525  \n",
       "38      SDT1_inet_scores  accuracy_titanic_10000  0.614525  \n",
       "48      SDT1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "50      SDT1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "52      SDT1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "39     SDT15_inet_scores  accuracy_titanic_10000  0.324022  \n",
       "40     SDT15_inet_scores  accuracy_titanic_10000  0.324022  \n",
       "44     SDT15_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "45     SDT15_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "53     SDT15_inet_scores  accuracy_titanic_10000  0.720670  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_accuracy_plot[(results_summary_reduced_accuracy_plot['result_identifier'] == 'accuracy_absenteeism_10000') &\n",
    "                                      #(results_summary_reduced_accuracy_plot['scores_type'] == 'vanilla1_inet_scores') & \n",
    "                                      (results_summary_reduced_accuracy_plot['scores_type'].str.contains('inet')) &\n",
    "                                      (results_summary_reduced_accuracy_plot['function_family_maximum_depth'] == 4)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42da84b6-076d-411f-b416-8fdadb3395ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:50:02.691395Z",
     "iopub.status.busy": "2022-01-04T19:50:02.690774Z",
     "iopub.status.idle": "2022-01-04T19:50:04.601780Z",
     "shell.execute_reply": "2022-01-04T19:50:04.600975Z",
     "shell.execute_reply.started": "2022-01-04T19:50:02.691331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/seaborn/axisgrid.py:670: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1138.65x648 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'inet_structure': '[1024, 1024, 256, 2048, 2048]',\n",
    "    'loss': 'binary_crossentropy', # 'binary_crossentropy', 'soft_binary_crossentropy'\n",
    "\n",
    "    'noise_injected_level': 0, #0, 0.2\n",
    "    'categorical_indices': '[]',\n",
    "    'data_reshape_version': 'None', #'None', '3' =autoencode\n",
    "    #'function_generation_type': 'random_decision_tree_trained', #make_classification_trained, random_decision_tree_trained\n",
    "\n",
    "    'nas': True, # 'True', 'False'\n",
    "    #'nas_trials': 20, #20, 100\n",
    "\n",
    "    'number_of_variables': [9], # [10]\n",
    "    'maximum_depth': [3, 4, 5], # [3, 4, 5]\n",
    "}\n",
    "\n",
    "results_summary_reduced_accuracy_plot = get_results_summary_reduced_for_metric(config, metric='accuracy', soft=False)\n",
    "\n",
    "plot = plot_results(\n",
    "                     data_reduced=results_summary_reduced_accuracy_plot,\n",
    "                     col='result_identifier',\n",
    "                     x='function_family_maximum_depth',\n",
    "                     y='score',\n",
    "                     hue='scores_type',\n",
    "                     plot_type=sns.barplot\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "484949a6-f7f5-4f9a-ba09-6bea66ab5e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:50:04.603085Z",
     "iopub.status.busy": "2022-01-04T19:50:04.602773Z",
     "iopub.status.idle": "2022-01-04T19:50:04.624320Z",
     "shell.execute_reply": "2022-01-04T19:50:04.623528Z",
     "shell.execute_reply.started": "2022-01-04T19:50:04.603063Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>result_identifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.648045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.921788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.575419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.374302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.888268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT9_inet_scores</td>\n",
       "      <td>accuracy_titanic_10000</td>\n",
       "      <td>0.938547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_decision_sparsity  \\\n",
       "27                              4                                  1   \n",
       "32                              4                                  1   \n",
       "25                              4                                  1   \n",
       "30                              4                                  1   \n",
       "24                              4                                  9   \n",
       "34                              4                                  9   \n",
       "\n",
       "   function_family_dt_type data_dt_type_train  data_maximum_depth_train  \\\n",
       "27                 vanilla            vanilla                         5   \n",
       "32                 vanilla            vanilla                         5   \n",
       "25                     SDT            vanilla                         5   \n",
       "30                     SDT            vanilla                         5   \n",
       "24                     SDT            vanilla                         5   \n",
       "34                     SDT            vanilla                         5   \n",
       "\n",
       "    data_number_of_variables  data_noise_injected_level  \\\n",
       "27                         9                          0   \n",
       "32                         9                          0   \n",
       "25                         9                          0   \n",
       "30                         9                          0   \n",
       "24                         9                          0   \n",
       "34                         9                          0   \n",
       "\n",
       "   data_function_generation_type data_categorical_indices  \\\n",
       "27  random_decision_tree_trained                       []   \n",
       "32   make_classification_trained                       []   \n",
       "25   make_classification_trained                       []   \n",
       "30  random_decision_tree_trained                       []   \n",
       "24   make_classification_trained                       []   \n",
       "34  random_decision_tree_trained                       []   \n",
       "\n",
       "   lambda_net_lambda_network_layers lambda_net_optimizer_lambda  \\\n",
       "27                            [128]                        adam   \n",
       "32                            [128]                        adam   \n",
       "25                            [128]                        adam   \n",
       "30                            [128]                        adam   \n",
       "24                            [128]                        adam   \n",
       "34                            [128]                        adam   \n",
       "\n",
       "               i_net_dense_layers              i_net_dropout  \\\n",
       "27  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "32  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "25  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "30  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "24  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "34  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "\n",
       "    i_net_learning_rate           i_net_loss  \\\n",
       "27               0.0001  binary_crossentropy   \n",
       "32               0.0001  binary_crossentropy   \n",
       "25               0.0001  binary_crossentropy   \n",
       "30               0.0001  binary_crossentropy   \n",
       "24               0.0001  binary_crossentropy   \n",
       "34               0.0001  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "27                              10000                                   3   \n",
       "32                              10000                                   3   \n",
       "25                              10000                                   3   \n",
       "30                              10000                                   3   \n",
       "24                              10000                                   1   \n",
       "34                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "27                       None       True                20   \n",
       "32                       None       True                60   \n",
       "25                       None       True                60   \n",
       "30                       None       True                20   \n",
       "24                       None       True                60   \n",
       "34                       None       True                60   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "27                                make_classification                    \n",
       "32                                make_classification                    \n",
       "25                                make_classification                    \n",
       "30                                make_classification                    \n",
       "24                                make_classification                    \n",
       "34                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "27                                                  0                 \n",
       "32                                                  0                 \n",
       "25                                                  0                 \n",
       "30                                                  0                 \n",
       "24                                                  0                 \n",
       "34                                                  0                 \n",
       "\n",
       "             scores_type       result_identifier     score  \n",
       "27  vanilla1_inet_scores  accuracy_titanic_10000  0.648045  \n",
       "32  vanilla1_inet_scores  accuracy_titanic_10000  0.921788  \n",
       "25      SDT1_inet_scores  accuracy_titanic_10000  0.575419  \n",
       "30      SDT1_inet_scores  accuracy_titanic_10000  0.374302  \n",
       "24      SDT9_inet_scores  accuracy_titanic_10000  0.888268  \n",
       "34      SDT9_inet_scores  accuracy_titanic_10000  0.938547  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_accuracy_plot[(results_summary_reduced_accuracy_plot['result_identifier'] == 'accuracy_titanic_10000') &\n",
    "                                      (results_summary_reduced_accuracy_plot['scores_type'].str.contains('inet')) & \n",
    "                                      (results_summary_reduced_accuracy_plot['function_family_maximum_depth'] == 4)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58c6798c-1a2b-4701-bfe3-7d7553eff5d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:50:04.625828Z",
     "iopub.status.busy": "2022-01-04T19:50:04.625478Z",
     "iopub.status.idle": "2022-01-04T19:50:06.437317Z",
     "shell.execute_reply": "2022-01-04T19:50:06.436632Z",
     "shell.execute_reply.started": "2022-01-04T19:50:04.625804Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/XAI/lib/python3.8/site-packages/seaborn/axisgrid.py:670: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1146.15x648 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'inet_structure': '[1024, 1024, 256, 2048, 2048]',\n",
    "    'loss': 'binary_crossentropy', # 'binary_crossentropy', 'soft_binary_crossentropy'\n",
    "\n",
    "    'noise_injected_level': 0, #0, 0.2\n",
    "    'categorical_indices': '[]',\n",
    "    'data_reshape_version': 'None', #'None', '3' =autoencode\n",
    "    #'function_generation_type': 'random_decision_tree_trained', #make_classification_trained, random_decision_tree_trained\n",
    "\n",
    "    'nas': True, # 'True', 'False'\n",
    "    #'nas_trials': 20, #20, 100\n",
    "\n",
    "    'number_of_variables': [15], # [10]\n",
    "    'maximum_depth': [3, 4, 5], # [3, 4, 5]\n",
    "}\n",
    "\n",
    "results_summary_reduced_accuracy_plot = get_results_summary_reduced_for_metric(config, metric='accuracy', soft=False)\n",
    "\n",
    "plot = plot_results(\n",
    "                     data_reduced=results_summary_reduced_accuracy_plot,\n",
    "                     col='result_identifier',\n",
    "                     x='function_family_maximum_depth',\n",
    "                     y='score',\n",
    "                     hue='scores_type',\n",
    "                     plot_type=sns.barplot\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0101f13-9656-4b06-9350-6dee99c3ad1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-04T19:50:06.439177Z",
     "iopub.status.busy": "2022-01-04T19:50:06.438791Z",
     "iopub.status.idle": "2022-01-04T19:50:06.460529Z",
     "shell.execute_reply": "2022-01-04T19:50:06.459948Z",
     "shell.execute_reply.started": "2022-01-04T19:50:06.439149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>function_family_maximum_depth</th>\n",
       "      <th>function_family_decision_sparsity</th>\n",
       "      <th>function_family_dt_type</th>\n",
       "      <th>data_dt_type_train</th>\n",
       "      <th>data_maximum_depth_train</th>\n",
       "      <th>data_number_of_variables</th>\n",
       "      <th>data_noise_injected_level</th>\n",
       "      <th>data_function_generation_type</th>\n",
       "      <th>data_categorical_indices</th>\n",
       "      <th>lambda_net_lambda_network_layers</th>\n",
       "      <th>lambda_net_optimizer_lambda</th>\n",
       "      <th>i_net_dense_layers</th>\n",
       "      <th>i_net_dropout</th>\n",
       "      <th>i_net_learning_rate</th>\n",
       "      <th>i_net_loss</th>\n",
       "      <th>i_net_interpretation_dataset_size</th>\n",
       "      <th>i_net_function_representation_type</th>\n",
       "      <th>i_net_data_reshape_version</th>\n",
       "      <th>i_net_nas</th>\n",
       "      <th>i_net_nas_trials</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_function_generation_type</th>\n",
       "      <th>evaluation_eval_data_description_eval_data_noise_injected_level</th>\n",
       "      <th>scores_type</th>\n",
       "      <th>result_identifier</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>vanilla1_inet_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT1_inet_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>make_classification_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.777027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>SDT</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>random_decision_tree_trained</td>\n",
       "      <td>[]</td>\n",
       "      <td>[128]</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1024, 1024, 256, 2048, 2048]</td>\n",
       "      <td>[0.3, 0.3, 0.3, 0.3, 0.3]</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>60</td>\n",
       "      <td>make_classification</td>\n",
       "      <td>0</td>\n",
       "      <td>SDT15_inet_scores</td>\n",
       "      <td>accuracy_absenteeism_10000</td>\n",
       "      <td>0.608108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    function_family_maximum_depth  function_family_decision_sparsity  \\\n",
       "29                              4                                  1   \n",
       "26                              4                                  1   \n",
       "28                              4                                  1   \n",
       "31                              4                                 15   \n",
       "33                              4                                 15   \n",
       "\n",
       "   function_family_dt_type data_dt_type_train  data_maximum_depth_train  \\\n",
       "29                 vanilla            vanilla                         5   \n",
       "26                     SDT            vanilla                         5   \n",
       "28                     SDT            vanilla                         5   \n",
       "31                     SDT            vanilla                         5   \n",
       "33                     SDT            vanilla                         5   \n",
       "\n",
       "    data_number_of_variables  data_noise_injected_level  \\\n",
       "29                        15                          0   \n",
       "26                        15                          0   \n",
       "28                        15                          0   \n",
       "31                        15                          0   \n",
       "33                        15                          0   \n",
       "\n",
       "   data_function_generation_type data_categorical_indices  \\\n",
       "29   make_classification_trained                       []   \n",
       "26  random_decision_tree_trained                       []   \n",
       "28   make_classification_trained                       []   \n",
       "31   make_classification_trained                       []   \n",
       "33  random_decision_tree_trained                       []   \n",
       "\n",
       "   lambda_net_lambda_network_layers lambda_net_optimizer_lambda  \\\n",
       "29                            [128]                        adam   \n",
       "26                            [128]                        adam   \n",
       "28                            [128]                        adam   \n",
       "31                            [128]                        adam   \n",
       "33                            [128]                        adam   \n",
       "\n",
       "               i_net_dense_layers              i_net_dropout  \\\n",
       "29  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "26  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "28  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "31  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "33  [1024, 1024, 256, 2048, 2048]  [0.3, 0.3, 0.3, 0.3, 0.3]   \n",
       "\n",
       "    i_net_learning_rate           i_net_loss  \\\n",
       "29               0.0001  binary_crossentropy   \n",
       "26               0.0001  binary_crossentropy   \n",
       "28               0.0001  binary_crossentropy   \n",
       "31               0.0001  binary_crossentropy   \n",
       "33               0.0001  binary_crossentropy   \n",
       "\n",
       "    i_net_interpretation_dataset_size  i_net_function_representation_type  \\\n",
       "29                              10000                                   3   \n",
       "26                              10000                                   3   \n",
       "28                              10000                                   3   \n",
       "31                              10000                                   1   \n",
       "33                              10000                                   1   \n",
       "\n",
       "   i_net_data_reshape_version  i_net_nas  i_net_nas_trials  \\\n",
       "29                       None       True                20   \n",
       "26                       None       True                60   \n",
       "28                       None       True                20   \n",
       "31                       None       True                20   \n",
       "33                       None       True                60   \n",
       "\n",
       "   evaluation_eval_data_description_eval_data_function_generation_type  \\\n",
       "29                                make_classification                    \n",
       "26                                make_classification                    \n",
       "28                                make_classification                    \n",
       "31                                make_classification                    \n",
       "33                                make_classification                    \n",
       "\n",
       "    evaluation_eval_data_description_eval_data_noise_injected_level  \\\n",
       "29                                                  0                 \n",
       "26                                                  0                 \n",
       "28                                                  0                 \n",
       "31                                                  0                 \n",
       "33                                                  0                 \n",
       "\n",
       "             scores_type           result_identifier     score  \n",
       "29  vanilla1_inet_scores  accuracy_absenteeism_10000  0.648649  \n",
       "26      SDT1_inet_scores  accuracy_absenteeism_10000  0.648649  \n",
       "28      SDT1_inet_scores  accuracy_absenteeism_10000  0.648649  \n",
       "31     SDT15_inet_scores  accuracy_absenteeism_10000  0.777027  \n",
       "33     SDT15_inet_scores  accuracy_absenteeism_10000  0.608108  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary_reduced_accuracy_plot[(results_summary_reduced_accuracy_plot['result_identifier'] == 'accuracy_absenteeism_10000') &\n",
    "                                      (results_summary_reduced_accuracy_plot['scores_type'].str.contains('inet')) & \n",
    "                                      (results_summary_reduced_accuracy_plot['function_family_maximum_depth'] == 4)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65330e6b-74db-413c-b986-af9cbfd7aa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
