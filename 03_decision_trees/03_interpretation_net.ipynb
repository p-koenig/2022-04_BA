{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 4,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,                      \n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 5, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'random_decision_tree', # 'make_classification' 'random_decision_tree'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 1000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [64],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [1056, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.2, 0.1],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 20, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, # 1=standard representation; 2=sparse representation\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 5000,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize1000_numLNets10000_var5_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1056-512_drop0.2-0.1e20b256_adam\n",
      "lNetSize1000_numLNets10000_var5_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-3)]: Done 758 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-3)]: Done 7254 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   11.9s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:  5.2min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   29.8s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if noise_injected_level > 0:\n",
    "    lambda_net_dataset_training = load_lambda_nets(config, no_noise=True, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_evaluation = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_training, test_split=0.1)\n",
    "    _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_evaluation, test_split=test_size)\n",
    "    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8955, 573)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 573)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 573)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>6671.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-1.227</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>3274.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.738</td>\n",
       "      <td>1.438</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.751</td>\n",
       "      <td>1.597</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-1.116</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.551</td>\n",
       "      <td>0.653</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>3095.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.798</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.632</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>8379.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.715</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.355</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.659</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.746</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>0.899</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.743</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.668</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.678</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.817</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.818</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.740</td>\n",
       "      <td>-0.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2   f0v3  f0v4  f1v0   f1v1  f1v2  f1v3  \\\n",
       "6671 6671.000    42 0.343 0.000 0.000  0.000 0.000 0.269  0.000 0.000 0.000   \n",
       "3274 3274.000    42 0.000 0.000 0.000  0.000 0.372 0.000  0.000 0.365 0.000   \n",
       "3095 3095.000    42 0.000 0.000 0.000  0.295 0.000 0.000 -0.234 0.000 0.000   \n",
       "8379 8379.000    42 0.000 0.000 0.000 -0.419 0.000 0.000  0.000 0.000 0.000   \n",
       "3043 3043.000    42 0.000 0.000 0.000  0.000 0.373 0.000  0.000 0.000 0.000   \n",
       "\n",
       "       f1v4  f2v0  f2v1   f2v2  f2v3  f2v4  f3v0  f3v1   f3v2  f3v3   f3v4  \\\n",
       "6671  0.000 0.000 0.000  0.000 0.273 0.000 0.000 0.000  0.000 0.330  0.000   \n",
       "3274  0.000 0.226 0.000  0.000 0.000 0.000 0.000 0.000 -0.169 0.000  0.000   \n",
       "3095  0.000 0.000 0.000  0.000 0.418 0.000 0.000 0.000  0.000 0.000 -0.394   \n",
       "8379 -0.293 0.328 0.000  0.000 0.000 0.000 0.000 0.000  0.000 0.000  0.264   \n",
       "3043  0.387 0.000 0.000 -0.375 0.000 0.000 0.000 0.000 -0.423 0.000  0.000   \n",
       "\n",
       "      f4v0   f4v1  f4v2   f4v3  f4v4  f5v0  f5v1   f5v2  f5v3   f5v4  f6v0  \\\n",
       "6671 0.000  0.411 0.000  0.000 0.000 0.000 0.000  0.000 0.000  0.307 0.291   \n",
       "3274 0.000  0.000 0.000 -0.366 0.000 0.000 0.000 -0.363 0.000  0.000 0.000   \n",
       "3095 0.000  0.000 0.000  0.345 0.000 0.000 0.000  0.000 0.000  0.421 0.000   \n",
       "8379 0.000  0.000 0.000 -0.406 0.000 0.000 0.000  0.000 0.000 -0.418 0.000   \n",
       "3043 0.000 -0.274 0.000  0.000 0.000 0.000 0.000  0.000 0.000  0.405 0.000   \n",
       "\n",
       "      f6v1   f6v2  f6v3   f6v4   f7v0  f7v1   f7v2  f7v3   f7v4  f8v0  f8v1  \\\n",
       "6671 0.000  0.000 0.000  0.000  0.000 0.331  0.000 0.000  0.000 0.000 0.000   \n",
       "3274 0.296  0.000 0.000  0.000  0.000 0.000 -0.297 0.000  0.000 0.000 0.191   \n",
       "3095 0.421  0.000 0.000  0.000 -0.380 0.000  0.000 0.000  0.000 0.000 0.000   \n",
       "8379 0.000 -0.315 0.000  0.000  0.000 0.000  0.000 0.000 -0.347 0.000 0.000   \n",
       "3043 0.000  0.000 0.000 -0.376  0.000 0.000  0.000 0.000  0.317 0.397 0.000   \n",
       "\n",
       "       f8v2  f8v3   f8v4  f9v0  f9v1  f9v2  f9v3  f9v4  f10v0  f10v1  f10v2  \\\n",
       "6671  0.000 0.000 -0.316 0.000 0.000 0.000 0.332 0.000  0.000  0.000  0.000   \n",
       "3274  0.000 0.000  0.000 0.000 0.000 0.000 0.433 0.000  0.000  0.000  0.000   \n",
       "3095 -0.311 0.000  0.000 0.000 0.424 0.000 0.000 0.000  0.000  0.000 -0.408   \n",
       "8379 -0.420 0.000  0.000 0.323 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "3043  0.000 0.000  0.000 0.000 0.400 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  f12v2  \\\n",
       "6671  0.350  0.000  0.000  0.000  0.000  0.000  0.360  0.000  0.000 -0.372   \n",
       "3274  0.000  0.410  0.000 -0.369  0.000  0.000  0.000  0.000 -0.440  0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000 -0.305  0.000  0.000  0.000   \n",
       "8379 -0.337  0.000  0.000  0.280  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.384  0.000  0.000  0.000  0.000  0.385  0.000  0.407  0.000  0.000   \n",
       "\n",
       "      f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  f14v2  \\\n",
       "6671  0.000  0.000  0.415  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274  0.000  0.000  0.000  0.371  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095  0.000  0.431  0.000  0.000  0.000 -0.406  0.000  0.000  0.000  0.000   \n",
       "8379  0.000 -0.404  0.000  0.000  0.263  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.363  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6     b7  \\\n",
       "6671  0.000 -0.195  0.015 -0.105 -0.399  0.381  0.088  0.077 -0.194 -0.304   \n",
       "3274 -0.363  0.000 -0.317  0.115  0.268  0.390 -0.230  0.196 -0.281 -0.183   \n",
       "3095  0.000  0.442  0.297 -0.063  0.301  0.146 -0.272  0.053 -0.001 -0.233   \n",
       "8379  0.000  0.420  0.372 -0.402  0.130  0.156 -0.086 -0.113 -0.138 -0.261   \n",
       "3043  0.000 -0.437 -0.093 -0.116 -0.176 -0.225 -0.409  0.116 -0.178  0.222   \n",
       "\n",
       "         b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  lp1c0  \\\n",
       "6671 -0.072 -0.092  0.165  0.150  0.023 -0.369 -0.334  0.069  0.163  0.207   \n",
       "3274  0.056  0.334 -0.403  0.193  0.080  0.308  0.126 -0.019 -0.241  0.170   \n",
       "3095  0.272 -0.090  0.345 -0.408 -0.210  0.211  0.201 -0.055 -0.042 -0.186   \n",
       "8379  0.428  0.311  0.063 -0.338  0.312  0.112 -0.097 -0.120 -0.035  0.201   \n",
       "3043  0.081  0.021  0.161 -0.189  0.020  0.374  0.138  0.058 -0.107 -0.158   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  wb_352  \\\n",
       "6671 -0.017  0.049 -0.221 -0.077 -0.231  ...  -0.186  -0.013  -0.074  -0.067   \n",
       "3274  0.182  0.005 -0.087  0.240  0.030  ...  -0.142  -0.071   0.219   0.510   \n",
       "3095 -0.169 -0.023  0.090  0.219 -0.030  ...   0.372  -0.171   0.302   0.348   \n",
       "8379  0.126  0.148  0.160  0.034  0.023  ...   0.005   0.287  -0.059  -0.020   \n",
       "3043  0.216 -0.047 -0.008 -0.233  0.073  ...  -0.043   0.388  -0.239  -0.087   \n",
       "\n",
       "      wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  \\\n",
       "6671   0.203  -0.048  -0.085  -0.005  -0.113  -0.060   0.000   0.342  -0.053   \n",
       "3274   0.175   0.025   0.162   0.396   0.248   0.298   0.000   0.164  -0.056   \n",
       "3095   0.185  -0.027  -0.023   0.469   0.016   0.239   0.000   0.217   0.342   \n",
       "8379  -0.074  -0.029  -0.054  -0.016  -0.076  -0.012   0.000  -0.117  -0.060   \n",
       "3043  -0.274   0.442  -0.265  -0.117  -0.173  -0.226   0.000  -0.358  -0.206   \n",
       "\n",
       "      wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  \\\n",
       "6671  -0.151  -0.145  -0.066  -0.058  -0.105   0.081  -0.076  -0.094  -0.128   \n",
       "3274   0.061  -0.082   0.367   0.497   0.163  -0.052   0.250   0.056   0.138   \n",
       "3095   0.271   0.297   0.407   0.413   0.222  -0.034   0.277  -0.059   0.202   \n",
       "8379  -0.043  -0.047  -0.061  -0.022  -0.081  -0.043  -0.029  -0.003  -0.025   \n",
       "3043  -0.175  -0.088  -0.131  -0.059  -0.253   0.445  -0.199   0.356  -0.151   \n",
       "\n",
       "      wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  \\\n",
       "6671   0.000   0.000  -0.018   0.006  -0.097   0.275   0.064  -0.034  -0.072   \n",
       "3274   0.000   0.000   0.063  -0.032  -0.079   0.006   0.008  -0.060   0.255   \n",
       "3095   0.000   0.000  -0.022   0.123   0.291   0.143  -0.076  -0.144   0.354   \n",
       "8379   0.000   0.000   0.410   0.344  -0.120  -0.104   0.028  -0.113  -0.068   \n",
       "3043   0.000   0.000   0.509   0.508  -0.215  -0.339   0.419   0.304  -0.186   \n",
       "\n",
       "      wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  wb_388  \\\n",
       "6671   0.130  -0.083  -0.099  -0.142  -0.337  -0.649   0.130  -0.428  -1.037   \n",
       "3274  -0.048   0.315  -0.071   0.152  -0.103   0.567   0.500  -0.873  -0.209   \n",
       "3095  -0.035   0.347  -0.045   0.176  -0.596  -0.443   0.422  -0.262  -0.736   \n",
       "8379  -0.039  -0.093   0.337  -0.092  -0.076  -0.680   0.114  -0.252  -0.110   \n",
       "3043   0.462  -0.091  -0.084  -0.264  -0.686   0.600   0.698  -0.981  -0.849   \n",
       "\n",
       "      wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  wb_397  \\\n",
       "6671   0.031   0.121   0.214  -0.592  -0.272   0.194  -0.695  -0.563  -0.149   \n",
       "3274   0.738   1.438  -0.673  -0.720  -0.272   0.469  -0.870  -0.542  -0.154   \n",
       "3095   0.650   0.798  -0.141  -0.178  -0.272   0.552  -0.110   0.560  -0.149   \n",
       "8379   0.912   0.137  -0.890  -0.174  -0.272   0.471  -0.120   0.715  -0.942   \n",
       "3043   0.036   0.114  -0.769  -0.794  -0.272   0.743  -0.782  -0.602  -0.149   \n",
       "\n",
       "      wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  wb_406  \\\n",
       "6671   0.438   0.136  -0.906  -0.409  -0.319  -0.133  -0.573  -0.644  -0.628   \n",
       "3274   0.036   1.268   0.849  -0.148   0.482  -0.135  -0.796  -0.597   0.603   \n",
       "3095   0.460   0.858   0.604  -0.620  -0.411  -0.118  -0.236  -0.045  -0.460   \n",
       "8379   0.700   1.355   0.830  -0.901   0.547  -0.962  -0.235  -0.057   0.823   \n",
       "3043   0.302   0.135  -0.871  -0.139   0.506  -1.038  -0.880  -0.670  -0.785   \n",
       "\n",
       "      wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  wb_415  \\\n",
       "6671  -0.271   0.237   0.730  -0.196   0.108  -0.895   0.582  -0.350   0.167   \n",
       "3274   0.572   0.536   0.639  -0.632   0.794  -0.741  -0.365  -0.661   0.751   \n",
       "3095   0.123   0.116   0.475  -0.064   0.790  -0.176   0.536  -0.615   0.682   \n",
       "8379   0.595   0.659  -0.008  -0.817   0.122  -0.179   0.746  -0.973   0.899   \n",
       "3043   0.585   0.660   0.668  -0.770   0.790  -0.868  -0.010  -0.835   0.842   \n",
       "\n",
       "      wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  wb_424  \\\n",
       "6671   0.214   0.478  -0.168   0.038   0.020   0.220  -0.054  -0.294   0.428   \n",
       "3274   1.597   0.536  -1.116   0.560   0.774   0.698   0.567  -0.294   0.314   \n",
       "3095   0.846   0.395  -0.089  -0.104   0.771   0.297   0.199  -0.294   0.232   \n",
       "8379   1.132   0.651  -0.090   0.707   0.013   0.260   0.679  -0.294   0.557   \n",
       "3043   0.212   0.678  -0.877   0.680   0.815   0.816   0.649  -0.294   0.506   \n",
       "\n",
       "      wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  wb_433  \\\n",
       "6671   0.077   0.477   0.483   0.184   0.143   0.183  -1.227   0.092  -0.220   \n",
       "3274   0.070   0.136   0.110   0.900   1.551   0.653  -0.261   0.666  -0.787   \n",
       "3095   0.625   0.607   0.607   0.983   0.859   0.509  -0.266   0.632  -0.252   \n",
       "8379   0.066   0.876   0.923   0.198   0.171   0.563  -0.266   0.771  -0.317   \n",
       "3043   0.619   0.393   0.137   0.967   0.136   0.817  -1.106   0.730  -0.882   \n",
       "\n",
       "      wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  wb_442  \\\n",
       "6671   0.092  -0.187  -0.257  -0.146  -0.647   0.117   0.367   0.307  -0.259   \n",
       "3274   0.116  -0.187  -0.257  -0.808  -0.604   0.069   0.500  -0.659  -0.510   \n",
       "3095   0.555  -0.187  -0.257  -0.145   0.251   0.585   0.073  -0.117  -0.637   \n",
       "8379   0.728  -0.187  -0.257  -0.988  -0.742   0.078   0.574  -0.237  -0.155   \n",
       "3043   0.190  -0.187  -0.257  -1.004  -0.924   0.663   0.579  -0.833  -0.608   \n",
       "\n",
       "      wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "6671   0.160  -0.913   0.175  -0.025   0.208   0.081  \n",
       "3274   0.830  -0.097   0.990  -0.162   0.603   0.124  \n",
       "3095   0.829  -0.106   0.720  -0.670   0.394   0.155  \n",
       "8379   0.162  -0.098   0.168  -0.903   0.784  -0.088  \n",
       "3043   0.818  -0.824   0.166  -0.150   0.740  -0.222  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-1.313</td>\n",
       "      <td>-1.513</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.533</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.571</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-1.350</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-1.041</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>0.892</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>0.767</td>\n",
       "      <td>1.274</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-1.306</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.628</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.547</td>\n",
       "      <td>1.393</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.629</td>\n",
       "      <td>0.776</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>-1.258</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>1.183</td>\n",
       "      <td>-1.204</td>\n",
       "      <td>1.464</td>\n",
       "      <td>-1.440</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>689.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>4148.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.467</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-1.014</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>1.078</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.573</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.654</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>1.202</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2815.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.162</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>5185.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-1.819</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.134</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.160</td>\n",
       "      <td>-1.668</td>\n",
       "      <td>-1.535</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-1.847</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.519</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.881</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.051</td>\n",
       "      <td>0.642</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.430</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.172</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.847</td>\n",
       "      <td>-2.119</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>0.623</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1   f0v2  f0v3   f0v4   f1v0   f1v1  f1v2  f1v3  \\\n",
       "3466 3466.000    42 0.000 0.000  0.000 0.337  0.000  0.000  0.000 0.000 0.000   \n",
       "689   689.000    42 0.000 0.000  0.000 0.000 -0.413 -0.432  0.000 0.000 0.000   \n",
       "4148 4148.000    42 0.000 0.000  0.442 0.000  0.000  0.000 -0.344 0.000 0.000   \n",
       "2815 2815.000    42 0.000 0.000  0.000 0.000 -0.443  0.000  0.379 0.000 0.000   \n",
       "5185 5185.000    42 0.000 0.000 -0.265 0.000  0.000  0.264  0.000 0.000 0.000   \n",
       "\n",
       "      f1v4  f2v0   f2v1   f2v2   f2v3  f2v4  f3v0  f3v1   f3v2   f3v3  f3v4  \\\n",
       "3466 0.423 0.000  0.000  0.000  0.000 0.441 0.000 0.000  0.000 -0.408 0.000   \n",
       "689  0.000 0.000  0.000 -0.288  0.000 0.000 0.000 0.433  0.000  0.000 0.000   \n",
       "4148 0.000 0.000 -0.252  0.000  0.000 0.000 0.399 0.000  0.000  0.000 0.000   \n",
       "2815 0.000 0.000  0.000  0.000 -0.428 0.000 0.000 0.000 -0.420  0.000 0.000   \n",
       "5185 0.000 0.000 -0.437  0.000  0.000 0.000 0.000 0.000  0.000  0.364 0.000   \n",
       "\n",
       "       f4v0   f4v1  f4v2  f4v3  f4v4  f5v0  f5v1   f5v2  f5v3  f5v4   f6v0  \\\n",
       "3466  0.000  0.000 0.000 0.413 0.000 0.000 0.000 -0.292 0.000 0.000 -0.362   \n",
       "689  -0.293  0.000 0.000 0.000 0.000 0.000 0.000 -0.331 0.000 0.000  0.000   \n",
       "4148  0.000 -0.345 0.000 0.000 0.000 0.398 0.000  0.000 0.000 0.000  0.000   \n",
       "2815  0.000 -0.421 0.000 0.000 0.000 0.000 0.000 -0.432 0.000 0.000  0.000   \n",
       "5185  0.000 -0.418 0.000 0.000 0.000 0.000 0.000  0.000 0.000 0.401 -0.273   \n",
       "\n",
       "      f6v1   f6v2   f6v3   f6v4  f7v0   f7v1  f7v2  f7v3   f7v4   f8v0  f8v1  \\\n",
       "3466 0.000  0.000  0.000  0.000 0.397  0.000 0.000 0.000  0.000  0.000 0.000   \n",
       "689  0.000  0.000 -0.439  0.000 0.447  0.000 0.000 0.000  0.000 -0.363 0.000   \n",
       "4148 0.000 -0.428  0.000  0.000 0.000  0.000 0.000 0.000 -0.410  0.000 0.000   \n",
       "2815 0.000  0.000  0.000 -0.414 0.000 -0.401 0.000 0.000  0.000  0.399 0.000   \n",
       "5185 0.000  0.000  0.000  0.000 0.425  0.000 0.000 0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f8v2  f8v3  f8v4  f9v0   f9v1   f9v2  f9v3  f9v4  f10v0  f10v1  f10v2  \\\n",
       "3466 0.425 0.000 0.000 0.435  0.000  0.000 0.000 0.000  0.000  0.000  0.382   \n",
       "689  0.000 0.000 0.000 0.000  0.000 -0.407 0.000 0.000  0.000 -0.423  0.000   \n",
       "4148 0.000 0.000 0.443 0.413  0.000  0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "2815 0.000 0.000 0.000 0.000  0.000  0.000 0.000 0.446  0.000  0.358  0.000   \n",
       "5185 0.000 0.431 0.000 0.000 -0.420  0.000 0.000 0.000 -0.350  0.000  0.000   \n",
       "\n",
       "      f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  f12v2  \\\n",
       "3466  0.000  0.000  0.000 -0.442  0.000  0.000  0.000  0.443  0.000  0.000   \n",
       "689   0.000  0.000  0.000 -0.242  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148  0.000 -0.442  0.000  0.000  0.000  0.333  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.428  0.000  0.000  0.000  0.000  0.000  0.000 -0.177   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.440  0.000 -0.436  0.000   \n",
       "\n",
       "      f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  f14v2  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.389  0.000  0.000  0.000  0.000  0.000   \n",
       "689   0.000  0.436  0.000  0.000  0.000  0.357  0.000  0.000  0.000  0.401   \n",
       "4148  0.315  0.000  0.000  0.000  0.000  0.372  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.444  0.000  0.000  0.000  0.000  0.000  0.000 -0.367   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.307  0.000  0.000 -0.409  0.000   \n",
       "\n",
       "      f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6     b7  \\\n",
       "3466  0.000  0.227  0.410 -0.065  0.297  0.409 -0.155  0.187  0.120 -0.277   \n",
       "689   0.000  0.000  0.381  0.093 -0.380  0.171 -0.023 -0.239  0.030 -0.010   \n",
       "4148 -0.332  0.000 -0.183 -0.404  0.376  0.281 -0.078 -0.194 -0.309  0.291   \n",
       "2815  0.000  0.000 -0.195  0.395  0.256 -0.356  0.351  0.372  0.123 -0.366   \n",
       "5185  0.000  0.000  0.190 -0.041  0.126 -0.156 -0.393  0.322 -0.112 -0.275   \n",
       "\n",
       "         b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  lp1c0  \\\n",
       "3466  0.149  0.041  0.190 -0.210  0.184 -0.093 -0.151 -0.250  0.081  0.229   \n",
       "689  -0.136 -0.171  0.370 -0.312 -0.340  0.034 -0.386 -0.106 -0.086 -0.156   \n",
       "4148  0.084 -0.018  0.291 -0.109 -0.034  0.094 -0.146 -0.198  0.084  0.240   \n",
       "2815  0.174  0.005  0.031  0.056 -0.253  0.010  0.187  0.192 -0.027 -0.184   \n",
       "5185 -0.381  0.415 -0.138 -0.150 -0.050 -0.062 -0.079  0.227  0.070 -0.226   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  wb_352  \\\n",
       "3466 -0.003 -0.162 -0.137  0.086  0.069  ...   0.051   0.391  -0.290   0.095   \n",
       "689  -0.069 -0.074 -0.215  0.120  0.192  ...   0.313   0.004  -0.070  -0.075   \n",
       "4148  0.017  0.084  0.038  0.091 -0.116  ...   0.440   0.298  -0.155  -0.068   \n",
       "2815 -0.173 -0.118 -0.087 -0.081 -0.118  ...   0.087   0.148  -0.077   0.099   \n",
       "5185 -0.235  0.068  0.210  0.040 -0.210  ...   0.560  -0.389   0.414   0.561   \n",
       "\n",
       "      wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  \\\n",
       "3466  -0.278   0.510  -0.466  -0.006  -0.243  -0.310   0.000  -0.471  -0.081   \n",
       "689   -0.024   0.077  -0.072   0.000  -0.113  -0.070   0.000   0.068   0.257   \n",
       "4148  -0.176  -0.031  -0.164   0.000  -0.245  -0.113   0.000  -0.386  -0.049   \n",
       "2815  -0.047   0.209   0.006   0.184   0.053  -0.076   0.000  -0.043  -0.011   \n",
       "5185   0.348  -0.031   0.440   0.513   0.258   0.498   0.000   0.361   0.462   \n",
       "\n",
       "      wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  \\\n",
       "3466  -0.231  -0.041   0.140  -0.033  -0.216   0.513  -0.304   0.415  -0.237   \n",
       "689    0.022   0.074  -0.064  -0.042  -0.094   0.092  -0.079  -0.093   0.092   \n",
       "4148  -0.094  -0.079  -0.048  -0.032  -0.374  -0.067  -0.122   0.284  -0.105   \n",
       "2815  -0.010   0.005   0.166  -0.032  -0.054   0.227  -0.083  -0.096   0.059   \n",
       "5185   0.432   0.470   0.500   0.599   0.352   0.220   0.467  -0.412   0.342   \n",
       "\n",
       "      wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  \\\n",
       "3466   0.000   0.000   0.583   0.543  -0.018  -0.503   0.483   0.267  -0.239   \n",
       "689    0.000   0.000  -0.007   0.035   0.179  -0.020  -0.014   0.012  -0.065   \n",
       "4148   0.000   0.000   0.475  -0.260  -0.100  -0.260   0.186   0.079  -0.431   \n",
       "2815   0.000   0.000  -0.011   0.187   0.068  -0.049   0.200   0.052  -0.070   \n",
       "5185   0.000   0.000  -0.017   0.314   0.434   0.301  -0.077  -0.395   0.463   \n",
       "\n",
       "      wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  wb_388  \\\n",
       "3466   0.358   0.089   0.559  -0.257  -0.337  -0.747   0.662  -1.313  -1.513   \n",
       "689    0.160   0.206   0.066  -0.130  -0.394  -0.452   0.323  -0.889  -0.836   \n",
       "4148   0.448  -0.070  -0.083  -0.174   0.029  -0.563   0.860  -0.290  -0.184   \n",
       "2815  -0.057   0.059   0.188  -0.066  -0.591   0.535   0.595  -0.741  -0.801   \n",
       "5185  -0.032   0.544  -0.275   0.352  -1.819  -0.528   0.014  -0.259  -2.134   \n",
       "\n",
       "      wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  wb_397  \\\n",
       "3466   0.027   1.533  -0.593  -0.932  -0.272   0.520  -0.512  -0.399  -1.568   \n",
       "689    0.031   0.125   0.161  -0.063  -0.272   0.238  -0.505  -0.359  -0.145   \n",
       "4148   0.035   0.132  -0.971  -0.925  -0.272   0.417  -0.684  -0.489  -0.151   \n",
       "2815   0.590   0.133  -0.140  -0.167  -0.272   0.701  -0.654   0.531  -0.759   \n",
       "5185   1.127   1.160  -1.668  -1.535  -0.272   0.455  -1.847   0.835  -0.148   \n",
       "\n",
       "      wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  wb_406  \\\n",
       "3466   0.452   1.571  -1.192  -1.381   0.080  -1.350  -0.664  -0.984  -0.574   \n",
       "689    0.344   0.144  -0.822  -0.539  -0.237  -0.431  -0.234  -0.340  -0.440   \n",
       "4148   0.278   0.134   1.467  -0.127   0.058  -0.118  -1.014  -0.614  -0.542   \n",
       "2815   0.493   1.122   0.705  -0.658  -0.353  -0.704  -0.225  -0.048   0.565   \n",
       "5185   0.519   1.084   0.881  -0.701  -0.481  -0.128  -0.787  -0.041  -0.573   \n",
       "\n",
       "      wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  wb_415  \\\n",
       "3466   0.579   0.559  -1.041  -0.658   0.892  -1.215  -1.324  -0.590   0.767   \n",
       "689   -0.184   0.092   0.329  -0.226   0.099  -0.675   0.584  -0.337   0.168   \n",
       "4148   0.462   0.505   0.936  -0.661   0.102  -0.829   1.078  -0.615   0.630   \n",
       "2815  -0.077   0.231   0.601  -0.655   0.124  -0.164   0.749  -0.662   0.162   \n",
       "5185   0.368   0.284   0.238  -0.186   0.105  -0.179   0.723  -0.748   1.058   \n",
       "\n",
       "      wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  wb_424  \\\n",
       "3466   1.274   0.533  -1.306   0.747   0.019   0.601   0.628  -0.294   0.547   \n",
       "689    0.217   0.125  -0.681   0.054   0.025   0.220   0.006  -0.294   0.151   \n",
       "4148   0.210   0.554  -0.086   0.549   0.025   0.681   0.573  -0.294   0.377   \n",
       "2815   1.027   0.053  -0.678   0.410   0.679   0.684  -0.010  -0.294   0.368   \n",
       "5185   1.051   0.642  -0.089   0.430   1.117   0.643   0.689  -0.294   0.172   \n",
       "\n",
       "      wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  wb_433  \\\n",
       "3466   1.393   0.512   0.943   1.488   1.629   0.776  -1.526   0.682  -0.744   \n",
       "689    0.679   0.355   0.512   0.204   0.157   0.185  -0.930   0.106  -0.238   \n",
       "4148   0.081   0.154   0.148   0.214   0.165   0.856  -0.240   0.654  -1.086   \n",
       "2815   0.788   0.755   0.801   0.853   0.161   0.268  -0.893   0.108  -0.232   \n",
       "5185   1.032   0.817   0.825   1.031   1.196   0.847  -2.119   0.935  -1.902   \n",
       "\n",
       "      wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  wb_442  \\\n",
       "3466   0.423  -0.187  -0.257  -1.024  -1.258   0.684   0.651  -1.219  -0.268   \n",
       "689    0.353  -0.187  -0.257  -0.152  -0.504   0.589   0.038  -0.478  -0.320   \n",
       "4148   0.112  -0.187  -0.257  -0.967   0.651   0.128   0.436  -0.541  -0.438   \n",
       "2815   0.600  -0.187  -0.257  -0.150  -0.575   0.644   0.299  -0.673  -0.293   \n",
       "5185   0.623  -0.187  -0.257  -0.147   0.758   0.429   0.118  -0.112  -0.704   \n",
       "\n",
       "      wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "3466   1.183  -1.204   1.464  -1.440   0.629  -0.238  \n",
       "689    0.164  -0.779   0.704  -0.636   0.208  -0.001  \n",
       "4148   1.202  -0.780   0.178  -0.162   0.539  -0.171  \n",
       "2815   0.161  -0.079   0.902  -0.704   0.305  -0.067  \n",
       "5185   1.211  -0.105   0.997  -0.701   0.539   0.324  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.891</td>\n",
       "      <td>1.166</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-1.152</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.774</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-1.488</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.885</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-1.367</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-1.436</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.727</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-1.399</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.842</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.878</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.612</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.589</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.869</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.853</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.965</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.131</td>\n",
       "      <td>1.269</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>0.607</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-1.667</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.188</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.002</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>1.288</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.474</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.348</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>1.565</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>1.493</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.238</td>\n",
       "      <td>-2.355</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.338</td>\n",
       "      <td>2.214</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.140</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-2.262</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-2.022</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>2.004</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.389</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.202</td>\n",
       "      <td>2.327</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>1.744</td>\n",
       "      <td>1.105</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>1.977</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>2.140</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0  f0v1   f0v2  f0v3  f0v4  f1v0  f1v1   f1v2   f1v3  \\\n",
       "7217 7217.000    42 -0.437 0.000  0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "8291 8291.000    42  0.000 0.000 -0.391 0.000 0.000 0.000 0.000  0.000  0.419   \n",
       "4607 4607.000    42  0.000 0.304  0.000 0.000 0.000 0.000 0.000 -0.418  0.000   \n",
       "5114 5114.000    42  0.000 0.000 -0.418 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "1859 1859.000    42  0.000 0.000  0.000 0.000 0.412 0.000 0.000  0.000 -0.342   \n",
       "\n",
       "       f1v4   f2v0   f2v1   f2v2  f2v3  f2v4  f3v0   f3v1   f3v2   f3v3  f3v4  \\\n",
       "7217 -0.431  0.000 -0.291  0.000 0.000 0.000 0.000  0.000  0.000 -0.250 0.000   \n",
       "8291  0.000 -0.389  0.000  0.000 0.000 0.000 0.000  0.000  0.000  0.000 0.290   \n",
       "4607  0.000  0.000  0.000 -0.406 0.000 0.000 0.000 -0.419  0.000  0.000 0.000   \n",
       "5114  0.395 -0.437  0.000  0.000 0.000 0.000 0.000  0.000 -0.434  0.000 0.000   \n",
       "1859  0.000  0.000  0.000 -0.344 0.000 0.000 0.439  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f4v0  f4v1   f4v2  f4v3  f4v4  f5v0   f5v1  f5v2  f5v3   f5v4  f6v0  \\\n",
       "7217 0.000 0.000  0.430 0.000 0.000 0.000  0.000 0.000 0.000  0.410 0.000   \n",
       "8291 0.000 0.000  0.000 0.000 0.443 0.000  0.000 0.000 0.000 -0.257 0.000   \n",
       "4607 0.438 0.000  0.000 0.000 0.000 0.000  0.000 0.000 0.000  0.443 0.000   \n",
       "5114 0.000 0.000  0.440 0.000 0.000 0.000 -0.436 0.000 0.000  0.000 0.000   \n",
       "1859 0.000 0.000 -0.373 0.000 0.000 0.000  0.000 0.000 0.000  0.440 0.000   \n",
       "\n",
       "       f6v1   f6v2   f6v3   f6v4  f7v0   f7v1   f7v2   f7v3  f7v4   f8v0  \\\n",
       "7217 -0.411  0.000  0.000  0.000 0.000  0.446  0.000  0.000 0.000  0.000   \n",
       "8291  0.000  0.000 -0.430  0.000 0.000  0.000 -0.418  0.000 0.000 -0.224   \n",
       "4607  0.000 -0.412  0.000  0.000 0.000 -0.434  0.000  0.000 0.000 -0.354   \n",
       "5114  0.000  0.000  0.000 -0.412 0.000  0.000  0.000  0.362 0.000  0.000   \n",
       "1859  0.000  0.000 -0.411  0.000 0.000  0.000  0.000 -0.313 0.000  0.000   \n",
       "\n",
       "      f8v1  f8v2   f8v3  f8v4  f9v0   f9v1  f9v2  f9v3  f9v4  f10v0  f10v1  \\\n",
       "7217 0.000 0.000  0.000 0.255 0.223  0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "8291 0.000 0.000  0.000 0.000 0.000  0.000 0.000 0.000 0.424  0.000  0.000   \n",
       "4607 0.000 0.000  0.000 0.000 0.000 -0.405 0.000 0.000 0.000 -0.351  0.000   \n",
       "5114 0.000 0.000 -0.443 0.000 0.000  0.187 0.000 0.000 0.000 -0.399  0.000   \n",
       "1859 0.000 0.446  0.000 0.000 0.349  0.000 0.000 0.000 0.000 -0.339  0.000   \n",
       "\n",
       "      f10v2  f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  \\\n",
       "7217  0.325  0.000  0.000  0.000  0.000  0.444  0.000  0.000  0.000 -0.337   \n",
       "8291 -0.343  0.000  0.000  0.000  0.000  0.000  0.294  0.000  0.000  0.408   \n",
       "4607  0.000  0.000  0.000  0.000  0.000 -0.395  0.000  0.000  0.000  0.378   \n",
       "5114  0.000  0.000  0.000  0.000 -0.416  0.000  0.000  0.000  0.000  0.447   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.386  0.000  0.000  0.000 -0.435   \n",
       "\n",
       "      f12v2  f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  \\\n",
       "7217  0.000  0.000  0.000  0.443  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000 -0.326  0.000  0.000  0.324   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.372  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.419  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000 -0.265  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f14v2  f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "7217  0.000  0.000  0.389 -0.065  0.393 -0.178 -0.007 -0.305 -0.298  0.252   \n",
       "8291  0.000  0.000  0.000  0.418  0.086 -0.061  0.131  0.193  0.094  0.142   \n",
       "4607  0.397  0.000  0.000 -0.421  0.434  0.280 -0.198 -0.349 -0.178 -0.264   \n",
       "5114  0.000  0.000  0.346  0.097 -0.287  0.039 -0.067 -0.312 -0.142  0.389   \n",
       "1859 -0.446  0.000  0.000 -0.346 -0.194  0.324  0.164 -0.045 -0.397  0.403   \n",
       "\n",
       "         b7     b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  \\\n",
       "7217  0.291 -0.429  0.275  0.119 -0.112  0.321 -0.383 -0.231 -0.184  0.025   \n",
       "8291  0.232  0.158 -0.350  0.075 -0.410  0.028 -0.125 -0.023 -0.213  0.242   \n",
       "4607  0.181  0.291  0.178  0.050  0.260  0.189  0.150  0.042 -0.069 -0.141   \n",
       "5114  0.014  0.229  0.082 -0.131 -0.058 -0.333  0.026 -0.312  0.173 -0.059   \n",
       "1859 -0.243 -0.165  0.222  0.373 -0.174  0.009 -0.416  0.101  0.068  0.206   \n",
       "\n",
       "      lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  \\\n",
       "7217  0.093  0.054 -0.007 -0.064 -0.003 -0.035  ...   0.358  -0.423  -0.064   \n",
       "8291  0.087 -0.014  0.150 -0.173 -0.107 -0.075  ...   0.380  -0.122   0.308   \n",
       "4607  0.114  0.117 -0.167  0.087  0.077  0.174  ...  -0.046   0.314  -0.083   \n",
       "5114  0.063 -0.006  0.009 -0.165  0.155 -0.147  ...   0.364  -0.035   0.326   \n",
       "1859 -0.198 -0.152 -0.151 -0.071 -0.165 -0.041  ...   0.156  -0.048   0.069   \n",
       "\n",
       "      wb_352  wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  \\\n",
       "7217   0.031   0.363  -0.081   0.527   0.000   0.457   0.567   0.000   0.346   \n",
       "8291   0.367   0.252  -0.024  -0.025  -0.010  -0.124   0.365   0.000  -0.203   \n",
       "4607  -0.071  -0.153   0.355  -0.149   0.000  -0.140   0.066   0.000  -0.187   \n",
       "5114   0.461   0.126  -0.026  -0.046  -0.005  -0.067   0.325   0.000  -0.133   \n",
       "1859   0.597   0.036  -0.030  -0.313  -0.022  -0.137   0.047   0.000  -0.105   \n",
       "\n",
       "      wb_361  wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  \\\n",
       "7217  -0.057  -0.100  -0.091  -0.012   0.173   0.311  -0.049   0.513  -0.291   \n",
       "8291   0.360   0.312   0.312  -0.065   0.411   0.234  -0.038   0.322  -0.047   \n",
       "4607  -0.050  -0.108  -0.099  -0.129  -0.028  -0.152   0.379  -0.096   0.207   \n",
       "5114   0.389   0.296   0.330  -0.063   0.473   0.140   0.223   0.276  -0.075   \n",
       "1859   0.108   0.083   0.076  -0.069   0.625  -0.001  -0.059   0.100  -0.080   \n",
       "\n",
       "      wb_370  wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  \\\n",
       "7217   0.312   0.000   0.000   0.000  -0.408  -0.122   0.465  -0.379  -0.283   \n",
       "8291  -0.097   0.000   0.000   0.000  -0.047  -0.106  -0.185  -0.075  -0.093   \n",
       "4607  -0.120   0.000   0.000   0.000   0.374  -0.088  -0.185   0.348  -0.120   \n",
       "5114  -0.110   0.000   0.000  -0.028  -0.046  -0.098  -0.078   0.087  -0.022   \n",
       "1859  -0.150   0.000   0.000  -0.006  -0.221   0.258  -0.238  -0.084   0.227   \n",
       "\n",
       "      wb_379  wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  \\\n",
       "7217  -0.047   0.221  -0.084  -0.084   0.294  -0.821   0.733   0.344  -1.460   \n",
       "8291   0.351  -0.048   0.378  -0.041   0.158  -0.154  -0.461   0.079  -0.261   \n",
       "4607  -0.072   0.418  -0.080  -0.048  -0.154  -0.556   0.491   0.574  -0.821   \n",
       "5114   0.450  -0.053   0.483  -0.021   0.036  -0.191  -0.528   0.070  -0.238   \n",
       "1859   0.399  -0.049   0.596   0.324  -0.026  -0.072  -0.470  -0.221  -0.248   \n",
       "\n",
       "      wb_388  wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  \\\n",
       "7217  -0.210   0.891   1.166  -0.724  -0.831  -0.272   0.153  -1.152  -0.689   \n",
       "8291  -0.216   0.030   0.842  -0.649  -0.707  -0.272   0.195  -0.659  -0.489   \n",
       "4607  -0.768   0.809   0.135  -0.174  -0.754  -0.272   0.853  -0.611  -0.497   \n",
       "5114  -1.667   0.033   1.249  -0.514  -0.642  -0.272   0.198  -0.549  -0.405   \n",
       "1859  -0.188   0.021   2.238  -2.355  -0.512  -0.272   0.289  -0.076   0.522   \n",
       "\n",
       "      wb_397  wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  \\\n",
       "7217  -0.148   0.083   0.774   1.012  -0.144   0.518  -0.938  -1.488  -0.682   \n",
       "8291  -0.145   0.479   0.874   0.698  -0.630  -0.370  -0.117  -0.774  -0.595   \n",
       "4607  -0.154   0.392   0.142  -0.610  -0.667   0.444  -0.122  -0.257  -0.226   \n",
       "5114  -0.154   0.188   1.292   1.002  -0.491  -0.255  -0.353  -0.649  -0.568   \n",
       "1859  -0.144   0.338   2.214   0.347  -0.540   1.140  -0.139  -0.574  -2.262   \n",
       "\n",
       "      wb_406  wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  \\\n",
       "7217   0.715   0.537   0.420   0.885  -0.829   0.960  -1.367   0.460  -0.942   \n",
       "8291  -0.482  -0.003   0.169   0.003  -0.101   0.878  -0.207   0.612  -0.586   \n",
       "4607   0.553   0.449   0.514   0.539  -0.582   0.122  -0.771   0.008  -0.615   \n",
       "5114  -0.512  -0.154   0.133   0.007  -0.348   1.288  -0.826   0.794  -0.372   \n",
       "1859  -0.525   0.618   0.684  -2.022  -0.064   2.004  -0.143   0.292  -0.088   \n",
       "\n",
       "      wb_415  wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  \\\n",
       "7217   0.174   0.425   0.349  -1.436   0.688   0.025   0.618   0.688  -0.294   \n",
       "8291   0.775   0.907   0.589  -0.092   0.062   0.017   0.216   0.635  -0.294   \n",
       "4607   0.157   0.223   0.268  -0.613   0.589   0.025   0.965  -0.362  -0.294   \n",
       "5114   1.162   1.474   0.366  -0.090   0.086   0.020   0.236   0.728  -0.294   \n",
       "1859   0.389   2.300   0.350  -0.090   1.384   0.007   0.212   0.347  -0.294   \n",
       "\n",
       "      wb_424  wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  \\\n",
       "7217   0.191   0.078   0.156   0.148   0.221   0.848   0.379  -0.262   0.727   \n",
       "8291  -0.266   0.711   0.714   0.729   0.207   0.913   0.710  -0.272   0.686   \n",
       "4607   0.464   0.080   0.145   0.131   1.269   0.167   0.443  -0.838   0.098   \n",
       "5114  -0.124   0.797   0.976   1.074   0.190   1.348   0.497  -1.686   0.693   \n",
       "1859   0.554   0.430   0.359   0.363   0.202   2.327   0.339  -0.255   0.353   \n",
       "\n",
       "      wb_433  wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  \\\n",
       "7217  -0.833   0.253  -0.187  -0.257  -0.159  -0.837   0.120   0.453  -0.839   \n",
       "8291  -0.550   0.102  -0.187  -0.257  -0.159  -0.007   0.150  -0.214  -0.121   \n",
       "4607  -0.853   0.607  -0.187  -0.257  -0.159  -0.563   0.617   0.484  -0.619   \n",
       "5114  -0.543   0.107  -0.187  -0.257  -0.139  -0.298   0.128  -0.066   0.178   \n",
       "1859  -0.241   0.419  -0.187  -0.257  -0.153   1.744   1.105   0.890  -0.111   \n",
       "\n",
       "      wb_442  wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "7217  -0.436   0.201  -1.399   0.177  -0.155   0.355   0.278  \n",
       "8291  -0.616   0.869  -0.122   0.849  -0.662   0.549   0.071  \n",
       "4607  -0.155   0.156  -0.802   0.169  -0.319   0.421  -0.138  \n",
       "5114  -0.436   1.565  -0.904   1.493  -0.477   0.368  -0.001  \n",
       "1859  -0.457   1.977  -0.092   2.140  -0.523   0.362  -0.158  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy_inet_decision_function_fv_metric\n",
      "\ttraining         \t (min:    0.680, max:    0.940, cur:    0.930)\n",
      "\tvalidation       \t (min:    0.762, max:    0.947, cur:    0.929)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.166, max:    0.573, cur:    0.186)\n",
      "\tvalidation       \t (min:    0.147, max:    0.477, cur:    0.186)\n",
      "Training Time: 0:08:17\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 449)]             0         \n",
      "_________________________________________________________________\n",
      "hidden1_1056 (Dense)         (None, 1056)              475200    \n",
      "_________________________________________________________________\n",
      "activation1_relu (Activation (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "dropout1_0.2 (Dropout)       (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "hidden2_512 (Dense)          (None, 512)               541184    \n",
      "_________________________________________________________________\n",
      "activation2_relu (Activation (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout2_0.1 (Dropout)       (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output_122 (Dense)           (None, 122)               62586     \n",
      "=================================================================\n",
      "Total params: 1,078,970\n",
      "Trainable params: 1,078,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3055bb8dc1f246b09fc570e826d66a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Target Lambda 0.9447200000000001\n",
      "Binary Crossentropy Target Lambda 0.16287197450032392\n",
      "Accuracy Lambda Decision 0.89168\n",
      "Binary Crossentropy Lambda Decision 0.26649287671306426\n",
      "Accuracy Target Decision 0.85976\n",
      "Binary Crossentropy Target Decision 0.4361215391042682\n"
     ]
    }
   ],
   "source": [
    "acc_target_lambda_list = []\n",
    "bc_target_lambda_list = []\n",
    "\n",
    "acc_lambda_decision_list = []\n",
    "bc_lambda_decision_list = []\n",
    "\n",
    "acc_target_decision_list = []\n",
    "bc_target_decision_list = []\n",
    "\n",
    "decision_function_parameters_list = []\n",
    "decision_functio_list = []\n",
    "\n",
    "for lambda_net in tqdm(lambda_net_dataset_test.lambda_net_list):\n",
    "    \n",
    "    target_function_parameters = lambda_net.target_function_parameters\n",
    "    target_function = lambda_net.target_function\n",
    "    \n",
    "    X_test_lambda = lambda_net.X_test_lambda\n",
    "    y_test_lambda = lambda_net.y_test_lambda\n",
    "    \n",
    "    network = lambda_net.network\n",
    "    network_parameters = lambda_net.network_parameters\n",
    "    \n",
    "    if config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['nas_type']['convolution_layers'] != 'SEQUENTIAL'):\n",
    "        network_parameters, network_parameters_flat = restructure_data_cnn_lstm(np.array([network_parameters]), config, subsequences=None)    \n",
    "      \n",
    "    decision_function_parameters= model.predict(np.array([network_parameters]))[0]\n",
    "    decision_function = generate_decision_tree_from_array(decision_function_parameters, config)\n",
    "    \n",
    "    decision_function_parameters_list.append(decision_function_parameters)\n",
    "    decision_functio_list.append(decision_function)\n",
    "    \n",
    "    y_test_network = network.predict(X_test_lambda)\n",
    "    y_test_decision_function = decision_function.predict_proba(X_test_lambda)\n",
    "    y_test_target_function = target_function.predict_proba(X_test_lambda)  \n",
    "    \n",
    "    acc_target_lambda = accuracy_score(np.round(y_test_target_function), np.round(y_test_network))\n",
    "    bc_target_lambda = log_loss(np.round(y_test_target_function), y_test_network, labels=[0, 1])\n",
    "    \n",
    "    acc_lambda_decision = accuracy_score(np.round(y_test_network), np.round(y_test_decision_function))\n",
    "    bc_lambda_decision = log_loss(np.round(y_test_network), y_test_decision_function, labels=[0, 1])        \n",
    "    \n",
    "    acc_target_decision = accuracy_score(np.round(y_test_target_function), np.round(y_test_decision_function))\n",
    "    bc_target_decision = log_loss(np.round(y_test_target_function), y_test_decision_function, labels=[0, 1])   \n",
    "    \n",
    "    \n",
    "    acc_target_lambda_list.append(acc_target_lambda)\n",
    "    bc_target_lambda_list.append(bc_target_lambda)\n",
    "\n",
    "    acc_lambda_decision_list.append(acc_lambda_decision)\n",
    "    bc_lambda_decision_list.append(bc_lambda_decision)\n",
    "\n",
    "    acc_target_decision_list.append(acc_target_decision)\n",
    "    bc_target_decision_list.append(bc_target_decision)\n",
    "    \n",
    "\n",
    "acc_target_lambda_array = np.array(acc_target_lambda_list)\n",
    "bc_target_lambda_array = np.array(bc_target_lambda_list)\n",
    "\n",
    "acc_lambda_decision_array = np.array(acc_lambda_decision_list)\n",
    "bc_lambda_decision_array = np.array(bc_lambda_decision_list)\n",
    "\n",
    "acc_target_decision_array = np.array(acc_target_decision_list)\n",
    "bc_target_decision_array = np.array(bc_target_decision_list)\n",
    "    \n",
    "    \n",
    "acc_target_lambda = np.mean(acc_target_lambda_array)\n",
    "bc_target_lambda = np.mean(bc_target_lambda_array[~np.isnan(bc_target_lambda_array)])\n",
    "\n",
    "acc_lambda_decision = np.mean(acc_lambda_decision_array)\n",
    "bc_lambda_decision = np.mean(bc_lambda_decision_array[~np.isnan(bc_lambda_decision_array)])\n",
    "\n",
    "acc_target_decision = np.mean(acc_target_decision_array)\n",
    "bc_target_decision = np.mean(bc_target_decision_array[~np.isnan(bc_target_decision_array)])\n",
    "\n",
    "\n",
    "print('Accuracy Target Lambda', acc_target_lambda)\n",
    "print('Binary Crossentropy Target Lambda', bc_target_lambda)\n",
    "print('Accuracy Lambda Decision', acc_lambda_decision)\n",
    "print('Binary Crossentropy Lambda Decision', bc_lambda_decision)\n",
    "print('Accuracy Target Decision', acc_target_decision)\n",
    "print('Binary Crossentropy Target Decision', bc_target_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02786644, 0.91616718, 0.60624075, 0.88591947, 0.03953329],\n",
       "       [0.78132991, 0.57253603, 0.19496315, 0.11438026, 0.57235053],\n",
       "       [0.84345932, 0.11760448, 0.57262199, 0.41297647, 0.93908842],\n",
       "       ...,\n",
       "       [0.8646321 , 0.14561154, 0.40179704, 0.07841369, 0.68174788],\n",
       "       [0.35746751, 0.99758493, 0.38932111, 0.83535797, 0.9646232 ],\n",
       "       [0.11789361, 0.17350065, 0.24059135, 0.84522561, 0.97066957]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29161674,  0.4584652 , -0.43084228, ...,  0.04972852,\n",
       "         0.22788163,  0.01619679],\n",
       "       [ 0.08819469, -0.0678779 , -0.11088647, ...,  0.35068122,\n",
       "         0.10858545,  0.01877911],\n",
       "       [ 0.2642992 ,  0.71580225,  0.37507978, ..., -0.5395654 ,\n",
       "         0.38570604, -0.26972985],\n",
       "       [ 0.377968  ,  0.14853731, -0.23176512, ..., -0.1642764 ,\n",
       "         0.08135869,  0.10158017],\n",
       "       [ 0.58881587,  0.44281018, -0.23438074, ..., -0.23245402,\n",
       "         0.6664687 , -0.12968457]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45681953, -0.44793075,  0.2816615 , -0.0492938 , -0.06176101,\n",
       "        0.4203166 ,  0.5660745 , -0.44356108, -0.08164763,  0.        ,\n",
       "        0.07437179, -0.41223946, -0.5200589 ,  0.        ,  0.5299024 ,\n",
       "        0.58825123,  0.6841029 , -0.36733192, -0.5526599 , -0.41840613,\n",
       "       -0.10449886, -0.41994047, -0.45792708, -0.576029  ,  0.31862995,\n",
       "        0.59918153, -0.06940714,  0.52732533, -0.07208212,  0.5470178 ,\n",
       "       -0.47873873,  0.493543  ,  0.51952726,  0.4665848 , -0.0254315 ,\n",
       "        0.02107218, -0.01309372, -0.02824431,  0.5001492 ,  0.        ,\n",
       "       -0.49418107,  0.55401963,  0.47315872,  0.520664  ,  0.61285615,\n",
       "        0.6254791 ,  0.47207806, -0.04172304,  0.45049247, -0.07797154,\n",
       "        0.393748  ,  0.        ,  0.        , -0.02696179, -0.04680119,\n",
       "        0.49806637, -0.45163694, -0.0701941 , -0.4050519 ,  0.59158844,\n",
       "       -0.06446765,  0.5156172 , -0.41351524,  0.38721523], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77346766],\n",
       "       [-0.57821596],\n",
       "       [ 0.55111617],\n",
       "       [-0.25357288],\n",
       "       [-0.21251057],\n",
       "       [ 0.90730554],\n",
       "       [ 1.0477254 ],\n",
       "       [-0.7378019 ],\n",
       "       [-0.16351095],\n",
       "       [-0.2718169 ],\n",
       "       [ 0.27941492],\n",
       "       [-0.77445775],\n",
       "       [-0.6732928 ],\n",
       "       [-0.15400417],\n",
       "       [ 0.65436   ],\n",
       "       [ 1.0790985 ],\n",
       "       [ 0.8534902 ],\n",
       "       [-0.7536076 ],\n",
       "       [-0.5020092 ],\n",
       "       [-0.8960187 ],\n",
       "       [-0.21985874],\n",
       "       [-0.66866595],\n",
       "       [-0.6072842 ],\n",
       "       [-0.5372026 ],\n",
       "       [ 0.18291588],\n",
       "       [ 0.97887987],\n",
       "       [-0.05379323],\n",
       "       [ 1.1296959 ],\n",
       "       [-0.22503386],\n",
       "       [ 0.7816153 ],\n",
       "       [-0.8748729 ],\n",
       "       [ 1.0477587 ],\n",
       "       [ 1.1145122 ],\n",
       "       [ 0.7316604 ],\n",
       "       [-0.09255359],\n",
       "       [ 0.07171552],\n",
       "       [ 0.00931237],\n",
       "       [ 0.28333545],\n",
       "       [ 0.8939037 ],\n",
       "       [-0.29366696],\n",
       "       [-0.3580425 ],\n",
       "       [ 0.87647027],\n",
       "       [ 0.89232785],\n",
       "       [ 0.9018052 ],\n",
       "       [ 0.97149974],\n",
       "       [ 1.1072325 ],\n",
       "       [ 0.91012734],\n",
       "       [-0.27103508],\n",
       "       [ 0.920401  ],\n",
       "       [-0.23905578],\n",
       "       [ 0.4427272 ],\n",
       "       [-0.18703991],\n",
       "       [-0.2574015 ],\n",
       "       [-0.14070949],\n",
       "       [-0.00701872],\n",
       "       [ 0.8501454 ],\n",
       "       [-0.3472605 ],\n",
       "       [-0.12021901],\n",
       "       [-0.7099311 ],\n",
       "       [ 1.0856591 ],\n",
       "       [-1.0580597 ],\n",
       "       [ 0.94303805],\n",
       "       [-0.8735245 ],\n",
       "       [ 0.49814296]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3486251], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_test_network).ravel()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_test_decision_function).ravel()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.908, 0.948, 0.992, 0.94 , 0.816, 0.848, 0.824, 0.92 , 0.944,\n",
       "       0.972, 0.856, 0.816, 0.904, 0.964, 0.968, 0.996, 0.976, 0.92 ,\n",
       "       0.988, 0.884, 0.816, 0.804, 0.88 , 0.984, 0.872, 0.976, 0.976,\n",
       "       0.848, 0.72 , 1.   , 0.928, 0.772, 0.98 , 1.   , 0.908, 0.8  ,\n",
       "       0.944, 0.824, 0.884, 0.948, 0.688, 0.784, 0.988, 0.872, 0.804,\n",
       "       0.824, 0.716, 0.988, 0.892, 0.78 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_lambda_decision_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BENCHMARK RANDOM GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3a710d2a84f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################################################################\n",
    "#################################################################################################### END WORKING CODE ####################################################################################################\n",
    "##########################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not optimize_decision_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if not optimize_decision_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
