{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'fully_grown': True,                      \n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'make_classification', # 'make_classification' 'random_decision_tree' 'random_decision_tree_trained'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 1000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [64],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [1056, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.2, 0.1],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 2000, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, \n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 5000,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize1000_numLNets10000_var3_class2_make_classification_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1056-512_drop0.2-0.1e2000b256_adam\n",
      "lNetSize1000_numLNets10000_var3_class2_make_classification_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   4 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-3)]: Done 161 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-3)]: Done 5006 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:    9.5s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:    9.2s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if noise_injected_level > 0:\n",
    "    lambda_net_dataset_training = load_lambda_nets(config, no_noise=True, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_evaluation = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_training, test_split=0.1)\n",
    "    _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_evaluation, test_split=test_size)\n",
    "    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8955, 367)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 367)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 367)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>6671.000</td>\n",
       "      <td>42</td>\n",
       "      <td>1.336</td>\n",
       "      <td>0.954</td>\n",
       "      <td>2.105</td>\n",
       "      <td>1.615</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>1.760</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>-2.711</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.551</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.790</td>\n",
       "      <td>-1.251</td>\n",
       "      <td>1.539</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.790</td>\n",
       "      <td>1.099</td>\n",
       "      <td>0.355</td>\n",
       "      <td>1.657</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-3.041</td>\n",
       "      <td>3.319</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>1.859</td>\n",
       "      <td>-1.850</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.999</td>\n",
       "      <td>-1.797</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.560</td>\n",
       "      <td>2.560</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>0.572</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>3274.000</td>\n",
       "      <td>42</td>\n",
       "      <td>1.245</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>0.149</td>\n",
       "      <td>1.065</td>\n",
       "      <td>-1.209</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-1.837</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-1.022</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>1.648</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.819</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-1.468</td>\n",
       "      <td>-1.644</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-2.253</td>\n",
       "      <td>2.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.067</td>\n",
       "      <td>1.067</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>2.392</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>-1.698</td>\n",
       "      <td>1.851</td>\n",
       "      <td>2.775</td>\n",
       "      <td>-3.009</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.479</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.485</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.581</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.824</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.519</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>3095.000</td>\n",
       "      <td>42</td>\n",
       "      <td>1.458</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-1.689</td>\n",
       "      <td>-1.649</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>0.076</td>\n",
       "      <td>1.561</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1.302</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.675</td>\n",
       "      <td>1.337</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>1.664</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-1.412</td>\n",
       "      <td>-2.145</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-2.188</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-1.813</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.764</td>\n",
       "      <td>-1.528</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-2.436</td>\n",
       "      <td>2.248</td>\n",
       "      <td>0.708</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.525</td>\n",
       "      <td>2.221</td>\n",
       "      <td>-2.013</td>\n",
       "      <td>-2.686</td>\n",
       "      <td>2.741</td>\n",
       "      <td>2.922</td>\n",
       "      <td>-2.846</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.719</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.745</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.731</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.564</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.752</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>1.162</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.822</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.658</td>\n",
       "      <td>1.165</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>1.072</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>8379.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-2.903</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-1.663</td>\n",
       "      <td>1.242</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-1.079</td>\n",
       "      <td>-1.599</td>\n",
       "      <td>1.813</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.894</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>1.044</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.055</td>\n",
       "      <td>1.343</td>\n",
       "      <td>-1.596</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-1.239</td>\n",
       "      <td>1.390</td>\n",
       "      <td>0.846</td>\n",
       "      <td>-1.007</td>\n",
       "      <td>-0.674</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>1.630</td>\n",
       "      <td>-1.630</td>\n",
       "      <td>-1.954</td>\n",
       "      <td>1.753</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.673</td>\n",
       "      <td>3.045</td>\n",
       "      <td>-1.217</td>\n",
       "      <td>1.217</td>\n",
       "      <td>2.448</td>\n",
       "      <td>-2.445</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.280</td>\n",
       "      <td>2.448</td>\n",
       "      <td>-2.422</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.724</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.805</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.972</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.839</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.302</td>\n",
       "      <td>1.101</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.721</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.903</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.964</td>\n",
       "      <td>-0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>2.538</td>\n",
       "      <td>0.690</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.612</td>\n",
       "      <td>1.229</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>0.275</td>\n",
       "      <td>1.069</td>\n",
       "      <td>2.365</td>\n",
       "      <td>-1.088</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.475</td>\n",
       "      <td>2.032</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-2.102</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>2.560</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.136</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-2.904</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>2.708</td>\n",
       "      <td>-3.134</td>\n",
       "      <td>-2.844</td>\n",
       "      <td>2.974</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>1.325</td>\n",
       "      <td>2.535</td>\n",
       "      <td>-2.535</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.573</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.623</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.621</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.517</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "6671 6671.000    42  1.336  0.954  2.105  1.615 -0.184  1.760  0.456 -0.705   \n",
       "3274 3274.000    42  1.245 -0.395 -1.320  0.149  1.065 -1.209 -0.191 -1.837   \n",
       "3095 3095.000    42  1.458 -0.321 -1.689 -1.649 -1.050  0.076  1.561  0.678   \n",
       "8379 8379.000    42  0.026 -2.903 -0.456 -1.663  1.242 -0.196 -1.079 -1.599   \n",
       "3043 3043.000    42 -0.391 -0.977  2.538  0.690 -0.600 -0.980  0.128  0.612   \n",
       "\n",
       "       f2v2  f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "6671 -2.711 0.765  0.551  1.249 -0.256  0.890 -0.162  0.008 -0.790 -1.251   \n",
       "3274  1.130 0.554  0.535 -1.022  0.068 -0.672 -0.179  1.648  0.105  0.819   \n",
       "3095  1.302 0.022 -0.486  0.299  0.675  1.337 -0.819 -0.393  1.664 -0.359   \n",
       "8379  1.813 0.613  0.894 -0.965 -0.561  1.044  0.667  0.055  1.343 -1.596   \n",
       "3043  1.229 0.249 -0.497  0.097 -0.475  0.275  1.069  2.365 -1.088  0.837   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2    b3     b4     b5     b6  \\\n",
       "6671  1.539  0.529 -0.236 -1.060  0.837  0.790 1.099  0.355  1.657 -0.103   \n",
       "3274 -0.736  0.012 -1.468 -1.644  0.251  0.862 0.338  0.195 -0.382  0.214   \n",
       "3095 -1.412 -2.145  0.602 -2.188  0.324 -1.813 0.390  0.040  0.764 -1.528   \n",
       "8379  0.232 -1.239  1.390  0.846 -1.007 -0.674 0.428 -0.911 -0.089 -0.164   \n",
       "3043  0.475  2.032  0.115 -2.102 -0.373  2.560 0.159  0.585  1.136 -0.410   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "6671 -3.041  3.319  0.221 -0.199  1.859 -1.850 -0.000 -0.000  1.999 -1.797   \n",
       "3274 -2.253  2.135  0.000  0.000 -1.067  1.067  0.003 -0.008  2.392 -2.360   \n",
       "3095 -0.935  0.935  0.318 -0.318 -2.436  2.248  0.708 -0.706 -0.525  0.525   \n",
       "8379  1.630 -1.630 -1.954  1.753  0.000  0.000 -2.673  3.045 -1.217  1.217   \n",
       "3043  0.811 -0.812  0.000 -0.000 -2.904  2.904  0.000 -0.000  2.708 -3.134   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3  \\\n",
       "6671 -0.000 -0.000 -0.000  0.000 -2.560  2.560 -0.031 -0.244  0.154 -0.162   \n",
       "3274 -1.698  1.851  2.775 -3.009 -0.099  0.098 -0.265 -0.649 -0.003 -0.224   \n",
       "3095  2.221 -2.013 -2.686  2.741  2.922 -2.846 -0.200  0.719 -0.084 -0.242   \n",
       "8379  2.448 -2.445 -0.280  0.280  2.448 -2.422  0.174  0.213  0.237  0.014   \n",
       "3043 -2.844  2.974 -1.324  1.325  2.535 -2.535 -0.075 -0.543 -0.214 -0.132   \n",
       "\n",
       "       wb_4   wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "6671 -0.073 -0.031  0.088 -0.116 -0.103 -0.064  0.352 -0.042  0.020 -0.045   \n",
       "3274 -0.178 -0.274 -0.045 -0.393 -0.210 -0.208  0.095 -0.077 -0.667 -0.161   \n",
       "3095 -0.189 -0.582 -0.249 -0.230 -0.173 -0.209  0.107 -0.062  0.745 -0.169   \n",
       "8379 -0.040 -0.278  0.039  0.145  0.123 -0.030  0.544 -0.019 -0.548  0.112   \n",
       "3043 -0.116 -0.566 -0.059 -0.059 -0.081 -0.133  0.106 -0.020  0.573 -0.054   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "6671  0.096  0.087  0.010 -0.080 -0.232 -0.106 -0.141 -0.020 -0.248  0.046   \n",
       "3274  0.479 -0.075 -0.597  0.158 -0.619 -0.335 -0.187  0.485 -0.801 -0.831   \n",
       "3095  0.688 -0.339 -0.741  0.031  0.731 -0.198 -0.171  0.382  0.692 -0.871   \n",
       "8379  0.231  0.053 -0.340 -0.027  0.205  0.164  0.029  0.063  0.123  0.237   \n",
       "3043 -0.236 -0.080  0.608 -0.063 -0.569 -0.046 -0.122  0.006  0.717 -0.494   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "6671  0.294  0.010 -0.056  0.074 -0.089 -0.009 -0.070  0.179  0.223  0.142   \n",
       "3274  0.073  0.754 -0.572 -0.114 -0.134 -0.621 -0.440  0.255  0.233  0.428   \n",
       "3095  0.165  0.538 -0.308 -0.311 -0.103  0.199 -0.196  0.304  0.197  0.636   \n",
       "8379  0.483  0.074  0.216 -0.001 -0.033 -0.229  0.201  0.585  0.450  0.428   \n",
       "3043  0.073 -0.456 -0.053  0.011 -0.085 -0.548 -0.027  0.071  0.117 -0.137   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "6671 -0.033  0.022 -0.239  0.362  0.021 -0.067  0.085  0.084  0.184  0.177   \n",
       "3274 -0.284 -0.083 -0.432  0.100 -0.262 -0.212 -0.847  0.375  0.409  0.365   \n",
       "3095 -0.224 -0.452 -0.736  0.028 -0.646 -0.192 -0.637  0.374  0.394  0.496   \n",
       "8379  0.167 -0.266 -0.389  0.473 -0.175 -0.046  0.216  0.577  0.595  0.649   \n",
       "3043  0.250 -0.360  0.715  0.143  0.018 -0.142 -0.332 -0.291 -0.032 -0.038   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "6671  0.218  0.125  0.229 -0.066  0.095 -0.207  0.283 -0.058 -0.065 -0.097   \n",
       "3274  0.063  0.132  0.301 -0.221 -0.126 -0.398  0.040 -0.082 -0.150 -0.355   \n",
       "3095 -0.015  0.018  0.544 -0.211  0.166 -0.312 -0.028 -0.078 -0.131 -0.316   \n",
       "8379  0.055  0.052  0.724 -0.036  0.097  0.018  0.316 -0.047 -0.046  0.170   \n",
       "3043  0.099  0.009  0.103 -0.142 -0.114 -0.133 -0.125  0.011 -0.106 -0.069   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "6671  ...   0.031  -0.035  -0.082  -0.088  -0.103   0.166   0.268   0.302   \n",
       "3274  ...   0.159  -0.209  -0.156  -0.132  -0.208  -0.137   0.009   0.073   \n",
       "3095  ...  -0.322   0.526  -0.242  -0.236  -0.206   0.564  -0.115  -0.104   \n",
       "8379  ...   0.084   0.249   0.399   0.236   0.120   0.164   0.151   0.159   \n",
       "3043  ...  -0.154  -0.091  -0.130  -0.150  -0.155   0.441   0.104   0.759   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "6671   0.204  -0.060   0.321   0.271  -0.082  -0.102  -0.097  -0.003  -0.085   \n",
       "3274  -0.271   0.092  -0.101   0.251  -0.211  -0.208  -0.190  -0.079  -0.143   \n",
       "3095   0.013  -0.003  -0.077   0.323  -0.295  -0.299  -0.262  -0.093  -0.149   \n",
       "8379   0.538   0.151  -0.077   0.350   0.406   0.323   0.461   0.266   0.196   \n",
       "3043  -0.104  -0.057  -0.186   0.090  -0.140  -0.154  -0.132  -0.098  -0.132   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "6671  -0.082   0.305  -0.096  -0.093   0.188   0.267   0.269   0.166   0.188   \n",
       "3274  -0.151  -0.071  -0.040  -0.213  -0.099  -0.123  -0.133  -0.181  -0.280   \n",
       "3095   0.044  -0.034  -0.295   0.491  -0.085  -0.101  -0.157   0.581   0.630   \n",
       "8379   0.539  -0.070   0.081   0.259  -0.057  -0.064  -0.131   0.161   0.243   \n",
       "3043  -0.110  -0.176  -0.153  -0.207   0.023  -0.088  -0.184  -0.107   0.623   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "6671   0.235   0.248   0.154  -0.125  -0.070   0.360  -0.107  -0.093  -0.080   \n",
       "3274  -0.085   0.060  -0.232   0.134  -0.106   0.186  -0.198  -0.112  -0.127   \n",
       "3095  -0.090   0.187   0.604  -0.177  -0.195  -0.176  -0.230  -0.064  -0.097   \n",
       "8379  -0.114   0.151   0.167   0.001   0.423  -0.037   0.337   0.162   0.508   \n",
       "3043  -0.100   0.187  -0.117  -0.150  -0.109   0.621  -0.152  -0.116  -0.100   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "6671  -0.356   0.231   0.329  -0.511  -0.588   0.340   0.077  -0.150  -0.189   \n",
       "3274  -0.462   0.488   0.077  -0.349  -0.160   0.277   0.559  -0.486  -0.406   \n",
       "3095  -0.776  -0.497   0.215  -0.501  -0.079   0.627   0.752  -0.486  -0.396   \n",
       "8379  -0.659  -0.500   0.044  -0.357  -0.249   0.566   0.267  -0.375  -0.364   \n",
       "3043   0.222   0.437   0.517  -0.197  -0.372   0.680   0.201  -0.085  -0.133   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "6671  -0.609   0.572  -0.229  -0.312  -0.391   0.112   0.062  -0.215  -0.143   \n",
       "3274  -0.205   0.157  -0.096   0.606  -0.273   0.700   0.449   0.507  -0.345   \n",
       "3095  -0.134   0.221  -0.091  -0.655  -0.601   1.162   0.701   0.822  -0.147   \n",
       "8379  -0.284   0.805  -0.218   0.522  -0.435   0.037   0.234   0.245  -0.428   \n",
       "3043  -0.269   0.692  -0.347  -0.619  -0.138   0.446   0.217  -0.545  -0.179   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "6671   0.236  -0.307  -0.188  -0.081   0.294   0.238   0.553   0.261  -0.276   \n",
       "3274   0.492  -0.658  -0.296  -0.626   0.540   0.860   0.109  -0.735  -0.737   \n",
       "3095  -0.523  -0.788  -0.251  -0.523  -0.535   0.931   0.174  -0.442  -0.686   \n",
       "8379  -0.515  -0.570  -0.188  -0.602  -0.717  -0.414   0.972  -0.380  -0.586   \n",
       "3043   0.425  -0.038  -0.192  -0.238  -0.691   0.559   0.512   0.474   0.149   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "6671   0.057  -0.264  -0.056  -0.187   0.174   0.212   0.108  -0.289   0.400   \n",
       "3274   0.422  -0.161   0.581  -0.742   0.705   0.502   0.824  -0.333   0.083   \n",
       "3095   0.673  -0.141   0.579  -0.774   0.892   0.600   1.158  -0.684   0.542   \n",
       "8379   0.294  -0.166   0.132  -0.596   0.696   0.507   0.434  -0.504   0.543   \n",
       "3043   0.110  -0.290   0.465  -0.053   0.124   0.158   0.299  -0.295   0.658   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "6671   0.333   0.643  -0.023  -0.723   0.216   0.066   0.163   0.163   0.242   \n",
       "3274   0.370   0.084   0.395  -0.217   0.786   0.749   0.848   0.860   0.039   \n",
       "3095   0.736   0.099   0.784  -0.153   0.651   0.967   0.983   1.090   0.265   \n",
       "8379   0.323   0.929   0.234  -0.261  -0.242   0.833   0.839   1.084   0.546   \n",
       "3043  -0.659   0.439   0.040  -0.303   0.312   0.420   0.228   0.202   0.247   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "6671   0.112   0.248  -0.714   0.079  -0.321   0.423  -0.465  -0.577  -0.357   \n",
       "3274   0.602   0.582  -0.244   0.519  -0.739   0.076  -0.051  -0.162  -0.535   \n",
       "3095   0.658   1.165  -0.179   0.646  -0.818   0.166   0.035  -0.133  -0.729   \n",
       "8379   0.302   1.101  -0.287   0.150  -0.477   0.149  -0.157  -0.163  -0.648   \n",
       "3043   0.158   0.161  -0.269   0.262  -0.150   0.689  -0.515  -0.383   0.195   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "6671  -0.229   0.414   0.254  -0.361  -0.133   0.188  -0.500   0.187  -0.151   \n",
       "3274  -0.693   0.292   0.213  -0.627  -0.339   0.473  -0.371   0.856  -0.272   \n",
       "3095  -0.669   0.497   0.382  -0.702  -0.100   0.757  -0.175   1.072  -0.252   \n",
       "8379  -0.792   0.122  -0.352  -0.701  -0.500   0.721  -0.095   0.903  -0.191   \n",
       "3043  -0.516   0.511   0.396   0.168  -0.095   0.124  -0.749   0.218  -0.163   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "6671   0.313  -0.038  \n",
       "3274   0.356  -0.132  \n",
       "3095   0.566  -0.129  \n",
       "8379   0.964  -0.029  \n",
       "3043   0.221  -0.191  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-1.206</td>\n",
       "      <td>3.094</td>\n",
       "      <td>-1.627</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>-1.459</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>1.146</td>\n",
       "      <td>-2.571</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-1.578</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.718</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.336</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.208</td>\n",
       "      <td>1.106</td>\n",
       "      <td>0.141</td>\n",
       "      <td>1.133</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.795</td>\n",
       "      <td>2.820</td>\n",
       "      <td>-2.655</td>\n",
       "      <td>1.752</td>\n",
       "      <td>-1.750</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-1.242</td>\n",
       "      <td>1.339</td>\n",
       "      <td>-2.968</td>\n",
       "      <td>2.688</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-3.012</td>\n",
       "      <td>2.946</td>\n",
       "      <td>2.835</td>\n",
       "      <td>-2.911</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.490</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.585</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.668</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.490</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.607</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>689.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-1.658</td>\n",
       "      <td>1.039</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-1.139</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>-1.273</td>\n",
       "      <td>0.105</td>\n",
       "      <td>1.247</td>\n",
       "      <td>0.534</td>\n",
       "      <td>-1.139</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>1.745</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-1.045</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-1.750</td>\n",
       "      <td>1.539</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.904</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-2.157</td>\n",
       "      <td>2.016</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-1.440</td>\n",
       "      <td>1.455</td>\n",
       "      <td>2.272</td>\n",
       "      <td>-2.571</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.583</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.674</td>\n",
       "      <td>0.524</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>4148.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-1.287</td>\n",
       "      <td>2.302</td>\n",
       "      <td>-1.756</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.905</td>\n",
       "      <td>1.891</td>\n",
       "      <td>-1.259</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-1.346</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>1.788</td>\n",
       "      <td>0.840</td>\n",
       "      <td>-1.264</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>2.392</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-2.265</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>1.132</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-1.262</td>\n",
       "      <td>-2.666</td>\n",
       "      <td>2.752</td>\n",
       "      <td>0.335</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>2.927</td>\n",
       "      <td>-3.076</td>\n",
       "      <td>-3.405</td>\n",
       "      <td>3.193</td>\n",
       "      <td>2.949</td>\n",
       "      <td>-2.949</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.637</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.722</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.879</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.564</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-1.162</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.654</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.572</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>1.016</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2815.000</td>\n",
       "      <td>42</td>\n",
       "      <td>1.450</td>\n",
       "      <td>2.108</td>\n",
       "      <td>1.196</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-1.157</td>\n",
       "      <td>-1.659</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.762</td>\n",
       "      <td>1.473</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-2.285</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.318</td>\n",
       "      <td>1.292</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>2.653</td>\n",
       "      <td>-2.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-3.086</td>\n",
       "      <td>3.064</td>\n",
       "      <td>-1.601</td>\n",
       "      <td>1.590</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>5185.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-3.210</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.406</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.879</td>\n",
       "      <td>-1.952</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>2.502</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.721</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>1.637</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>1.187</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-1.990</td>\n",
       "      <td>-2.304</td>\n",
       "      <td>2.264</td>\n",
       "      <td>1.592</td>\n",
       "      <td>-1.682</td>\n",
       "      <td>2.932</td>\n",
       "      <td>-3.115</td>\n",
       "      <td>-2.747</td>\n",
       "      <td>2.483</td>\n",
       "      <td>1.173</td>\n",
       "      <td>-1.173</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.594</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.728</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.661</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.638</td>\n",
       "      <td>0.621</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.474</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "3466 3466.000    42 -1.206  3.094 -1.627 -1.719 -1.459 -0.294 -0.504  1.146   \n",
       "689   689.000    42 -0.628 -1.658  1.039  0.540 -1.139 -0.656 -1.273  0.105   \n",
       "4148 4148.000    42 -1.287  2.302 -1.756  0.567  0.942  0.905  1.891 -1.259   \n",
       "2815 2815.000    42  1.450  2.108  1.196 -0.512 -1.506 -0.133 -1.157 -1.659   \n",
       "5185 5185.000    42 -3.210 -0.494 -0.702  1.043  1.406 -1.443  0.844  0.180   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "3466 -2.571 -0.440 -0.474 -1.578 -0.376  0.511 -0.921  0.511 -0.973 -0.718   \n",
       "689   1.247  0.534 -1.139  0.526  0.723 -0.068 -0.304 -0.482  1.745  0.180   \n",
       "4148  0.811 -0.270 -0.463  0.268  0.470 -0.810 -1.346 -0.438  1.788  0.840   \n",
       "2815 -0.888 -0.033 -0.293  1.041  0.762  1.473  0.503 -0.237  0.265 -2.285   \n",
       "5185  0.074  0.879 -1.952 -0.416 -0.650  2.502 -0.134  1.122  0.721 -0.812   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "3466  1.693  1.336 -0.353  0.208  1.106  0.141  1.133 -0.608  0.694  0.795   \n",
       "689  -0.116 -1.045 -0.414 -1.750  1.539  0.131  0.525  0.904 -0.253 -0.112   \n",
       "4148 -1.264 -0.160  2.392 -1.190  0.538 -2.265 -0.558 -0.210 -0.309  1.132   \n",
       "2815  0.000  0.000  0.000 -0.515  0.184  0.364 -0.477 -0.010  0.880  0.000   \n",
       "5185 -0.000 -0.000 -0.000 -0.424 -0.117  1.637  0.287 -0.312  1.187 -0.000   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "3466  2.820 -2.655  1.752 -1.750 -0.311  0.314 -1.242  1.339 -2.968  2.688   \n",
       "689  -2.157  2.016  0.188 -0.188 -0.081  0.081 -1.440  1.455  2.272 -2.571   \n",
       "4148 -0.000  0.000  1.250 -1.262 -2.666  2.752  0.335 -0.335 -0.211  0.211   \n",
       "2815 -1.318  1.292  0.092 -0.089  2.653 -2.284  0.000 -0.000 -3.086  3.064   \n",
       "5185  1.990 -1.990 -2.304  2.264  1.592 -1.682  2.932 -3.115 -2.747  2.483   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3  \\\n",
       "3466  0.025 -0.022 -3.012  2.946  2.835 -2.911 -0.040 -0.036  0.370 -0.301   \n",
       "689   0.324 -0.324 -0.934  0.961  0.890 -0.816  0.133 -0.125 -0.058 -0.327   \n",
       "4148  2.927 -3.076 -3.405  3.193  2.949 -2.949 -0.094 -0.684  0.012 -0.183   \n",
       "2815 -1.601  1.590  0.000 -0.000 -0.000  0.000  0.208  0.473 -0.001 -0.149   \n",
       "5185  1.173 -1.173 -0.000 -0.000  0.003  0.001 -0.094 -0.590  0.050 -0.236   \n",
       "\n",
       "       wb_4   wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "3466 -0.322  0.415  0.116 -0.325 -0.357 -0.327  0.490 -0.080  0.026 -0.199   \n",
       "689  -0.268 -0.060  0.092  0.116 -0.127 -0.292 -0.004 -0.104  0.006 -0.118   \n",
       "4148 -0.040 -0.021 -0.025 -0.299 -0.170 -0.082  0.097  0.116  0.637 -0.113   \n",
       "2815 -0.126 -0.227 -0.104  0.163  0.179 -0.134  0.098 -0.050 -0.351 -0.010   \n",
       "5185 -0.327  0.350  0.059 -0.233 -0.258 -0.303  0.543 -0.299 -0.407 -0.167   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "3466  0.337  0.135  0.019 -0.095 -0.022 -0.055 -0.290 -0.051  0.008  0.326   \n",
       "689   0.021  0.098 -0.039 -0.127 -0.111 -0.018 -0.225 -0.011 -0.078  0.290   \n",
       "4148  0.044  0.037 -0.595  0.102 -0.671 -0.169 -0.180  0.291  0.630 -0.723   \n",
       "2815  0.023 -0.086 -0.307 -0.098 -0.283  0.178 -0.139  0.197 -0.289  0.421   \n",
       "5185 -0.215  0.086 -0.265 -0.406 -0.567 -0.101 -0.310 -0.405 -0.515 -0.389   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "3466  0.513  0.338 -0.066  0.118 -0.273  0.023 -0.128  0.410  0.229  0.308   \n",
       "689  -0.035 -0.083  0.144  0.070 -0.180  0.084  0.156  0.077  0.138  0.039   \n",
       "4148  0.066 -0.633  0.784  0.024  0.002 -0.323 -0.269  0.068  0.058 -0.021   \n",
       "2815  0.039 -0.129  0.242 -0.086 -0.092 -0.239  0.230  0.042  0.085  0.035   \n",
       "5185  0.533  0.365 -0.035  0.458 -0.245 -0.165 -0.193  0.550  0.594 -0.136   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "3466 -0.061  0.262 -0.044  0.507  0.068 -0.321  0.323  0.292  0.370  0.332   \n",
       "689  -0.003 -0.019 -0.079  0.027 -0.032 -0.283  0.281  0.020  0.081  0.063   \n",
       "4148  0.168  0.228 -0.598  0.129 -0.320 -0.150 -0.506 -0.078  0.010  0.066   \n",
       "2815  0.157 -0.195 -0.244  0.104 -0.181 -0.150  0.316  0.005  0.063  0.051   \n",
       "5185 -0.037  0.444 -0.211  0.597  0.054 -0.358 -0.423 -0.186 -0.033 -0.107   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "3466  0.182  0.139  0.383 -0.326  0.111 -0.151  0.424 -0.281 -0.381 -0.059   \n",
       "689   0.047  0.102  0.090 -0.302  0.078 -0.171 -0.034 -0.173 -0.216 -0.031   \n",
       "4148  0.091 -0.000  0.078 -0.158 -0.047 -0.219  0.059  0.062 -0.019 -0.062   \n",
       "2815  0.003 -0.048  0.071 -0.160 -0.077 -0.007  0.027 -0.089 -0.105  0.174   \n",
       "5185  0.575  0.508  0.551 -0.326  0.026 -0.164  0.413 -0.125 -0.217 -0.054   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "3466  ...  -0.031   0.154   0.055   0.054  -0.179   0.232   0.087   0.108   \n",
       "689   ...  -0.048   0.291  -0.009   0.032  -0.059   0.381   0.018   0.047   \n",
       "4148  ...  -0.476   0.068  -0.143  -0.203  -0.279   0.363  -0.722   0.686   \n",
       "2815  ...   0.241   0.259  -0.085  -0.043  -0.084   0.093   0.404   0.437   \n",
       "5185  ...  -0.055   0.015   0.352   0.212   0.037  -0.038   0.238   0.187   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "3466   0.125  -0.009   0.268  -0.118  -0.165  -0.106  -0.144   0.103  -0.011   \n",
       "689    0.073   0.199  -0.099   0.342  -0.035  -0.043  -0.052   0.110   0.026   \n",
       "4148  -0.097   0.443  -0.158   0.879  -0.428  -0.231  -0.127  -0.173  -0.255   \n",
       "2815  -0.072   0.314  -0.147   0.126  -0.051  -0.104  -0.103   0.205   0.079   \n",
       "5185   0.289   0.192  -0.021   0.169   0.011   0.055   0.050   0.202   0.231   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "3466  -0.049   0.266   0.047   0.245   0.093   0.262   0.140   0.226   0.296   \n",
       "689   -0.069  -0.093  -0.041   0.324  -0.088  -0.128  -0.094   0.371   0.326   \n",
       "4148  -0.153  -0.152  -0.384   0.041  -0.184   0.088  -0.282  -0.031   0.229   \n",
       "2815  -0.119  -0.147  -0.008   0.222  -0.096  -0.146  -0.130   0.186   0.269   \n",
       "5185   0.375  -0.012   0.061  -0.154   0.314   0.045   0.083  -0.011  -0.098   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "3466  -0.041  -0.072   0.226  -0.058   0.076  -0.159  -0.150   0.066   0.154   \n",
       "689   -0.134  -0.148   0.352   0.067   0.004  -0.093  -0.052   0.050  -0.007   \n",
       "4148  -0.113   0.233  -0.032  -0.523  -0.138  -0.189  -0.124  -0.080  -0.186   \n",
       "2815  -0.124  -0.048   0.256   0.083  -0.060  -0.113  -0.113  -0.077  -0.140   \n",
       "5185   0.190   0.272   0.006   0.097   0.352   0.123   0.061   0.019   0.359   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "3466  -0.263  -0.557   0.503  -0.349  -0.269   0.391   0.326  -0.154  -0.462   \n",
       "689   -0.636   0.270   0.450  -0.460  -0.200   0.375   0.491  -0.602  -0.521   \n",
       "4148  -0.138   0.526   0.381  -0.214  -0.206   0.332   0.258  -0.826  -0.248   \n",
       "2815  -0.376  -0.362   0.115  -0.204  -0.141   0.505   0.418  -0.501  -0.664   \n",
       "5185  -0.185  -0.379   0.024  -0.329  -0.392   0.114   0.206  -0.219  -0.511   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "3466  -0.295   0.585  -0.546  -0.738  -0.271   0.539   0.347   0.235  -0.596   \n",
       "689   -0.242   0.408  -0.116   0.350  -0.443   0.018   0.535   0.417  -0.106   \n",
       "4148  -0.164   0.265  -0.503  -0.692  -0.117   0.564  -0.168   0.638  -0.519   \n",
       "2815  -0.159   0.200  -0.079   0.442  -0.112   0.026   0.387   0.339  -0.063   \n",
       "5185  -0.384   0.375  -0.557  -0.500  -0.249   0.487   0.138   0.242  -0.563   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "3466  -0.589  -0.349  -0.493  -0.606  -0.569   0.514   0.589   0.579  -0.381   \n",
       "689    0.272  -0.602  -0.337  -0.037   0.266  -0.583   0.621   0.421  -0.468   \n",
       "4148   0.559  -0.462  -0.220  -0.840  -0.673   0.887   0.185   0.710  -1.162   \n",
       "2815   0.419  -0.550  -0.203  -0.229   0.419  -0.395   0.187   0.288  -0.422   \n",
       "5185  -0.398  -0.125  -0.728  -0.541  -0.382   0.527   0.401  -0.422  -0.037   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "3466   0.380  -0.408   0.160  -0.160   0.324   0.431   0.625  -0.366   0.171   \n",
       "689    0.475  -0.187   0.245  -0.588   0.257   0.475   0.085  -0.438   0.398   \n",
       "4148  -0.089  -0.332   0.654  -0.862   0.167   0.152   0.254  -0.627  -0.637   \n",
       "2815   0.403  -0.140   0.349  -0.494   0.160   0.283   0.063  -0.143   0.497   \n",
       "5185   0.234  -0.598   0.221  -0.288   0.608   0.582   0.435  -0.173   0.208   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "3466   0.198   0.540   0.302  -0.365   0.435   0.640   0.552   0.654   0.458   \n",
       "689    0.270   0.612   0.416  -0.257  -0.364   0.085   0.168   0.133   0.549   \n",
       "4148   0.572  -0.013   0.365  -0.167   1.016   0.582   0.183   0.158   0.066   \n",
       "2815   0.346   0.321   0.341  -0.192  -0.187   0.066   0.120   0.103   0.443   \n",
       "5185  -0.252   0.364   0.059  -0.429   0.589   0.412   0.430   0.489   0.404   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "3466   0.363   0.668  -0.365   0.272  -0.434   0.541  -0.161  -0.384  -0.440   \n",
       "689    0.523   0.145  -0.282   0.325  -0.674   0.524  -0.099  -0.202  -0.510   \n",
       "4148   0.194   0.255  -0.175   0.354  -0.601   0.241  -0.238  -0.298  -0.192   \n",
       "2815   0.415   0.132  -0.195   0.337  -0.549   0.168  -0.070  -0.162  -0.321   \n",
       "5185   0.450   0.661  -0.409   0.226  -0.214   0.191  -0.358  -0.545  -0.160   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "3466  -0.432   0.535   0.490  -0.446  -0.620   0.330  -0.519   0.649  -0.407   \n",
       "689   -0.422   0.111   0.415  -0.456  -0.150   0.287  -0.057   0.163  -0.321   \n",
       "4148  -0.917   0.628   0.704  -0.245  -0.558   0.190  -0.660   0.106  -0.181   \n",
       "2815  -0.390   0.096   0.026  -0.389  -0.205   0.169  -0.043   0.126  -0.173   \n",
       "5185   0.139   0.735   0.241  -0.133  -0.638   0.621  -0.439   0.474  -0.592   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "3466   0.607  -0.003  \n",
       "689    0.227  -0.091  \n",
       "4148   0.255   0.059  \n",
       "2815   0.190  -0.111  \n",
       "5185   0.702   0.138  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.731</td>\n",
       "      <td>1.297</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-1.845</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.392</td>\n",
       "      <td>2.575</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-2.281</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-2.107</td>\n",
       "      <td>2.301</td>\n",
       "      <td>3.346</td>\n",
       "      <td>-3.273</td>\n",
       "      <td>1.657</td>\n",
       "      <td>-1.657</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.489</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.888</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.778</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.817</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.634</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.628</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.956</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.652</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.785</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.741</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>2.266</td>\n",
       "      <td>1.251</td>\n",
       "      <td>1.779</td>\n",
       "      <td>0.617</td>\n",
       "      <td>1.243</td>\n",
       "      <td>-1.668</td>\n",
       "      <td>-1.548</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>1.215</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.927</td>\n",
       "      <td>1.140</td>\n",
       "      <td>-0.827</td>\n",
       "      <td>-1.101</td>\n",
       "      <td>-1.273</td>\n",
       "      <td>0.169</td>\n",
       "      <td>1.089</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-1.146</td>\n",
       "      <td>1.476</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.101</td>\n",
       "      <td>1.349</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-2.662</td>\n",
       "      <td>2.888</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.745</td>\n",
       "      <td>-2.077</td>\n",
       "      <td>-1.666</td>\n",
       "      <td>1.769</td>\n",
       "      <td>2.983</td>\n",
       "      <td>-3.061</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-2.508</td>\n",
       "      <td>2.580</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.594</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.454</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.619</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.521</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.737</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>0.674</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.297</td>\n",
       "      <td>2.074</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-1.558</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.412</td>\n",
       "      <td>1.207</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.474</td>\n",
       "      <td>-1.105</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>2.527</td>\n",
       "      <td>-2.641</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-1.648</td>\n",
       "      <td>1.651</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.485</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.479</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.645</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.752</td>\n",
       "      <td>-2.145</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-1.872</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>0.053</td>\n",
       "      <td>1.860</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-1.341</td>\n",
       "      <td>0.253</td>\n",
       "      <td>2.402</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.644</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.634</td>\n",
       "      <td>-2.688</td>\n",
       "      <td>2.865</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-2.911</td>\n",
       "      <td>2.979</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>0.980</td>\n",
       "      <td>2.243</td>\n",
       "      <td>-2.286</td>\n",
       "      <td>2.980</td>\n",
       "      <td>-3.094</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.247</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.433</td>\n",
       "      <td>2.848</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>0.623</td>\n",
       "      <td>1.682</td>\n",
       "      <td>1.949</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-1.630</td>\n",
       "      <td>-1.172</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.968</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>-1.556</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-1.745</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-1.580</td>\n",
       "      <td>-1.991</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.804</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.253</td>\n",
       "      <td>2.154</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>2.765</td>\n",
       "      <td>-2.799</td>\n",
       "      <td>-2.217</td>\n",
       "      <td>2.391</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>0.932</td>\n",
       "      <td>2.651</td>\n",
       "      <td>-2.661</td>\n",
       "      <td>1.264</td>\n",
       "      <td>-1.264</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.719</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.576</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.657</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.619</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.683</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0  f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "7217 7217.000    42 -0.246 0.021  0.731  1.297 -0.938 -1.845  0.079  0.101   \n",
       "8291 8291.000    42 -0.999 2.266  1.251  1.779  0.617  1.243 -1.668 -1.548   \n",
       "4607 4607.000    42  0.297 2.074  0.232  0.491 -1.558  0.437  0.412  1.207   \n",
       "5114 5114.000    42  1.582 1.752 -2.145 -0.503 -1.872 -1.798  0.053  1.860   \n",
       "1859 1859.000    42  0.433 2.848  0.709 -1.768  0.623  1.682  1.949  0.181   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2  f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "7217 -0.389 -0.926 -1.695 -0.950 1.443  0.373 -0.861  0.049  0.106 -0.396   \n",
       "8291 -0.731  1.215  0.111  1.137 0.927  1.140 -0.827 -1.101 -1.273  0.169   \n",
       "4607  0.401 -0.230  0.218  0.418 0.424 -1.509  0.361 -0.000 -0.000  0.000   \n",
       "5114  1.164  0.519  0.086 -1.060 0.752  0.378  0.012 -0.364 -0.215  0.497   \n",
       "1859 -1.630 -1.172 -0.824  0.280 0.968 -0.938 -0.653 -0.886 -1.556 -0.808   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "7217  0.000 -0.000  0.000  2.392  2.575 -0.046 -2.281 -0.162 -0.013 -0.000   \n",
       "8291  1.089  0.580 -0.097 -1.146  1.476  1.084  0.961  0.101  1.349  0.323   \n",
       "4607  1.474 -1.105 -0.389 -0.095 -0.390 -0.770  0.153 -0.454  0.000  0.248   \n",
       "5114 -1.341  0.253  2.402 -0.087 -0.513 -0.564  1.160  1.644 -0.515  0.634   \n",
       "1859 -1.621  0.485  0.837  0.529 -1.745  0.144 -1.580 -1.991  0.035  0.804   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "7217 -2.107  2.301  3.346 -3.273  1.657 -1.657 -1.214  0.967  0.566 -0.565   \n",
       "8291 -2.662  2.888 -0.237  0.250  1.745 -2.077 -1.666  1.769  2.983 -3.061   \n",
       "4607  0.516 -0.516  0.226 -0.225  0.001 -0.004  2.527 -2.641  0.000  0.000   \n",
       "5114 -2.688  2.865 -0.008 -0.001 -2.911  2.979  0.000 -0.000  0.411 -0.411   \n",
       "1859 -0.133  0.131 -2.253  2.154  0.173 -0.173  2.765 -2.799 -2.217  2.391   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3  \\\n",
       "7217  0.001 -0.001 -0.000  0.000 -0.000 -0.000  0.078 -0.504 -0.315 -0.109   \n",
       "8291 -0.430  0.429 -2.508  2.580 -0.000 -0.000  0.020 -0.457 -0.246 -0.117   \n",
       "4607 -0.027 -0.002 -1.148  1.309 -1.648  1.651  0.118 -0.046  0.012 -0.273   \n",
       "5114 -0.980  0.980  2.243 -2.286  2.980 -3.094 -0.227 -0.284  0.198 -0.405   \n",
       "1859 -0.932  0.932  2.651 -2.661  1.264 -1.264  0.412 -0.412 -0.380 -0.124   \n",
       "\n",
       "       wb_4   wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "7217 -0.108 -0.394 -0.176  0.142 -0.057 -0.111  0.052 -0.047 -0.451  0.007   \n",
       "8291  0.034 -0.343  0.594 -0.015 -0.043  0.010  0.289  0.005  0.082  0.005   \n",
       "4607 -0.221 -0.048 -0.047  0.091  0.045 -0.224  0.099 -0.151 -0.086  0.030   \n",
       "5114 -0.290  0.166  0.279 -0.301 -0.213 -0.305  0.310 -0.161 -0.169 -0.284   \n",
       "1859 -0.228 -0.151  0.152  0.013 -0.123 -0.203 -0.221 -0.323 -0.433  0.238   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "7217  0.108 -0.152 -0.295 -0.099 -0.497  0.127 -0.099 -0.028 -0.504 -0.493   \n",
       "8291  0.495  0.526  0.089 -0.043 -0.445  0.001 -0.134  0.007 -0.484 -0.326   \n",
       "4607  0.004  0.010 -0.117 -0.167 -0.056  0.098 -0.061  0.006 -0.062  0.226   \n",
       "5114  0.215  0.224  0.103 -0.199 -0.269 -0.354 -0.234 -0.181 -0.244  0.176   \n",
       "1859 -0.234  0.184 -0.134 -0.320 -0.399  0.145 -0.184 -0.334 -0.409 -0.454   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "7217  0.072 -0.458  0.489 -0.162 -0.109 -0.198  0.416 -0.120 -0.152  0.079   \n",
       "8291  0.537 -0.317  0.035  0.454 -0.013  0.587  0.012  0.623  0.707  0.529   \n",
       "4607 -0.000 -0.032  0.150 -0.043 -0.190 -0.042  0.138  0.049  0.073  0.023   \n",
       "5114  0.233  0.080 -0.153  0.204 -0.232  0.177 -0.159  0.284  0.411  0.247   \n",
       "1859 -0.125 -0.484  0.466  0.197 -0.287  0.056  0.371  0.120  0.181  0.025   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "7217  0.347 -0.353 -0.319  0.101 -0.211 -0.137 -0.493  0.306  0.091  0.059   \n",
       "8291 -0.021 -0.332 -0.499  0.159  0.401 -0.003 -0.278  0.520  0.599  0.576   \n",
       "4607 -0.095 -0.049 -0.065  0.077 -0.076 -0.220  0.225 -0.009  0.068  0.064   \n",
       "5114 -0.045  0.175 -0.121  0.335  0.158 -0.315  0.112  0.213  0.274  0.240   \n",
       "1859  0.413  0.174 -0.517  0.260  0.201 -0.282 -0.420  0.026  0.073  0.049   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "7217 -0.161 -0.165  0.106 -0.139 -0.159 -0.022 -0.100 -0.076 -0.126  0.151   \n",
       "8291  0.566  0.666  0.619 -0.156  0.608 -0.082 -0.210  0.039 -0.029 -0.019   \n",
       "4607  0.008 -0.020  0.090 -0.248  0.022  0.040  0.031 -0.188 -0.165  0.115   \n",
       "5114  0.267  0.301  0.203 -0.353  0.255 -0.446  0.250 -0.228 -0.276 -0.214   \n",
       "1859  0.230  0.162  0.120 -0.261  0.125 -0.049 -0.335 -0.256 -0.313  0.310   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "7217  ...  -0.289   0.888  -0.232  -0.314  -0.056   0.354  -0.334  -0.356   \n",
       "8291  ...  -0.086  -0.049   0.325   0.061   0.249   0.171   0.121   0.002   \n",
       "4607  ...   0.308   0.269   0.167   0.192  -0.112   0.007   0.238   0.304   \n",
       "5114  ...   0.068   0.142   0.285   0.104   0.359  -0.107  -0.046   0.172   \n",
       "1859  ...  -0.074   0.289  -0.066  -0.106  -0.145   0.394  -0.080  -0.200   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "7217  -0.306  -0.376   0.016  -0.541   0.778  -0.142  -0.132  -0.341  -0.329   \n",
       "8291   0.143  -0.074   0.260   0.125   0.296   0.236   0.290   0.352   0.029   \n",
       "4607   0.189   0.283  -0.063   0.094  -0.045  -0.088  -0.110   0.226   0.231   \n",
       "5114  -0.096  -0.165   0.157   0.384   0.319   0.275   0.346  -0.113   0.007   \n",
       "1859  -0.125  -0.110  -0.133  -0.150  -0.052  -0.065  -0.141  -0.122  -0.089   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "7217  -0.146   0.019  -0.299   0.139  -0.253   0.096  -0.062   0.114   0.999   \n",
       "8291   0.349   0.202  -0.038  -0.117   0.171   0.567   0.329   0.096   0.146   \n",
       "4607  -0.105  -0.042   0.275   0.272  -0.036  -0.103  -0.079   0.313   0.342   \n",
       "5114   0.348   0.107   0.022  -0.044   0.106   0.178   0.159  -0.113  -0.034   \n",
       "1859  -0.097  -0.133  -0.046  -0.031  -0.242  -0.121  -0.143   0.325   0.445   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "7217  -0.155  -0.525   0.333   0.048  -0.274  -0.011  -0.159  -0.132  -0.169   \n",
       "8291   0.244   0.136   0.044  -0.098   0.365   0.636   0.248   0.107   0.347   \n",
       "4607  -0.148  -0.057   0.348   0.116   0.149  -0.096  -0.127   0.218   0.086   \n",
       "5114   0.340   0.217  -0.119   0.193   0.269   0.148   0.348   0.129   0.262   \n",
       "1859  -0.216  -0.166   0.293  -0.124  -0.077  -0.157  -0.140  -0.096  -0.087   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "7217  -0.119   0.523   0.468  -0.158  -0.140   0.745   0.817  -0.409  -0.095   \n",
       "8291  -0.418   0.351   0.372  -0.410  -0.595   0.486   0.658  -0.270  -0.215   \n",
       "4607  -0.667   0.369   0.102  -0.347  -0.152   0.447   0.485  -0.458  -0.542   \n",
       "5114  -0.127  -0.298   0.108  -0.258  -0.229  -0.128   0.385  -0.315  -0.558   \n",
       "1859  -0.605  -0.373   0.719  -0.091  -0.219  -0.114   0.582  -0.065  -0.191   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "7217  -0.154   0.194  -0.077   0.634  -0.064  -0.111   0.626   0.438  -0.114   \n",
       "8291  -0.515   0.521  -0.513  -0.855  -0.428   0.474   0.377  -0.533  -0.192   \n",
       "4607  -0.182   0.272  -0.014   0.511  -0.431   0.038   0.443   0.456  -0.093   \n",
       "5114  -0.221   0.241  -0.509  -0.425  -0.169   0.510   0.333   0.275  -0.496   \n",
       "1859  -0.168   0.513  -0.591  -0.585  -0.371   0.428   0.611   0.386  -0.576   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "7217   0.516  -0.230  -0.149  -0.046   0.556   0.799   0.172   0.829  -0.770   \n",
       "8291   0.358  -0.314  -0.193  -0.247   0.456   0.324   0.866   0.370  -0.357   \n",
       "4607   0.390  -0.595  -0.495  -0.283   0.376  -0.356   0.509   0.312  -0.469   \n",
       "5114  -0.311  -0.346  -0.589  -0.437  -0.320   0.494   0.182  -0.062  -0.012   \n",
       "1859  -0.398  -0.257  -0.313  -0.563  -0.409   0.626   0.262   0.657  -0.635   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "7217   0.628  -0.131   0.339  -0.759   0.599   0.956  -0.042  -0.339   0.718   \n",
       "8291   0.278  -0.430   0.304  -0.325   0.767   0.737   0.601  -0.394   0.553   \n",
       "4607   0.479  -0.066   0.288  -0.500   0.312   0.517   0.033  -0.107   0.459   \n",
       "5114   0.286  -0.548   0.368  -0.547   0.549   0.470   0.597   0.112  -0.068   \n",
       "1859   0.651  -0.560   0.172  -0.509   0.425   0.615   0.052  -0.593   0.376   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "7217   0.471   0.219   0.566  -0.156   0.617  -0.607   0.065   0.074   0.944   \n",
       "8291   0.542   0.485   0.213  -0.500   0.232   0.644   0.676   0.671   0.781   \n",
       "4607   0.374   0.559   0.432  -0.186  -0.158   0.027   0.085   0.077   0.552   \n",
       "5114  -0.318   0.133   0.254  -0.326   0.245   0.555   0.598   0.623   0.198   \n",
       "1859  -0.450   0.360   0.619  -0.412   0.563   0.081   0.156   0.101   0.603   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "7217   0.845   0.151  -0.166   0.652  -0.235   0.246  -0.073  -0.160  -0.197   \n",
       "8291   0.650   0.737  -0.438   0.552  -0.338   0.429  -0.615  -0.595  -0.425   \n",
       "4607   0.498   0.131  -0.207   0.371  -0.593   0.228  -0.066  -0.118  -0.613   \n",
       "5114   0.392   0.649  -0.282   0.431  -0.301   0.142  -0.193  -0.482  -0.048   \n",
       "1859   0.607   0.187  -0.357   0.390  -0.145   0.683  -0.177  -0.577  -0.532   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "7217  -0.831   0.203   0.785  -0.327  -0.092   0.741  -0.068   0.077  -0.106   \n",
       "8291  -0.395   0.438   0.324  -0.424  -0.131   0.790  -0.886   0.674  -0.200   \n",
       "4607  -0.492   0.093  -0.009  -0.645  -0.380   0.418  -0.020   0.097  -0.569   \n",
       "5114   0.341   0.415   0.102  -0.035  -0.488   0.531  -0.453   0.641  -0.572   \n",
       "1859  -0.640   0.834   0.641  -0.512  -0.361   0.518  -0.606   0.119  -0.319   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "7217   0.191  -0.404  \n",
       "8291   0.827   0.004  \n",
       "4607   0.293  -0.077  \n",
       "5114   0.608   0.028  \n",
       "1859   0.448  -0.116  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy_inet_decision_function_fv_metric\n",
      "\ttraining         \t (min:    0.783, max:    0.947, cur:    0.915)\n",
      "\tvalidation       \t (min:    0.827, max:    0.939, cur:    0.913)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.113, max:    0.436, cur:    0.177)\n",
      "\tvalidation       \t (min:    0.140, max:    0.403, cur:    0.192)\n",
      "Training Time: 0:42:54\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 321)]             0         \n",
      "_________________________________________________________________\n",
      "hidden1_1056 (Dense)         (None, 1056)              340032    \n",
      "_________________________________________________________________\n",
      "activation1_relu (Activation (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "dropout1_0.2 (Dropout)       (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "hidden2_512 (Dense)          (None, 512)               541184    \n",
      "_________________________________________________________________\n",
      "activation2_relu (Activation (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout2_0.1 (Dropout)       (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output_44 (Dense)            (None, 44)                22572     \n",
      "=================================================================\n",
      "Total params: 903,788\n",
      "Trainable params: 903,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c031e3d9ef9f4589a3dc2bb524091673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Target Lambda 0.9692799999999999\n",
      "Binary Crossentropy Target Lambda 0.10549376948265259\n",
      "Accuracy Lambda Decision 0.7242399999999999\n",
      "Binary Crossentropy Lambda Decision 1.3566034122989679\n",
      "Accuracy Target Decision 0.71496\n",
      "Binary Crossentropy Target Decision 1.4156571072780748\n"
     ]
    }
   ],
   "source": [
    "acc_target_lambda_list = []\n",
    "bc_target_lambda_list = []\n",
    "\n",
    "acc_lambda_decision_list = []\n",
    "bc_lambda_decision_list = []\n",
    "\n",
    "acc_target_decision_list = []\n",
    "bc_target_decision_list = []\n",
    "\n",
    "decision_function_parameters_list = []\n",
    "decision_functio_list = []\n",
    "\n",
    "for lambda_net in tqdm(lambda_net_dataset_test.lambda_net_list):\n",
    "    \n",
    "    target_function_parameters = lambda_net.target_function_parameters\n",
    "    target_function = lambda_net.target_function\n",
    "    \n",
    "    X_test_lambda = lambda_net.X_test_lambda\n",
    "    y_test_lambda = lambda_net.y_test_lambda\n",
    "    \n",
    "    network = lambda_net.network\n",
    "    network_parameters = lambda_net.network_parameters\n",
    "    \n",
    "    if config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['nas_type']['convolution_layers'] != 'SEQUENTIAL'):\n",
    "        network_parameters, network_parameters_flat = restructure_data_cnn_lstm(np.array([network_parameters]), config, subsequences=None)    \n",
    "      \n",
    "    decision_function_parameters= model.predict(np.array([network_parameters]))[0]\n",
    "    decision_function = generate_decision_tree_from_array(decision_function_parameters, config)\n",
    "    \n",
    "    decision_function_parameters_list.append(decision_function_parameters)\n",
    "    decision_functio_list.append(decision_function)\n",
    "    \n",
    "    y_test_network = network.predict(X_test_lambda)\n",
    "    y_test_decision_function = decision_function.predict_proba(X_test_lambda)\n",
    "    y_test_target_function = target_function.predict_proba(X_test_lambda)  \n",
    "    \n",
    "    acc_target_lambda = accuracy_score(np.round(y_test_target_function), np.round(y_test_network))\n",
    "    bc_target_lambda = log_loss(np.round(y_test_target_function), y_test_network, labels=[0, 1])\n",
    "    \n",
    "    acc_lambda_decision = accuracy_score(np.round(y_test_network), np.round(y_test_decision_function))\n",
    "    bc_lambda_decision = log_loss(np.round(y_test_network), y_test_decision_function, labels=[0, 1])        \n",
    "    \n",
    "    acc_target_decision = accuracy_score(np.round(y_test_target_function), np.round(y_test_decision_function))\n",
    "    bc_target_decision = log_loss(np.round(y_test_target_function), y_test_decision_function, labels=[0, 1])   \n",
    "    \n",
    "    \n",
    "    acc_target_lambda_list.append(acc_target_lambda)\n",
    "    bc_target_lambda_list.append(bc_target_lambda)\n",
    "\n",
    "    acc_lambda_decision_list.append(acc_lambda_decision)\n",
    "    bc_lambda_decision_list.append(bc_lambda_decision)\n",
    "\n",
    "    acc_target_decision_list.append(acc_target_decision)\n",
    "    bc_target_decision_list.append(bc_target_decision)\n",
    "    \n",
    "\n",
    "acc_target_lambda_array = np.array(acc_target_lambda_list)\n",
    "bc_target_lambda_array = np.array(bc_target_lambda_list)\n",
    "\n",
    "acc_lambda_decision_array = np.array(acc_lambda_decision_list)\n",
    "bc_lambda_decision_array = np.array(bc_lambda_decision_list)\n",
    "\n",
    "acc_target_decision_array = np.array(acc_target_decision_list)\n",
    "bc_target_decision_array = np.array(bc_target_decision_list)\n",
    "    \n",
    "    \n",
    "acc_target_lambda = np.mean(acc_target_lambda_array)\n",
    "bc_target_lambda = np.mean(bc_target_lambda_array[~np.isnan(bc_target_lambda_array)])\n",
    "\n",
    "acc_lambda_decision = np.mean(acc_lambda_decision_array)\n",
    "bc_lambda_decision = np.mean(bc_lambda_decision_array[~np.isnan(bc_lambda_decision_array)])\n",
    "\n",
    "acc_target_decision = np.mean(acc_target_decision_array)\n",
    "bc_target_decision = np.mean(bc_target_decision_array[~np.isnan(bc_target_decision_array)])\n",
    "\n",
    "\n",
    "print('Accuracy Target Lambda', acc_target_lambda)\n",
    "print('Binary Crossentropy Target Lambda', bc_target_lambda)\n",
    "print('Accuracy Lambda Decision', acc_lambda_decision)\n",
    "print('Binary Crossentropy Lambda Decision', bc_lambda_decision)\n",
    "print('Accuracy Target Decision', acc_target_decision)\n",
    "print('Binary Crossentropy Target Decision', bc_target_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.87835145,        nan, 0.3315129 , 1.84782498, 2.6737047 ,\n",
       "       0.98578728, 2.79318077, 1.11750519, 0.04059605, 1.13371203,\n",
       "       0.71593341, 0.73553898, 1.36269945, 1.21460975, 0.77976785,\n",
       "       0.59619638, 2.02672674, 1.49740196, 1.98324726, 0.40103348,\n",
       "       0.83884258, 1.29594132, 0.71956024, 0.89399608, 2.49531339,\n",
       "       0.29574561, 0.15801875, 2.12550824, 0.30690749, 1.59527147,\n",
       "       0.35149486, 1.31130772, 2.02894139, 2.31439072, 2.99973943,\n",
       "       0.60090846, 0.76955028, 0.38118154, 1.24988822, 2.72524036,\n",
       "       1.13304493, 0.5170824 , 0.1027604 , 2.09862221, 1.6672928 ,\n",
       "       0.81063014, 3.72472238, 0.83960252, 0.65012727,        nan])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_lambda_decision_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BENCHMARK RANDOM GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3a710d2a84f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################################################################\n",
    "#################################################################################################### END WORKING CODE ####################################################################################################\n",
    "##########################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not optimize_decision_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if not optimize_decision_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
