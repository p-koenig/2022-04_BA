{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.6 (default, Aug 18 2021, 19:38:01) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 4,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,    \n",
    "        'dt_type': 'vanilla', #'vanilla', 'SDT'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 30, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'make_classification_vanilla_decision_tree_trained', # 'make_classification' 'random_decision_tree' 'random_decision_tree_trained' 'random_vanilla_decision_tree_trained' 'make_classification_vanilla_decision_tree_trained'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [256],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.5, 0],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 200, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 2, # 1=standard representation; 2=sparse representation\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': True,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 15,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500, \n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "        'different_eval_data': False,\n",
    "        \n",
    "        'eval_data_description': {\n",
    "            ######### data #########\n",
    "            'eval_data_function_generation_type': 'make_classification',\n",
    "            'eval_data_lambda_dataset_size': 1000, #number of samples per function\n",
    "            'eval_data_noise_injected_level': 0, \n",
    "            'eval_data_noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'     \n",
    "            ######### lambda_net #########\n",
    "            'eval_data_number_of_trained_lambda_nets': 500,\n",
    "            ######### i_net #########\n",
    "            'eval_data_interpretation_dataset_size': 500,\n",
    "            \n",
    "        }\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from itertools import product       \n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "#from prettytable import PrettyTable\n",
    "#import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score, log_loss\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "\n",
    "\n",
    "#from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "#import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['basic_function_representation_length'] = (2 ** maximum_depth - 1) * number_of_variables + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes\n",
    "config['function_family']['function_representation_length'] = ( \n",
    "       ((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes  if function_representation_type == 1 and dt_type == 'SDT'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes if function_representation_type == 2 and dt_type == 'SDT'\n",
    "  else ((2 ** maximum_depth - 1) * decision_sparsity) * 2 + (2 ** maximum_depth)  if function_representation_type == 1 and dt_type == 'vanilla'\n",
    "  else (2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) if function_representation_type == 2 and dt_type == 'vanilla'\n",
    "  else None\n",
    "                                                            )\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize5000_numLNets10000_var30_class2_make_classification_vanilla_decision_tree_trained_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/256_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense512-1024_drop0.5-0e200b256_adam\n",
      "lNetSize5000_numLNets10000_var30_class2_make_classification_vanilla_decision_tree_trained_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/256_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    #if psutil.virtual_memory().percent > 80:\n",
    "        #raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   4 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-3)]: Done 116 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-3)]: Done 732 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-3)]: Done 1628 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-3)]: Done 2780 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-3)]: Done 4188 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-3)]: Done 5852 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-3)]: Done 7772 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-3)]: Done 9973 out of 10000 | elapsed:   44.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   44.1s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   19.4s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if different_eval_data:\n",
    "    config_train = deepcopy(config)\n",
    "    config_eval = deepcopy(config)\n",
    "    \n",
    "    config_eval['data']['function_generation_type'] = config['evaluation']['eval_data_description']['eval_data_function_generation_type']\n",
    "    config_eval['data']['lambda_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_lambda_dataset_size']\n",
    "    config_eval['data']['noise_injected_level'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_level']\n",
    "    config_eval['data']['noise_injected_type'] = config['evaluation']['eval_data_description']['eval_data_noise_injected_type'] \n",
    "    config_eval['lambda_net']['number_of_trained_lambda_nets'] = config['evaluation']['eval_data_description']['eval_data_number_of_trained_lambda_nets']   \n",
    "    config_eval['i_net']['interpretation_dataset_size'] = config['evaluation']['eval_data_description']['eval_data_interpretation_dataset_size']   \n",
    "    \n",
    "\n",
    "    lambda_net_dataset_train = load_lambda_nets(config_train, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_eval = load_lambda_nets(config_eval, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_eval, test_split=test_size)    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8955, 8692)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 8692)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 8692)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f0v17</th>\n",
       "      <th>f0v18</th>\n",
       "      <th>f0v19</th>\n",
       "      <th>f0v20</th>\n",
       "      <th>f0v21</th>\n",
       "      <th>f0v22</th>\n",
       "      <th>f0v23</th>\n",
       "      <th>f0v24</th>\n",
       "      <th>f0v25</th>\n",
       "      <th>f0v26</th>\n",
       "      <th>f0v27</th>\n",
       "      <th>f0v28</th>\n",
       "      <th>f0v29</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f1v17</th>\n",
       "      <th>f1v18</th>\n",
       "      <th>f1v19</th>\n",
       "      <th>f1v20</th>\n",
       "      <th>f1v21</th>\n",
       "      <th>f1v22</th>\n",
       "      <th>f1v23</th>\n",
       "      <th>f1v24</th>\n",
       "      <th>f1v25</th>\n",
       "      <th>f1v26</th>\n",
       "      <th>f1v27</th>\n",
       "      <th>f1v28</th>\n",
       "      <th>f1v29</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f2v17</th>\n",
       "      <th>f2v18</th>\n",
       "      <th>f2v19</th>\n",
       "      <th>f2v20</th>\n",
       "      <th>f2v21</th>\n",
       "      <th>f2v22</th>\n",
       "      <th>f2v23</th>\n",
       "      <th>f2v24</th>\n",
       "      <th>f2v25</th>\n",
       "      <th>f2v26</th>\n",
       "      <th>f2v27</th>\n",
       "      <th>f2v28</th>\n",
       "      <th>f2v29</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_8093</th>\n",
       "      <th>wb_8094</th>\n",
       "      <th>wb_8095</th>\n",
       "      <th>wb_8096</th>\n",
       "      <th>wb_8097</th>\n",
       "      <th>wb_8098</th>\n",
       "      <th>wb_8099</th>\n",
       "      <th>wb_8100</th>\n",
       "      <th>wb_8101</th>\n",
       "      <th>wb_8102</th>\n",
       "      <th>wb_8103</th>\n",
       "      <th>wb_8104</th>\n",
       "      <th>wb_8105</th>\n",
       "      <th>wb_8106</th>\n",
       "      <th>wb_8107</th>\n",
       "      <th>wb_8108</th>\n",
       "      <th>wb_8109</th>\n",
       "      <th>wb_8110</th>\n",
       "      <th>wb_8111</th>\n",
       "      <th>wb_8112</th>\n",
       "      <th>wb_8113</th>\n",
       "      <th>wb_8114</th>\n",
       "      <th>wb_8115</th>\n",
       "      <th>wb_8116</th>\n",
       "      <th>wb_8117</th>\n",
       "      <th>wb_8118</th>\n",
       "      <th>wb_8119</th>\n",
       "      <th>wb_8120</th>\n",
       "      <th>wb_8121</th>\n",
       "      <th>wb_8122</th>\n",
       "      <th>wb_8123</th>\n",
       "      <th>wb_8124</th>\n",
       "      <th>wb_8125</th>\n",
       "      <th>wb_8126</th>\n",
       "      <th>wb_8127</th>\n",
       "      <th>wb_8128</th>\n",
       "      <th>wb_8129</th>\n",
       "      <th>wb_8130</th>\n",
       "      <th>wb_8131</th>\n",
       "      <th>wb_8132</th>\n",
       "      <th>wb_8133</th>\n",
       "      <th>wb_8134</th>\n",
       "      <th>wb_8135</th>\n",
       "      <th>wb_8136</th>\n",
       "      <th>wb_8137</th>\n",
       "      <th>wb_8138</th>\n",
       "      <th>wb_8139</th>\n",
       "      <th>wb_8140</th>\n",
       "      <th>wb_8141</th>\n",
       "      <th>wb_8142</th>\n",
       "      <th>wb_8143</th>\n",
       "      <th>wb_8144</th>\n",
       "      <th>wb_8145</th>\n",
       "      <th>wb_8146</th>\n",
       "      <th>wb_8147</th>\n",
       "      <th>wb_8148</th>\n",
       "      <th>wb_8149</th>\n",
       "      <th>wb_8150</th>\n",
       "      <th>wb_8151</th>\n",
       "      <th>wb_8152</th>\n",
       "      <th>wb_8153</th>\n",
       "      <th>wb_8154</th>\n",
       "      <th>wb_8155</th>\n",
       "      <th>wb_8156</th>\n",
       "      <th>wb_8157</th>\n",
       "      <th>wb_8158</th>\n",
       "      <th>wb_8159</th>\n",
       "      <th>wb_8160</th>\n",
       "      <th>wb_8161</th>\n",
       "      <th>wb_8162</th>\n",
       "      <th>wb_8163</th>\n",
       "      <th>wb_8164</th>\n",
       "      <th>wb_8165</th>\n",
       "      <th>wb_8166</th>\n",
       "      <th>wb_8167</th>\n",
       "      <th>wb_8168</th>\n",
       "      <th>wb_8169</th>\n",
       "      <th>wb_8170</th>\n",
       "      <th>wb_8171</th>\n",
       "      <th>wb_8172</th>\n",
       "      <th>wb_8173</th>\n",
       "      <th>wb_8174</th>\n",
       "      <th>wb_8175</th>\n",
       "      <th>wb_8176</th>\n",
       "      <th>wb_8177</th>\n",
       "      <th>wb_8178</th>\n",
       "      <th>wb_8179</th>\n",
       "      <th>wb_8180</th>\n",
       "      <th>wb_8181</th>\n",
       "      <th>wb_8182</th>\n",
       "      <th>wb_8183</th>\n",
       "      <th>wb_8184</th>\n",
       "      <th>wb_8185</th>\n",
       "      <th>wb_8186</th>\n",
       "      <th>wb_8187</th>\n",
       "      <th>wb_8188</th>\n",
       "      <th>wb_8189</th>\n",
       "      <th>wb_8190</th>\n",
       "      <th>wb_8191</th>\n",
       "      <th>wb_8192</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>6671.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.729</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>-1.410</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.956</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-1.078</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-1.635</td>\n",
       "      <td>-1.711</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.879</td>\n",
       "      <td>-0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>3274.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.551</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-1.539</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.866</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.925</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-1.155</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>1.322</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-1.871</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.610</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.887</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.702</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-1.332</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>3095.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.774</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.988</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>1.504</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-1.152</td>\n",
       "      <td>1.351</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.764</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.152</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.066</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.410</td>\n",
       "      <td>1.539</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.780</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.219</td>\n",
       "      <td>1.252</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>1.128</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>8379.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.572</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>1.004</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>1.834</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>1.111</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-0.865</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-1.061</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>1.493</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.861</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>1.123</td>\n",
       "      <td>-1.631</td>\n",
       "      <td>1.621</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.163</td>\n",
       "      <td>1.681</td>\n",
       "      <td>0.665</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.369</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.931</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-1.823</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.685</td>\n",
       "      <td>1.330</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>2.001</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>1.041</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.673</td>\n",
       "      <td>1.042</td>\n",
       "      <td>-2.535</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-1.675</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>1.840</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.956</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.791</td>\n",
       "      <td>1.231</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1.065</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-1.626</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>1.630</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8692 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "6671 6671.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3274 3274.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3095 3095.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8379 8379.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3043 3043.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f0v17  f0v18  \\\n",
       "6671 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8379 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v19  f0v20  f0v21  f0v22  f0v23  f0v24  f0v25  f0v26  f0v27  f0v28  \\\n",
       "6671  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8379  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v29  f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  \\\n",
       "6671  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3274  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3095  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8379  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "3043  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v10  f1v11  f1v12  f1v13  f1v14  f1v15  f1v16  f1v17  f1v18  f1v19  \\\n",
       "6671  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8379  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v20  f1v21  f1v22  f1v23  f1v24  f1v25  f1v26  f1v27  f1v28  f1v29  \\\n",
       "6671  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8379  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f2v10  \\\n",
       "6671 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "3274 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "3095 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "8379 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "3043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "\n",
       "      f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f2v17  f2v18  f2v19  f2v20  \\\n",
       "6671  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8379  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v21  f2v22  f2v23  f2v24  f2v25  f2v26  f2v27  f2v28  f2v29  f3v0  \\\n",
       "6671  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "3274  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "8379  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f3v1  f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  ...  wb_8093  wb_8094  \\\n",
       "6671 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.298    0.729   \n",
       "3274 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -1.551   -0.187   \n",
       "3095 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.325    0.774   \n",
       "8379 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.119    0.572   \n",
       "3043 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -1.480   -0.899   \n",
       "\n",
       "      wb_8095  wb_8096  wb_8097  wb_8098  wb_8099  wb_8100  wb_8101  wb_8102  \\\n",
       "6671   -0.173   -0.559   -1.410   -0.142    0.070    0.030    0.360   -0.092   \n",
       "3274   -0.187   -0.067   -0.061   -0.142    0.070    0.030    0.152   -0.354   \n",
       "3095   -0.108   -0.284   -0.252   -0.142    0.070    0.030    0.019   -0.101   \n",
       "8379   -0.075   -0.109   -0.055   -0.142    0.070    0.030    0.119   -0.098   \n",
       "3043    0.329   -1.823   -0.064   -0.142    0.070    0.030    0.020   -0.100   \n",
       "\n",
       "      wb_8103  wb_8104  wb_8105  wb_8106  wb_8107  wb_8108  wb_8109  wb_8110  \\\n",
       "6671   -0.120    0.112   -0.368   -0.100   -0.956    0.028   -0.337   -0.029   \n",
       "3274   -0.125    0.039   -0.393   -0.100   -0.059    0.029   -0.012   -0.023   \n",
       "3095   -0.116    0.173    0.988   -0.100   -0.885    0.560   -0.097   -0.297   \n",
       "8379   -0.152    0.073   -0.759   -0.100   -0.057    1.004   -0.203   -0.029   \n",
       "3043   -0.435    0.076   -0.167   -0.100   -0.201    0.028   -0.276   -0.029   \n",
       "\n",
       "      wb_8111  wb_8112  wb_8113  wb_8114  wb_8115  wb_8116  wb_8117  wb_8118  \\\n",
       "6671   -0.114   -0.137   -0.156    0.175   -0.146   -1.078   -0.126   -0.251   \n",
       "3274   -0.109   -0.137   -0.256    0.091   -0.146   -1.539   -0.126   -0.852   \n",
       "3095   -0.257   -0.137   -0.113    0.230   -0.146   -0.022   -0.628    0.061   \n",
       "8379   -0.109   -0.137   -0.067    0.100   -0.146   -0.039   -0.126   -0.610   \n",
       "3043   -0.108   -0.137   -0.522    0.199   -0.146   -0.033   -0.122   -0.685   \n",
       "\n",
       "      wb_8119  wb_8120  wb_8121  wb_8122  wb_8123  wb_8124  wb_8125  wb_8126  \\\n",
       "6671    0.070    0.163   -0.152    0.126   -0.190   -0.276   -0.181    0.461   \n",
       "3274    0.220    0.866   -0.152    0.925   -0.037    0.102   -1.155    0.156   \n",
       "3095    0.099    0.226   -0.146    1.504   -0.037   -0.187   -0.264   -0.578   \n",
       "8379    0.270    0.135   -0.152    0.127   -0.038    0.023   -0.125    0.131   \n",
       "3043    1.330    0.175   -0.152    2.001   -0.838    1.041   -0.050   -0.115   \n",
       "\n",
       "      wb_8127  wb_8128  wb_8129  wb_8130  wb_8131  wb_8132  wb_8133  wb_8134  \\\n",
       "6671   -0.199    0.250   -0.282   -0.053    0.086    0.119    0.014   -0.297   \n",
       "3274   -0.052    1.322   -0.964   -0.053    0.999    0.062    0.014   -0.275   \n",
       "3095   -1.152    1.351   -0.234   -0.052    0.204    0.143    0.014   -0.181   \n",
       "8379   -0.059    1.834   -0.144   -0.053    1.111    0.711    0.014   -0.477   \n",
       "3043    0.673    1.042   -2.535   -0.054    1.132    0.110    0.014   -0.851   \n",
       "\n",
       "      wb_8135  wb_8136  wb_8137  wb_8138  wb_8139  wb_8140  wb_8141  wb_8142  \\\n",
       "6671   -0.613    0.264   -0.233   -0.236    0.135    0.205   -0.810    0.110   \n",
       "3274   -1.871    0.125   -0.568    0.038    0.135    0.156   -0.578    0.108   \n",
       "3095   -1.421    0.541    0.216   -0.389    0.135    0.203   -0.438    0.392   \n",
       "8379   -0.865    0.886    0.672   -1.061    0.135    0.243   -0.216    1.493   \n",
       "3043   -1.675    0.104   -0.105   -0.324    0.135    0.069   -0.074    1.840   \n",
       "\n",
       "      wb_8143  wb_8144  wb_8145  wb_8146  wb_8147  wb_8148  wb_8149  wb_8150  \\\n",
       "6671   -0.176    0.239   -0.341    0.132   -0.028    0.858    0.145    0.371   \n",
       "3274   -0.136    0.360   -0.069    0.610   -0.025    0.260    0.152    0.001   \n",
       "3095   -0.114    0.564    0.115    0.764   -0.032    0.913    0.152    1.378   \n",
       "8379    0.044    0.861   -0.078    1.123   -1.631    1.621    0.152    0.163   \n",
       "3043   -0.974    0.007   -0.822    0.152   -0.026    0.135    0.152    0.370   \n",
       "\n",
       "      wb_8151  wb_8152  wb_8153  wb_8154  wb_8155  wb_8156  wb_8157  wb_8158  \\\n",
       "6671    0.088    0.382   -0.252   -0.025   -0.180    0.324    0.119   -0.221   \n",
       "3274    0.127    0.141   -1.526   -0.004   -0.068    0.326    0.919   -0.370   \n",
       "3095    1.066    0.269   -0.181   -0.129   -0.134    0.410    1.539   -0.138   \n",
       "8379    1.681    0.665   -0.353   -0.099   -0.171    0.137    0.121    0.951   \n",
       "3043    0.252    0.956   -1.460   -0.022   -0.162    0.791    1.231    0.946   \n",
       "\n",
       "      wb_8159  wb_8160  wb_8161  wb_8162  wb_8163  wb_8164  wb_8165  wb_8166  \\\n",
       "6671    0.002    0.017    0.130    0.024    0.066   -0.537   -0.730    0.114   \n",
       "3274    0.020    0.017    0.039    0.019    0.076   -0.435   -0.376    0.455   \n",
       "3095    0.003    0.017    0.096    0.024    0.076   -0.254   -0.505    0.230   \n",
       "8379    0.006    0.017   -0.146    0.024    0.076   -0.283   -0.173    0.230   \n",
       "3043    1.065    0.017    0.127    0.024    0.070   -0.074   -0.098    0.166   \n",
       "\n",
       "      wb_8167  wb_8168  wb_8169  wb_8170  wb_8171  wb_8172  wb_8173  wb_8174  \\\n",
       "6671   -0.150    0.230   -0.969    0.365    0.033   -0.142   -0.082   -0.012   \n",
       "3274   -0.150    0.887   -0.867    0.160    0.034   -0.368   -0.098   -0.012   \n",
       "3095   -0.150    0.849   -0.059    0.391    0.035   -0.136   -0.099   -0.012   \n",
       "8379   -0.150    0.160   -0.075    0.902    0.880   -0.142   -0.105   -0.012   \n",
       "3043   -0.150    0.218    0.017    0.152    0.037   -0.137   -0.063   -0.012   \n",
       "\n",
       "      wb_8175  wb_8176  wb_8177  wb_8178  wb_8179  wb_8180  wb_8181  wb_8182  \\\n",
       "6671    0.161    0.286    0.050    0.119   -0.059   -0.046   -0.104   -1.635   \n",
       "3274    0.239    0.010    0.050    1.702   -0.090   -0.046   -0.123   -0.065   \n",
       "3095    0.041    0.009    0.037    0.780   -0.133   -0.046   -0.200   -0.112   \n",
       "8379    0.150    0.187    0.043    0.116   -0.155   -0.038   -0.118   -0.063   \n",
       "3043    0.116    0.016    0.041    0.113   -0.812   -0.046   -0.109   -1.626   \n",
       "\n",
       "      wb_8183  wb_8184  wb_8185  wb_8186  wb_8187  wb_8188  wb_8189  wb_8190  \\\n",
       "6671   -1.711    0.174   -0.341    1.030   -0.097    0.215   -0.153   -0.266   \n",
       "3274   -0.378    0.209    0.036    0.122   -1.332    0.083   -0.072   -0.119   \n",
       "3095   -0.594    0.212    0.219    1.252   -0.306    1.478   -0.127   -0.270   \n",
       "8379   -0.114    0.697    0.004    1.369   -0.356    0.931   -0.076   -0.301   \n",
       "3043   -0.927    1.630    0.004    0.166   -0.662    0.085   -0.197   -0.122   \n",
       "\n",
       "      wb_8191  wb_8192  \n",
       "6671    0.879   -0.059  \n",
       "3274    0.157    0.038  \n",
       "3095    1.128    0.059  \n",
       "8379    0.520    0.063  \n",
       "3043    0.548    0.096  \n",
       "\n",
       "[5 rows x 8692 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f0v17</th>\n",
       "      <th>f0v18</th>\n",
       "      <th>f0v19</th>\n",
       "      <th>f0v20</th>\n",
       "      <th>f0v21</th>\n",
       "      <th>f0v22</th>\n",
       "      <th>f0v23</th>\n",
       "      <th>f0v24</th>\n",
       "      <th>f0v25</th>\n",
       "      <th>f0v26</th>\n",
       "      <th>f0v27</th>\n",
       "      <th>f0v28</th>\n",
       "      <th>f0v29</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f1v17</th>\n",
       "      <th>f1v18</th>\n",
       "      <th>f1v19</th>\n",
       "      <th>f1v20</th>\n",
       "      <th>f1v21</th>\n",
       "      <th>f1v22</th>\n",
       "      <th>f1v23</th>\n",
       "      <th>f1v24</th>\n",
       "      <th>f1v25</th>\n",
       "      <th>f1v26</th>\n",
       "      <th>f1v27</th>\n",
       "      <th>f1v28</th>\n",
       "      <th>f1v29</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f2v17</th>\n",
       "      <th>f2v18</th>\n",
       "      <th>f2v19</th>\n",
       "      <th>f2v20</th>\n",
       "      <th>f2v21</th>\n",
       "      <th>f2v22</th>\n",
       "      <th>f2v23</th>\n",
       "      <th>f2v24</th>\n",
       "      <th>f2v25</th>\n",
       "      <th>f2v26</th>\n",
       "      <th>f2v27</th>\n",
       "      <th>f2v28</th>\n",
       "      <th>f2v29</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_8093</th>\n",
       "      <th>wb_8094</th>\n",
       "      <th>wb_8095</th>\n",
       "      <th>wb_8096</th>\n",
       "      <th>wb_8097</th>\n",
       "      <th>wb_8098</th>\n",
       "      <th>wb_8099</th>\n",
       "      <th>wb_8100</th>\n",
       "      <th>wb_8101</th>\n",
       "      <th>wb_8102</th>\n",
       "      <th>wb_8103</th>\n",
       "      <th>wb_8104</th>\n",
       "      <th>wb_8105</th>\n",
       "      <th>wb_8106</th>\n",
       "      <th>wb_8107</th>\n",
       "      <th>wb_8108</th>\n",
       "      <th>wb_8109</th>\n",
       "      <th>wb_8110</th>\n",
       "      <th>wb_8111</th>\n",
       "      <th>wb_8112</th>\n",
       "      <th>wb_8113</th>\n",
       "      <th>wb_8114</th>\n",
       "      <th>wb_8115</th>\n",
       "      <th>wb_8116</th>\n",
       "      <th>wb_8117</th>\n",
       "      <th>wb_8118</th>\n",
       "      <th>wb_8119</th>\n",
       "      <th>wb_8120</th>\n",
       "      <th>wb_8121</th>\n",
       "      <th>wb_8122</th>\n",
       "      <th>wb_8123</th>\n",
       "      <th>wb_8124</th>\n",
       "      <th>wb_8125</th>\n",
       "      <th>wb_8126</th>\n",
       "      <th>wb_8127</th>\n",
       "      <th>wb_8128</th>\n",
       "      <th>wb_8129</th>\n",
       "      <th>wb_8130</th>\n",
       "      <th>wb_8131</th>\n",
       "      <th>wb_8132</th>\n",
       "      <th>wb_8133</th>\n",
       "      <th>wb_8134</th>\n",
       "      <th>wb_8135</th>\n",
       "      <th>wb_8136</th>\n",
       "      <th>wb_8137</th>\n",
       "      <th>wb_8138</th>\n",
       "      <th>wb_8139</th>\n",
       "      <th>wb_8140</th>\n",
       "      <th>wb_8141</th>\n",
       "      <th>wb_8142</th>\n",
       "      <th>wb_8143</th>\n",
       "      <th>wb_8144</th>\n",
       "      <th>wb_8145</th>\n",
       "      <th>wb_8146</th>\n",
       "      <th>wb_8147</th>\n",
       "      <th>wb_8148</th>\n",
       "      <th>wb_8149</th>\n",
       "      <th>wb_8150</th>\n",
       "      <th>wb_8151</th>\n",
       "      <th>wb_8152</th>\n",
       "      <th>wb_8153</th>\n",
       "      <th>wb_8154</th>\n",
       "      <th>wb_8155</th>\n",
       "      <th>wb_8156</th>\n",
       "      <th>wb_8157</th>\n",
       "      <th>wb_8158</th>\n",
       "      <th>wb_8159</th>\n",
       "      <th>wb_8160</th>\n",
       "      <th>wb_8161</th>\n",
       "      <th>wb_8162</th>\n",
       "      <th>wb_8163</th>\n",
       "      <th>wb_8164</th>\n",
       "      <th>wb_8165</th>\n",
       "      <th>wb_8166</th>\n",
       "      <th>wb_8167</th>\n",
       "      <th>wb_8168</th>\n",
       "      <th>wb_8169</th>\n",
       "      <th>wb_8170</th>\n",
       "      <th>wb_8171</th>\n",
       "      <th>wb_8172</th>\n",
       "      <th>wb_8173</th>\n",
       "      <th>wb_8174</th>\n",
       "      <th>wb_8175</th>\n",
       "      <th>wb_8176</th>\n",
       "      <th>wb_8177</th>\n",
       "      <th>wb_8178</th>\n",
       "      <th>wb_8179</th>\n",
       "      <th>wb_8180</th>\n",
       "      <th>wb_8181</th>\n",
       "      <th>wb_8182</th>\n",
       "      <th>wb_8183</th>\n",
       "      <th>wb_8184</th>\n",
       "      <th>wb_8185</th>\n",
       "      <th>wb_8186</th>\n",
       "      <th>wb_8187</th>\n",
       "      <th>wb_8188</th>\n",
       "      <th>wb_8189</th>\n",
       "      <th>wb_8190</th>\n",
       "      <th>wb_8191</th>\n",
       "      <th>wb_8192</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-1.064</td>\n",
       "      <td>-1.676</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.661</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-1.061</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1.246</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>1.405</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>1.149</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>1.725</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>1.478</td>\n",
       "      <td>1.289</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.459</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.587</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>1.787</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>689.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-1.255</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.931</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.511</td>\n",
       "      <td>1.332</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-2.001</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-1.944</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>1.004</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>4148.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-1.279</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>1.138</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>1.616</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.247</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2815.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-1.578</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-1.844</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>2.463</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.245</td>\n",
       "      <td>1.701</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>1.660</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>1.211</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.884</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.944</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>5185.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>0.858</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.911</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.168</td>\n",
       "      <td>1.241</td>\n",
       "      <td>1.425</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-1.355</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>1.150</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-1.416</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8692 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "3466 3466.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "689   689.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4148 4148.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "2815 2815.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5185 5185.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f0v17  f0v18  \\\n",
       "3466 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "689  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "2815 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5185 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v19  f0v20  f0v21  f0v22  f0v23  f0v24  f0v25  f0v26  f0v27  f0v28  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "689   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v29  f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  \\\n",
       "3466  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "689   0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4148  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "2815  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5185  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v10  f1v11  f1v12  f1v13  f1v14  f1v15  f1v16  f1v17  f1v18  f1v19  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "689   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v20  f1v21  f1v22  f1v23  f1v24  f1v25  f1v26  f1v27  f1v28  f1v29  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "689   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f2v10  \\\n",
       "3466 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "689  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "4148 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "2815 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "5185 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "\n",
       "      f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f2v17  f2v18  f2v19  f2v20  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "689   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v21  f2v22  f2v23  f2v24  f2v25  f2v26  f2v27  f2v28  f2v29  f3v0  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "689   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "4148  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "2815  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f3v1  f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  ...  wb_8093  wb_8094  \\\n",
       "3466 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.272   -0.202   \n",
       "689  0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -1.006   -0.281   \n",
       "4148 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.755   -0.185   \n",
       "2815 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.468   -1.578   \n",
       "5185 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.340   -0.214   \n",
       "\n",
       "      wb_8095  wb_8096  wb_8097  wb_8098  wb_8099  wb_8100  wb_8101  wb_8102  \\\n",
       "3466   -0.469   -0.776   -0.269   -0.142    0.070    0.030    0.193   -0.101   \n",
       "689    -0.102   -0.505   -0.059   -0.142    0.070    0.030    0.145   -0.102   \n",
       "4148   -0.190   -0.555   -0.558   -0.142    0.070    0.030    0.062   -0.101   \n",
       "2815   -0.237   -0.167   -1.844   -0.142    0.070    0.030    0.460   -0.102   \n",
       "5185   -0.088   -0.859   -0.061   -0.142    0.070    0.030    0.091   -0.152   \n",
       "\n",
       "      wb_8103  wb_8104  wb_8105  wb_8106  wb_8107  wb_8108  wb_8109  wb_8110  \\\n",
       "3466   -0.086    0.280   -0.176   -0.100   -0.407    0.029   -1.064   -1.676   \n",
       "689    -0.272    0.123   -0.040   -0.100   -0.275    0.288   -2.024   -0.029   \n",
       "4148   -0.151    0.201   -1.421   -0.100   -0.060    0.275   -0.162   -0.271   \n",
       "2815   -0.019    0.117   -0.077   -0.100   -0.060    0.771   -0.481   -0.022   \n",
       "5185   -0.040    0.217   -0.151   -0.100   -0.683    0.858   -0.937   -0.018   \n",
       "\n",
       "      wb_8111  wb_8112  wb_8113  wb_8114  wb_8115  wb_8116  wb_8117  wb_8118  \\\n",
       "3466   -0.114   -0.137   -0.082    0.661   -0.146   -0.031   -0.126   -0.137   \n",
       "689    -0.108   -0.137   -1.255    0.306   -0.146   -0.031   -0.126   -1.037   \n",
       "4148   -0.114   -0.137    0.057    0.177   -0.146   -0.249   -0.126   -0.267   \n",
       "2815   -0.107   -0.137   -0.265    0.108   -0.146   -0.033   -0.126   -0.140   \n",
       "5185   -0.237   -0.137   -0.837    0.236   -0.146   -0.032   -0.126   -0.569   \n",
       "\n",
       "      wb_8119  wb_8120  wb_8121  wb_8122  wb_8123  wb_8124  wb_8125  wb_8126  \\\n",
       "3466    0.568    0.150   -0.152    0.086   -0.042   -0.132   -0.108    0.496   \n",
       "689     0.075    0.200   -0.152    0.381   -0.039   -0.213   -0.212    0.238   \n",
       "4148    0.303    0.098   -0.152    0.259   -0.036   -0.543   -0.635   -1.279   \n",
       "2815    0.309    0.171   -0.152    0.378   -0.362   -0.217   -0.251   -0.245   \n",
       "5185    0.081    0.307   -0.152    0.234   -0.037   -0.076   -0.808   -0.122   \n",
       "\n",
       "      wb_8127  wb_8128  wb_8129  wb_8130  wb_8131  wb_8132  wb_8133  wb_8134  \\\n",
       "3466   -0.508    0.062   -0.120   -0.050    0.082    0.112    0.014   -0.072   \n",
       "689    -0.271    0.427   -0.283   -0.338    0.453    0.290    0.014   -0.361   \n",
       "4148   -0.209    1.138   -0.474   -0.237    0.082    0.114    0.014   -0.365   \n",
       "2815   -0.489    2.463   -0.282   -0.053    0.094    0.158    0.014   -0.799   \n",
       "5185    0.045    0.403   -0.414   -0.051    0.792    0.007    0.014   -0.241   \n",
       "\n",
       "      wb_8135  wb_8136  wb_8137  wb_8138  wb_8139  wb_8140  wb_8141  wb_8142  \\\n",
       "3466   -0.067    0.116   -0.151   -1.061    0.135    0.158   -0.887    0.210   \n",
       "689    -0.053    0.037   -0.336   -0.009    0.135    0.177   -0.365    0.931   \n",
       "4148   -0.141    0.323    0.230   -0.492    0.135    0.201   -0.142    0.110   \n",
       "2815   -0.545    0.166    0.245    1.701    0.135    0.381   -0.297    0.156   \n",
       "5185   -0.134    0.090   -0.426   -0.695    0.135    0.243   -0.143    0.106   \n",
       "\n",
       "      wb_8143  wb_8144  wb_8145  wb_8146  wb_8147  wb_8148  wb_8149  wb_8150  \\\n",
       "3466    1.246    0.026   -0.088    0.239   -0.032    1.405    0.152    0.157   \n",
       "689    -0.522    0.500    0.126    0.374   -0.032    0.921    0.874    0.800   \n",
       "4148   -0.466    0.174   -0.043    0.326   -0.586    0.309    0.152    0.169   \n",
       "2815   -0.218    0.183   -0.077    1.660   -0.032    0.109    0.147    0.007   \n",
       "5185   -0.031    0.724    0.019    0.911   -0.027    1.424    0.152    0.168   \n",
       "\n",
       "      wb_8151  wb_8152  wb_8153  wb_8154  wb_8155  wb_8156  wb_8157  wb_8158  \\\n",
       "3466    0.851    0.147   -0.104   -0.144   -0.155    1.149    0.221   -0.139   \n",
       "689     0.511    1.332   -0.397   -0.248   -0.106    0.399    0.473   -0.179   \n",
       "4148    0.190    0.321   -0.910   -0.453   -0.182    0.242    0.152   -0.179   \n",
       "2815    0.085    0.142   -0.640   -0.174   -0.127    1.211    0.091   -0.089   \n",
       "5185    1.241    1.425   -0.096   -0.377   -0.053    0.239    0.205   -0.477   \n",
       "\n",
       "      wb_8159  wb_8160  wb_8161  wb_8162  wb_8163  wb_8164  wb_8165  wb_8166  \\\n",
       "3466    0.093    0.017    0.022    0.024    0.076   -0.199   -0.314    0.414   \n",
       "689     0.003    0.017    0.027    0.277    0.405   -0.334   -2.001    0.166   \n",
       "4148    0.013    0.017    0.078    0.024    0.076   -0.087   -0.247    1.616   \n",
       "2815    0.315    0.017    0.124    0.291    0.076   -0.269   -0.321    0.884   \n",
       "5185    0.006    0.017    0.001    0.024    0.076   -0.140   -1.355    0.084   \n",
       "\n",
       "      wb_8167  wb_8168  wb_8169  wb_8170  wb_8171  wb_8172  wb_8173  wb_8174  \\\n",
       "3466   -0.150    1.725   -0.678    1.478    1.289   -0.142   -0.057   -0.012   \n",
       "689    -0.150    0.063   -1.944    0.407    0.437   -0.413   -0.191   -0.012   \n",
       "4148   -0.150    0.071   -0.096    0.055    0.036   -0.140   -0.106   -0.012   \n",
       "2815   -0.150    0.244    0.275    0.307    0.041   -0.142   -0.066   -0.012   \n",
       "5185   -0.150    1.150   -0.089    0.050    0.123   -0.140   -0.113   -0.012   \n",
       "\n",
       "      wb_8175  wb_8176  wb_8177  wb_8178  wb_8179  wb_8180  wb_8181  wb_8182  \\\n",
       "3466    0.078    1.459    0.044    0.357   -0.171   -0.046   -0.145   -0.055   \n",
       "689     0.105    0.282    0.050    0.120   -0.119   -0.046   -0.198   -0.345   \n",
       "4148    0.045    0.010    0.055    0.293   -0.115   -0.046   -0.207   -0.056   \n",
       "2815    0.177    0.009    0.050    0.119   -0.079   -0.046   -0.214   -0.074   \n",
       "5185    0.048    0.009    0.044    0.119   -0.112   -1.416   -0.165   -0.059   \n",
       "\n",
       "      wb_8183  wb_8184  wb_8185  wb_8186  wb_8187  wb_8188  wb_8189  wb_8190  \\\n",
       "3466   -0.114    0.513    0.196    0.587   -0.746    1.787   -0.055   -0.772   \n",
       "689    -0.114    0.371   -0.208    1.004   -0.095    0.243   -0.272   -0.141   \n",
       "4148   -0.113    0.071    0.453    0.207   -0.260    0.247   -0.102   -0.332   \n",
       "2815   -0.114    0.281   -0.013    0.944   -0.098    0.064   -0.133   -0.919   \n",
       "5185   -0.109    0.187    0.001    0.198   -0.155    0.054   -0.174   -0.203   \n",
       "\n",
       "      wb_8191  wb_8192  \n",
       "3466    0.186    0.088  \n",
       "689     0.806    0.120  \n",
       "4148    0.126    0.054  \n",
       "2815    0.753   -0.046  \n",
       "5185    0.241   -0.028  \n",
       "\n",
       "[5 rows x 8692 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f0v5</th>\n",
       "      <th>f0v6</th>\n",
       "      <th>f0v7</th>\n",
       "      <th>f0v8</th>\n",
       "      <th>f0v9</th>\n",
       "      <th>f0v10</th>\n",
       "      <th>f0v11</th>\n",
       "      <th>f0v12</th>\n",
       "      <th>f0v13</th>\n",
       "      <th>f0v14</th>\n",
       "      <th>f0v15</th>\n",
       "      <th>f0v16</th>\n",
       "      <th>f0v17</th>\n",
       "      <th>f0v18</th>\n",
       "      <th>f0v19</th>\n",
       "      <th>f0v20</th>\n",
       "      <th>f0v21</th>\n",
       "      <th>f0v22</th>\n",
       "      <th>f0v23</th>\n",
       "      <th>f0v24</th>\n",
       "      <th>f0v25</th>\n",
       "      <th>f0v26</th>\n",
       "      <th>f0v27</th>\n",
       "      <th>f0v28</th>\n",
       "      <th>f0v29</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f1v5</th>\n",
       "      <th>f1v6</th>\n",
       "      <th>f1v7</th>\n",
       "      <th>f1v8</th>\n",
       "      <th>f1v9</th>\n",
       "      <th>f1v10</th>\n",
       "      <th>f1v11</th>\n",
       "      <th>f1v12</th>\n",
       "      <th>f1v13</th>\n",
       "      <th>f1v14</th>\n",
       "      <th>f1v15</th>\n",
       "      <th>f1v16</th>\n",
       "      <th>f1v17</th>\n",
       "      <th>f1v18</th>\n",
       "      <th>f1v19</th>\n",
       "      <th>f1v20</th>\n",
       "      <th>f1v21</th>\n",
       "      <th>f1v22</th>\n",
       "      <th>f1v23</th>\n",
       "      <th>f1v24</th>\n",
       "      <th>f1v25</th>\n",
       "      <th>f1v26</th>\n",
       "      <th>f1v27</th>\n",
       "      <th>f1v28</th>\n",
       "      <th>f1v29</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f2v5</th>\n",
       "      <th>f2v6</th>\n",
       "      <th>f2v7</th>\n",
       "      <th>f2v8</th>\n",
       "      <th>f2v9</th>\n",
       "      <th>f2v10</th>\n",
       "      <th>f2v11</th>\n",
       "      <th>f2v12</th>\n",
       "      <th>f2v13</th>\n",
       "      <th>f2v14</th>\n",
       "      <th>f2v15</th>\n",
       "      <th>f2v16</th>\n",
       "      <th>f2v17</th>\n",
       "      <th>f2v18</th>\n",
       "      <th>f2v19</th>\n",
       "      <th>f2v20</th>\n",
       "      <th>f2v21</th>\n",
       "      <th>f2v22</th>\n",
       "      <th>f2v23</th>\n",
       "      <th>f2v24</th>\n",
       "      <th>f2v25</th>\n",
       "      <th>f2v26</th>\n",
       "      <th>f2v27</th>\n",
       "      <th>f2v28</th>\n",
       "      <th>f2v29</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f3v5</th>\n",
       "      <th>f3v6</th>\n",
       "      <th>f3v7</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_8093</th>\n",
       "      <th>wb_8094</th>\n",
       "      <th>wb_8095</th>\n",
       "      <th>wb_8096</th>\n",
       "      <th>wb_8097</th>\n",
       "      <th>wb_8098</th>\n",
       "      <th>wb_8099</th>\n",
       "      <th>wb_8100</th>\n",
       "      <th>wb_8101</th>\n",
       "      <th>wb_8102</th>\n",
       "      <th>wb_8103</th>\n",
       "      <th>wb_8104</th>\n",
       "      <th>wb_8105</th>\n",
       "      <th>wb_8106</th>\n",
       "      <th>wb_8107</th>\n",
       "      <th>wb_8108</th>\n",
       "      <th>wb_8109</th>\n",
       "      <th>wb_8110</th>\n",
       "      <th>wb_8111</th>\n",
       "      <th>wb_8112</th>\n",
       "      <th>wb_8113</th>\n",
       "      <th>wb_8114</th>\n",
       "      <th>wb_8115</th>\n",
       "      <th>wb_8116</th>\n",
       "      <th>wb_8117</th>\n",
       "      <th>wb_8118</th>\n",
       "      <th>wb_8119</th>\n",
       "      <th>wb_8120</th>\n",
       "      <th>wb_8121</th>\n",
       "      <th>wb_8122</th>\n",
       "      <th>wb_8123</th>\n",
       "      <th>wb_8124</th>\n",
       "      <th>wb_8125</th>\n",
       "      <th>wb_8126</th>\n",
       "      <th>wb_8127</th>\n",
       "      <th>wb_8128</th>\n",
       "      <th>wb_8129</th>\n",
       "      <th>wb_8130</th>\n",
       "      <th>wb_8131</th>\n",
       "      <th>wb_8132</th>\n",
       "      <th>wb_8133</th>\n",
       "      <th>wb_8134</th>\n",
       "      <th>wb_8135</th>\n",
       "      <th>wb_8136</th>\n",
       "      <th>wb_8137</th>\n",
       "      <th>wb_8138</th>\n",
       "      <th>wb_8139</th>\n",
       "      <th>wb_8140</th>\n",
       "      <th>wb_8141</th>\n",
       "      <th>wb_8142</th>\n",
       "      <th>wb_8143</th>\n",
       "      <th>wb_8144</th>\n",
       "      <th>wb_8145</th>\n",
       "      <th>wb_8146</th>\n",
       "      <th>wb_8147</th>\n",
       "      <th>wb_8148</th>\n",
       "      <th>wb_8149</th>\n",
       "      <th>wb_8150</th>\n",
       "      <th>wb_8151</th>\n",
       "      <th>wb_8152</th>\n",
       "      <th>wb_8153</th>\n",
       "      <th>wb_8154</th>\n",
       "      <th>wb_8155</th>\n",
       "      <th>wb_8156</th>\n",
       "      <th>wb_8157</th>\n",
       "      <th>wb_8158</th>\n",
       "      <th>wb_8159</th>\n",
       "      <th>wb_8160</th>\n",
       "      <th>wb_8161</th>\n",
       "      <th>wb_8162</th>\n",
       "      <th>wb_8163</th>\n",
       "      <th>wb_8164</th>\n",
       "      <th>wb_8165</th>\n",
       "      <th>wb_8166</th>\n",
       "      <th>wb_8167</th>\n",
       "      <th>wb_8168</th>\n",
       "      <th>wb_8169</th>\n",
       "      <th>wb_8170</th>\n",
       "      <th>wb_8171</th>\n",
       "      <th>wb_8172</th>\n",
       "      <th>wb_8173</th>\n",
       "      <th>wb_8174</th>\n",
       "      <th>wb_8175</th>\n",
       "      <th>wb_8176</th>\n",
       "      <th>wb_8177</th>\n",
       "      <th>wb_8178</th>\n",
       "      <th>wb_8179</th>\n",
       "      <th>wb_8180</th>\n",
       "      <th>wb_8181</th>\n",
       "      <th>wb_8182</th>\n",
       "      <th>wb_8183</th>\n",
       "      <th>wb_8184</th>\n",
       "      <th>wb_8185</th>\n",
       "      <th>wb_8186</th>\n",
       "      <th>wb_8187</th>\n",
       "      <th>wb_8188</th>\n",
       "      <th>wb_8189</th>\n",
       "      <th>wb_8190</th>\n",
       "      <th>wb_8191</th>\n",
       "      <th>wb_8192</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-1.154</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.669</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>1.487</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.521</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.196</td>\n",
       "      <td>1.402</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>0.660</td>\n",
       "      <td>1.041</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>1.824</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>1.575</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.651</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.576</td>\n",
       "      <td>-0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-1.057</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.785</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-1.145</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-1.042</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>1.111</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>1.712</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.015</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.956</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>1.634</td>\n",
       "      <td>-0.047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8692 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2  f0v3  f0v4  f0v5  f0v6  f0v7  f0v8  \\\n",
       "7217 7217.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291 8291.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607 4607.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114 5114.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859 1859.000    42 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f0v9  f0v10  f0v11  f0v12  f0v13  f0v14  f0v15  f0v16  f0v17  f0v18  \\\n",
       "7217 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v19  f0v20  f0v21  f0v22  f0v23  f0v24  f0v25  f0v26  f0v27  f0v28  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f0v29  f1v0  f1v1  f1v2  f1v3  f1v4  f1v5  f1v6  f1v7  f1v8  f1v9  \\\n",
       "7217  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "8291  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4607  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "5114  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1859  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "\n",
       "      f1v10  f1v11  f1v12  f1v13  f1v14  f1v15  f1v16  f1v17  f1v18  f1v19  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f1v20  f1v21  f1v22  f1v23  f1v24  f1v25  f1v26  f1v27  f1v28  f1v29  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v0  f2v1  f2v2  f2v3  f2v4  f2v5  f2v6  f2v7  f2v8  f2v9  f2v10  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  0.000   \n",
       "\n",
       "      f2v11  f2v12  f2v13  f2v14  f2v15  f2v16  f2v17  f2v18  f2v19  f2v20  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f2v21  f2v22  f2v23  f2v24  f2v25  f2v26  f2v27  f2v28  f2v29  f3v0  \\\n",
       "7217  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f3v1  f3v2  f3v3  f3v4  f3v5  f3v6  f3v7  ...  wb_8093  wb_8094  \\\n",
       "7217 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.293   -0.244   \n",
       "8291 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.139    0.004   \n",
       "4607 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.425   -0.211   \n",
       "5114 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.204   -0.210   \n",
       "1859 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ...   -0.523    0.007   \n",
       "\n",
       "      wb_8095  wb_8096  wb_8097  wb_8098  wb_8099  wb_8100  wb_8101  wb_8102  \\\n",
       "7217   -0.145   -0.114   -0.274   -0.142    0.070    0.030    0.186   -0.368   \n",
       "8291   -0.082   -0.186   -0.624   -0.142    0.070    0.030    0.238   -0.198   \n",
       "4607   -0.103   -0.324   -0.122   -0.142    0.070    0.030    0.106   -0.093   \n",
       "5114   -0.161   -0.194   -0.262   -0.142    0.070    0.030    0.130   -0.106   \n",
       "1859   -0.026   -1.057   -0.061   -0.142    0.070    0.030    0.785   -0.099   \n",
       "\n",
       "      wb_8103  wb_8104  wb_8105  wb_8106  wb_8107  wb_8108  wb_8109  wb_8110  \\\n",
       "7217   -0.246    0.153   -0.283   -0.100   -0.061    0.221   -0.072   -0.023   \n",
       "8291   -0.108    0.175   -1.154   -0.100   -0.276    0.640   -0.319   -0.364   \n",
       "4607   -0.090    0.069   -0.256   -0.100   -0.067    0.031   -0.429   -0.110   \n",
       "5114   -0.155    0.194   -0.130   -0.100   -0.062    0.028   -0.194   -0.029   \n",
       "1859   -0.221    0.035   -0.207   -0.100   -0.502    0.028   -1.145   -0.204   \n",
       "\n",
       "      wb_8111  wb_8112  wb_8113  wb_8114  wb_8115  wb_8116  wb_8117  wb_8118  \\\n",
       "7217   -0.107   -0.137   -0.215    0.290   -0.146   -0.034   -0.120   -0.317   \n",
       "8291   -0.106   -0.137   -0.194    0.170   -0.146   -0.034   -0.126   -0.669   \n",
       "4607   -0.109   -0.137   -0.429    0.376   -0.146   -0.303   -0.126   -0.411   \n",
       "5114   -0.111   -0.137    0.010    0.108   -0.146   -0.093   -0.332   -0.070   \n",
       "1859   -0.114   -0.137   -0.047    0.260   -0.146   -1.042   -0.126   -0.213   \n",
       "\n",
       "      wb_8119  wb_8120  wb_8121  wb_8122  wb_8123  wb_8124  wb_8125  wb_8126  \\\n",
       "7217    0.076    0.208   -0.152    0.203   -0.285    0.137   -0.088    0.236   \n",
       "8291    0.266    0.113   -0.229    0.491   -0.038   -0.096   -0.174    0.186   \n",
       "4607    1.132    0.102   -0.152    0.161   -0.037   -0.133   -0.133   -0.125   \n",
       "5114    0.291    0.207   -0.152    0.260   -0.051   -0.129   -0.199   -0.212   \n",
       "1859    0.051    0.209   -0.152    0.308   -0.042   -0.662   -0.810   -0.139   \n",
       "\n",
       "      wb_8127  wb_8128  wb_8129  wb_8130  wb_8131  wb_8132  wb_8133  wb_8134  \\\n",
       "7217   -0.023    0.063   -0.081   -0.053    0.083    0.104    0.014   -0.217   \n",
       "8291   -0.603    0.343   -0.525   -0.053    1.487    0.058    0.014   -0.191   \n",
       "4607   -0.399    0.176   -0.072   -0.236    0.997    0.133    0.014   -0.581   \n",
       "5114    0.069    0.068   -0.238   -0.052    0.281    0.095    0.014   -0.020   \n",
       "1859   -0.003    0.068   -0.083   -0.053    0.188    0.411    0.014   -0.104   \n",
       "\n",
       "      wb_8135  wb_8136  wb_8137  wb_8138  wb_8139  wb_8140  wb_8141  wb_8142  \\\n",
       "7217   -0.224    0.291   -0.249   -0.018    0.437    0.191   -0.250    0.106   \n",
       "8291   -0.043    0.986   -0.720   -0.007    0.135    0.147   -0.812    0.496   \n",
       "4607   -1.106    0.660    1.041   -0.387    0.135    0.111   -0.174    0.230   \n",
       "5114   -0.009    0.268   -0.187   -0.172    0.135    0.188   -0.268    0.275   \n",
       "1859   -1.907    1.111    0.233   -0.169    0.135    0.142   -0.819    0.111   \n",
       "\n",
       "      wb_8143  wb_8144  wb_8145  wb_8146  wb_8147  wb_8148  wb_8149  wb_8150  \\\n",
       "7217   -0.127    0.065   -0.193    0.374   -0.035    0.115    0.147    0.005   \n",
       "8291    0.138    0.521   -0.147    0.578   -0.630    0.112    0.152    0.496   \n",
       "4607    0.157    0.112   -0.066    1.824   -0.032    1.575    0.152    0.191   \n",
       "5114   -0.186    0.238   -0.021    0.333   -0.025    0.115    0.145    0.008   \n",
       "1859    0.032    0.344   -0.010    0.233   -0.032    0.111    0.152    0.599   \n",
       "\n",
       "      wb_8151  wb_8152  wb_8153  wb_8154  wb_8155  wb_8156  wb_8157  wb_8158  \\\n",
       "7217    0.201    0.142   -0.248   -0.218   -0.161    0.200    0.117   -0.155   \n",
       "8291    0.882    0.147   -0.296   -0.586   -0.148    0.466    0.320   -0.115   \n",
       "4607    0.314    0.300   -0.765   -0.397   -0.150    0.651    1.038    0.095   \n",
       "5114    0.084    0.142   -0.183   -0.021   -0.119    0.264    0.190   -0.203   \n",
       "1859    0.485    0.547   -0.773   -0.927   -0.135    0.737    0.153    0.064   \n",
       "\n",
       "      wb_8159  wb_8160  wb_8161  wb_8162  wb_8163  wb_8164  wb_8165  wb_8166  \\\n",
       "7217    0.184    0.017    0.101    0.024    0.076   -0.034   -0.039    0.218   \n",
       "8291    0.008    0.017    0.016    0.024    0.076   -0.131   -0.041    0.105   \n",
       "4607    0.198    0.017    0.044    0.024    0.076   -0.211   -0.219    0.190   \n",
       "5114    0.207    0.017    0.158    0.024    0.071   -0.138   -0.270    0.141   \n",
       "1859    0.298    0.017    0.044    0.024    0.072   -0.947   -0.579    1.712   \n",
       "\n",
       "      wb_8167  wb_8168  wb_8169  wb_8170  wb_8171  wb_8172  wb_8173  wb_8174  \\\n",
       "7217   -0.150    0.297   -0.219    0.271    0.233   -0.141   -0.152   -0.012   \n",
       "8291   -0.150    0.027   -0.446    0.196    1.402   -0.135   -0.041   -0.012   \n",
       "4607   -0.150    0.056   -0.010    0.690    0.036   -0.480   -0.131   -0.012   \n",
       "5114   -0.150    0.067   -0.102    0.054    0.034   -0.207   -0.107   -0.012   \n",
       "1859   -0.150    0.231    1.015    0.054    0.059   -0.141   -0.114   -0.012   \n",
       "\n",
       "      wb_8175  wb_8176  wb_8177  wb_8178  wb_8179  wb_8180  wb_8181  wb_8182  \\\n",
       "7217    0.077    0.270    0.046    0.355   -0.090   -0.046   -0.134   -0.062   \n",
       "8291    0.084    0.008    0.044    0.751   -0.204   -0.046   -0.197   -0.276   \n",
       "4607    0.116    0.176    0.050    0.118   -0.086   -0.046   -0.141   -0.879   \n",
       "5114    0.176    0.009    0.050    0.199   -0.102   -0.046   -0.186   -0.233   \n",
       "1859    0.421    0.017    0.042    0.956   -0.205   -0.046   -0.118   -0.946   \n",
       "\n",
       "      wb_8183  wb_8184  wb_8185  wb_8186  wb_8187  wb_8188  wb_8189  wb_8190  \\\n",
       "7217   -0.107    0.193    0.215    0.263   -0.095    0.249   -0.231   -0.216   \n",
       "8291   -0.283    0.070    0.189    0.431   -0.094    0.140   -0.059   -0.358   \n",
       "4607   -0.114    0.260    0.668    0.315   -0.707    0.131   -0.164   -0.190   \n",
       "5114   -0.109    0.072    0.228    0.233   -0.294    0.221   -0.079   -0.117   \n",
       "1859   -0.114    0.312    0.938    0.116   -0.216    0.057   -0.128   -0.684   \n",
       "\n",
       "      wb_8191  wb_8192  \n",
       "7217    0.328   -0.004  \n",
       "8291    0.299   -0.089  \n",
       "4607    0.576   -0.025  \n",
       "5114    0.197   -0.006  \n",
       "1859    1.634   -0.047  \n",
       "\n",
       "[5 rows x 8692 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [00h 07m 03s]\n",
      "val_loss: 0.6930855512619019\n",
      "\n",
      "Best val_loss So Far: 0.6674543619155884\n",
      "Total elapsed time: 01h 56m 41s\n",
      "\n",
      "Search: Running Trial #10\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "dense_block_1/u...|False             |False             \n",
      "dense_block_1/n...|2                 |2                 \n",
      "dense_block_1/u...|512               |512               \n",
      "dense_block_1/d...|0                 |0                 \n",
      "dense_block_1/u...|128               |128               \n",
      "optimizer         |adam              |adam              \n",
      "learning_rate     |1e-05             |0.001             \n",
      "\n",
      "Epoch 1/200\n",
      "35/35 [==============================] - 17s 295ms/step - loss: 0.6934 - binary_accuracy_inet_decision_function_fv_metric: 0.5081 - val_loss: 0.6922 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5176\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 7s 201ms/step - loss: 0.6912 - binary_accuracy_inet_decision_function_fv_metric: 0.5262 - val_loss: 0.6907 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5276\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6893 - binary_accuracy_inet_decision_function_fv_metric: 0.5376 - val_loss: 0.6888 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5392\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.6872 - binary_accuracy_inet_decision_function_fv_metric: 0.5471 - val_loss: 0.6866 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5482\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.6848 - binary_accuracy_inet_decision_function_fv_metric: 0.5554 - val_loss: 0.6842 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5555\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.6826 - binary_accuracy_inet_decision_function_fv_metric: 0.5614 - val_loss: 0.6824 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5604\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 8s 212ms/step - loss: 0.6810 - binary_accuracy_inet_decision_function_fv_metric: 0.5652 - val_loss: 0.6811 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5638\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 7s 200ms/step - loss: 0.6797 - binary_accuracy_inet_decision_function_fv_metric: 0.5676 - val_loss: 0.6798 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5672\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 7s 201ms/step - loss: 0.6784 - binary_accuracy_inet_decision_function_fv_metric: 0.5706 - val_loss: 0.6785 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5699\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.6773 - binary_accuracy_inet_decision_function_fv_metric: 0.5725 - val_loss: 0.6776 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5713\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 7s 203ms/step - loss: 0.6765 - binary_accuracy_inet_decision_function_fv_metric: 0.5741 - val_loss: 0.6769 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5727\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6757 - binary_accuracy_inet_decision_function_fv_metric: 0.5751 - val_loss: 0.6763 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5745\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 7s 199ms/step - loss: 0.6749 - binary_accuracy_inet_decision_function_fv_metric: 0.5766 - val_loss: 0.6756 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5762\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.6743 - binary_accuracy_inet_decision_function_fv_metric: 0.5777 - val_loss: 0.6752 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5773\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 7s 198ms/step - loss: 0.6738 - binary_accuracy_inet_decision_function_fv_metric: 0.5788 - val_loss: 0.6746 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5781\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.6733 - binary_accuracy_inet_decision_function_fv_metric: 0.5796 - val_loss: 0.6744 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5785\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 7s 212ms/step - loss: 0.6730 - binary_accuracy_inet_decision_function_fv_metric: 0.5801 - val_loss: 0.6740 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5793\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.6726 - binary_accuracy_inet_decision_function_fv_metric: 0.5807 - val_loss: 0.6738 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5800\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 7s 209ms/step - loss: 0.6723 - binary_accuracy_inet_decision_function_fv_metric: 0.5813 - val_loss: 0.6736 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5802\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 7s 198ms/step - loss: 0.6720 - binary_accuracy_inet_decision_function_fv_metric: 0.5820 - val_loss: 0.6734 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5808\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.6716 - binary_accuracy_inet_decision_function_fv_metric: 0.5827 - val_loss: 0.6734 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5806\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 7s 201ms/step - loss: 0.6714 - binary_accuracy_inet_decision_function_fv_metric: 0.5831 - val_loss: 0.6732 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5805\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 7s 201ms/step - loss: 0.6711 - binary_accuracy_inet_decision_function_fv_metric: 0.5837 - val_loss: 0.6731 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5811\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 7s 200ms/step - loss: 0.6708 - binary_accuracy_inet_decision_function_fv_metric: 0.5841 - val_loss: 0.6729 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5805\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 8s 221ms/step - loss: 0.6705 - binary_accuracy_inet_decision_function_fv_metric: 0.5847 - val_loss: 0.6728 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5805\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 7s 202ms/step - loss: 0.6703 - binary_accuracy_inet_decision_function_fv_metric: 0.5852 - val_loss: 0.6726 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5812\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.6701 - binary_accuracy_inet_decision_function_fv_metric: 0.5856 - val_loss: 0.6726 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5811\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 7s 200ms/step - loss: 0.6699 - binary_accuracy_inet_decision_function_fv_metric: 0.5860 - val_loss: 0.6725 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5809\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 7s 200ms/step - loss: 0.6696 - binary_accuracy_inet_decision_function_fv_metric: 0.5864 - val_loss: 0.6724 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5813\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 7s 208ms/step - loss: 0.6695 - binary_accuracy_inet_decision_function_fv_metric: 0.5866 - val_loss: 0.6722 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5818\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 7s 200ms/step - loss: 0.6692 - binary_accuracy_inet_decision_function_fv_metric: 0.5870 - val_loss: 0.6720 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5819\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.6690 - binary_accuracy_inet_decision_function_fv_metric: 0.5874 - val_loss: 0.6719 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5821\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 7s 199ms/step - loss: 0.6689 - binary_accuracy_inet_decision_function_fv_metric: 0.5878 - val_loss: 0.6717 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5824\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 7s 211ms/step - loss: 0.6687 - binary_accuracy_inet_decision_function_fv_metric: 0.5879 - val_loss: 0.6716 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5828\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 7s 197ms/step - loss: 0.6685 - binary_accuracy_inet_decision_function_fv_metric: 0.5882 - val_loss: 0.6716 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5829\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 7s 200ms/step - loss: 0.6683 - binary_accuracy_inet_decision_function_fv_metric: 0.5886 - val_loss: 0.6715 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5829\n",
      "Epoch 37/200\n",
      "24/35 [===================>..........] - ETA: 1s - loss: 0.6681 - binary_accuracy_inet_decision_function_fv_metric: 0.5888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6601 - binary_accuracy_inet_decision_function_fv_metric: 0.6000 - val_loss: 0.6881 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5573\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 8s 229ms/step - loss: 0.6592 - binary_accuracy_inet_decision_function_fv_metric: 0.6010 - val_loss: 0.6883 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5613\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 8s 228ms/step - loss: 0.6589 - binary_accuracy_inet_decision_function_fv_metric: 0.6017 - val_loss: 0.6873 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5604\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 8s 224ms/step - loss: 0.6586 - binary_accuracy_inet_decision_function_fv_metric: 0.6019 - val_loss: 0.6869 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5592\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6584 - binary_accuracy_inet_decision_function_fv_metric: 0.6018 - val_loss: 0.6862 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5602\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 8s 220ms/step - loss: 0.6585 - binary_accuracy_inet_decision_function_fv_metric: 0.6017 - val_loss: 0.6861 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5612\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6586 - binary_accuracy_inet_decision_function_fv_metric: 0.6019 - val_loss: 0.6850 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5602\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 8s 214ms/step - loss: 0.6584 - binary_accuracy_inet_decision_function_fv_metric: 0.6020 - val_loss: 0.6856 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5594\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6580 - binary_accuracy_inet_decision_function_fv_metric: 0.6029 - val_loss: 0.6857 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5584\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 8s 237ms/step - loss: 0.6578 - binary_accuracy_inet_decision_function_fv_metric: 0.6027 - val_loss: 0.6866 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5565\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 8s 226ms/step - loss: 0.6577 - binary_accuracy_inet_decision_function_fv_metric: 0.6029 - val_loss: 0.6856 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5573\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6576 - binary_accuracy_inet_decision_function_fv_metric: 0.6031 - val_loss: 0.6858 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5576\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 8s 222ms/step - loss: 0.6577 - binary_accuracy_inet_decision_function_fv_metric: 0.6028 - val_loss: 0.6853 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5595\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 8s 216ms/step - loss: 0.6574 - binary_accuracy_inet_decision_function_fv_metric: 0.6030 - val_loss: 0.6853 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5609\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6565 - binary_accuracy_inet_decision_function_fv_metric: 0.6040 - val_loss: 0.6841 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5638\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 8s 216ms/step - loss: 0.6567 - binary_accuracy_inet_decision_function_fv_metric: 0.6038 - val_loss: 0.6845 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5633\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 8s 222ms/step - loss: 0.6570 - binary_accuracy_inet_decision_function_fv_metric: 0.6037 - val_loss: 0.6838 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5626\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6564 - binary_accuracy_inet_decision_function_fv_metric: 0.6045 - val_loss: 0.6853 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5597\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6561 - binary_accuracy_inet_decision_function_fv_metric: 0.6052 - val_loss: 0.6848 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5584\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6558 - binary_accuracy_inet_decision_function_fv_metric: 0.6053 - val_loss: 0.6854 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5560\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6560 - binary_accuracy_inet_decision_function_fv_metric: 0.6049 - val_loss: 0.6860 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5564\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 7s 210ms/step - loss: 0.6553 - binary_accuracy_inet_decision_function_fv_metric: 0.6057 - val_loss: 0.6855 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5580\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 8s 233ms/step - loss: 0.6554 - binary_accuracy_inet_decision_function_fv_metric: 0.6056 - val_loss: 0.6840 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5605\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6554 - binary_accuracy_inet_decision_function_fv_metric: 0.6059 - val_loss: 0.6833 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5628\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 8s 222ms/step - loss: 0.6554 - binary_accuracy_inet_decision_function_fv_metric: 0.6059 - val_loss: 0.6842 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5615\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6549 - binary_accuracy_inet_decision_function_fv_metric: 0.6065 - val_loss: 0.6836 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5656\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 7s 210ms/step - loss: 0.6549 - binary_accuracy_inet_decision_function_fv_metric: 0.6065 - val_loss: 0.6835 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5641\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 8s 234ms/step - loss: 0.6548 - binary_accuracy_inet_decision_function_fv_metric: 0.6068 - val_loss: 0.6839 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5626\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 8s 212ms/step - loss: 0.6546 - binary_accuracy_inet_decision_function_fv_metric: 0.6069 - val_loss: 0.6842 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5617\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 8s 214ms/step - loss: 0.6545 - binary_accuracy_inet_decision_function_fv_metric: 0.6071 - val_loss: 0.6845 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5605\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 8s 232ms/step - loss: 0.6542 - binary_accuracy_inet_decision_function_fv_metric: 0.6077 - val_loss: 0.6838 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5612\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6541 - binary_accuracy_inet_decision_function_fv_metric: 0.6075 - val_loss: 0.6834 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5620\n",
      "Epoch 54/200\n",
      "35/35 [==============================] - 8s 217ms/step - loss: 0.6542 - binary_accuracy_inet_decision_function_fv_metric: 0.6078 - val_loss: 0.6834 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5622\n",
      "Epoch 55/200\n",
      "35/35 [==============================] - 7s 209ms/step - loss: 0.6542 - binary_accuracy_inet_decision_function_fv_metric: 0.6077 - val_loss: 0.6834 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5646\n",
      "Epoch 56/200\n",
      "35/35 [==============================] - 8s 231ms/step - loss: 0.6541 - binary_accuracy_inet_decision_function_fv_metric: 0.6077 - val_loss: 0.6839 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5627\n",
      "Epoch 57/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6540 - binary_accuracy_inet_decision_function_fv_metric: 0.6078 - val_loss: 0.6836 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5639\n",
      "Epoch 58/200\n",
      "35/35 [==============================] - 8s 214ms/step - loss: 0.6540 - binary_accuracy_inet_decision_function_fv_metric: 0.6079 - val_loss: 0.6835 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5637\n",
      "Epoch 59/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6539 - binary_accuracy_inet_decision_function_fv_metric: 0.6081 - val_loss: 0.6827 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5641\n",
      "Epoch 60/200\n",
      "35/35 [==============================] - 8s 225ms/step - loss: 0.6537 - binary_accuracy_inet_decision_function_fv_metric: 0.6081 - val_loss: 0.6831 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5631\n",
      "Epoch 61/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6539 - binary_accuracy_inet_decision_function_fv_metric: 0.6081 - val_loss: 0.6836 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5616\n",
      "Epoch 62/200\n",
      "35/35 [==============================] - 8s 221ms/step - loss: 0.6537 - binary_accuracy_inet_decision_function_fv_metric: 0.6083 - val_loss: 0.6834 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5637\n",
      "Epoch 63/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6536 - binary_accuracy_inet_decision_function_fv_metric: 0.6083 - val_loss: 0.6829 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5654\n",
      "Epoch 64/200\n",
      "35/35 [==============================] - 8s 212ms/step - loss: 0.6536 - binary_accuracy_inet_decision_function_fv_metric: 0.6083 - val_loss: 0.6831 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5653\n",
      "Epoch 65/200\n",
      "35/35 [==============================] - 8s 231ms/step - loss: 0.6539 - binary_accuracy_inet_decision_function_fv_metric: 0.6079 - val_loss: 0.6827 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5651\n",
      "Epoch 66/200\n",
      "35/35 [==============================] - 8s 217ms/step - loss: 0.6538 - binary_accuracy_inet_decision_function_fv_metric: 0.6078 - val_loss: 0.6826 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5661\n",
      "Epoch 67/200\n",
      "35/35 [==============================] - 8s 217ms/step - loss: 0.6539 - binary_accuracy_inet_decision_function_fv_metric: 0.6078 - val_loss: 0.6823 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5668\n",
      "Epoch 68/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6537 - binary_accuracy_inet_decision_function_fv_metric: 0.6078 - val_loss: 0.6823 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5650\n",
      "Epoch 69/200\n",
      "35/35 [==============================] - 8s 226ms/step - loss: 0.6540 - binary_accuracy_inet_decision_function_fv_metric: 0.6077 - val_loss: 0.6827 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5646\n",
      "Epoch 70/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6535 - binary_accuracy_inet_decision_function_fv_metric: 0.6082 - val_loss: 0.6824 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5630\n",
      "Epoch 71/200\n",
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6533 - binary_accuracy_inet_decision_function_fv_metric: 0.6086 - val_loss: 0.6833 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5614\n",
      "Epoch 72/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6533 - binary_accuracy_inet_decision_function_fv_metric: 0.6086 - val_loss: 0.6828 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5633\n",
      "Epoch 73/200\n",
      "35/35 [==============================] - 9s 239ms/step - loss: 0.6531 - binary_accuracy_inet_decision_function_fv_metric: 0.6089 - val_loss: 0.6830 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5622\n",
      "Epoch 74/200\n",
      "35/35 [==============================] - 8s 212ms/step - loss: 0.6530 - binary_accuracy_inet_decision_function_fv_metric: 0.6089 - val_loss: 0.6836 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5632\n",
      "Epoch 75/200\n",
      "35/35 [==============================] - 8s 221ms/step - loss: 0.6532 - binary_accuracy_inet_decision_function_fv_metric: 0.6089 - val_loss: 0.6838 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5631\n",
      "Epoch 76/200\n",
      "35/35 [==============================] - 8s 220ms/step - loss: 0.6534 - binary_accuracy_inet_decision_function_fv_metric: 0.6086 - val_loss: 0.6833 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5632\n",
      "Epoch 77/200\n",
      "35/35 [==============================] - 8s 215ms/step - loss: 0.6534 - binary_accuracy_inet_decision_function_fv_metric: 0.6087 - val_loss: 0.6828 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5625\n",
      "Epoch 78/200\n",
      "35/35 [==============================] - 8s 214ms/step - loss: 0.6532 - binary_accuracy_inet_decision_function_fv_metric: 0.6088 - val_loss: 0.6824 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5625\n",
      "Epoch 79/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6532 - binary_accuracy_inet_decision_function_fv_metric: 0.6088 - val_loss: 0.6821 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5622\n",
      "Epoch 80/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6530 - binary_accuracy_inet_decision_function_fv_metric: 0.6090 - val_loss: 0.6821 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5645\n",
      "Epoch 81/200\n",
      "35/35 [==============================] - 8s 216ms/step - loss: 0.6530 - binary_accuracy_inet_decision_function_fv_metric: 0.6091 - val_loss: 0.6828 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5631\n",
      "Epoch 82/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6532 - binary_accuracy_inet_decision_function_fv_metric: 0.6089 - val_loss: 0.6823 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5638\n",
      "Epoch 83/200\n",
      "35/35 [==============================] - 8s 230ms/step - loss: 0.6531 - binary_accuracy_inet_decision_function_fv_metric: 0.6089 - val_loss: 0.6819 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5653\n",
      "Epoch 84/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6529 - binary_accuracy_inet_decision_function_fv_metric: 0.6091 - val_loss: 0.6819 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5660\n",
      "Epoch 85/200\n",
      "35/35 [==============================] - 8s 216ms/step - loss: 0.6527 - binary_accuracy_inet_decision_function_fv_metric: 0.6095 - val_loss: 0.6826 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5649\n",
      "Epoch 86/200\n",
      "35/35 [==============================] - 8s 224ms/step - loss: 0.6524 - binary_accuracy_inet_decision_function_fv_metric: 0.6100 - val_loss: 0.6822 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5657\n",
      "Epoch 87/200\n",
      "35/35 [==============================] - 8s 221ms/step - loss: 0.6521 - binary_accuracy_inet_decision_function_fv_metric: 0.6102 - val_loss: 0.6819 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5655\n",
      "Epoch 88/200\n",
      "35/35 [==============================] - 8s 212ms/step - loss: 0.6524 - binary_accuracy_inet_decision_function_fv_metric: 0.6096 - val_loss: 0.6822 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5662\n",
      "Epoch 89/200\n",
      "35/35 [==============================] - 8s 217ms/step - loss: 0.6521 - binary_accuracy_inet_decision_function_fv_metric: 0.6102 - val_loss: 0.6821 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5661\n",
      "Epoch 90/200\n",
      "35/35 [==============================] - 8s 219ms/step - loss: 0.6519 - binary_accuracy_inet_decision_function_fv_metric: 0.6105 - val_loss: 0.6814 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5676\n",
      "Epoch 91/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6517 - binary_accuracy_inet_decision_function_fv_metric: 0.6106 - val_loss: 0.6816 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5668\n",
      "Epoch 92/200\n",
      "35/35 [==============================] - 8s 225ms/step - loss: 0.6517 - binary_accuracy_inet_decision_function_fv_metric: 0.6108 - val_loss: 0.6821 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5676\n",
      "Epoch 93/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6516 - binary_accuracy_inet_decision_function_fv_metric: 0.6107 - val_loss: 0.6823 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5654\n",
      "Epoch 94/200\n",
      "35/35 [==============================] - 8s 217ms/step - loss: 0.6513 - binary_accuracy_inet_decision_function_fv_metric: 0.6112 - val_loss: 0.6819 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5673\n",
      "Epoch 95/200\n",
      "35/35 [==============================] - 8s 234ms/step - loss: 0.6517 - binary_accuracy_inet_decision_function_fv_metric: 0.6108 - val_loss: 0.6813 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5665\n",
      "Epoch 96/200\n",
      "35/35 [==============================] - 8s 213ms/step - loss: 0.6519 - binary_accuracy_inet_decision_function_fv_metric: 0.6106 - val_loss: 0.6813 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5659\n",
      "Epoch 97/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6520 - binary_accuracy_inet_decision_function_fv_metric: 0.6106 - val_loss: 0.6821 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5645\n",
      "Epoch 98/200\n",
      "35/35 [==============================] - 8s 216ms/step - loss: 0.6521 - binary_accuracy_inet_decision_function_fv_metric: 0.6102 - val_loss: 0.6820 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5652\n",
      "Epoch 99/200\n",
      "35/35 [==============================] - 8s 218ms/step - loss: 0.6521 - binary_accuracy_inet_decision_function_fv_metric: 0.6104 - val_loss: 0.6822 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5645\n",
      "Epoch 100/200\n",
      "35/35 [==============================] - 8s 214ms/step - loss: 0.6522 - binary_accuracy_inet_decision_function_fv_metric: 0.6101 - val_loss: 0.6819 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5643\n",
      "Epoch 101/200\n",
      "35/35 [==============================] - 8s 220ms/step - loss: 0.6519 - binary_accuracy_inet_decision_function_fv_metric: 0.6104 - val_loss: 0.6824 - val_binary_accuracy_inet_decision_function_fv_metric: 0.5641\n",
      "Epoch 102/200\n",
      "26/35 [=====================>........] - ETA: 1s - loss: 0.6516 - binary_accuracy_inet_decision_function_fv_metric: 0.6104"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(\n",
    "                                      lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      #callback_names=['plot_losses']\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8193)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cast_to_float32 (CastToFloat32) (None, 8193)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          4195328     cast_to_float32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65664       re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           4128        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 15)           495         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 30)           990         re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 1)            33          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 481)          0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "                                                                 dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "                                                                 dense_11[0][0]                   \n",
      "                                                                 dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "                                                                 dense_14[0][0]                   \n",
      "                                                                 dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "                                                                 dense_20[0][0]                   \n",
      "                                                                 dense_21[0][0]                   \n",
      "                                                                 dense_22[0][0]                   \n",
      "                                                                 dense_23[0][0]                   \n",
      "                                                                 dense_24[0][0]                   \n",
      "                                                                 dense_25[0][0]                   \n",
      "                                                                 dense_26[0][0]                   \n",
      "                                                                 dense_27[0][0]                   \n",
      "                                                                 dense_28[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "                                                                 dense_30[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "                                                                 dense_33[0][0]                   \n",
      "                                                                 dense_34[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,280,993\n",
      "Trainable params: 4,280,993\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/use_batchnorm: False\n",
      "dense_block_1/num_layers: 3\n",
      "dense_block_1/units_0: 512\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "optimizer: adam_weight_decay\n",
      "learning_rate: 0.001\n",
      "dense_block_1/units_2: 32\n",
      "Score: 0.6655382513999939\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/use_batchnorm: False\n",
      "dense_block_1/num_layers: 2\n",
      "dense_block_1/units_0: 512\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "optimizer: adam_weight_decay\n",
      "learning_rate: 0.001\n",
      "Score: 0.666150689125061\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/use_batchnorm: False\n",
      "dense_block_1/num_layers: 2\n",
      "dense_block_1/units_0: 512\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "Score: 0.6674543619155884\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/use_batchnorm: False\n",
      "dense_block_1/num_layers: 2\n",
      "dense_block_1/units_0: 512\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 256\n",
      "optimizer: adam\n",
      "learning_rate: 0.001\n",
      "Score: 0.6674912571907043\n",
      "None\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "dense_block_1/use_batchnorm: False\n",
      "dense_block_1/num_layers: 3\n",
      "dense_block_1/units_0: 128\n",
      "dense_block_1/dropout: 0.0\n",
      "dense_block_1/units_1: 128\n",
      "optimizer: adam_weight_decay\n",
      "learning_rate: 0.001\n",
      "dense_block_1/units_2: 32\n",
      "Score: 0.6675047278404236\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if nas:\n",
    "    for trial in history: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edff6362fd2b4aa9bc22524ec6360538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dab7f820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439e11af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb5d840e160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb5d840e8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439d04a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb5d840e5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439cb7d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439d044c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439d04ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438e2fca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439d04550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438c40280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438e2faf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438e2fdc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438e2fc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438c40700> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c53820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c53550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c38f70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438d71d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c53310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438dbfe50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c533a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c1c4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438dbfca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c1cee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438be2b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb43871fd30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb438d2ca60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c86550> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb43680c5e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb43680cee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb43680cc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb43681c9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb439c86d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb4366d0940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb43681cc10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb4366d0f70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb4366ef820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb4366efd30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb436446820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb4364464c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb436620160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb4364463a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb4366d0ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb436831670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb436446ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb436857d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb436857040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb436857160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fb43632bee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Binary Crossentropy:\t\t 9.381 (Sklearn DT) \t 0.721 (I-Net DT)\n",
      "Accuracy:\t\t 0.728 (Sklearn DT) \t 0.506 (I-Net DT)\n",
      "F1 Score:\t\t 0.708 (Sklearn DT) \t 0.409 (I-Net DT)\n"
     ]
    }
   ],
   "source": [
    "y_test_inet_dt_list = []\n",
    "y_test_distilled_sklearn_dt_list = []\n",
    "\n",
    "binary_crossentropy_distilled_sklearn_dt_list =[]\n",
    "accuracy_distilled_sklearn_dt_list = []\n",
    "f1_score_distilled_sklearn_dt_list = []\n",
    "\n",
    "binary_crossentropy_inet_dt_list =[]\n",
    "accuracy_inet_dt_list = []\n",
    "f1_score_inet_dt_list = []\n",
    "\n",
    "#inet_metric_function_list = []\n",
    "\n",
    "number = lambda_net_dataset_test.y_test_lambda_array.shape[0]#10\n",
    "\n",
    "for lambda_net_parameters, lambda_net, X_test_lambda, y_test_lambda in tqdm(zip(lambda_net_dataset_test.network_parameters_array[:number], lambda_net_dataset_test.network_list[:number], lambda_net_dataset_test.X_test_lambda_array[:number], lambda_net_dataset_test.y_test_lambda_array[:number]), total=lambda_net_dataset_test.y_test_lambda_array[:number].shape[0]):\n",
    "    dt_inet = model.predict(np.array([lambda_net_parameters]))[0]\n",
    "    if nas:\n",
    "        dt_inet = dt_inet[:function_representation_length]\n",
    "\n",
    "    \n",
    "    X_data_random = generate_random_data_points_custom(config['data']['x_min'], config['data']['x_max'], config['evaluation']['random_evaluation_dataset_size'], config['data']['number_of_variables'])\n",
    "    y_data_random_lambda_pred = lambda_net.predict(X_data_random)\n",
    "    y_data_random_lambda_pred = np.round(y_data_random_lambda_pred).astype(np.int64)\n",
    "    \n",
    "    dt_sklearn_distilled = DecisionTreeClassifier(max_depth=config['function_family']['maximum_depth'])\n",
    "    dt_sklearn_distilled.fit(X_data_random, y_data_random_lambda_pred)\n",
    "    \n",
    "    \n",
    "    if dt_type == 'SDT':\n",
    "        y_test_inet_dt  = calculate_function_value_from_decision_tree_parameters_wrapper(X_test_lambda, config)(dt_inet).numpy()\n",
    "    elif dt_type == 'vanilla':\n",
    "        y_test_inet_dt  = calculate_function_value_from_vanilla_decision_tree_parameters_wrapper(X_test_lambda, config)(dt_inet).numpy()\n",
    "    y_test_distilled_sklearn_dt = dt_sklearn_distilled.predict(X_test_lambda)\n",
    "    \n",
    "    y_test_lambda_pred = lambda_net.predict(X_test_lambda)\n",
    "    y_test_lambda_pred = np.round(y_test_lambda_pred)\n",
    "    \n",
    "    \n",
    "    #random_model = generate_base_model(config)        \n",
    "    #random_network_parameters = random_model.get_weights()\n",
    "    #network_parameters_structure = [network_parameter.shape for network_parameter in random_network_parameters]         \n",
    "    \n",
    "    #function_true_placeholder = np.array([0 for i in range(basic_function_representation_length)])\n",
    "    #function_true_with_network_parameters = np.concatenate([function_true_placeholder, lambda_net_parameters])\n",
    "    #inet_metric_function = inet_decision_function_fv_metric_wrapper(X_test_lambda, \n",
    "    #                                                                 random_model, \n",
    "    #                                                                 network_parameters_structure, \n",
    "    #                                                                 config, \n",
    "    #                                                                 'binary_accuracy')(np.array([function_true_with_network_parameters]), \n",
    "    #                                                                                     np.array([dt_inet]))    \n",
    "    #inet_metric_function_list.append(inet_metric_function)\n",
    "        \n",
    "    \n",
    "    \n",
    "    binary_crossentropy_distilled_sklearn_dt = log_loss(y_test_lambda_pred, y_test_distilled_sklearn_dt)\n",
    "    accuracy_distilled_sklearn_dt = accuracy_score(y_test_lambda_pred, np.round(y_test_distilled_sklearn_dt))\n",
    "    f1_score_distilled_sklearn_dt = f1_score(y_test_lambda_pred, np.round(y_test_distilled_sklearn_dt))\n",
    "    \n",
    "    binary_crossentropy_inet_dt = log_loss(y_test_lambda_pred, y_test_inet_dt)\n",
    "    accuracy_inet_dt = accuracy_score(y_test_lambda_pred, np.round(y_test_inet_dt))\n",
    "    f1_score_inet_dt = f1_score(y_test_lambda_pred, np.round(y_test_inet_dt))\n",
    "    \n",
    "    \n",
    "    y_test_inet_dt_list.append(y_test_inet_dt)\n",
    "    y_test_distilled_sklearn_dt_list.append(y_test_distilled_sklearn_dt)    \n",
    "\n",
    "    binary_crossentropy_distilled_sklearn_dt_list.append(np.nan_to_num(binary_crossentropy_distilled_sklearn_dt))\n",
    "    accuracy_distilled_sklearn_dt_list.append(np.nan_to_num(accuracy_distilled_sklearn_dt))\n",
    "    f1_score_distilled_sklearn_dt_list.append(np.nan_to_num(f1_score_distilled_sklearn_dt))\n",
    "\n",
    "    binary_crossentropy_inet_dt_list.append(np.nan_to_num(binary_crossentropy_inet_dt))\n",
    "    accuracy_inet_dt_list.append(np.nan_to_num(accuracy_inet_dt))\n",
    "    f1_score_inet_dt_list.append(np.nan_to_num(f1_score_inet_dt))\n",
    "    \n",
    "y_test_inet_dt_list = np.array(y_test_inet_dt_list)\n",
    "y_test_distilled_sklearn_dt_list = np.array(y_test_distilled_sklearn_dt_list)\n",
    "\n",
    "binary_crossentropy_distilled_sklearn_dt_list = np.array(binary_crossentropy_distilled_sklearn_dt_list)\n",
    "accuracy_distilled_sklearn_dt_list = np.array(accuracy_distilled_sklearn_dt_list)\n",
    "f1_score_distilled_sklearn_dt_list = np.array(f1_score_distilled_sklearn_dt_list)\n",
    "\n",
    "binary_crossentropy_inet_dt_list = np.array(binary_crossentropy_inet_dt_list)\n",
    "accuracy_inet_dt_list = np.array(accuracy_inet_dt_list)\n",
    "f1_score_inet_dt_list = np.array(f1_score_inet_dt_list)    \n",
    "\n",
    "    \n",
    "print('Binary Crossentropy:\\t\\t', np.round(np.mean(binary_crossentropy_distilled_sklearn_dt_list), 3), '(Sklearn DT)' , '\\t', np.round(np.mean(binary_crossentropy_inet_dt_list), 3), '(I-Net DT)')\n",
    "print('Accuracy:\\t\\t', np.round(np.mean(accuracy_distilled_sklearn_dt_list), 3), '(Sklearn DT)' , '\\t', np.round(np.mean(accuracy_inet_dt_list), 3), '(I-Net DT)')\n",
    "print('F1 Score:\\t\\t', np.round(np.mean(f1_score_distilled_sklearn_dt_list), 3), '(Sklearn DT)' , '\\t', np.round(np.mean(f1_score_inet_dt_list), 3), '(I-Net DT)')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL DATA EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADULT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "                 \"Age\", #0\n",
    "                 \"Workclass\",  #1\n",
    "                 \"fnlwgt\",  #2\n",
    "                 \"Education\",  #3\n",
    "                 \"Education-Num\",  #4\n",
    "                 \"Marital Status\", #5\n",
    "                 \"Occupation\",  #6\n",
    "                 \"Relationship\",  #7\n",
    "                 \"Race\",  #8\n",
    "                 \"Sex\",  #9\n",
    "                 \"Capital Gain\",  #10\n",
    "                 \"Capital Loss\", #11\n",
    "                 \"Hours per week\",  #12\n",
    "                 \"Country\", #13\n",
    "                 \"capital_gain\" #14\n",
    "                ] \n",
    "\n",
    "\n",
    "\n",
    "dataframe = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', names=feature_names, index_col=False)\n",
    "dataframe = dataframe.drop([\n",
    "                             #\"Age\", #0\n",
    "                             \"Workclass\",  #1\n",
    "                             \"fnlwgt\",  #2\n",
    "                             \"Education\",  #3\n",
    "                             \"Education-Num\",  #4\n",
    "                             \"Marital Status\", #5\n",
    "                             \"Occupation\",  #6\n",
    "                             \"Relationship\",  #7\n",
    "                             \"Race\",  #8\n",
    "                             #\"Sex\",  #9 \n",
    "                             #\"Capital Gain\",  #10\n",
    "                             #\"Capital Loss\", #11\n",
    "                             \"Hours per week\",  #12\n",
    "                             \"Country\", #13\n",
    "                            ] , axis=1)\n",
    "#dataframe = dataframe.drop(['Country'], axis=1)\n",
    "\n",
    "categorical_features = [\n",
    "                        1, #\"Workclass\"\n",
    "                        3, #\"Education\"\n",
    "                        5, #\"Marital Status\"\n",
    "                        6, #\"Occupation\"\n",
    "                        7, #\"Relationship\"\n",
    "                        8, #\"Race\"\n",
    "                        9, #\"Sex\"\n",
    "                        #13, #\"Country\n",
    "                       ] \n",
    "\n",
    "categorical_features = [1]\n",
    "\n",
    "\n",
    "data = dataframe.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['capital_gain'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[:,data.shape[1]-1]\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)\n",
    "class_names = le.classes_\n",
    "data = data[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
    "transformer.fit(data)\n",
    "\n",
    "data = transformer.transform(data)\n",
    "data = data#.toarray()\n",
    "data = data.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose = data.transpose()\n",
    "normalizer_list = []\n",
    "transpose_normalized = []\n",
    "for column in transpose:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(column.reshape(-1, 1))\n",
    "    column_new = scaler.transform(column.reshape(-1, 1)).ravel()\n",
    "    transpose_normalized.append(column_new)\n",
    "    normalizer_list.append(scaler)\n",
    "data = np.array(transpose_normalized).transpose()\n",
    "\n",
    "#data = data / data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_valid, test, labels_train_with_valid, labels_test = train_test_split(data, labels, train_size=0.8)\n",
    "train, valid, labels_train, labels_valid = train_test_split(train_with_valid, labels_train_with_valid, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_network = generate_lambda_net_from_config(config)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=50, \n",
    "                                                  min_delta=0.001, \n",
    "                                                  verbose=0, \n",
    "                                                  mode='min', \n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "model_history = test_network.fit(train,\n",
    "                                  labels_train, \n",
    "                                  epochs=1000, \n",
    "                                  batch_size=256, \n",
    "                                  callbacks=[early_stopping, PlotLossesKerasTF()],\n",
    "                                  validation_data=(valid, labels_valid),\n",
    "                                  verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_network_parameters = shaped_network_parameters_to_array(test_network.get_weights(), config)\n",
    "test_network_dt_inet = model.predict(np.array([test_network_parameters]))[0]\n",
    "\n",
    "if nas:\n",
    "    test_network_dt_inet = test_network_dt_inet[:function_representation_length]\n",
    "\n",
    "X_data_random = generate_random_data_points_custom(config['data']['x_min'], config['data']['x_max'], config['evaluation']['random_evaluation_dataset_size'], config['data']['number_of_variables'])\n",
    "y_data_random_test_network_pred = test_network.predict(X_data_random)\n",
    "y_data_random_test_network_pred = np.round(y_data_random_test_network_pred).astype(np.int64)\n",
    "\n",
    "dt_sklearn_distilled = DecisionTreeClassifier(max_depth=config['function_family']['maximum_depth'])\n",
    "dt_sklearn_distilled.fit(X_data_random, y_data_random_test_network_pred)\n",
    "\n",
    "\n",
    "if dt_type == 'SDT':\n",
    "    y_test_inet_dt  = calculate_function_value_from_decision_tree_parameters_wrapper(test, config)(test_network_dt_inet).numpy()\n",
    "elif dt_type == 'vanilla':\n",
    "    y_test_inet_dt  = calculate_function_value_from_vanilla_decision_tree_parameters_wrapper(test, config)(test_network_dt_inet).numpy()\n",
    "y_test_distilled_sklearn_dt = dt_sklearn_distilled.predict(test)\n",
    "\n",
    "y_test_test_network_pred = test_network.predict(test)\n",
    "y_test_test_network_pred = np.round(y_test_test_network_pred)\n",
    "\n",
    "binary_crossentropy_distilled_sklearn_dt = log_loss(y_test_test_network_pred, y_test_distilled_sklearn_dt)\n",
    "accuracy_distilled_sklearn_dt = accuracy_score(y_test_test_network_pred, np.round(y_test_distilled_sklearn_dt))\n",
    "f1_score_distilled_sklearn_dt = f1_score(y_test_test_network_pred, np.round(y_test_distilled_sklearn_dt))\n",
    "\n",
    "binary_crossentropy_inet_dt = log_loss(y_test_test_network_pred, y_test_inet_dt)\n",
    "accuracy_inet_dt = accuracy_score(y_test_test_network_pred, np.round(y_test_inet_dt))\n",
    "f1_score_inet_dt = f1_score(y_test_test_network_pred, np.round(y_test_inet_dt))\n",
    "\n",
    "\n",
    "print('Binary Crossentropy:\\t\\t', np.round(binary_crossentropy_distilled_sklearn_dt, 3), '(Sklearn DT)' , '\\t', np.round(binary_crossentropy_inet_dt, 3), '(I-Net DT)')\n",
    "print('Accuracy:\\t\\t', np.round(accuracy_distilled_sklearn_dt, 3), '(Sklearn DT)' , '\\t', np.round(accuracy_inet_dt, 3), '(I-Net DT)')\n",
    "print('F1 Score:\\t\\t', np.round(f1_score_distilled_sklearn_dt, 3), '(Sklearn DT)' , '\\t', np.round(f1_score_inet_dt, 3), '(I-Net DT)')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits, leaf_classes = get_shaped_parameters_for_decision_tree(test_network_dt_inet, config)\n",
    "print(splits, leaf_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "image, nodes = anytree_decision_tree_from_parameters(test_network_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "image\n",
    "#tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = treelib_decision_tree_from_parameters(test_network_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "#tree = treelib_decision_tree_from_parameters(test_network_dt_inet, config=config)\n",
    "\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree = treelib_decision_tree_from_parameters(test_network_dt_inet, config=config, normalizer_list=normalizer_list)\n",
    "tree = treelib_decision_tree_from_parameters(test_network_dt_inet, config=config)\n",
    "\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,12))  # set plot size (denoted in inches)\n",
    "plot_tree(dt_sklearn_distilled, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
