{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 4,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,                      \n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 5, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'random_decision_tree', # 'make_classification' 'random_decision_tree'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 1000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [64],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [1056, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.2, 0.1],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 200, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, # 1=standard representation; 2=sparse representation, 3=vanilla_dt\n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 ** maximum_depth - 1) * number_of_variables + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['basic_function_representation_length'] = (2 ** maximum_depth - 1) * number_of_variables + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes\n",
    "config['function_family']['function_representation_length'] =( (2 ** maximum_depth - 1) * number_of_variables + (2 ** maximum_depth - 1) + (2 ** maximum_depth) * num_classes  if function_representation_type == 1 \n",
    "                                                              else (2 ** maximum_depth - 1) * decision_sparsity + (2 ** maximum_depth - 1) + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes if function_representation_type == 2\n",
    "                                                              else (2 ** maximum_depth - 1) * decision_sparsity + ((2 ** maximum_depth - 1)  * decision_sparsity * number_of_variables) + (2 ** maximum_depth) * num_classes)\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize1000_numLNets10000_var5_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1056-512_drop0.2-0.1e200b256_adam\n",
      "lNetSize1000_numLNets10000_var5_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth4_beta1_decisionSpars1_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-3)]: Done 742 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-3)]: Done 5670 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-3)]: Done 9864 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   28.5s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:  4.5min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   29.0s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if noise_injected_level > 0:\n",
    "    lambda_net_dataset_training = load_lambda_nets(config, no_noise=True, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_evaluation = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_training, test_split=0.1)\n",
    "    _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_evaluation, test_split=test_size)\n",
    "    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8955, 573)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 573)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 573)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>6671.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-1.227</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>3274.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.738</td>\n",
       "      <td>1.438</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.751</td>\n",
       "      <td>1.597</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-1.116</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.551</td>\n",
       "      <td>0.653</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.666</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>3095.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.798</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.536</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.632</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>8379.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.715</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.355</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.659</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.746</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>0.899</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.771</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.743</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.668</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.678</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.817</td>\n",
       "      <td>-1.106</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.882</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>-0.924</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.818</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.740</td>\n",
       "      <td>-0.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1  f0v2   f0v3  f0v4  f1v0   f1v1  f1v2  f1v3  \\\n",
       "6671 6671.000    42 0.343 0.000 0.000  0.000 0.000 0.269  0.000 0.000 0.000   \n",
       "3274 3274.000    42 0.000 0.000 0.000  0.000 0.372 0.000  0.000 0.365 0.000   \n",
       "3095 3095.000    42 0.000 0.000 0.000  0.295 0.000 0.000 -0.234 0.000 0.000   \n",
       "8379 8379.000    42 0.000 0.000 0.000 -0.419 0.000 0.000  0.000 0.000 0.000   \n",
       "3043 3043.000    42 0.000 0.000 0.000  0.000 0.373 0.000  0.000 0.000 0.000   \n",
       "\n",
       "       f1v4  f2v0  f2v1   f2v2  f2v3  f2v4  f3v0  f3v1   f3v2  f3v3   f3v4  \\\n",
       "6671  0.000 0.000 0.000  0.000 0.273 0.000 0.000 0.000  0.000 0.330  0.000   \n",
       "3274  0.000 0.226 0.000  0.000 0.000 0.000 0.000 0.000 -0.169 0.000  0.000   \n",
       "3095  0.000 0.000 0.000  0.000 0.418 0.000 0.000 0.000  0.000 0.000 -0.394   \n",
       "8379 -0.293 0.328 0.000  0.000 0.000 0.000 0.000 0.000  0.000 0.000  0.264   \n",
       "3043  0.387 0.000 0.000 -0.375 0.000 0.000 0.000 0.000 -0.423 0.000  0.000   \n",
       "\n",
       "      f4v0   f4v1  f4v2   f4v3  f4v4  f5v0  f5v1   f5v2  f5v3   f5v4  f6v0  \\\n",
       "6671 0.000  0.411 0.000  0.000 0.000 0.000 0.000  0.000 0.000  0.307 0.291   \n",
       "3274 0.000  0.000 0.000 -0.366 0.000 0.000 0.000 -0.363 0.000  0.000 0.000   \n",
       "3095 0.000  0.000 0.000  0.345 0.000 0.000 0.000  0.000 0.000  0.421 0.000   \n",
       "8379 0.000  0.000 0.000 -0.406 0.000 0.000 0.000  0.000 0.000 -0.418 0.000   \n",
       "3043 0.000 -0.274 0.000  0.000 0.000 0.000 0.000  0.000 0.000  0.405 0.000   \n",
       "\n",
       "      f6v1   f6v2  f6v3   f6v4   f7v0  f7v1   f7v2  f7v3   f7v4  f8v0  f8v1  \\\n",
       "6671 0.000  0.000 0.000  0.000  0.000 0.331  0.000 0.000  0.000 0.000 0.000   \n",
       "3274 0.296  0.000 0.000  0.000  0.000 0.000 -0.297 0.000  0.000 0.000 0.191   \n",
       "3095 0.421  0.000 0.000  0.000 -0.380 0.000  0.000 0.000  0.000 0.000 0.000   \n",
       "8379 0.000 -0.315 0.000  0.000  0.000 0.000  0.000 0.000 -0.347 0.000 0.000   \n",
       "3043 0.000  0.000 0.000 -0.376  0.000 0.000  0.000 0.000  0.317 0.397 0.000   \n",
       "\n",
       "       f8v2  f8v3   f8v4  f9v0  f9v1  f9v2  f9v3  f9v4  f10v0  f10v1  f10v2  \\\n",
       "6671  0.000 0.000 -0.316 0.000 0.000 0.000 0.332 0.000  0.000  0.000  0.000   \n",
       "3274  0.000 0.000  0.000 0.000 0.000 0.000 0.433 0.000  0.000  0.000  0.000   \n",
       "3095 -0.311 0.000  0.000 0.000 0.424 0.000 0.000 0.000  0.000  0.000 -0.408   \n",
       "8379 -0.420 0.000  0.000 0.323 0.000 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "3043  0.000 0.000  0.000 0.000 0.400 0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  f12v2  \\\n",
       "6671  0.350  0.000  0.000  0.000  0.000  0.000  0.360  0.000  0.000 -0.372   \n",
       "3274  0.000  0.410  0.000 -0.369  0.000  0.000  0.000  0.000 -0.440  0.000   \n",
       "3095  0.000  0.000  0.000  0.000  0.000  0.000 -0.305  0.000  0.000  0.000   \n",
       "8379 -0.337  0.000  0.000  0.280  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.384  0.000  0.000  0.000  0.000  0.385  0.000  0.407  0.000  0.000   \n",
       "\n",
       "      f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  f14v2  \\\n",
       "6671  0.000  0.000  0.415  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3274  0.000  0.000  0.000  0.371  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "3095  0.000  0.431  0.000  0.000  0.000 -0.406  0.000  0.000  0.000  0.000   \n",
       "8379  0.000 -0.404  0.000  0.000  0.263  0.000  0.000  0.000  0.000  0.000   \n",
       "3043  0.000  0.000  0.000  0.000  0.000  0.363  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6     b7  \\\n",
       "6671  0.000 -0.195  0.015 -0.105 -0.399  0.381  0.088  0.077 -0.194 -0.304   \n",
       "3274 -0.363  0.000 -0.317  0.115  0.268  0.390 -0.230  0.196 -0.281 -0.183   \n",
       "3095  0.000  0.442  0.297 -0.063  0.301  0.146 -0.272  0.053 -0.001 -0.233   \n",
       "8379  0.000  0.420  0.372 -0.402  0.130  0.156 -0.086 -0.113 -0.138 -0.261   \n",
       "3043  0.000 -0.437 -0.093 -0.116 -0.176 -0.225 -0.409  0.116 -0.178  0.222   \n",
       "\n",
       "         b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  lp1c0  \\\n",
       "6671 -0.072 -0.092  0.165  0.150  0.023 -0.369 -0.334  0.069  0.163  0.207   \n",
       "3274  0.056  0.334 -0.403  0.193  0.080  0.308  0.126 -0.019 -0.241  0.170   \n",
       "3095  0.272 -0.090  0.345 -0.408 -0.210  0.211  0.201 -0.055 -0.042 -0.186   \n",
       "8379  0.428  0.311  0.063 -0.338  0.312  0.112 -0.097 -0.120 -0.035  0.201   \n",
       "3043  0.081  0.021  0.161 -0.189  0.020  0.374  0.138  0.058 -0.107 -0.158   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  wb_352  \\\n",
       "6671 -0.017  0.049 -0.221 -0.077 -0.231  ...  -0.186  -0.013  -0.074  -0.067   \n",
       "3274  0.182  0.005 -0.087  0.240  0.030  ...  -0.142  -0.071   0.219   0.510   \n",
       "3095 -0.169 -0.023  0.090  0.219 -0.030  ...   0.372  -0.171   0.302   0.348   \n",
       "8379  0.126  0.148  0.160  0.034  0.023  ...   0.005   0.287  -0.059  -0.020   \n",
       "3043  0.216 -0.047 -0.008 -0.233  0.073  ...  -0.043   0.388  -0.239  -0.087   \n",
       "\n",
       "      wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  \\\n",
       "6671   0.203  -0.048  -0.085  -0.005  -0.113  -0.060   0.000   0.342  -0.053   \n",
       "3274   0.175   0.025   0.162   0.396   0.248   0.298   0.000   0.164  -0.056   \n",
       "3095   0.185  -0.027  -0.023   0.469   0.016   0.239   0.000   0.217   0.342   \n",
       "8379  -0.074  -0.029  -0.054  -0.016  -0.076  -0.012   0.000  -0.117  -0.060   \n",
       "3043  -0.274   0.442  -0.265  -0.117  -0.173  -0.226   0.000  -0.358  -0.206   \n",
       "\n",
       "      wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  \\\n",
       "6671  -0.151  -0.145  -0.066  -0.058  -0.105   0.081  -0.076  -0.094  -0.128   \n",
       "3274   0.061  -0.082   0.367   0.497   0.163  -0.052   0.250   0.056   0.138   \n",
       "3095   0.271   0.297   0.407   0.413   0.222  -0.034   0.277  -0.059   0.202   \n",
       "8379  -0.043  -0.047  -0.061  -0.022  -0.081  -0.043  -0.029  -0.003  -0.025   \n",
       "3043  -0.175  -0.088  -0.131  -0.059  -0.253   0.445  -0.199   0.356  -0.151   \n",
       "\n",
       "      wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  \\\n",
       "6671   0.000   0.000  -0.018   0.006  -0.097   0.275   0.064  -0.034  -0.072   \n",
       "3274   0.000   0.000   0.063  -0.032  -0.079   0.006   0.008  -0.060   0.255   \n",
       "3095   0.000   0.000  -0.022   0.123   0.291   0.143  -0.076  -0.144   0.354   \n",
       "8379   0.000   0.000   0.410   0.344  -0.120  -0.104   0.028  -0.113  -0.068   \n",
       "3043   0.000   0.000   0.509   0.508  -0.215  -0.339   0.419   0.304  -0.186   \n",
       "\n",
       "      wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  wb_388  \\\n",
       "6671   0.130  -0.083  -0.099  -0.142  -0.337  -0.649   0.130  -0.428  -1.037   \n",
       "3274  -0.048   0.315  -0.071   0.152  -0.103   0.567   0.500  -0.873  -0.209   \n",
       "3095  -0.035   0.347  -0.045   0.176  -0.596  -0.443   0.422  -0.262  -0.736   \n",
       "8379  -0.039  -0.093   0.337  -0.092  -0.076  -0.680   0.114  -0.252  -0.110   \n",
       "3043   0.462  -0.091  -0.084  -0.264  -0.686   0.600   0.698  -0.981  -0.849   \n",
       "\n",
       "      wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  wb_397  \\\n",
       "6671   0.031   0.121   0.214  -0.592  -0.272   0.194  -0.695  -0.563  -0.149   \n",
       "3274   0.738   1.438  -0.673  -0.720  -0.272   0.469  -0.870  -0.542  -0.154   \n",
       "3095   0.650   0.798  -0.141  -0.178  -0.272   0.552  -0.110   0.560  -0.149   \n",
       "8379   0.912   0.137  -0.890  -0.174  -0.272   0.471  -0.120   0.715  -0.942   \n",
       "3043   0.036   0.114  -0.769  -0.794  -0.272   0.743  -0.782  -0.602  -0.149   \n",
       "\n",
       "      wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  wb_406  \\\n",
       "6671   0.438   0.136  -0.906  -0.409  -0.319  -0.133  -0.573  -0.644  -0.628   \n",
       "3274   0.036   1.268   0.849  -0.148   0.482  -0.135  -0.796  -0.597   0.603   \n",
       "3095   0.460   0.858   0.604  -0.620  -0.411  -0.118  -0.236  -0.045  -0.460   \n",
       "8379   0.700   1.355   0.830  -0.901   0.547  -0.962  -0.235  -0.057   0.823   \n",
       "3043   0.302   0.135  -0.871  -0.139   0.506  -1.038  -0.880  -0.670  -0.785   \n",
       "\n",
       "      wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  wb_415  \\\n",
       "6671  -0.271   0.237   0.730  -0.196   0.108  -0.895   0.582  -0.350   0.167   \n",
       "3274   0.572   0.536   0.639  -0.632   0.794  -0.741  -0.365  -0.661   0.751   \n",
       "3095   0.123   0.116   0.475  -0.064   0.790  -0.176   0.536  -0.615   0.682   \n",
       "8379   0.595   0.659  -0.008  -0.817   0.122  -0.179   0.746  -0.973   0.899   \n",
       "3043   0.585   0.660   0.668  -0.770   0.790  -0.868  -0.010  -0.835   0.842   \n",
       "\n",
       "      wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  wb_424  \\\n",
       "6671   0.214   0.478  -0.168   0.038   0.020   0.220  -0.054  -0.294   0.428   \n",
       "3274   1.597   0.536  -1.116   0.560   0.774   0.698   0.567  -0.294   0.314   \n",
       "3095   0.846   0.395  -0.089  -0.104   0.771   0.297   0.199  -0.294   0.232   \n",
       "8379   1.132   0.651  -0.090   0.707   0.013   0.260   0.679  -0.294   0.557   \n",
       "3043   0.212   0.678  -0.877   0.680   0.815   0.816   0.649  -0.294   0.506   \n",
       "\n",
       "      wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  wb_433  \\\n",
       "6671   0.077   0.477   0.483   0.184   0.143   0.183  -1.227   0.092  -0.220   \n",
       "3274   0.070   0.136   0.110   0.900   1.551   0.653  -0.261   0.666  -0.787   \n",
       "3095   0.625   0.607   0.607   0.983   0.859   0.509  -0.266   0.632  -0.252   \n",
       "8379   0.066   0.876   0.923   0.198   0.171   0.563  -0.266   0.771  -0.317   \n",
       "3043   0.619   0.393   0.137   0.967   0.136   0.817  -1.106   0.730  -0.882   \n",
       "\n",
       "      wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  wb_442  \\\n",
       "6671   0.092  -0.187  -0.257  -0.146  -0.647   0.117   0.367   0.307  -0.259   \n",
       "3274   0.116  -0.187  -0.257  -0.808  -0.604   0.069   0.500  -0.659  -0.510   \n",
       "3095   0.555  -0.187  -0.257  -0.145   0.251   0.585   0.073  -0.117  -0.637   \n",
       "8379   0.728  -0.187  -0.257  -0.988  -0.742   0.078   0.574  -0.237  -0.155   \n",
       "3043   0.190  -0.187  -0.257  -1.004  -0.924   0.663   0.579  -0.833  -0.608   \n",
       "\n",
       "      wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "6671   0.160  -0.913   0.175  -0.025   0.208   0.081  \n",
       "3274   0.830  -0.097   0.990  -0.162   0.603   0.124  \n",
       "3095   0.829  -0.106   0.720  -0.670   0.394   0.155  \n",
       "8379   0.162  -0.098   0.168  -0.903   0.784  -0.088  \n",
       "3043   0.818  -0.824   0.166  -0.150   0.740  -0.222  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-1.313</td>\n",
       "      <td>-1.513</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.533</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-1.568</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.571</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-1.350</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-1.041</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>0.892</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>0.767</td>\n",
       "      <td>1.274</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-1.306</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.628</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.547</td>\n",
       "      <td>1.393</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.629</td>\n",
       "      <td>0.776</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>-1.258</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.651</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>1.183</td>\n",
       "      <td>-1.204</td>\n",
       "      <td>1.464</td>\n",
       "      <td>-1.440</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>689.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>4148.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.475</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.467</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-1.014</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.936</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>1.078</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.573</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.654</td>\n",
       "      <td>-1.086</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.967</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>1.202</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2815.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.162</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>5185.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-1.819</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.134</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.160</td>\n",
       "      <td>-1.668</td>\n",
       "      <td>-1.535</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-1.847</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.519</td>\n",
       "      <td>1.084</td>\n",
       "      <td>0.881</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.051</td>\n",
       "      <td>0.642</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.430</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.172</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.847</td>\n",
       "      <td>-2.119</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>0.623</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed  f0v0  f0v1   f0v2  f0v3   f0v4   f1v0   f1v1  f1v2  f1v3  \\\n",
       "3466 3466.000    42 0.000 0.000  0.000 0.337  0.000  0.000  0.000 0.000 0.000   \n",
       "689   689.000    42 0.000 0.000  0.000 0.000 -0.413 -0.432  0.000 0.000 0.000   \n",
       "4148 4148.000    42 0.000 0.000  0.442 0.000  0.000  0.000 -0.344 0.000 0.000   \n",
       "2815 2815.000    42 0.000 0.000  0.000 0.000 -0.443  0.000  0.379 0.000 0.000   \n",
       "5185 5185.000    42 0.000 0.000 -0.265 0.000  0.000  0.264  0.000 0.000 0.000   \n",
       "\n",
       "      f1v4  f2v0   f2v1   f2v2   f2v3  f2v4  f3v0  f3v1   f3v2   f3v3  f3v4  \\\n",
       "3466 0.423 0.000  0.000  0.000  0.000 0.441 0.000 0.000  0.000 -0.408 0.000   \n",
       "689  0.000 0.000  0.000 -0.288  0.000 0.000 0.000 0.433  0.000  0.000 0.000   \n",
       "4148 0.000 0.000 -0.252  0.000  0.000 0.000 0.399 0.000  0.000  0.000 0.000   \n",
       "2815 0.000 0.000  0.000  0.000 -0.428 0.000 0.000 0.000 -0.420  0.000 0.000   \n",
       "5185 0.000 0.000 -0.437  0.000  0.000 0.000 0.000 0.000  0.000  0.364 0.000   \n",
       "\n",
       "       f4v0   f4v1  f4v2  f4v3  f4v4  f5v0  f5v1   f5v2  f5v3  f5v4   f6v0  \\\n",
       "3466  0.000  0.000 0.000 0.413 0.000 0.000 0.000 -0.292 0.000 0.000 -0.362   \n",
       "689  -0.293  0.000 0.000 0.000 0.000 0.000 0.000 -0.331 0.000 0.000  0.000   \n",
       "4148  0.000 -0.345 0.000 0.000 0.000 0.398 0.000  0.000 0.000 0.000  0.000   \n",
       "2815  0.000 -0.421 0.000 0.000 0.000 0.000 0.000 -0.432 0.000 0.000  0.000   \n",
       "5185  0.000 -0.418 0.000 0.000 0.000 0.000 0.000  0.000 0.000 0.401 -0.273   \n",
       "\n",
       "      f6v1   f6v2   f6v3   f6v4  f7v0   f7v1  f7v2  f7v3   f7v4   f8v0  f8v1  \\\n",
       "3466 0.000  0.000  0.000  0.000 0.397  0.000 0.000 0.000  0.000  0.000 0.000   \n",
       "689  0.000  0.000 -0.439  0.000 0.447  0.000 0.000 0.000  0.000 -0.363 0.000   \n",
       "4148 0.000 -0.428  0.000  0.000 0.000  0.000 0.000 0.000 -0.410  0.000 0.000   \n",
       "2815 0.000  0.000  0.000 -0.414 0.000 -0.401 0.000 0.000  0.000  0.399 0.000   \n",
       "5185 0.000  0.000  0.000  0.000 0.425  0.000 0.000 0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f8v2  f8v3  f8v4  f9v0   f9v1   f9v2  f9v3  f9v4  f10v0  f10v1  f10v2  \\\n",
       "3466 0.425 0.000 0.000 0.435  0.000  0.000 0.000 0.000  0.000  0.000  0.382   \n",
       "689  0.000 0.000 0.000 0.000  0.000 -0.407 0.000 0.000  0.000 -0.423  0.000   \n",
       "4148 0.000 0.000 0.443 0.413  0.000  0.000 0.000 0.000  0.000  0.000  0.000   \n",
       "2815 0.000 0.000 0.000 0.000  0.000  0.000 0.000 0.446  0.000  0.358  0.000   \n",
       "5185 0.000 0.431 0.000 0.000 -0.420  0.000 0.000 0.000 -0.350  0.000  0.000   \n",
       "\n",
       "      f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  f12v2  \\\n",
       "3466  0.000  0.000  0.000 -0.442  0.000  0.000  0.000  0.443  0.000  0.000   \n",
       "689   0.000  0.000  0.000 -0.242  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "4148  0.000 -0.442  0.000  0.000  0.000  0.333  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.428  0.000  0.000  0.000  0.000  0.000  0.000 -0.177   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.000  0.440  0.000 -0.436  0.000   \n",
       "\n",
       "      f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  f14v2  \\\n",
       "3466  0.000  0.000  0.000  0.000  0.389  0.000  0.000  0.000  0.000  0.000   \n",
       "689   0.000  0.436  0.000  0.000  0.000  0.357  0.000  0.000  0.000  0.401   \n",
       "4148  0.315  0.000  0.000  0.000  0.000  0.372  0.000  0.000  0.000  0.000   \n",
       "2815  0.000  0.000  0.444  0.000  0.000  0.000  0.000  0.000  0.000 -0.367   \n",
       "5185  0.000  0.000  0.000  0.000  0.000  0.307  0.000  0.000 -0.409  0.000   \n",
       "\n",
       "      f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6     b7  \\\n",
       "3466  0.000  0.227  0.410 -0.065  0.297  0.409 -0.155  0.187  0.120 -0.277   \n",
       "689   0.000  0.000  0.381  0.093 -0.380  0.171 -0.023 -0.239  0.030 -0.010   \n",
       "4148 -0.332  0.000 -0.183 -0.404  0.376  0.281 -0.078 -0.194 -0.309  0.291   \n",
       "2815  0.000  0.000 -0.195  0.395  0.256 -0.356  0.351  0.372  0.123 -0.366   \n",
       "5185  0.000  0.000  0.190 -0.041  0.126 -0.156 -0.393  0.322 -0.112 -0.275   \n",
       "\n",
       "         b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  lp1c0  \\\n",
       "3466  0.149  0.041  0.190 -0.210  0.184 -0.093 -0.151 -0.250  0.081  0.229   \n",
       "689  -0.136 -0.171  0.370 -0.312 -0.340  0.034 -0.386 -0.106 -0.086 -0.156   \n",
       "4148  0.084 -0.018  0.291 -0.109 -0.034  0.094 -0.146 -0.198  0.084  0.240   \n",
       "2815  0.174  0.005  0.031  0.056 -0.253  0.010  0.187  0.192 -0.027 -0.184   \n",
       "5185 -0.381  0.415 -0.138 -0.150 -0.050 -0.062 -0.079  0.227  0.070 -0.226   \n",
       "\n",
       "      lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  wb_352  \\\n",
       "3466 -0.003 -0.162 -0.137  0.086  0.069  ...   0.051   0.391  -0.290   0.095   \n",
       "689  -0.069 -0.074 -0.215  0.120  0.192  ...   0.313   0.004  -0.070  -0.075   \n",
       "4148  0.017  0.084  0.038  0.091 -0.116  ...   0.440   0.298  -0.155  -0.068   \n",
       "2815 -0.173 -0.118 -0.087 -0.081 -0.118  ...   0.087   0.148  -0.077   0.099   \n",
       "5185 -0.235  0.068  0.210  0.040 -0.210  ...   0.560  -0.389   0.414   0.561   \n",
       "\n",
       "      wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  wb_361  \\\n",
       "3466  -0.278   0.510  -0.466  -0.006  -0.243  -0.310   0.000  -0.471  -0.081   \n",
       "689   -0.024   0.077  -0.072   0.000  -0.113  -0.070   0.000   0.068   0.257   \n",
       "4148  -0.176  -0.031  -0.164   0.000  -0.245  -0.113   0.000  -0.386  -0.049   \n",
       "2815  -0.047   0.209   0.006   0.184   0.053  -0.076   0.000  -0.043  -0.011   \n",
       "5185   0.348  -0.031   0.440   0.513   0.258   0.498   0.000   0.361   0.462   \n",
       "\n",
       "      wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  wb_370  \\\n",
       "3466  -0.231  -0.041   0.140  -0.033  -0.216   0.513  -0.304   0.415  -0.237   \n",
       "689    0.022   0.074  -0.064  -0.042  -0.094   0.092  -0.079  -0.093   0.092   \n",
       "4148  -0.094  -0.079  -0.048  -0.032  -0.374  -0.067  -0.122   0.284  -0.105   \n",
       "2815  -0.010   0.005   0.166  -0.032  -0.054   0.227  -0.083  -0.096   0.059   \n",
       "5185   0.432   0.470   0.500   0.599   0.352   0.220   0.467  -0.412   0.342   \n",
       "\n",
       "      wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  wb_379  \\\n",
       "3466   0.000   0.000   0.583   0.543  -0.018  -0.503   0.483   0.267  -0.239   \n",
       "689    0.000   0.000  -0.007   0.035   0.179  -0.020  -0.014   0.012  -0.065   \n",
       "4148   0.000   0.000   0.475  -0.260  -0.100  -0.260   0.186   0.079  -0.431   \n",
       "2815   0.000   0.000  -0.011   0.187   0.068  -0.049   0.200   0.052  -0.070   \n",
       "5185   0.000   0.000  -0.017   0.314   0.434   0.301  -0.077  -0.395   0.463   \n",
       "\n",
       "      wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  wb_388  \\\n",
       "3466   0.358   0.089   0.559  -0.257  -0.337  -0.747   0.662  -1.313  -1.513   \n",
       "689    0.160   0.206   0.066  -0.130  -0.394  -0.452   0.323  -0.889  -0.836   \n",
       "4148   0.448  -0.070  -0.083  -0.174   0.029  -0.563   0.860  -0.290  -0.184   \n",
       "2815  -0.057   0.059   0.188  -0.066  -0.591   0.535   0.595  -0.741  -0.801   \n",
       "5185  -0.032   0.544  -0.275   0.352  -1.819  -0.528   0.014  -0.259  -2.134   \n",
       "\n",
       "      wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  wb_397  \\\n",
       "3466   0.027   1.533  -0.593  -0.932  -0.272   0.520  -0.512  -0.399  -1.568   \n",
       "689    0.031   0.125   0.161  -0.063  -0.272   0.238  -0.505  -0.359  -0.145   \n",
       "4148   0.035   0.132  -0.971  -0.925  -0.272   0.417  -0.684  -0.489  -0.151   \n",
       "2815   0.590   0.133  -0.140  -0.167  -0.272   0.701  -0.654   0.531  -0.759   \n",
       "5185   1.127   1.160  -1.668  -1.535  -0.272   0.455  -1.847   0.835  -0.148   \n",
       "\n",
       "      wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  wb_406  \\\n",
       "3466   0.452   1.571  -1.192  -1.381   0.080  -1.350  -0.664  -0.984  -0.574   \n",
       "689    0.344   0.144  -0.822  -0.539  -0.237  -0.431  -0.234  -0.340  -0.440   \n",
       "4148   0.278   0.134   1.467  -0.127   0.058  -0.118  -1.014  -0.614  -0.542   \n",
       "2815   0.493   1.122   0.705  -0.658  -0.353  -0.704  -0.225  -0.048   0.565   \n",
       "5185   0.519   1.084   0.881  -0.701  -0.481  -0.128  -0.787  -0.041  -0.573   \n",
       "\n",
       "      wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  wb_415  \\\n",
       "3466   0.579   0.559  -1.041  -0.658   0.892  -1.215  -1.324  -0.590   0.767   \n",
       "689   -0.184   0.092   0.329  -0.226   0.099  -0.675   0.584  -0.337   0.168   \n",
       "4148   0.462   0.505   0.936  -0.661   0.102  -0.829   1.078  -0.615   0.630   \n",
       "2815  -0.077   0.231   0.601  -0.655   0.124  -0.164   0.749  -0.662   0.162   \n",
       "5185   0.368   0.284   0.238  -0.186   0.105  -0.179   0.723  -0.748   1.058   \n",
       "\n",
       "      wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  wb_424  \\\n",
       "3466   1.274   0.533  -1.306   0.747   0.019   0.601   0.628  -0.294   0.547   \n",
       "689    0.217   0.125  -0.681   0.054   0.025   0.220   0.006  -0.294   0.151   \n",
       "4148   0.210   0.554  -0.086   0.549   0.025   0.681   0.573  -0.294   0.377   \n",
       "2815   1.027   0.053  -0.678   0.410   0.679   0.684  -0.010  -0.294   0.368   \n",
       "5185   1.051   0.642  -0.089   0.430   1.117   0.643   0.689  -0.294   0.172   \n",
       "\n",
       "      wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  wb_433  \\\n",
       "3466   1.393   0.512   0.943   1.488   1.629   0.776  -1.526   0.682  -0.744   \n",
       "689    0.679   0.355   0.512   0.204   0.157   0.185  -0.930   0.106  -0.238   \n",
       "4148   0.081   0.154   0.148   0.214   0.165   0.856  -0.240   0.654  -1.086   \n",
       "2815   0.788   0.755   0.801   0.853   0.161   0.268  -0.893   0.108  -0.232   \n",
       "5185   1.032   0.817   0.825   1.031   1.196   0.847  -2.119   0.935  -1.902   \n",
       "\n",
       "      wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  wb_442  \\\n",
       "3466   0.423  -0.187  -0.257  -1.024  -1.258   0.684   0.651  -1.219  -0.268   \n",
       "689    0.353  -0.187  -0.257  -0.152  -0.504   0.589   0.038  -0.478  -0.320   \n",
       "4148   0.112  -0.187  -0.257  -0.967   0.651   0.128   0.436  -0.541  -0.438   \n",
       "2815   0.600  -0.187  -0.257  -0.150  -0.575   0.644   0.299  -0.673  -0.293   \n",
       "5185   0.623  -0.187  -0.257  -0.147   0.758   0.429   0.118  -0.112  -0.704   \n",
       "\n",
       "      wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "3466   1.183  -1.204   1.464  -1.440   0.629  -0.238  \n",
       "689    0.164  -0.779   0.704  -0.636   0.208  -0.001  \n",
       "4148   1.202  -0.780   0.178  -0.162   0.539  -0.171  \n",
       "2815   0.161  -0.079   0.902  -0.704   0.305  -0.067  \n",
       "5185   1.211  -0.105   0.997  -0.701   0.539   0.324  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f0v3</th>\n",
       "      <th>f0v4</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f1v3</th>\n",
       "      <th>f1v4</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f2v3</th>\n",
       "      <th>f2v4</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f3v3</th>\n",
       "      <th>f3v4</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f4v3</th>\n",
       "      <th>f4v4</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f5v3</th>\n",
       "      <th>f5v4</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>f6v3</th>\n",
       "      <th>f6v4</th>\n",
       "      <th>f7v0</th>\n",
       "      <th>f7v1</th>\n",
       "      <th>f7v2</th>\n",
       "      <th>f7v3</th>\n",
       "      <th>f7v4</th>\n",
       "      <th>f8v0</th>\n",
       "      <th>f8v1</th>\n",
       "      <th>f8v2</th>\n",
       "      <th>f8v3</th>\n",
       "      <th>f8v4</th>\n",
       "      <th>f9v0</th>\n",
       "      <th>f9v1</th>\n",
       "      <th>f9v2</th>\n",
       "      <th>f9v3</th>\n",
       "      <th>f9v4</th>\n",
       "      <th>f10v0</th>\n",
       "      <th>f10v1</th>\n",
       "      <th>f10v2</th>\n",
       "      <th>f10v3</th>\n",
       "      <th>f10v4</th>\n",
       "      <th>f11v0</th>\n",
       "      <th>f11v1</th>\n",
       "      <th>f11v2</th>\n",
       "      <th>f11v3</th>\n",
       "      <th>f11v4</th>\n",
       "      <th>f12v0</th>\n",
       "      <th>f12v1</th>\n",
       "      <th>f12v2</th>\n",
       "      <th>f12v3</th>\n",
       "      <th>f12v4</th>\n",
       "      <th>f13v0</th>\n",
       "      <th>f13v1</th>\n",
       "      <th>f13v2</th>\n",
       "      <th>f13v3</th>\n",
       "      <th>f13v4</th>\n",
       "      <th>f14v0</th>\n",
       "      <th>f14v1</th>\n",
       "      <th>f14v2</th>\n",
       "      <th>f14v3</th>\n",
       "      <th>f14v4</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b13</th>\n",
       "      <th>b14</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-1.460</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.891</td>\n",
       "      <td>1.166</td>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-1.152</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.774</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-1.488</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.885</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-1.367</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-1.436</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.727</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.837</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-1.399</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.842</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.698</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.878</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.612</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.589</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.635</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.686</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.869</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.853</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.965</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.131</td>\n",
       "      <td>1.269</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>0.607</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-1.667</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.188</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.002</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>1.288</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.474</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.348</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>1.565</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>1.493</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.021</td>\n",
       "      <td>2.238</td>\n",
       "      <td>-2.355</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.338</td>\n",
       "      <td>2.214</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.140</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-2.262</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-2.022</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>2.004</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.389</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.202</td>\n",
       "      <td>2.327</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>1.744</td>\n",
       "      <td>1.105</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>1.977</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>2.140</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0  f0v1   f0v2  f0v3  f0v4  f1v0  f1v1   f1v2   f1v3  \\\n",
       "7217 7217.000    42 -0.437 0.000  0.000 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "8291 8291.000    42  0.000 0.000 -0.391 0.000 0.000 0.000 0.000  0.000  0.419   \n",
       "4607 4607.000    42  0.000 0.304  0.000 0.000 0.000 0.000 0.000 -0.418  0.000   \n",
       "5114 5114.000    42  0.000 0.000 -0.418 0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "1859 1859.000    42  0.000 0.000  0.000 0.000 0.412 0.000 0.000  0.000 -0.342   \n",
       "\n",
       "       f1v4   f2v0   f2v1   f2v2  f2v3  f2v4  f3v0   f3v1   f3v2   f3v3  f3v4  \\\n",
       "7217 -0.431  0.000 -0.291  0.000 0.000 0.000 0.000  0.000  0.000 -0.250 0.000   \n",
       "8291  0.000 -0.389  0.000  0.000 0.000 0.000 0.000  0.000  0.000  0.000 0.290   \n",
       "4607  0.000  0.000  0.000 -0.406 0.000 0.000 0.000 -0.419  0.000  0.000 0.000   \n",
       "5114  0.395 -0.437  0.000  0.000 0.000 0.000 0.000  0.000 -0.434  0.000 0.000   \n",
       "1859  0.000  0.000  0.000 -0.344 0.000 0.000 0.439  0.000  0.000  0.000 0.000   \n",
       "\n",
       "      f4v0  f4v1   f4v2  f4v3  f4v4  f5v0   f5v1  f5v2  f5v3   f5v4  f6v0  \\\n",
       "7217 0.000 0.000  0.430 0.000 0.000 0.000  0.000 0.000 0.000  0.410 0.000   \n",
       "8291 0.000 0.000  0.000 0.000 0.443 0.000  0.000 0.000 0.000 -0.257 0.000   \n",
       "4607 0.438 0.000  0.000 0.000 0.000 0.000  0.000 0.000 0.000  0.443 0.000   \n",
       "5114 0.000 0.000  0.440 0.000 0.000 0.000 -0.436 0.000 0.000  0.000 0.000   \n",
       "1859 0.000 0.000 -0.373 0.000 0.000 0.000  0.000 0.000 0.000  0.440 0.000   \n",
       "\n",
       "       f6v1   f6v2   f6v3   f6v4  f7v0   f7v1   f7v2   f7v3  f7v4   f8v0  \\\n",
       "7217 -0.411  0.000  0.000  0.000 0.000  0.446  0.000  0.000 0.000  0.000   \n",
       "8291  0.000  0.000 -0.430  0.000 0.000  0.000 -0.418  0.000 0.000 -0.224   \n",
       "4607  0.000 -0.412  0.000  0.000 0.000 -0.434  0.000  0.000 0.000 -0.354   \n",
       "5114  0.000  0.000  0.000 -0.412 0.000  0.000  0.000  0.362 0.000  0.000   \n",
       "1859  0.000  0.000 -0.411  0.000 0.000  0.000  0.000 -0.313 0.000  0.000   \n",
       "\n",
       "      f8v1  f8v2   f8v3  f8v4  f9v0   f9v1  f9v2  f9v3  f9v4  f10v0  f10v1  \\\n",
       "7217 0.000 0.000  0.000 0.255 0.223  0.000 0.000 0.000 0.000  0.000  0.000   \n",
       "8291 0.000 0.000  0.000 0.000 0.000  0.000 0.000 0.000 0.424  0.000  0.000   \n",
       "4607 0.000 0.000  0.000 0.000 0.000 -0.405 0.000 0.000 0.000 -0.351  0.000   \n",
       "5114 0.000 0.000 -0.443 0.000 0.000  0.187 0.000 0.000 0.000 -0.399  0.000   \n",
       "1859 0.000 0.446  0.000 0.000 0.349  0.000 0.000 0.000 0.000 -0.339  0.000   \n",
       "\n",
       "      f10v2  f10v3  f10v4  f11v0  f11v1  f11v2  f11v3  f11v4  f12v0  f12v1  \\\n",
       "7217  0.325  0.000  0.000  0.000  0.000  0.444  0.000  0.000  0.000 -0.337   \n",
       "8291 -0.343  0.000  0.000  0.000  0.000  0.000  0.294  0.000  0.000  0.408   \n",
       "4607  0.000  0.000  0.000  0.000  0.000 -0.395  0.000  0.000  0.000  0.378   \n",
       "5114  0.000  0.000  0.000  0.000 -0.416  0.000  0.000  0.000  0.000  0.447   \n",
       "1859  0.000  0.000  0.000  0.000  0.000  0.386  0.000  0.000  0.000 -0.435   \n",
       "\n",
       "      f12v2  f12v3  f12v4  f13v0  f13v1  f13v2  f13v3  f13v4  f14v0  f14v1  \\\n",
       "7217  0.000  0.000  0.000  0.443  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "8291  0.000  0.000  0.000  0.000  0.000  0.000 -0.326  0.000  0.000  0.324   \n",
       "4607  0.000  0.000  0.000  0.000  0.000  0.000  0.000 -0.372  0.000  0.000   \n",
       "5114  0.000  0.000  0.000  0.000  0.419  0.000  0.000  0.000  0.000  0.000   \n",
       "1859  0.000  0.000  0.000  0.000  0.000 -0.265  0.000  0.000  0.000  0.000   \n",
       "\n",
       "      f14v2  f14v3  f14v4     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "7217  0.000  0.000  0.389 -0.065  0.393 -0.178 -0.007 -0.305 -0.298  0.252   \n",
       "8291  0.000  0.000  0.000  0.418  0.086 -0.061  0.131  0.193  0.094  0.142   \n",
       "4607  0.397  0.000  0.000 -0.421  0.434  0.280 -0.198 -0.349 -0.178 -0.264   \n",
       "5114  0.000  0.000  0.346  0.097 -0.287  0.039 -0.067 -0.312 -0.142  0.389   \n",
       "1859 -0.446  0.000  0.000 -0.346 -0.194  0.324  0.164 -0.045 -0.397  0.403   \n",
       "\n",
       "         b7     b8     b9    b10    b11    b12    b13    b14  lp0c0  lp0c1  \\\n",
       "7217  0.291 -0.429  0.275  0.119 -0.112  0.321 -0.383 -0.231 -0.184  0.025   \n",
       "8291  0.232  0.158 -0.350  0.075 -0.410  0.028 -0.125 -0.023 -0.213  0.242   \n",
       "4607  0.181  0.291  0.178  0.050  0.260  0.189  0.150  0.042 -0.069 -0.141   \n",
       "5114  0.014  0.229  0.082 -0.131 -0.058 -0.333  0.026 -0.312  0.173 -0.059   \n",
       "1859 -0.243 -0.165  0.222  0.373 -0.174  0.009 -0.416  0.101  0.068  0.206   \n",
       "\n",
       "      lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  ...  wb_349  wb_350  wb_351  \\\n",
       "7217  0.093  0.054 -0.007 -0.064 -0.003 -0.035  ...   0.358  -0.423  -0.064   \n",
       "8291  0.087 -0.014  0.150 -0.173 -0.107 -0.075  ...   0.380  -0.122   0.308   \n",
       "4607  0.114  0.117 -0.167  0.087  0.077  0.174  ...  -0.046   0.314  -0.083   \n",
       "5114  0.063 -0.006  0.009 -0.165  0.155 -0.147  ...   0.364  -0.035   0.326   \n",
       "1859 -0.198 -0.152 -0.151 -0.071 -0.165 -0.041  ...   0.156  -0.048   0.069   \n",
       "\n",
       "      wb_352  wb_353  wb_354  wb_355  wb_356  wb_357  wb_358  wb_359  wb_360  \\\n",
       "7217   0.031   0.363  -0.081   0.527   0.000   0.457   0.567   0.000   0.346   \n",
       "8291   0.367   0.252  -0.024  -0.025  -0.010  -0.124   0.365   0.000  -0.203   \n",
       "4607  -0.071  -0.153   0.355  -0.149   0.000  -0.140   0.066   0.000  -0.187   \n",
       "5114   0.461   0.126  -0.026  -0.046  -0.005  -0.067   0.325   0.000  -0.133   \n",
       "1859   0.597   0.036  -0.030  -0.313  -0.022  -0.137   0.047   0.000  -0.105   \n",
       "\n",
       "      wb_361  wb_362  wb_363  wb_364  wb_365  wb_366  wb_367  wb_368  wb_369  \\\n",
       "7217  -0.057  -0.100  -0.091  -0.012   0.173   0.311  -0.049   0.513  -0.291   \n",
       "8291   0.360   0.312   0.312  -0.065   0.411   0.234  -0.038   0.322  -0.047   \n",
       "4607  -0.050  -0.108  -0.099  -0.129  -0.028  -0.152   0.379  -0.096   0.207   \n",
       "5114   0.389   0.296   0.330  -0.063   0.473   0.140   0.223   0.276  -0.075   \n",
       "1859   0.108   0.083   0.076  -0.069   0.625  -0.001  -0.059   0.100  -0.080   \n",
       "\n",
       "      wb_370  wb_371  wb_372  wb_373  wb_374  wb_375  wb_376  wb_377  wb_378  \\\n",
       "7217   0.312   0.000   0.000   0.000  -0.408  -0.122   0.465  -0.379  -0.283   \n",
       "8291  -0.097   0.000   0.000   0.000  -0.047  -0.106  -0.185  -0.075  -0.093   \n",
       "4607  -0.120   0.000   0.000   0.000   0.374  -0.088  -0.185   0.348  -0.120   \n",
       "5114  -0.110   0.000   0.000  -0.028  -0.046  -0.098  -0.078   0.087  -0.022   \n",
       "1859  -0.150   0.000   0.000  -0.006  -0.221   0.258  -0.238  -0.084   0.227   \n",
       "\n",
       "      wb_379  wb_380  wb_381  wb_382  wb_383  wb_384  wb_385  wb_386  wb_387  \\\n",
       "7217  -0.047   0.221  -0.084  -0.084   0.294  -0.821   0.733   0.344  -1.460   \n",
       "8291   0.351  -0.048   0.378  -0.041   0.158  -0.154  -0.461   0.079  -0.261   \n",
       "4607  -0.072   0.418  -0.080  -0.048  -0.154  -0.556   0.491   0.574  -0.821   \n",
       "5114   0.450  -0.053   0.483  -0.021   0.036  -0.191  -0.528   0.070  -0.238   \n",
       "1859   0.399  -0.049   0.596   0.324  -0.026  -0.072  -0.470  -0.221  -0.248   \n",
       "\n",
       "      wb_388  wb_389  wb_390  wb_391  wb_392  wb_393  wb_394  wb_395  wb_396  \\\n",
       "7217  -0.210   0.891   1.166  -0.724  -0.831  -0.272   0.153  -1.152  -0.689   \n",
       "8291  -0.216   0.030   0.842  -0.649  -0.707  -0.272   0.195  -0.659  -0.489   \n",
       "4607  -0.768   0.809   0.135  -0.174  -0.754  -0.272   0.853  -0.611  -0.497   \n",
       "5114  -1.667   0.033   1.249  -0.514  -0.642  -0.272   0.198  -0.549  -0.405   \n",
       "1859  -0.188   0.021   2.238  -2.355  -0.512  -0.272   0.289  -0.076   0.522   \n",
       "\n",
       "      wb_397  wb_398  wb_399  wb_400  wb_401  wb_402  wb_403  wb_404  wb_405  \\\n",
       "7217  -0.148   0.083   0.774   1.012  -0.144   0.518  -0.938  -1.488  -0.682   \n",
       "8291  -0.145   0.479   0.874   0.698  -0.630  -0.370  -0.117  -0.774  -0.595   \n",
       "4607  -0.154   0.392   0.142  -0.610  -0.667   0.444  -0.122  -0.257  -0.226   \n",
       "5114  -0.154   0.188   1.292   1.002  -0.491  -0.255  -0.353  -0.649  -0.568   \n",
       "1859  -0.144   0.338   2.214   0.347  -0.540   1.140  -0.139  -0.574  -2.262   \n",
       "\n",
       "      wb_406  wb_407  wb_408  wb_409  wb_410  wb_411  wb_412  wb_413  wb_414  \\\n",
       "7217   0.715   0.537   0.420   0.885  -0.829   0.960  -1.367   0.460  -0.942   \n",
       "8291  -0.482  -0.003   0.169   0.003  -0.101   0.878  -0.207   0.612  -0.586   \n",
       "4607   0.553   0.449   0.514   0.539  -0.582   0.122  -0.771   0.008  -0.615   \n",
       "5114  -0.512  -0.154   0.133   0.007  -0.348   1.288  -0.826   0.794  -0.372   \n",
       "1859  -0.525   0.618   0.684  -2.022  -0.064   2.004  -0.143   0.292  -0.088   \n",
       "\n",
       "      wb_415  wb_416  wb_417  wb_418  wb_419  wb_420  wb_421  wb_422  wb_423  \\\n",
       "7217   0.174   0.425   0.349  -1.436   0.688   0.025   0.618   0.688  -0.294   \n",
       "8291   0.775   0.907   0.589  -0.092   0.062   0.017   0.216   0.635  -0.294   \n",
       "4607   0.157   0.223   0.268  -0.613   0.589   0.025   0.965  -0.362  -0.294   \n",
       "5114   1.162   1.474   0.366  -0.090   0.086   0.020   0.236   0.728  -0.294   \n",
       "1859   0.389   2.300   0.350  -0.090   1.384   0.007   0.212   0.347  -0.294   \n",
       "\n",
       "      wb_424  wb_425  wb_426  wb_427  wb_428  wb_429  wb_430  wb_431  wb_432  \\\n",
       "7217   0.191   0.078   0.156   0.148   0.221   0.848   0.379  -0.262   0.727   \n",
       "8291  -0.266   0.711   0.714   0.729   0.207   0.913   0.710  -0.272   0.686   \n",
       "4607   0.464   0.080   0.145   0.131   1.269   0.167   0.443  -0.838   0.098   \n",
       "5114  -0.124   0.797   0.976   1.074   0.190   1.348   0.497  -1.686   0.693   \n",
       "1859   0.554   0.430   0.359   0.363   0.202   2.327   0.339  -0.255   0.353   \n",
       "\n",
       "      wb_433  wb_434  wb_435  wb_436  wb_437  wb_438  wb_439  wb_440  wb_441  \\\n",
       "7217  -0.833   0.253  -0.187  -0.257  -0.159  -0.837   0.120   0.453  -0.839   \n",
       "8291  -0.550   0.102  -0.187  -0.257  -0.159  -0.007   0.150  -0.214  -0.121   \n",
       "4607  -0.853   0.607  -0.187  -0.257  -0.159  -0.563   0.617   0.484  -0.619   \n",
       "5114  -0.543   0.107  -0.187  -0.257  -0.139  -0.298   0.128  -0.066   0.178   \n",
       "1859  -0.241   0.419  -0.187  -0.257  -0.153   1.744   1.105   0.890  -0.111   \n",
       "\n",
       "      wb_442  wb_443  wb_444  wb_445  wb_446  wb_447  wb_448  \n",
       "7217  -0.436   0.201  -1.399   0.177  -0.155   0.355   0.278  \n",
       "8291  -0.616   0.869  -0.122   0.849  -0.662   0.549   0.071  \n",
       "4607  -0.155   0.156  -0.802   0.169  -0.319   0.421  -0.138  \n",
       "5114  -0.436   1.565  -0.904   1.493  -0.477   0.368  -0.001  \n",
       "1859  -0.457   1.977  -0.092   2.140  -0.523   0.362  -0.158  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAI4CAYAAAB3HEhGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAADja0lEQVR4nOzdd5xU5fX48c9tMzvbl7b03qVIUVBQBEVURFQw9t5jEk1MLFHRaIwa28+SaIwGv7bYjQW72HtBUAGld5ayvc3Mvff5/XFnZgtLn90pe96vlwm7c+fO8wzD3DlzznMeTSmlEEIIIYQQQgix1/RED0AIIYQQQggh0oUEWEIIIYQQQggRJxJgCSGEEEIIIUScSIAlhBBCCCGEEHEiAZYQQgghhBBCxIkEWEIIIYQQQggRJxJgxcmkSZP47LPPtvn9N998w5QpUxIwIhH1yiuvcM455yR6GAwYMIBVq1bt1TnOO+88XnrppZ0eN2LECNasWbNXj7UjW7Zs4dRTT2XEiBHceuutzfY4TUnnue2NBx98kGuuuSbRwxBCCCFaPTPRA0h3o0eP5q233kr0MFq1Y445hmOOOSYu5xowYABvv/02PXr0iMv5dtfDDz+8S8fNmzevWcfxzDPPUFBQwHfffYemac32OKeffjrHHHMMJ5xwQux36TK3XfXll1/ypz/9iY8++miHx1100UUtNCIhRLqZNGkSf/3rXznwwAMTPRQh0oJksFKUUgrXdRM9jD1i23aihyD20vr16+nTp09SBCDxlopzk39TQgghRPKQACuOfvjhB4466ij2228/rr76aoLBIF9++SUHH3xw7JhJkybxyCOPMG3aNEaNGsVll11GMBgEoKysjAsvvJCxY8ey3377ceGFF7Jx48bYfU8//XTuvvtuTjrpJIYPH85//vMfjj/++AZjmD17NhdffPEOx/nBBx9w7LHHMnLkSCZMmMB9993X4PZvvvmGk046idGjRzNhwgRefPFFAGpra7n11luZOHEio0aN4uSTT6a2tnabOUbnGS2ZvO+++/jd737HH//4R0aOHMlLL73EggULOPHEExk9ejTjx4/nxhtvJBQKxe6/ZMkSzj77bPbff38OPPBAHnzwQTZv3szw4cMpKSmJHffTTz8xduxYwuHwduf74osvcvLJJ8d+HjBgAP/97385/PDDGT16NH/5y19QSsVuf/755znyyCPZb7/9OPfcc1m3bh0Ap556KgDTp09nxIgRvP766zt8nh9++GHGjx/P+PHjef755xvcFgqFuO222zjkkEM48MADmTVrFrW1tbHb3333XaZPn87IkSM57LDDYtmL008/neeeew6AVatWcdpppzFq1CjGjBnDZZdd1mCO0XLEiooKrrjiCsaOHcvEiRP55z//GQvOo8/Nbbfdxn777cekSZP48MMPdzivq666iv/973888sgjjBgxgs8++4yrrrqKu+++O3bM7rzutzffu+++m2+++YYbb7yRESNGcOONN7b43P73v/8xbNgwSktLY8csXLiQMWPG7PQ1d9JJJ/G3v/2N0aNHc+ihh/Ldd9/x4osvMmHCBA444IAGpZ7bez1UV1dz/vnns2nTJkaMGMGIESMoKipq8t/Ufffdxx//+MfYObf371gIIXZFKBTi5ptvjl3Hbr755th1uri4mAsvvJDRo0ez//77c8opp8Teex966CEOOuggRowYwZQpU/j8888TOQ0hEkOJuJg4caKaOnWqWr9+vSopKVEnnniiuuuuu9QXX3yhDjrooAbHzZgxQ23cuFGVlJSoI444Qj311FNKKaWKi4vVm2++qaqrq1VFRYX67W9/qy6++OLYfU877TQ1YcIE9csvv6hwOKyCwaDab7/91NKlS2PHTJ8+Xb355ps7HOsXX3yhFi9erBzHUYsWLVIHHHCAeuedd5RSSq1du1btu+++6tVXX1WhUEgVFxerhQsXKqWUuuGGG9Rpp52mNm7cqGzbVt9++60KBoPbzDE6z08//VQppdS9996rBg8erN555x3lOI6qqalRP/zwg5o3b54Kh8NqzZo16ogjjlCzZ89WSilVUVGhxo0bpx555BFVW1urKioq1Pfff6+UUuq8885TTz75ZOxxbr75ZnXjjTfucL4vvPCCOumkk2I/9+/fX11wwQWqrKxMrVu3To0ZM0Z9+OGHSiml3nnnHXXYYYeppUuXqnA4rP7xj3+oE088scF9V65cucPHU0qpDz/8UB1wwAHq559/VlVVVeoPf/hDg/vefPPN6sILL1QlJSWqoqJCXXjhheqOO+5QSik1f/58NXLkSPXJJ58ox3HUxo0bY3/Hp512mnr22WeVUkr9/ve/V//85z+V4ziqtrZWff31102O809/+pO66KKLVEVFhVqzZo06/PDDY+d44YUX1ODBg9UzzzyjbNtWTz75pBo3bpxyXXeH87vyyivVXXfdtd2fd+d1v6vzTdTcTj/9dPXMM8/Efr711lvVddddt8NzvPDCC2rQoEHq+eefV7Ztq7vuuktNmDBB3XDDDSoYDKqPP/5Y7bvvvqqyslIptePXQ1P/vpr6N3Xvvfeqyy+/XCm143/HQgjRWP1rdtT/+3//T51wwglqy5YtauvWrerEE09Ud999t1JKqTvuuENdd911KhQKqVAopL7++mvluq5atmyZOvjgg9XGjRuVUkqtWbNGrVq1qqWnI0TCSQYrjk499VQ6depEfn4+F198MXPmzGnyuNNPP53CwkLy8/OZOHEiixYtAqCgoIApU6YQCATIzs7m4osv5uuvv25w3+OOO45+/fphmiY+n48jjzySV155BfCyPuvWrWPixIk7HOeYMWMYMGAAuq4zcOBApk6dyldffQXAa6+9xoEHHsjRRx+NZVkUFBQwaNAgXNflhRde4JprrqGwsBDDMBg5ciQ+n2+Xnpt9992Xww47DF3XycjIYMiQIey7776YpknXrl058cQTY3P94IMPaNeuHeeccw5+v5/s7GyGDx8em390vo7jMGfOHKZPn75LY6jv/PPPJzc3l86dOzNmzBgWL14MwNNPP80FF1xAnz59ME2Tiy66iEWLFsWyWLvqjTfe4Pjjj6d///5kZmbym9/8JnabUopnn32WP//5z+Tn55Odnc2FF14Ye708//zzzJgxg3HjxqHrOoWFhfTp02ebxzBNk/Xr17Np0yb8fj+jR4/e5hjHcXj99de5/PLLyc7OpmvXrpx99tmx5xCgc+fO/OpXv8IwDI477jg2b97Mli1bdmu+u2J7r/tdnW+i5jZt2jRee+01wPu7e/3115k2bdpO79e1a1dmzJiBYRgcddRRbNiwgUsuuQSfz8f48ePx+XysXr16p6+H7Wn8b6q+7f07FkKIXfXqq69yySWX0LZtW9q0acMll1wSe381TZPNmzezfv16LMti9OjRaJqGYRiEQiGWLVtGOByma9eudO/ePcEzEaLlSZOLOOrUqVPsz507d2bTpk1NHte+ffvYnwOBQOy4mpoabrnlFj7++GPKysoAqKqqwnEcDMPY5jHACzj+8Ic/cNlll/Hyyy9z5JFH7jTomT9/PnfccQdLliwhHA4TCoU44ogjANiwYUOTb4YlJSUEg0G6deu2s6ehSR07dmzw84oVK7j11lv58ccfqampwXEc9tlnnx2OAeDQQw/l+uuvZ82aNaxYsYLs7GyGDRu22+Np/HdQVVUFeOtv/va3v3HbbbfFbldKUVRURJcuXXb5/Js2bWLIkCGxn+vft7i4mJqamgblnaremroNGzYwYcKEnT7Gn/70J+655x5mzpxJXl4eZ599NjNnzmxwTElJCeFwmM6dO8d+17lzZ4qKimI/t2vXLvbnQCAAQHV19a5OdZdt73W/q/NtrKXmdvjhh3PTTTexadMmVq5cia7rTQazjbVt2zb252gAVH88fr+fqqqqnb4etqfxv6n6dvRvSAghdsWmTZu2eX+Nvm+fe+653H///bEOvSeeeCIXXHABPXr04M9//jP33XcfS5cuZfz48Vx11VUUFhYmZA5CJIoEWHG0YcOG2J/Xr19Phw4dduv+//nPf1ixYgXPPvss7du3Z9GiRRx77LEN1gc1Xni/7777YlkW33zzDa+99hp33HHHTh/n8ssv57TTTuPhhx/G7/dz8803x9Y1derUiQULFmxzn4KCAvx+P2vWrGHgwIENbgsEAg3WDzmOQ3FxcYNjGo/7hhtuYPDgwdx5551kZ2fz6KOPxrotdurUabvrm/x+fyxrt3z58j3KXu1Ip06duOiii/a662CHDh22eT1EFRQUkJGRwZw5c5q86HTq1InVq1fv9DHat2/PX//6V8Bbb3P22Wez3377NehwWFBQgGVZrF+/nr59+wLe6zTeF7vGr4HdyRLt6nwba6m55eXlMW7cOF5//XWWL1/OUUcdFdcGGDt7PWzvsXY0hu39OxZCiF3VoUMH1q9fT79+/QDv/TX6uSY7O5urrrqKq666il9++YUzzzyToUOHcsABBzBt2jSmTZtGZWUls2bN4o477uD2229P5FSEaHFSIhhHTz31FBs3bqS0tJQHH3yQo446arfuX1VVhd/vJzc3l9LSUu6///5dut+xxx7LjTfeiGmau/TNelVVFXl5efj9fhYsWBArfwKvHOqzzz7j9ddfx7ZtSkpKWLRoEbquM2PGDG655RaKiopwHId58+YRCoXo1asXwWCQDz74gHA4zAMPPNCgYcX2xpCVlUVWVhbLli3jv//9b+y2Qw45hM2bN/Poo48SCoWorKxk/vz5sdunT5/OSy+9xNy5c+MeYJ100kk89NBDLFmyBPCaKLzxxhux29u1a7dLezAdccQRvPTSSyxdupSampoGf5e6rnPCCSfwt7/9ja1btwJQVFTExx9/DMDMmTN58cUX+fzzz3Fdl6KiIpYtW7bNY7zxxhuxJih5eXlomoauN/wnbRgGRxxxBHfffTeVlZWsW7eO2bNnx61tfdSgQYP48MMPKS0tZfPmzfzf//3fLt93R/Pd0fPdUnMD79/Fyy+/zFtvvbVL5YG7Y2evh7Zt21JaWkpFRcVujbepf8dCCLE94XCYYDAY+2/q1Kk88MADFBcXU1xczD/+8Y/Y+9/777/PqlWrUEqRk5ODYRhomsby5cv5/PPPCYVC+Hw+/H7/NtclIVoDedXH0dFHH80555zDYYcdRvfu3Xfaza+xM888k2AwyNixYznxxBM56KCDdul+06dPZ8mSJbv8wfL666/n3nvvZcSIEfzjH//gyCOPjN3WuXNn/v3vfzN79mz2339/jj322Nj6pCuvvJL+/fszc+ZM9t9/f+644w5c1yUnJ4frr7+ea6+9loMPPphAILDD8qXouV577TVGjhzJdddd1yAYzc7O5j//+Q/vv/8+48aNY8qUKXz55Zex20eNGoWu6+yzzz67Vba3KyZPnsx5553HH/7wB0aOHMnRRx/dYP+h3/zmN1x11VWMHj16h10EJ0yYwJlnnsmZZ57J5MmTGTt2bIPb//SnP9GjRw9+9atfMXLkSM466yxWrFgBwLBhw7jlllv429/+xqhRozjttNMaZMCifvjhB0444QRGjBjBxRdfzDXXXNNkCed1111HIBDgsMMO45RTTuHoo49mxowZe/oUNWn69OkMHDiQSZMmcc455+zWlws7mu8ZZ5zBW2+9xX777RfL1tXXEnMDrwviypUradeu3TYZ3HjY0euhT58+TJ06lcMOO4zRo0c3KIHcnh39OxZCiKZccMEFDBs2LPZfKBRiyJAhsb0k99lnH379618DXhfbs88+mxEjRnDiiSdy8sknM3bsWEKhEHfeeSdjxoxh/PjxFBcX84c//CHBMxOi5Wmqfv2ZSEm1tbWxts89e/ZM9HBaxBlnnMG0adMabEArhBBCCCFEokkGKw3897//ZejQoa0muFqwYAELFy5skHkTQgghhBAiGUiTixQ3adIklFL84x//aPD7qVOnNllW9pe//KVZ1qi0lCuvvJJ3332Xa665huzs7NjvZ82axauvvrrN8dOmTYttUBtvDz74IP/617+2+f2oUaN4+OGHm+UxW9KIESOa/P2///3vXVrrl8ziMbdEvOaEEEIIkfykRFAIIYQQQggh4kRKBIUQQgghhBAiTpKuRNB1XRxn75NqhqHF5TzJKF3nlq7zgvSdW7rOC9J3bqk4L8syEj0EQK5PO5Ou84L0nVu6zgvSd27pOi9Izblt7/qUdAGW4yhKS6v3+jz5+ZlxOU8ySte5peu8IH3nlq7zgvSdWyrOq337nEQPAZDr086k67wgfeeWrvOC9J1bus4LUnNu27s+SYmgEEIIIYQQQsSJBFhCCCGEEEIIEScSYAkhhBBCCCFEnEiAJYQQQgghhBBxIgGWEEIIIYQQQsSJBFhCCCGEEEIIEScSYAkhhBBCCCFEnEiAJYQQQgghhBBxIgGWEEIIIYQQQsSJBFhCCCGEEEIIEScSYAkhhBBCCCFEnEiAJYQQQgghhBBxIgGWEEIIIYQQQsSJBFhCCCGEEEIIESfmrhz00UcfcfPNN+O6LieccAIXXHBBg9vXrVvHn//8Z4qLi8nPz+f222+nY8eOAAwaNIj+/fsD0KlTJx588ME4T0EIIYQQQgghksNOAyzHcbjxxhuZPXs2hYWFzJw5k0mTJtG3b9/YMbfddhvHHnssxx13HJ9//jl33nknt99+OwAZGRm8/PLLzTcDIYQQQgghhEgSOy0RXLBgAT169KBbt274fD6mTp3Ke++91+CYZcuWMXbsWADGjh27ze1CCCGEEEII0RrsNINVVFQUK/cDKCwsZMGCBQ2OGThwIG+//TZnnnkm77zzDlVVVZSUlFBQUEAwGOT444/HNE0uuOACDjvssB0+nmFo5Odn7uF06p9Hj8t5klG6zi1d5wXpO7d0nRek79zSdV5CCCFEstilNVg7c8UVV3DTTTfx0ksvMXr0aAoLCzEMA4D333+fwsJC1qxZw5lnnkn//v3p3r37ds/lOIrS0uq9HlN+fmZczpOM0nVu6TovSN+5peu8IH3nlorzat8+J9FDEEIIIXbZTgOswsJCNm7cGPu5qKiIwsLCbY65//77AaiqquLtt98mNzc3dhtAt27d2H///Vm4cOEOAywhhBBCCCGESFU7XYM1dOhQVq5cyZo1awiFQsyZM4dJkyY1OKa4uBjXdQF46KGHmDFjBgBlZWWEQqHYMd99912D5hgiuRkly9CqNyd6GEIIkTZWbK1GKZXoYQghhGhGO81gmabJrFmzOO+883AchxkzZtCvXz/uuecehgwZwqGHHspXX33FXXfdhaZpjB49muuvvx7wml9cf/31aJqGUorzzz9fAqzmYNegV2/Fze4I+l5WfboOvpXvEFjwH3zrPsPJLKTsmCdx2g6Mz1iFEKKV2lhey68e/Yb/nDmaoe1kHZwQQqSrXfo0PmHCBCZMmNDgd5deemnsz0cccQRHHHHENvcbOXIkr7766l4OUWyPXrWRjB/+j8CPj6MHS1G6hZPTFSevJ05eT9zcbqAUmlOLFq4BpxbNrkUZftycrji5Xb3/z+kKQMaiZwj88ChGxVqc7C5U7fcHMhY+Sf5LMyg7+jHsjqMSPGMhhEhdQdur9CipCoEEWEIIkbbi0uRCxJ9WW4K5ZSFuZnvczA4ofx5omnfjxh/I+eRe/EteAdcm1PsIQt0OxqhYi1G2Er1sFdaGr9HDlbHzKc1AmQEwM9DC1Wh204vcQ53HUjluFqFeh4NuUjtwJvkvn0z+yydTdtQjhLsd1BLTF0KItGMa3nt42HETPBIhhBDNSQKsJKNXbiDw/UMEfnqyQRCkDD9uViHKDGAW/4xhZVEz5HRqhp2Dm9dz2xMphRYsBd1EGRlgWNvcZlSsRa9Yi1GxDq22lGCfo3DaDW5wGje3O6XHv0jeq6eS99qZlB9+P6E+RzXP5IUQIo1ZurfsOezIGiwhhEhnEmAlCaN0OYF5D5Cx+HlQLsF+06ntfxx6qBy9ahN61Ub06k3otcU4I06hpNdML6u1PZqGyijY4W12RgG0H7rTsblZhZQe+xx5r51J7lsXUTHxdoKDTtzDmQohROtkSQZLCCFaBQmwEkyvWEfW57fgX/oK6Ba1g0+hesSFuLnbb2Wfn5+JauF9bFRGAaXTnybvjfPInXs5laEKaoaf16JjEEKIVGYZ0QyWBFhCCJHOJMBqDsol+/0/oSlFzbCzsZvKEtm1ZH7/EJnf3gcoava9kOrh56OyOrT4cHeZlUnZ1Nnkvv0bsj+5AS1YTvV+v69bG7YXMn56AmPNu3DEo3s/TiGESEKmHs1gSYmgEEKkMwmwmkHGoqcJLHoGpfvIWPws4U77Uz3sHEK9jwDdxLfyPbI/noVRvopgn6OoPHAWbm7XRA971xh+yqc8QM77V5D19V1ooQqqxs3a6yDLv+QV9HWfodUUowJt4jRYIYRIHpLBEkKI1kECrDjTq4rI+vSvhLocQPmRD5Ox6FkCP8wm762LcLK74OT3wrf2E+yCvpQe8xThbgcnesi7TzepmHQHri+bzPn/RgtVUHnIbaAbe3Y+18Eq+h4Ac8tCwt3Gx2+sQgiRJAxdQ9ckgyWEEOlOAqw4y/54FpoTpPKQ21D+PGr2PZ+aYefgW/kugQWPYG5ZSOWB11Ez7GwwfIke7p7TdKrG/wXlyyHrm3vQQpVUTL53j+ZkFP8c65hobvlJAiwhRNqyDF0yWEIIkeYkwIoj34q38S+bQ9WYK3Hye9fdoBuEek8h1HtK4gbXHDSN6jF/Qvlyyf7sJvTarZQf/gAqs91uncba+B0AyszA3LqwOUYqhBBJwdQ1CbCEECLN6YkeQLrQQhVkf/hn7LYDqR5xUaKH06JqRlxI+WH3YG38joLnjsKMlPvtKqvoO9yMNqge4zG3SIAlhEhfXgZLSgSFECKdSQYrTrK+uBW9qojSIx5quKlvigs7LpsqgxRVRP4rDxK0XSYPbE/vtlmx44IDZuC06U/uG+eT/9IMKib8bZf3yjKLviPccSRm4RCMFR+AEwTD30wzEkKIxLEMyWAJIUS6kwArDsyN35Lxw2PUDDsHu+PIRA8nxnYVlUGbyqBNVdChMuT9uaQ6zKbKIJsqQ2yuDLK5MkRxdRjHVSilUICrFEpBdcih8XetGvDwF6sZ1S2PE/btzIQ+bTENHbv9UEpOeJ3ct39N7tzLqdk0n8rxN+xwXZZWW4pZspRg/+MxCvujuTZG8VKc9vs04zMjhBCJYUmJoBBCpD0JsPaWEyLn/StwsztTNeaKRI+G6pDDR8u28s7Pm/l8ZfEOS1HaZFq0z/ZTmONncMccLF1D07wuVwCappHjNyjM8Y7pmJNBhxw/Qdvh5R828uKCDVz16iLaZ/s4bmgnZu7biYLMNpRNe4KsL24lc96DGMW/UHbss6A1XY1qbvoegHDHUfg69PB+t+UnCbCEEGnJNHRCEmAJIURakwBrbyiXnLl/xCz+mbKp/we+rJ3fpxlUBm2+WlXCOz9v5uPlxQRtlw7ZPo4f1omu+QGy/QZZPpNsv0G23yQ/YNEuyxfbk2V3ZfoMzhrTndP368ZnK4p5fv56/v35Kp6Zt45LJ/Tm6H0KqTrwWtysjmR/cgPmxm+xO+3X5Lmsjd+h0LA7DIc2bWONLoJ784QIIUSS8koEZQ2WEEKkMwmw9pRSZH9wNRm/vEjVmCsJ9Ty02R/Sdlx+3lTJuqVb+XFNKSu2VrN8axWbKkOAl5E6ZkhHDh/QnmFdctH3cvPfnTF0jYP6tOWgPm1ZvrWKW95Zwo1v/cLrC4u4enJ/egw6kazPb8G/bM72A6yi73DaDkD5skE3sNsMxNzyU7OOWwghEsXSpU27EEKkOwmw9oRSZH36FwILn6Rq1G+pHv3bZnkY21UsLqrg2zVlfLOmlPnryqgJexdmv6nTq00mo7vn07ttFvt0zGHfrnmYevMGVdvTu20W/zpxOP9bsIF7P1rByf/3Decd0IPfdTvYa10/bta2ZYLKxSyaR7DP1Niv7Hb74F/2GigFzRwgCiFES5MmF0IIkf4kwNoDmV/eTub8h6kedi7VcV53pZTi+3XlvLRgAx8u3Up12AGgV9tMpg4uZGS3fPbv155sFEaCgqnt0TWN44d35qA+bbnz/WX885OVaAVD+FPNO5hF32/TAMQoXY4eLMMurPu93W4wgYVPoleux83p0tJTEEKIZuXtgyUlgkIIkc4kwNpNgW/vJ+vbe6kZfDJV42+IW5alrCbMnIVF/G/BRlYUV5PlMzh8YHvG9ChgRNc82mbVdeLLz8+ktLQ6Lo/bHNpn+7l12mBe+XEj97xVxR8Cplcm2CjAMiMbDIc71g+wvOYW5paFhCTAEkKkGdOQEkEhhEh3EmBtj3LRarZiVG5Ar1yPXrkBc+siAguforbfsVROuHWvg6uw4/LVqlLeXLyJub9sJuQohnTK4bop/Zk8oD0By4jTZBJj2j6FPPVte76pGc7oZa9TdeC1DZ4zq+g7XF8uTkHf2O+ctgMBMLcuJNRrcouPWQghmpNlaNSGnEQPQwghRDOSACtCC5ZhbfwWc8PXWOu/wtr0PZrTsJed0i1qB8ygYtKdoO9Z8OO4iu/WlvL24s28v2QLZbU2OX6TY4Z05LhhnejfITse00kKmqZx6qiuPP/uKMY4D2Fu/gG7w7DY7dbG77ALRzRYm6V82dh5PaXRhRAiLVm6Tti2Ez0MIYQQzajVB1j+Ja+S+e29GFsXo6FQuondbgg1Q07HyeuJm9UJN7sTTnYnVKDtdvdz2hXvL9nC399bypaqEAFLZ0Lfdhw+oD1jexbsccv0ZDdlYAee+PgAHPdh/MterwuwQlUYxYsJ9jp8m/s47QZjbFnYwiMVQojmZxma7IMlhBBprtUHWIHvH0QLllK9/x8Id9qfcOEIsDLj/jhvLtrEDW8spn+HbC6f2IfxvduQkeIlgLvCZ+ocMXIAn365D/v98iqMvRI0DWvzfDTlbrMuC6KdBF9HC1V67duFECJNmIaO7UqTCyGESGfpmTbZVU4Qc8tCgv2mU73f7wl3HdcswdUrP2xk1uuL2bdrHg/+ajiHDWjfKoKrqOOHdeJdxhCoXIWxdRFQr8FF4YhtjrfbDgZA37KI2rCsVRBCpA9LlzbtQgiR7lp1gGVuXYzmhgl32LfZHuO579dz09u/MKZHAf/vuCFk+lpPYBWVF7AwB07FURrOolcAsIrmYef3RmUUAFBUEeTRz1dy5/vLuP5bL7F6/wuvctC9n/Lp8uKEjV0IIeLJki6CQgiR9lp3gLVpPgB2h+HNcv4nv1nL399bykG923Dnsfu0qqxVY9P2H8JXahDq51dBqUiDC688cP66Mk5//Dtufn0xL/+wgR8qs6nUsplcsAmA5VurEjl0IYSIG2+jYSkRFEKIdNaq12CZm+bjZhTg5nSN+7n/76s13P/xCg7t346bjhqYtk0sdlXX/ABvtZnEAaX3U7TsXfSazYQ7juStRZu48a2f6ZDj54lz96e9T0fTNHz/G8ro8FpMXaO0RjpuCSHSg6lLBksIIdJdq/7Ub22a72Wv4rRZcNSC9eX84+MVTB7Qnr9OHdTqg6uonmNn4ioN86ObAHh2YyeufX0x+3TKZfYpI+hfmIMW+buw2+2DWbyYNhk6ZTXhRA5bCCHixstgSYAlhBDprPV+8g9XYxT/QjjO5YEh2+Wvb/1Chxw/1xzeD1OPb/CWygb06csiaxBta5YT1DL423yTqfsUcv+MoeQHrAbH2m0Ho9m1DMnYQqkEWEKINBEtEVRKygSFECJdtdoAy9zyk9cmPM4B1qNfrWZFcTVXH9aPLF+rrsBsUqjPVADmOb24cHwfrp/SH5+57cvQbrcPAEPM1RJgCSHSRrSiQVq1CyFE+mq1AZbVDA0ulm2pYvaXa5gysD3jereJ23nTSZf9Z+KikdvnAM4e0z1WEtiY06YvSjcZwCoJsIQQaSNa1SCNLoQQIn212hSLuWk+TlZH3KzCuJzPcRV/ffsXsnwGl0/sE5dzpiMttwvlxz1HhzYD2eHHC8OPU9CP3rUrJMASQqSNaAbLW4fVejvLCiFEOmu1GSwz2uAiTp79fj0/bqjg8kl9KMj0xe286SjceSwqI3+nx9ntBtMltIzyWhtHymmEEGnAMiIZLHlPE0KItNUqAywtWIZZujxuAdb6slr++fEKDuxVwBEDO8TlnMJbh5Ub3kIB5VTUSqt2IUTqs/TIGizpJCiEEGmrVQZY5uYfAeLSQVApxS3vLEHXNK4+rN921xSJ3We3HQzAIF3WYQkh0oNpyBosIYRId61yDZa56XsA7A7D9uj+jqv4cUM5Hy0r5uNlW1lRXM2fJvWhY25GHEcpnPzeAHTXNlFSE6ZnYocjhBB7LdbkwpUMlhBCpKtWGWBZm+bj5PZAZRTs1v1+3FDOC/M38MnyYkprwhi6xsiueZw0sjPHDuvUTKNtvZQvG4BMaiWDJYRIC3VNLiSDJYQQ6apVBljmpgWEC0fu1n1+3lTJxc8uwDJ0DuxVwMF92nJgrzZk+1vlU9gilJUJQBZBCbCEEGkh2uRC1mAJIUT6anXRgVazFaNiLTVDz9rl+5RUh/jj/34iN8Pk/04bSbss6RLYInQT1/CTaUsGSwiRHqJNLiSDJYQQ6avVNbnY3Q2GbcflylcXUVIT5vbp+0hw1dKsLPJ0yWAJIdJDtMmFLW3ahRAibbW6AMvcNB+Fht1+6C4df+f7y5i3toxrDu/H4I45zTw60ZjyZZNvhiiTAEsIkQZia7CkyYUQQqStVhlgOQV9Yw0UduTFBRt4fv4GTh/dlSMHFbbA6ERjysokVw9SWiP7YAkhUp8lbdqFECnsw6VbWFlcnehhJL3WFWAphblpwS6VB85bW8bf31vKAT0LuOSgXi0wONEUZWWRIyWCQog0IRsNCyFS2U1v/cI/Pl6R6GEkvVYVYOlVGzCqN+10g+GS6hBXvbqQLnkZ3Dx1EIYumwcnirKyyNIkwBJCpAfZaFgIkcpqbZevV5fKOtKdaFUBlrlpAbDzDYbfX7KF4uowf506kJyMVtdoMakoK1P2wRJCpI1YiaCswRJCpBilFCHbpSrk8NOG8kQPJ6m1sgBrPko3sdvts8PjPl1RQudcPwM77HydlmheysoioGqoCjmEbPlAIoRIbdKmXQiRqmxXEX3n+mpVaSKHkvRaVYBlbZqP3WYgmBnbPSZku3y9uoQDerVB06Q0MNGUlYXfrQGgrFayWEKI1CZNLoQQqSpUb+3oF6tKEjiS5Nd6AiylMDfN32l54Lx1ZdSEXcb1atNCAxM7oqxMrEiAJWWCQohUF23TbkuJoBAixUQrifIyTH7aUE5FrXR43p5WE2DplRvQg2U73f/qsxXF+AyN0d3zW2ZgYoeUlYXpBjFwJMASQqQ8U5cMlhAiNQUjAdb43m1wFHyzpjSxA0pirSbA0myvZ7/y7Xiz4M9WFDOyaz4By2iJYYmdUFYWAJnIXlhCiNQX22hY2rQLIVJM9Iuhkd3yybQMvpQywe1qNQEWrpf9UIa13UPWldWwsriGA3tLeWCyUFYmgHQSFEKkBUPX0DUIS4tjIUSKCUa+GMryGYzqlicB1g60mgBLcyPZD337AdZnK7wXyoE9C1piSGIXRDNYWZoEWEKI9GAaumw0LIRIOdHMu2XojOlRwNrSWtaW1iR4VMmp1QRYOJEP5/r297X6bEUxXfMz6F4QaKFBiZ1Rltcqv73PpkwCLCFEGrAMTdZgCSFSTrTJhd/QGRNJRkgWq2mtJ8CKZLDUdjJYwcjO1OOkPXtSiZYIdvCHKamWAEsIkfp8hi5rsIQQKSfapt0yNXoUBOiY4+dL2Q+rSa0mwNLcHWewvltbStB2OUDasyeVaIlgO19YSgSFEGnBMnRZgyWESDkh23vf8hs6mqYxpmcBX68uwZb3s220mgAr1uRiOxmsT5cX4zd1RnXNa8lRiZ2IBlhtLAmwhBDpwTI0+UAihEg50SYXPtMLH8b2KKAy6LBwY0Uih5WUWk2AFWtysZ0ugp+vLGFUtzwypD17UokFWGZIAiwhRFqwpMmFECIFhe26JhcA+3XPRwO+XCnrsBprNQHWjjJYq0tqWF1SwzgpD0w60TVYBUaQslobpeRbXyFEarMMXZpcCCFSTjSD5Y9ksPICFoM65kijiya0mgBLc6Jt2rddg/XZimIADpQAK+lEA6xcI0TQdqm15VtfIURqs6TJhRAiBdVv0x41tkc+P24opzJoJ2pYSanVBFjsoMnFZyuK6V4QoGu+tGdPOoYPpfvI0YMAUiYohEh5lqFJkwshRMoJ1mvTHjWmZwGOgm9WlyZoVMmp1QVYjUsEa8MO364plfLAJKasTLKoBSTAEkKkPlmDJYRIRdF9sKJNLgCGdsol0zL4QsoEG2g1Adb22rR/s6aUkKM4sFdBAkYldoWyssiUAEsIkSZ8sgZLCJHkjJKl5L16Gtaaj2O/i75vWUbdfrGWoTOqWx5frCzBlXXyMa0mwIptNGz4Gvz68xUlZJg6I7rmJ2BQYlcoK4sMCbCEEGlCSgSFEMnO2vAVvtUfkP/KyeS893u02hKCjotlaOia1uDYwwa0Z11ZLfd/tCJBo00+Te+6m4ZibdobZbBWl9bQq21mrCOKSD7KysTv1gBQUi0BlhAitUmTCyFE0nNCANQMOZOMhU/iW/keg9v+Gp8xZJtDjxzUgR83VPD4N2tpl+3jlFFdW3q0SafVBFg4Ta/Bqgra5Phbz9OQipQvG8uuQdegTDJYQogU563BkgyWECJ5aZHPzVVjr6BmyGnkvH8FJ677K1314fhW/h4ntwdOblcwA2iaxuUT+7C1KsQ9Hyylh76FQ9pVYFSsI9RlLG5ez8ROJgFaTWSxvQxWZdChfbY/ASMSu0pZWRg1xeRlWJTWSBtQIURq80oEJYMlhEhirpfBUroPp+0gSo//Hx88dwdHbfk3WXPOih3mZBbi5nbD9eXwr4o1kLES67O6z2pKt6gZcgbVoy9FBVpPQ7lWE2BFXyg0ymBVhmyy/UYCBiR2lbIy0cJV5GdasgZLCJHyLFOaXAghkpsWKREk2rtAN3g3+xj+U7E/T03NwShfjVG+Gr18DUb5KvTqTbht+lHT7VAe+cVkQXVbLj1iNIPWPU3gh9lkLH6W6lG/oWbYOWCm/7ZIrSjAslGaAY0W5lUGbbKlRDCpKSsLLVxNfpYEWEKI1GfqmqzBEkIkNyeM0nTQ65IQIUcRsnKwO43G7jR6u3c9YkSQ5/77Pee9F+Thk/5Cz+HnkfX5LWR/fguBHx6lasyVBAccD1r69j9I35k1orlhMBpmr2xXURN2yfZJgJXMvACrivyABFhCiNTnM3Rs6SIohEhimhvapuor7Lj4jJ2HDh1y/Nw3YyiOq7jgmfn87HahfOpsSo99FjezA7nvXUb+C9Mxi75vptEnXqsJsHDtJhtcAGRJiWBSU1Ymml1NQYYuAZYQIuVJF0EhRNJzwiijYY+CoL1rARZAz7aZPHTScDQNLnxmPj+sLyfc5UBKZ75K+aF3Y5SvJf/5aWTP/SNa9ZbmmEFCtZoAS3PD2za4CHkBlnQRTG7KygKgvd+hrCaMko3shBApzDI0WYMlhEhqmhPapvIr7LhYu7GtUe+2WTx80r7kZZj8+rkFfLmyBDSd4MATKD71Q2r2vYCMn5+nzZMHE5j/MLhOvKeRMK0mwMLZNoNVGfT+ImUNVnKLBljtfGEcVff3JoQQqciKlAjKl0VCiKTlhrb53By0Xfy7mMGK6pyXwb9P2pduBQEue+lH3vtlMwDKn0vVuOtYd/ybFOcNIfuTGwi/fV3chp9orSayaDKDFSkRlC6CyU1ZmQC0tbzywJKaMDkZrealK4RIM1bkA4rjKkxD28nRQgjR8jQnXNdBMCLsKKw9eM9qm+XjX78azu9f+pGrX13EjOGlbK0Os2RzJWtLa4FL+KsZ4LRvH2SxbwTdRk6N0ywSp/VksNzwti3aJYOVEqIZrALTaxkq67CEEKksWmITlkYXQohk5YRQjQKskOPi340SwfpyMkzunzmU8b3b8ML8DSzbUsXADtn8enxP7j5uCO2PuY3lWje6fXYFL34+P+Uz/K0nsnBtVKNa0qrIGizpIpjcogFWvhkCDAmwhBApLfoNcNhxCVhSQSGESD5aE4mJ3Wly0ZQMy+Cu44YQsl18TQRqvjMeJ/DY4Qz4+s9csfFWrjtyILkZVhNnSn6tJoMlJYKpK1oimKsHAclgCSF27qOPPmLKlClMnjyZhx56aJvbX3zxRcaOHcv06dOZPn06zz33XIuNLVoiKI0uhBBJq4kMVthpOjDaXds7R2bXYQQPmsUhxnz6rn6K0x//jp82Vuz14yVC60ndNNGmvSIWYLWepyEVRTNYuXoIyKRMAiwhxA44jsONN97I7NmzKSwsZObMmUyaNIm+ffs2OO6oo45i1qxZLT4+XyzAklbtQojk5O0f2zDA2tsM1q6oHXImvjUf8+eV/2WJO4xznwpy5v7dOHdsj7gEdy0ldUa6l5rOYDn4TT32baJITsrKBsDvVuMzNMlgCSF2aMGCBfTo0YNu3brh8/mYOnUq7733XqKHFRMtEZTNhoUQyUpztu0iGHbc5v/MrGlUTLoDldmOf2f9k+kDc/nPl2s4/YnUymbtUurmo48+4uabb8Z1XU444QQuuOCCBrevW7eOP//5zxQXF5Ofn8/tt99Ox44dAXjppZd44IEHALj44os57rjj4jyFXeQ01eTCJssn5YHJTvm8DJZmV5MfsCTAEkLsUFFRUewaBFBYWMiCBQu2Oe7tt9/m66+/plevXlx99dV06tRph+c1DI38/My9Hp/fKgcgI9Mfl/MlC8PQ02o+9aXr3NJ1XpC+c2upeRmaDf7s2GMppQg5irxsX7M9ft3cMlHH/gvryWO5I/82zh1/JNfMz+ecp6o5d1wvLp3UF3+Sr1/daYC1K6UWt912G8ceeyzHHXccn3/+OXfeeSe33347paWl3H///bzwwgtomsbxxx/PpEmTyMvLa9ZJNUVzbVQTGSwpD0x+0TVYWriavIBFaY2d4BEJIVLdxIkTOfroo/H5fDz99NNceeWVPPbYYzu8j+MoSkur9/qxDc3LYBWXVlPqS58Kivz8zLg8P8koXeeWrvOC9J1bS80rPxTE9RmURx4raHslzU7YabbHbzC3/FEExl9P5jf3ss+qj/kfsDWrkPe/HMC98/dln4mnc0DfjmhaYre6aN8+p8nf7/SdfVdKLZYtW8bYsWMBGDt2bOz2Tz75hHHjxpGfn09eXh7jxo3j448/3tu57Bk3vM2O1JUhWwKsVKD7ULqJHqqUDJYQYqcKCwvZuHFj7OeioiIKCwsbHFNQUIDP560vOOGEE/jpp59abHyxLoJSIiiESFKaE2qwBiu6ZnRP27TviZrh57H1nO8pPuldKg66ieweozgmYwF/Dt9L/zeP5+YnXuLbNaUtNp7dsdPoYldKLQYOHMjbb7/NmWeeyTvvvENVVRUlJSVN3reoqGiHjxevEozGKVRTc1G+jAa/q3Vc8rOaL9XZXFpl2tuXhV8P0SE3g4UbylNu/q3y7yzFpevc0nVe9Q0dOpSVK1eyZs0aCgsLmTNnDnfeeWeDYzZt2kSHDh0AmDt3Ln369Gmx8UXXMNjS5EIIkaQar8GKZrCau8nFtgPRcdoOxGk7kNphZ4NyMZa/Tff3ruTO8t9zzwvH8XiXM7ngoL4M7th0NikR4pK+ueKKK7jpppt46aWXGD16NIWFhRjGntVGxqsEo3EKtSAcxHG1WKoToKwqTJsMK+VSyK0x7d3GzCRUWUamqbO1KpRy82+Nf2epLl3nlorz2l4JxvaYpsmsWbM477zzcByHGTNm0K9fP+655x6GDBnCoYceyuOPP87cuXMxDIO8vDxuueWWZhr9tqRNuxAi6TXqIhjNYLV4gNWYpuP0OYKaLmPgg2u4fNnz/LTpOy576iImjxvP2WO6J3Z8ETsNsHal1KKwsJD7778fgKqqKt5++21yc3MpLCzkq6++anDf/fffP15j3z1NtGn3SgSTe5Gc8CgrCy1cRX6uSXmtje0qTD2xdbdCiOQ1YcIEJkyY0OB3l156aezPl19+OZdffnlLDwsAX6xEUDJYQojkpDnhBvtgxTJYSdIqXWUUUH3EP7GXTWXgB1fzOn/m1h9+B2P+kOihAbuwBqt+qUUoFGLOnDlMmjSpwTHFxcW4kQvFQw89xIwZMwAYP348n3zyCWVlZZSVlfHJJ58wfvz4ZpjGzmlO0xsNyxqs1BALsAJekFxeK+uwhBCpyZQMlhAi2bkNSwSj71fRL4iSRajPVEpOnkuZrxNH1LyOkyRrW3caXexKqcVXX33FXXfdhaZpjB49muuvvx6A/Px8fv3rXzNz5kwALrnkEvLz85t1QtvlNmzTbruKmrArAVaKUFYmWrg6FmCV1oRpk+nbyb2EECL5yBosIUSya9zkIugkVwarPpXZjqrcvuTULmVDeS1d8wOJHtKurcHaWanFEUccwRFHHNHkfWfOnBkLsBKqUYlgVdBr9S0BVmpQVhZ65cYGAZYQQqSiWBdByWAJIZJVo8REKFIi2OwbDe8hX2Y+fq2KRcXVSRFgJeez1Aw0NwxGXTBVGYoEWLLRcEpoXCIoe2EJIVJVrMmFrMESQiQj5Xr7x9bLYIWibdqTNMAK5LQhlypWFdckeihAKwqwGmewKoMOIBmsVNFUiaAQQqQi6SIohEhqTgigYYCVZE0uGvNlFZClBVlTXJ7ooQCtKMDS3IZNLipjJYKSwUoFyspGC1eRFwmwyiTAEkKkqOgicTtJFmMLIUR9mhv5jNWgyUWStGnfDtefB8CWrVsSPBJPcj5LzcEJN8pgyRqsVOJlsKrwGxqZliEZLCFEyqrLYEmJoBAiCTneZyyVIk0uAFQkwCor2ZzgkXiS81mKN6XQlNMogxUpEfRJgJUKlJWFhgK7lvyASUm1BFhCiNRU10VQMlhCiOSjOUHvD8a2TS6SrU17VDTAoraMitrEr9NvHQFWE6lOKRFMLcrKAkALV5IXsCSDJYRIWZZsNCyESGaRz81Kr9/kIrIPVpJmsKIlgrlaFatKqhM8mlYTYHnBVIMSwZCUCKaSugDL6yQoAZYQIlUZuoaGNLkQQiQnLVIi2HQGKzlDh2gGKy9JOgkm57MUZ7HFekbDLoJ+U0/afv6iIWVlAsQ6CZYlQfpXCCH2hKZpWIYmAZYQIjm5TXQRTPImF8qfC0C+Xi0ZrBYTy2A17CKYJXtgpYz6GazcDFO6CAohUppl6NhSIiiESEJapE07+rZt2q0kXYMVLRHslhFipWSwWkbdC6VhkwspD0wd9QOsvIBFVcjBlg5cQogUZeqSwRJCJKlYF8F6JYKOwmdoaFpyBliYGSjDT5eMIKuKJYPVMrazBksCrNRRVyJYRV5GZC8sKRMUQqQoy9ClTbsQIilpkRJBGpUIJmuDiyjXn0ehVcPa0hqcBO8zmNzPVJzUbZhWF1BVBW1ypINgyqjLYFWTl+H9PZZLgCWESFGWoRGWjYaFEMkoUvmlGpUIJuv6qyjlz6OtUUPIUWwor03oWJL7mYqXSAarfi2plAimloYlgt7fm6zDEkKkKsvQpcxZCJGUmuwi6KRCgJVLrlYFkPBOgsn9TMVLtETQqLcGK2TLJsMppPEaLICyWgmwhBCpyZA1WEKIZBXtIqg3bNOeCiWCWW4kwEpwJ8HkfqbiRNvORsNZUiKYOswMlKZHSgRlDZYQIrVZuiZrsIQQSakug+WP/S41Mlh5WOFy8jJMyWC1iGg3lMgaLNtV1IRdKRFMJZqGsrJibdpBSgSFEKlFC1WQ//w02LrEa3Iha7CEEMkougarcYlgkmewlD8XLVhKjzaZrExwJ8HkfqbipHEGqzLoZT4kwEotyspEC1eR5TMwdE0yWEKIlKJXbsQqmodW9AOWockaLCFEUop1EWxcIpike2BFuf58tFAFPfP9rCqRDFbzizW58AKqWIAlGw2nFGVloYWq0DSNvAyTclmDJYRIIbHN7p0wpqHLGiwhRHKK7YNVv027Sv4SQV8umnLplwdbq0Kxz/uJkNzPVJxEM1jRxXpVQQeQDFaqiZYIAuRlWJTVSAZLCJFCot8GO2EsXcOWEkEhRBLSnCb2wUqBJhfKnwdAnxxv/InccDi5n6l4iWawIrWklaFoiaBksFJJtEQQIC9gShdBIURqiXSy1VxbNhoWQiSvRokJSI0mF26GF2B1D3jjT2SZYHI/U/HiNmxyIWuwUpOXwfK+jZAMlhAi1aj6GSzZaFgIkaSazGClQpMLXy4AHX21GJpksJpdrN1krMlFpERQ9sFKKQ1KBAOyBksIkWKiAZYbwtSlyYUQIkm5YS8podWFCanS5ALAsivokh9gZQJbtbeKACu20fA2GSwpEUwl9UsEczMs6SIohEgpdRmsaImgZLCEEMlHc0INOghCijS58HsZLL22jB4FgYRuNpzcz1ScbNOmPSQlgqmoYYmgSdB2qQ07CR6VEELsomgXQVdKBIUQScwJNeggCKnV5EILltGjTSZrSmpwEvQ+m9zPVLzEMlh1JYJ+U8dK8khcNBQrEVSKvID3dylZLCFEyqjXpt3SdSkRFEIkJc0NN5HBSv4mF8qXjdJ0tFA5PdsECDmKjRW1CRlLcj9T8RLNYBl1Gw1L9ir1KCsLTTngBMnL8P7+ympkHZYQIkVomvdFnxvGNDQpERRCJCWtUQbLVQrbTf4SQTQd5ctBD5bSoyATIGHrsJL8mYoPbZuNhh3ZZDgFKSsLAC1cXS+DJQGWECKF6Gaki6BO2JUMlhAiCTVagxWyvfeqZC8RBFD+fLTaMnq0CQCJ6ySY/M9UPETaTcaaXIQkg5WK6gKsKvIyvH/45VIiKIRIIdEMlqV7GSylJIslhEgumhtukMEKOakTYLn+PLRQOfkBi9wMk9UJ2gsr+Z+pOKjLYHkfyquCtnQQTEHK8tK9WriSXCkRFEKkIt2MdREEErYAWwghtstpFGBFM1hJ3qYdvE6CerAMTdPoUZDJSslgNaPoGizNC6oqg45ksFJQ0yWCksESQqQOZVhokS6CgHQSFEIkHc0NNiwRjKwXTfo1WHidBLVgOQA92gRYJWuwmo/m2ijdB5p3QasI2rLJcAqqXyLoN3UyTJ1SyWAJIVKJboETxtAjAZZ0EhRCJBsnDE1msJI/bHD9uWjBMgB6FATYUhWK7X/bkpL/mYoH165rj4vXRTBLSgRTTv0ACyAvYMkaLCFESlG6GdkHy7v8SidBIUSy0dxw3cbopNYaLOXPQ48EWN0KvEYX68tavlV78j9T8eCGUZEW7bbjUmu7UiKYgurWYHkBVm6GKWuwhBCpJZLBsiSDJYRIVo3atKdSgOX689GcINi1ZEU6hteEnRYfR/I/U3GgOeG6Fu0h70mWACv11F+DBV4GS9ZgCSFSim6BW9fkwpY1WEKIJKM5oYYlgk5qNbkA0INlZJhegFUbbvkvslpFgIUbrmvRHqnDlH2wUk8swAp5Gaz8DJNy2QdLCJFClGGBE6prciElgkKIZNO4RDCF1mApfx4AWrCMgBUJsGzJYDULzbXrtWiXDFbKsrxa2roSQYuyGslgCSFSSGQNlhnLYEmJoBAiuWiNmlwE7UgXwZQoEYwGWOX4LW+8NZLBaib1IvHKUCSDJU0uUo+mo8zMeiWCXgZLNuoUQqQKFd0HS5cMlhAiSTnBWO8CqFsrmhIZLF9diWAsgyVrsJpH/QxWrERQMlgpSVlZdV0EMywcBVWhlv+HI4QQe0S3Il0EpcmFECI5aW4Y9KbWYCV/2KAy8gGvRDAjknGrsSWD1TzqtWmvjJYIyj5YKcn11QuwAt7foeyFJYRIFSraRVCaXAghkpUTbpDBiq3BSoUSwUgGS5MMVgtwQrEXSjSDlSMZrJTkZbC8EsHcDO/vVDoJCiFShm6hOWFMadMuhEhSmhNssougPxUyWJE1WHqwDMvQ0DUJsJqNVj+DJWuwUpuVhRauBCAvw/s7lb2whBApw5CNhoUQSc4No/T6TS68AMsyk79NO4blrdcPlqNpGgHLoFZKBJtJgzbtDn5Tj3VwEqlFWZn1SgS9DFa5ZLCEEClCxfbBimSwpERQCJFMXAdNudCgyYX3PpUKGSwA15+LFiwDwG/qstFwc/EyWF4kXhm0pcFFCqtfIigZLCFEytEj+2DpkTVYUiIohEgmbggAVb9Nu+OiAYaeAhksvDJBPVgK4GWwpE17M2m00bBsMpy66ncRzImtwZIASwiRGqJt2k3ZaFgIkYQ0xwuw6ncRDNsuPlNH01IjwHL9+WihcgAyLMlgNZ8GbdodyWClsPolgqaukeM3pURQCJE6Ym3ao2uwJIMlhEgijveldYMugo6bEi3ao5Q/F73WKxGUNVjNyNuRuq7JhTS4SF31SwTBa9UubdqFEKlC6WbDfbBkDZYQIolobjSD1SjASoEW7VHKnxdbg5Vh6tJFsNm4YW9hMbIGK9UpK8vbAC+Sws7NsKRNuxAidRg+r0RQ2rQLIZKRE12D5Y/9KmS7+I3UKA+ESJOLWImgrMFqNlqjjYZlk+HUpaxMgLpOghmmNLkQQqQO3fSaXEQ3GpY1WEKIJKJFSgTrdxEM2ir2npUKlD8PPVQBrkOGaVBrSwareTTKYGVJiWDKUlYWQF0nwYAla7CEEClD6SaaG8aKfBkcdiWDJYRIIm5kDZZev0176pUIAmih8kiTC8lgNQst0uTCdlxqbVdKBFNYXYBVL4MlXQSFEKki8qHF0LwLvnQRFEIkk1gXwXolgsEUa3LhRgOsYFmkTbtksJpHpE17Zch7giXASl3bBlgWlUEHWxaKCyFSQLQzl6a8zYYlwBJCJJXYGqzUz2DpwTKvyYV0EWwemhMG3aIy6JWSyT5YqUv5GpcIesFyhWSxhBCpIJLB0pwwlq5jS4mgECKJaJESwQZdBG0XXwo1uVD+XAC0YDkByyBou7iqZb/MahUBFq4NhklVUDJYqS6WwQpVAl4XQYCyGlmHJYRIftFN73G9DJY0uRBCJJVYBqtuo+GgnbolghmWN+6W7iSYOs/W3og0uagMeR/CcyTASlnblAhGMliyDksIkRKiGSw3jGno0uRCCJFU6tZg1QVYYUfhT8kSwVIyLK9qraU7CabOs7WnXAcN1bBEULoIpqy6Nu2REsFoBks6CQohUkH9DJYua7CEEEmmiS6CQcdNqTbtri+awSonIxIY1rRwo4vUebb2VOyFYlIpJYIpb7sZLNkLSwiRAmILx51QpMmFZLCEEMmj6QxWajW5wMpE6SZ6pIsgSIlg3GluJLPRoMmFBFipSpmNNxqWDJYQIoXESgRtTEOXDqhCiOQSS0zUBVihFFuDhaah/HleBiu2BksyWPEV64ZiUiElgqlPN1BmRizAyvIZGLomGSwhREqoa3IRlhJBIUTSqctg1esimGL7YAG4vtzYPlhAi7dqT61na084kUjc8PZL8ps6Zoq9SERDysqKrcHSNI28DJNyyWAJIVJBvQyWZehSIiiESCpaE10EQ7aL30ydNu3gNbqI7oMFsgYr7upKBE0qQ7asv0oDXoBVFfs5N8OULoJCiJQQWzjuhr01WFIiKIRIJo32wbJdhaNIqSYXQKREsKyui6CswYqzerWkVUFbNhlOA8rKbBBg5WVYUiIohEgNjdq025LBEkIkkcYZrGiWPZXatIO3F1b9fbAkgxVnDTJYQUcyWGmgfokgQF7AkiYXQojUYESuQY60aRdCJKHoGqzIl0HByNqlVMxg6bIGqxnVb9MesqXBRRpQVnajDJYpGSwhREpoWCIoa7CEEMlFc8Pe+5TmrbmKvkelVJt2QPlz0ULlZBjSRbBZaPVqSSuDsgYrHShfFlqwNPZzboZksIQQKaJ+m3Zd1mAJIZKME27Q4CKawfIZqdXkwvXnobk2Aa0WkDVY8efUtWmvDDqyB1YasNvtg1m6HK22BPA2Gw7abot/OyGEELurrk27t9GwrMESQiQTzQ3GvggCr0U7kHJt2pU/DwAjVI7f1Km1JYMVX5E1WF6bdpssKRFMeaEu4wCw1n0GeGuwAGnVLoRIfo02GpY1WEKIpNIogxW2vfeoVGxyAXiNLkydGslgxVe0RNDBpNZ2pUQwDdgdhqPMTHxrIwFWhvd3Kq3ahRDJbpuNhqVEUAiRRDQ33CCDFXRSt8kF4O2FZRmyBivuIhmsasebqgRYacCwCHUeg7XuU8Br0w5QViMZLCFEkot8M6w53kbDUiIohEgqTqhhBitF27Qrfy4AWrBcMljNQYsFWN7ivCzZBysthLuOwyxZil61kbyAFzSXSwZLCJHs6mewDA1bMlhCiCSiOaHYF0GQum3a65cIBixD1mDFnev18w/jBVaptkhPNC3cNbIOa+1nsQxWqazBEkIkufpt2k1d2rQLIZJMtE17RCyDlWKfnxuWCOpSIhhvmuN96LbxvjU09dRqMymaZrcdjOvPw1r3KbnRNViyF5YQItlFMliaa2MZ3kbDSkkWSwiRHDQn3HQGy0ytz8/KFy0RjKzBko2G4yzS5MKOZLAMCbDSg24Q7nIAvrWfkWEZ+E1duggKIZKeMupvNKyhAGkkKIRIGk6w7n2K1G3Tjm7g+nLqdRGUDFZ8uZLBSlehLuMwKtagl68mL8OUDJYQIvlF27Q7YSzduwRLowshRLLwugjWZbBCTmq2aQcvi6UHy701WNLkIr60WAYrEmCl2E7UYvui67B8az8lL2BRJhksIUSy0yKNltxw7Hoke2EJIZKGE26YwUrRJhfgrcPSImuwJIMVb5EMVjgyVUOTACtdOAX9cAPtsdZ+Sl6GKV0EhRDJT9NQuhVZg+Vdl8KuZLCEEMlBcxt2EUzVNu0AbkYeeqSLYFDWYMVXNIMVcr1vDSWDlUY0jVDXA7HWfRYpEZQMlhAiBRgWuDaWLhksIUSScUIoPfXbtINXIlh/DVZLNhRKvWdrdzmREkHNKxGUDFZ6CXcdh1G9iX76BsokgyWESAWGFWlyEclgyRosIUSS8LoINmxyYWip2cPA9efHugi6qmW/zNqlAOujjz5iypQpTJ48mYceemib29evX8/pp5/Osccey7Rp0/jwww8BWLt2LcOGDWP69OlMnz6dWbNmxXf0uyC60XBYRTNY6R9TtiahLt46rKHh+ZTV2tLuWAiR/HTLa3IRqaiwJYMlhEgWbqjBPlghW6Vk9gq8NVh6JMACWnQdlrmzAxzH4cYbb2T27NkUFhYyc+ZMJk2aRN++fWPHPPDAAxx55JGccsopLF26lAsuuIC5c+cC0L17d15++eXmm8HOREoEYwFWCkbgYvvc3O44OV3pXzMPx92PqpBDtn+nL2shhEgc3Yo0uZA1WEKI5KI5ITD8sZ9DjpuS668AlD8Xza4hS/cCq1rbJa+FHnunz9iCBQvo0aMH3bp1w+fzMXXqVN57770Gx2iaRmVlJQAVFRV06NCheUa7J1wbpRnYygusZB+sNKNphLqMo3vFd2i4UiYohEh+huk1uZA1WEKIZNO4i6Dj4kvRAMv1e+FUrlYNJFkGq6ioiI4dO8Z+LiwsZMGCBQ2O+c1vfsO5557LE088QU1NDbNnz47dtnbtWo499liys7O57LLLGD169A4fzzA08vMzd3ceTZxHJz8/02vlb1j4MrwXS5u8QFzOn0jRuaWbPZ2X1n8i5uJnGKytxjXHJeVzI39nqSdd55au80ophq9Rm3bJYAkhkoO3D1bDNu2pWyKYC0AOXhIo2IJ7YcWllmrOnDkcd9xxnHPOOcybN48rrriC1157jQ4dOvD+++9TUFDAjz/+yCWXXMKcOXPIzs7e7rkcR1FaWr3XY8rPz6S0tJqs6moyNJOKyiAA1VVBSq3UfKFEReeWbvZ0XnrBKNoCB+g/sXxDOd2yrJ3ep6XJ31nqSde5peK82rfPSfQQ4ks30dx6Gw27ksESQiQBpbwugo3atPtTNcDyeQFWlqoBoNZuuQzWTp+xwsJCNm7cGPu5qKiIwsLCBsc8//zzHHnkkQCMGDGCYDBISUkJPp+PgoICAIYMGUL37t1ZsWJFPMe/U14kbmJHatxlDVb6cbM7Ec7rzXhjIV+uKkn0cIQQYsd0Cxw71uRCMlhCiKTg2mioBvtgBW039l6VapQvC4BMvACrJUsEdxpgDR06lJUrV7JmzRpCoRBz5sxh0qRJDY7p1KkTn3/+OQDLli0jGAzSpk0biouLcRxvMmvWrGHlypV069atGaaxA44NuoUT+YZQ1mClJ7vrOMYYi/lkyUbpJCiESGrKaNTkQtZgCSGSQaQxXIMugqnc5MLyAqwAtQDUJlOJoGmazJo1i/POOw/HcZgxYwb9+vXjnnvuYciQIRx66KFcddVVXHvttTz66KNomsatt96Kpml8/fXX3HvvvZimia7r/OUvfyE/P78FplWPa6MMM1aCIRms9BTqOo68nx6nU/ViFm8azqDCNCspEkKkD8Nq2ORCSgSFEElAc0LeH+plsEKOStkmF8ryliRlqBogn5oWLBHcpTVYEyZMYMKECQ1+d+mll8b+3LdvX55++ult7jdlyhSmTJmyl0PcO16JoK9egJWaLxKxY+Gu41GawWHGd3y49DAJsIQQyUu3wA7FFo7bUiIohEgGkQCr/hqskO2Sk5Ga298oy2volOF6645bMoOV/tGGa6N0U0oE05zKyCfceSzT/PP4aNnWRA9HCCG2L9qmPbrRsGSwhBBJQIuUCNK4RDBlm1x4GSyfm4RrsFKd5oYiTS6kRDDdhXodTjdnDfaWpawrq0n0cIQQomm6Ba4dux5JkwshRFKIZbDSpE276WWw/I6XwQraksGKH9dG6VYswJIMVvoK9vLKUSfr3/DhUsliCSGSlOFDc8KxDy3S5EIIkQzqMlj+2O+8Jhcp+tlZN1BmAMOuxtAkgxVXdW3aFRoSYKUzN7cr4Xb7MM3/vZQJCiGSl26CG65r0y4lgkKIJKA1lcFyVMpmsMDrJKjZ1WRYhqzBiivHBsNr0y7BVfoL9ZrCEHcxa9auprQmnOjhCCHEtiJt2qXJhRAiqUS7COoNSwRTtU07RAKsUCUZliEZrLiKlgg6StZftQLBXlPQUByif8eny4sTPRwhhNiW3qhNu5QICiGSQLREsEEXQcfFl+oZrHA1AUunVtZgxY9XImjhKIWZojtRi13ntBuMk9OVo33z+FDKBIUQyShSImhIkwshRDJptA+WUsprcpHKGSxfFlq4kgzToFYyWHHkhlG6ie24GJoEWGlP0wj2OpwDWMD8leub/sekFHrZyhYfmhBCgPftsObaaJqGZWiyBksIkRRia7AiJYKOq1CQsm3aIZrBqvIyWLIGK37qN7kwU/gFInZdqNcULBViP2c+X68u3eb2zG/+H22fGI9v1dyWH5wQQhgmOF4pjqXrksESQiSHaBfBSAYrGHlvslK4AiwaYPllDVacRdZgOa4ihV8fYjeEO+2P68/jSOu7bcoEzU3zyfzmHgAyv7oTlHxzLIRoYboVW+tgGRq2rMESQiQBzWm4BisUWbOUyk0uXCvby2CZsgYrrjRHMlitjmER6nEohxnf8dnSTbjRIMquIefdy3AD7ag88DqsTfPxrXovsWMVQjSLjz76iClTpjB58mQeeuih7R731ltvMWDAAH744YeWG5zhbTQM3tYhYVcyWEKIJOAGgboSwVDky5/UbtOeiRauli6CcefaYPhwXOki2JoEe08h262gd+1P/LihAoCsL/6OWbKEikPvombYOTi5Pcj86i7JYgmRZhzH4cYbb+Thhx9mzpw5vPbaayxdunSb4yorK3nssccYPnx4yw4wmsFS3v4y0kVQCJEMohks0iiDpXzZaKFKAqYmTS7iyrW9JheyD1arEup2CK7hY4r5DR8s2YK19lMy5/+bmqFnEu52MBgWVaN/h7V5Ab6V7yR6uEKIOFqwYAE9evSgW7du+Hw+pk6dynvvbZutvueeezj//PPx+/0tO8DoJp6u7TW5kDVYQohkEG3THstgee9NKd+mXTlkGw5BKRGMH80NeW3aJYPVuviyCHc9iKN983j9+6VkvH0Zdl4vKg+4JnZIcMAMyWIJkYaKioro2LFj7OfCwkKKiooaHPPTTz+xceNGDjnkkBYeHXWbeLo2lq5jSxdBIUQS0Bq1aQ/FmlykbrigrEwAco1gi5YImi32SIlSL4MlAVbrEup1OB1WvcfD1h1YNRtZdvAzFET+oQGgm1Ttdxm57/0e34q3CPU+InGDFUK0GNd1ufXWW7nlllt2636GoZGfn7nzA3dCM70AKz/Hwu8zQNfjct5EM4z0mEdT0nVu6TovSN+5Nee89Mj+wnlt8sCXib/cW5PVJj/QIs9lc8xNy2sDQLuAQ8hR5OQGWqSiLe0DrLo27a4EWK1MsOdksrmKEWoh/+Y4nvrYxyNdbXIy6l72wf7HYX9zL1lf3UWo1+Ggpe63NEIIT2FhIRs3boz9XFRURGFhYeznqqoqfvnlF8444wwANm/ezMUXX8wDDzzA0KFDt3tex1GUllbv9fjaaF6AVVZSjg5U14bjct5Ey8/PTIt5NCVd55au84L0nVtzziuzqoosoLTCBqOarZHHCdWEWuS5bI65+cImeYA/XAkYbNxSQZYvfuFP+/Y5Tf4+/T9NunasRFDWYLUuKqsD4S4HEm4/lO5Hz2JNaQ1XvLqw4XoH3aR6v0sxty7Et/zNxA1WCBE3Q4cOZeXKlaxZs4ZQKMScOXOYNGlS7PacnBy+/PJL5s6dy9y5c9l33313GlzFVWQNluaGsHRNSgSFEMnBCaHQQPcCkJDtvTelepMLgCy8bFxLbTacus/YrlAKzbVRhiUlgq1U2dRHKT3+RUb2aM+1h/fnm9Wl3PruElS9NVfBfsdi5/cm6+u7QMlicyFSnWmazJo1i/POO4+jjjqKI488kn79+nHPPfc02eyipanIhxccO7IPlrzvCCEST3NC3vorzfu8nB5rsLIAyNZqAVpsHVZ6lwhG9hmJZrBSOQIXe8gKxP44dZ9C1pbW8PAXq+maH+DsMd29G3ST6tGXkfvu78hY9Ay1g09O0GCFEPEyYcIEJkyY0OB3l156aZPHPv744y0xpDqxLoJhTEOn1rZb9vGFEKIpbjjWQRDqtWlPgwArkxqAFttsOHWfsV0RazcZbXKR3tMVO3fBgT2YMrA9//xkJS/OXx/7fbDfdMLthpDz/p/If/F4rNUfSGdBIUTziJUI2li6JvtgCSGSguaE674Aol6b9hROUEQDrIDyMlgttRdW6j5ju0CLBFjoUiIoPJqmMWvKAMb3bsMt7y7lue8jQZZuUDrjJSoOugm9Yg35r55G/vNH41v+lpQNCiHiS6/LYHkbDct7jBAiCThBVKRFO9TfByt1Pz9H12AFohksWYMVB5ESQdloWNTnM3VumzaYg3q34e/vLeWZ79Z5N5gBaoedTfFpn1Ix8e/otaXkvXEu+S9MB7s2sYMWQqQPPZrBCntrsKTJhRAiCXidt+sCrOjGvKmdwfLavmeoll2DlbrP2C6on8GSjYZFfT5T57ZjBnNI37bc8f4ynvp2bd2Nho/awadQfOqHVB54LVbRPKx1nydusEKI9GJElj+7NqZksIQQycIJo+qVCEbLl30pvAYLIwOl6fhdr/27rMGKByeSwYp0EZQMlqjPMnRuOXoQk/q14+4PlvPEN2sbHqCb1Aw9E2Vm4Fs1NzGDFEKkn/oZLFmDJYRIEpob6SIYEW1ykcpdBNE0lJWNz/FKBCWDFQ+xDJaJ7chGw2JbpqFz89SBHNa/Hfd8uJzHv17T6IAAoS7j8EuAJYSIl+gHGMeWNVhCiOThhFB6wzVYpq6lfIJCWZn43CpA1mDFRYMSQUXKv0BE8zANnZumDmLygPbc+9EKnmyUyQr1mIRRvgqjdHmCRiiESCuREkHNDckaLCFE0miqi2BKlwdGKF82phNtciH7YO29+k0uJIMldsDUNW48aiBKKf7fh8sxdI2TRnYBINRjIgC+VXOpye+dyGEKIdJAbJ8Z18bUJYMlhEgSbmibfbBSucFFlLKyMO3oGiwpEdxrDTNYCjMNonDRfExd46ajBjKxXzvufH8Zz87zWri7ud2xC/rhW/V+gkcohEgLev2Nhr01WEr23RNCJJjmhMDwx372Mlipn5xQVhZ6uAq/qVMjJYJx0CCDpTC01H+RiOYVXZM1oU9bbp+7lBcimxGHekzyOgmGqxM8QiFEymu00bACpM+FECLhGnURDKZRBotwFQHLkI2G4yGWwTJ83kbDaRCFi+ZnGTq3TBvE+N5tuPXdpby0YAOh7hPR3BC+tZ8menhCiFQX/QDjhGPduWwpExRCJJi3D1bDNu0p3UEwQllZ6KFKMkxd2rTHRaRNO7qJI23axW6wDG8z4nG92nDLO0v4LNwP18qSdu1CiL3XaKNhQFq1CyESzwmhjIZdBP1pEmBp4WrJYMWL5oYAcDBRIE0uxG7xmTq3ThtEzzaZXPf2Cqo6jcO3+n2QtRJCiL2h19toWPcuw2FXMlhCiMTy1mA13AcrXUoEtXAlGZZksOIjsgbL0QxAAiyx+zIsg78dPYiK2jBPlw7EqFiLUbIk0cMSQqSy6AeYehksWzJYQohEc8MNuwimS5MLXxaaXUOmKRsNx0dkDZYd6UYvAZbYE33bZ3HZIX14ZFM/ACkTFELsneg+WE69EkHJYAkhEszLYKVpkwsg1wjLRsPxoMUyWN7FTNZgiT01c3gnBvbtzyK3O+El7zR5TGD+w+S9fDI4wRYenRAipdRr025FSwQlgyWESDDNCaH0ujbtYUelx0bDVjYABWZIMlhxEctgSYmg2DuapnHt4f35yhxFzuZvqKooaXC7+eNTZH9yA761H+Nf/HyCRimESAn127RLiaAQIlm44QYZLK9EMPVDBWVlApCrB2UNVjxokS6CYSkRFHGQF7AYfMCxmDi8+ebzVAZt3lq0iWeeeYTcD67kQ2cY893e1Hz8/6iork30cIUQyUozUGiRjYalyYUQIgkohdZ4DZbtYqVDiaDPy2DlGbXSRTAuIhksJ5LBkhJBsbd6Dj2YWiObvPUfcdg/P+flN/7HeVv+ytrAAMqPfIi1gy6i0NnAE0/cx9LNVYkerhAiGWka6FaDDJaUCAohEiq2d2xdiWA6tWkHyNaCsgYrLiIvlnCsRDC9pytagGGheh7CURk/8PvBtTyVfTdmfneyTnmWsf26sv+hJ1GZ05dTwy9w9lPf8uaiTU2eJjD/YXJfPxeteksLT0AIkRR009toOLYGSzJYQojE0RxvayPVuEQwHTJYsQCrlpqwg2qB7XZS/1nbgWiTC+kiKOIp3GMSefYWfr36UnRfFmXHPIUKtPFu1HTcsZfSh7WcVfAT172+mDvmLiVUr+bXWvMJWZ/8Bf+Ktyh4birG5p8SNBMhRKIow5I27UKI5BHNYDUqEUyLNu31AiwFhFrg/TatA6zGGSwpERTxEOp+iPcHTaNs2hO4OV0a3B7sOw0ntwd/yHiFU0Z25pl56zl99ldsrgyiVW0i953f4hT0pfS4FwCXghePxbf0tRafh3/Rs5gbv2vxxxVCALqJ5tqyBksIkRTqMljePn1KKUJp00XQC7Ay8bo8t0QnQbPZHyGBohksaXIh4klldaD8sHux2w7EaTtw2wN0k+pRl5Dz/hVcMXYtQzrvw1/f/oUzH/+GN9vchRauoHz60zhtB1Aycw55b55P3lsXUVX8e6r3+z1oLfBm5oTI+eAqwp32o+zYZ5r/8dKU49iUlGzGtkOJHsouKyrSWqQ8Yk+Ypo+CgvYYRlpfmgC8heRuGEuXNVhCiPjao2uT67Bx8jO4gQLYuAql4KGjO5KT4bBx46rmG2w9zXZ9Ui4bJz9DgZXLQ739VJWso7Zs92KC3b0+pfdVzIm0aVfeB1bJYIl4CQ44foe31w6YSebXd5P5zX1MPv4F9u3Zhi8evZK2W77knd7XMrxNfzS8YK302GfJ+eBqsr6+G3PrYsoPuwciLUV3yAljVKzBKF2BURb9byXBPlOpHXzKDu9qbl2M5oawNnyJFixH+XN3Y/YiqqRkMxkZmWRldUTTUuP9xTB0nCRc76OUoqqqnJKSzbRr1ynRw2l+uoXmhrEMWYMlhIivPbo22UFMswIntxMqowDHVZRolRTk+Gmb5WveAUc02/VJKUyjnGp/OypqsmnXLhO/aezG3Xf/+pTWAVa03aTtetGwmQZ1pCJFGD5q9r2I7E+ux1r/Jf0zDQbZz/Bp5qGcv3AQU9zFXHN4fwKWAYafikl3YrcdRNZnN5H3xnmUTZ3doJNPY76lr5H73u/R7JrY71yfFyTplRt3HmBtWgBE9uFZ8xGhvkfHYdKtj22HUiq4SmaappGVlUtlZWmih9IilG6Ca8cqK6LXKSGE2Ft7dm2KBjbefaKZpLS4vGkaoKMrb467+3a7J9en1C+s3BHXBt3EibxIjLR4lYhUUTP4FNxAW7K+uBXjfxfiFPSh36n/4Nfje/HOz5s5+6l5rC2NBEiaRs2+51Mx8Q58az4i961fxzKwjfl/+R+5b1+C3W4w5YfeTcnx/2PLOfPZet5P1Ox7AUbxL2jB8h2Ozdz0Pa4/HzejAP/Kd+I99VZFgqv4aVXPZSyDFS0RlAyWECJ+dvv9NFqaF7lfNAZJl0BBaTpaJIjckzLE3X0+0+V5a1o0g+VIBkskgBWgevj5WBu+hmAZ5VMeQPNlc/aY7tx7/FA2V4Y468l5fL26JHaX4KBfUXHQjfhXvEXO3D+Aavihy//zC+S8+zvCnfajdNpTBAeegN1pNCrQFjSNcMeRaCjMTd/veGibFmAXDifUfSK+VXPBbZmN90R8VVRU8OKLz+32/f74x99RUVGxw2MefvhBvv76yz0dmtgJZVjg1GtyIWuwhBAJFX0P8j4ru7EM1u5/dt7Ta9Mf/vDb5rs26To6e5bB2hNpHWBpjTJYsg+WaGm1Q88k3Gl/nKn34rQdFPv9mJ4FPHrKCNpk+vjt8z/w7Lx1sW9UaoedQ9WYK8n45SWyP7wm9q2Sf9Gz5Lx7GeHOB1B29GPgy9rm8ewO+6LQsHbUHTBcg1H8M+H2wwn1nIxeW4JZJN0EU1FlZQUvvbTtRcy27R3e74477iUnJ2eHx5x33kXst9+YvRqf2AHd9NZBRptcSImgECKBtFgGy/us3CihtVv29Np01133Nd+1SdPRYiWCzf9+m9ZrsBpnsKTJhWhpypdD6fEvkp+fCaXVDW7rVhDgP6fsy6zXF3P73GX8srmKKw/ti2XoVI/+LVq4kszv/oGyMnEK+pD9/pWEux1M2VEPgxlo+vH8uTht+mNu/Ha7YzK3LkRTDnaH4YS7jEXpJv6V72J32i+ucxfN78EH72PdunWcddYpmKaJz+cjJyeHVatW8fTTL3L11ZdTVFREKBTihBNOYvp0rznLzJnTePjhx6mpqeaPf/wdw4btyw8/LKB9+/bceuud+P0Z3HzzDRx44HgmTjyMmTOnceSRR/Pppx9h2zY33XQbPXr0pKSkhL/85Rq2bNnCkCFD+frrL3nkkSfIz89P7BOTCnQLXDvW5MKWEkEhREKpyP82LBHU2P3Pznt6bTruuKnNdm169M4byc31wp6WaKSb1gGW5tpg1GtyIQGWSDLZfpM7jt2HBz9dyewv17ByazWXT+rDwA7ZVI29yguyvv8XAMHuEyk/8t9gZuzwnOHCEfiXv+G9gzTx1ZNV9D0AdodhKH8e4U7741v5LlUHXB33+bUmc34q4pUfN8b1nMcM6cjUfQq3e/tFF/2W5cuX8eijT/Hdd99wxRWX8dhjz9C5s7c329VXzyI3N49gsJbzzjuDQw6ZRJs2bRqcY+3aNdxww81ceeW1XHfdVXzwwVymTDlqm8fKy8vjP/95khdffI7//vdxrrrqOmbPfohRo/bj9NPP5osvPuO1116O6/zTmdfkov4aLMlgCSHib5evTa6N5tSizJ9BM3CVojbs4jf1bRIUzXFtysvLb3COuF+b6q3BkgzW3nJCKN3EcSWDJZKXrmn8enwv+rbL4q9v/8IZT8yjX/ssjhnSkSNGz6KLZqIHy6iYeFuss2BpTZiqkE2XvG0zWXbHUQQWPY1RtgInv/c2t5ubF+BkdsDN6ghAqOdksj/9C3r5Gtzcbrs87sC392N+dz/Z/Y6ldp9TsdsP3cNnQMTLoEH7xC5gAM899zQfffQBAJs2FbFmzZptAqxOnTrTr98AAAYMGMiGDeubPPeECZMixwziww/fB2DBgvn87W+3AzB27IHk5Ei7/11m+NDCVfW6CEoGSwiRDKJdBCM/xeGj865cmxoHWPG+NqkGJYJ7P6edSesAy1uDJRkskRoOH9iBA3q24a3Fm3jlx43c+f4y7v1oORP6nES/9lmsfmclq4trWF1STVmtV8d8wYE9OP+AHg3OEy4cCYC58bumA6xNC7A7DI+9a4Z6Hgqf/gXfynepHXb2rg1WuQR+fAx8OWT8/DyBn54g3GE4tYNPIdhvOsqXvRfPRGqauk/hDr/RawmBQF3A/d133/DNN1/xr3/NJiMjg9/85gJCoeA297EsK/ZnXTdwnG2P8Y7z9kHx9inZcR292Dmlm2iujaZpmLomGSwhRLPY1WuTVluKUb4Ku80AMDOoCNqsKamhV9tMb0uZvZAU1ybNiDUOa5bNjBtJ764P0SYXEmCJFJGTYTJz3848dtpInjpjJDOGd+br1SU88OlKvl5Vgt/UOLR/e35/SG8OH9Cehz5bxdPfrWtwDqdNP1xfDlYT67C0UCVGyVLsDsPqjs/vjZ3fG/+qd3d5nNa6zzEq1+McdiNbz/qWioNuQrNryfngSto8OhprzSd7/iSIXZaZmUl1dXWTt1VVVZKTk0tGRgarVq1k4cIf4/74Q4cOZ+5cr83/V199QUXFjrcHEPXoFpoTAsAyJMASQiSY2s4+WHtwqqS8Nmk6KBdNkwzW3ottNOy9aCTAEqmkX/tsLp+YzaUH9yLsqm2+QbJdRchxufP9ZWT7DY7exyv5Q9OxO+zbZGdAc/MPaCgvg1VPqOdkAgtmo4Uqdyn75P/lRVwrG9X/KFSVonbY2dQOPQuz6Dty3r2M7A+vpuTk98Bomd3fW6u8vHyGDh3O6af/Cr8/o0H535gxB/K//73IqafOpHv3HgwePCTuj3/OOedzww3X8NZbrzNkyDDatm1LZmZm3B8nLUU2GgawDF1KBIUQCdZoH6y9KBFMzmtTFppdiq5pLbIGS1MtkSfbDeGwQ2lp01Hv7sjPz0Q9dixaqJL/9HuQW95dyhsXjqFdtj8Oo0ys/PzMuDxHySZd5wXNN7eQ7fKH//3I16tLuWXaYCb1awdA5pe3k/ntfWw5fzFYdR94A/MeJPuzv7Ll7O9Rme1iv7fWfUb+/35F2ZH/JtT7yB0/qF1D29kjCfY+CnPGA9vMy7fyPfLmnEnl+BuoGX5efCZq1+Jf/gbBftNjLWSb2678nW3cuIqOHXvs8Jhk45VRxOfDfCgUQtd1TNPkxx8XcMcdt/Loo0/t1Tmbek7bt99x296WEs/rk/PsOZibvqfktE+Y8sDnHNK3HVdP7heHUSaOvIennnSdF6Tv3Jrr2qRVb8GoXIfddjAYFiXVITaUB+nXPivW7bS5xev61NS16bF/3I1etZFF9CbLb9E5b8cNw5qyO9en9M9g1esiKE0uRLrxmTq3T9+HS577gWvnLOLuY4cwpmcBduFINOVibfqecJcDY8ebmxbgZHdpEFwBhDvuh+vLxbfy3Z0GWP4V76CHKggOOL7JN5BQj0mEuh1M5td3UztgBiqjYLvn0svX4OZ03elXZJnf3kfWN/dQmt2JcOexOzxWtJyioo3MmnUVrquwLIsrr7wm0UNKHYblrRPGy2CFpU27ECKhGqasolXLRjy6XLSwpq5NKvLlrKmpFlmDldYBluaEUWZGvSYX6b3kTLROAcvg/x2/Dxc9u4A/vvwTVxzal8F5/RmL1+iifoBlbZqPXeiVB9qOixn9VsqwCPWYiH/lXCqVu8Mskf/nF3CyOxHuckDTB2galeNmUfDM4WR+dRdVB9/U9Li/e4Dsz2+m4uC/Ujv0rO0+nla9hczv/+0Ns/gXCbCSSLdu3Zk9e+8yVq2V0k1wwkBkDZZsNCyESKRGGw07rkLX4tNFsKU1eW2qKQbA0FSLrMFK74jDDTdociEZLJGucjMs7p0xlMIcPze+9QsnPbuCZW4nvv7iPc5/+nv+/Noirn7uM4zyVTyysoCD7/2EA/7fJ9z01s+xb3JCPQ5Fr9mMuWn+dh9Hq9mKb/UHBPsft8MgzGk7kNrBpxL48TGM4iXb3O5f9CzZn9+MMgNkffF3tOot2z1X5nf3g1OLMvyYxT/vxrMiRBLTLTQ3EmDpumw0LIRIKK3e1sIQDbA0tFSMsJrSwhmsNA+w7EiTC+kiKNJfuywfT50xiidPH8mt0wZR035fRhlL0JRiUVEFHSoXARDuMJzjhnXiyEEdeOXHIp761utCGOoxEaXp+FZuv5ugf8nLaMqhtv/xOx1P1Zg/oqxMsj67iY3ltSwqqgDAt+Idct7/E6FuB1My42U0u5qsL25p8hx6xToCPzxG7YATsNsNbjJYEyIVqXpNLkzpIiiESDSlAK1eiaBKr8REJMAycKWL4N7SIm3aZQ2WaC18pk7/Dtn075BNRuggcj58g4entsXN7U7mN1/Cl3Dq1Kkofx5KKWptl3s/Wk7/Dlns170Au+NofCvfpXrMn5o8f8bPLxButw9O24E7HYsKtKV61O/I/vxmZj/9GK9UDeKJiWHGfHERdvshlB/xEMqXTc3w88ic9yC1g0/F7jiywTkyv/l/AFTv/wcyv7oL/6q5e/0cCZEU6mewDJ2wdBEUQiSUS/2m7I6r0isxEQ2wNIXbAhFWmmewwrEMloYEWKJ1sSMbDlsbvXbt5ub52Hm9UP48ADRN4/oj+tOjTSZXv7qI9WW1BPschbXlJwLf/WOb8xkly7A2zSc4YAZrS2uY/eVq7n53yQ7bndYMP4diqzMXBv/DmMBaBn96McHMzpQd/VisHXz16MtwsgrJ/uhacJ26xytdTsaiZ6kZcjpuThecNv3Qazaj1ZbE7TkSImF0yytjByzZaFgIkWhKobSGAZaeRp+bleZtdaPj0hJfZ6V1gKW5NhiWF4Ub6fMiEWJX2G0HosxAbD8sc9P8BhsMA2T5TO6Yvg+OUvzp5Z8oGXg6tf2mk/35LWR+c0/D8/3wLC46F/7Qh+Me+Zp/frKSf364jH9/tmq7Y1i0JcS11b9igL6Wx9U11OLjtNCVlJAbO0b5sqk68DqszQvIWPTf2O8zv7wDDD/Vo34LgFPgtbCWMsE9N3nyQQBs2bKZa6+9osljfvObC1i8eOEOz/Pss09RW1sb+/mPf/wdFRUV8RtoK6B007tGKe/6JGuwhBCJpRp0tHBc1aIdBJv9+hQrEZQ1WHvPCaF0E9tp2ReJEElBNwl3GI618Vu0qk0YlRu22WAYoHtBgJuOGsiSzVXc/N4Kyg+9h9oBM8j68nZq3/8bj36xivOf+o7g/Gf5xNmHrRTwu4N78cr5+zNzZBce/mI1c3/ZvM15bcflprd+4Sv/OGo6HQBWgGWTZrOgMo8rXv6JkF33gTLYbzqhzmPI+vxWtNoSzM0/krH0Far3PT/WUt5u0x8As+SXZnrCWo927drz17/+fY/v/+yz/21wAbvjjnvJyUmOvapShmF5/+/aWIZOUDJYQohEiq7BApRSCVuD1WzXp0iApcsarL3nrcGysF1XMliiVbI7jiTw/UNYG77yfm4iwAIY37stF47rwYOfriI/YBF0LmKiVsIxC/9Jpr2W3jlj6aZvJjz2Ch4fVbdO6oZp+/DzhnKuf+NnuhUE6Nc+O3bb49+sZcnmKm4/ZjCVPR+jyq6hb6AN12ubuGbOYm5+5xduOGKA16FI06g8+K8UPHMEWV/8Hb1yHa4/j5p9L4idz83pgjIDcc1g6WUr8a39lNp9To3bOVvSAw/cR4cOhcyY8SsAHnnkXxiGwbx531JRUY5t25x//sUcdNAhDe63YcN6rrjiMh5//FmCwVr+9re/sHTpErp370kwGIwdd8cdt7Bo0UKCwSATJx7KuedeyHPPPc2WLZv53e8uJC8vn/vu+xczZ07j4YcfJz8/n6effoI5c14BYNq0Y/nVr05hw4b1/PGPv2PYsH354YcFtG/fnltvvRO/f/c3ekwXSq8LsDItg6KK4I7vIIQQzUrFghBXefHW3gRYSXd9ys3hqVfe4uW5XxBWOjOPO75Zr09pHWDF2rTbksESrVO4cBSZrk1g4VMoTSfcbsh2jz17THcWF1Xy9HfryPIZlHe/gn5OPpdseBFH/wplZpIzbHqD+/hNnb8fM5gznpzHH//3E/936kjyMy1WFlfz8OerOLR/Ow7p52WglBUA4PCBHVhTWsODn66iW36A8w7wdkV32g6iZuhZBBb8Bw1F5QF/jq0XA0DTsdv0xyyJX4CV/elN+Fe8RajreNy83dv1vjH/4ufJWPR0nEbmqR10EsGBM7d7+6GHTubee++KXcDef/9d7rzzPk444SSysrIpLS3lwgvPYvz4CdtttfvSS8/j92fw5JPPs3TpEs4997TYbRdc8Gtyc/NwHIdLL72YpUuXcMIJJ/HMM09y773/Ij8/v8G5Fi9exOuvv8pDD/0fSikuuOAs9t13JDk5uaxdu4YbbriZK6+8luuuu4oPPpjLlClH7f2TlKoiAZbmhghYOrVhZyd3EEKI3ber1ybNDoJyUVYApWB42MFn6k02utjZtQmS8Pr0y8/Mmfsp9995Fytqc7jrukua9fqU5gFWXZv22IaqQrQi4cIRAPjWfITdZgD4srZ7rK5p3Dx1EEs2V9K/QzaWoYO6h5qPcwn88KjXmt3K3OZ+7bL93H7MYC54Zj5Xv7aQe2cM5ea3fyHDMvjjpL5NPtY5Y7qzpqSGf322ivd+2cLwLrkM75LLyEGXMHjJy7iaQc3Qs7e5n1PQD2vtJ3v4bDSab/lafCvfAcC/8h1qhp8Xl/MCaK4DKK8VdzPq338gJSXFbNmymZKSEnJycmjbth333nsn8+fPQ9N0Nm/eTHHxVtq2bdfkOebPn8fMmScB0LdvP/r0qfs7mzv3HV555SUcx2Hr1i2sXLmcvn37bXc8CxZ8z8EHTyQQ8ILpCRMmMn/+94wffzCdOnWmX78BAAwYMJANG9bH62lISbHXhmsTsAyqQxJgCSESScWaCDbcEWvPJN/1aT4HjxlFVoaPDC3AwQc37/UprQOsaJt2b6FeokcjRMtTWR1wcrphVKzZpsFFU3ymzj6d6hpQoGlUHnQT4U77Eep8wHbvt0+nXP48uT83vPkzZz45jyWbq5g1pT/tsnxNHq9pGn+e3J/ebbP4ek0pby7axAvzNwAwKut6xvduw0x8NE7Q2236kfHz82jBcpQ/d9sT74bAT48D4GR1xLfyvb0OsIIDZ3rf6Lk2xtbF3rnbDd7hhszxMHHiYbz//nsUF29l0qTDefvtNygtLeWRR57ANE1mzpxGKBTa7fOuX7+O//73Cf7978fIzc3l5ptv2KPzRFmWFfuzrhs4TisviYtlsMIELINaW5pcCCHiL3Zt2gm9dDma6+C06Udl0GZ1SQ092gTI8u15qJB01ydNQ1Pee239ZVjNcX1K77SOG0YZPslgiVYtHNlbKryd9Vc7pWkE+01HZXXY4WFT9ynklFFdWLK5ijE98jl6n8IdHu8zdc7Yvxv3zRjKe5ccyJOnj+RPk/qS23kA9/ygccaT8/h5U2WD+zgFXqMLY2/LBO1aMhb+l1DPyQT7H4e1/gu0UHy64OnVm9GU4/0Xro7LOXdk0qTJvPfe27z//ntMnHgYlZWVFBQUYJom3333DRs3btjh/YcPH8E777wJwPLlS1m2bCkAVVVVZGQEyM7Oprh4K1988VnsPpmZmVRXVzV5ro8//oDa2lpqamr46KP3GT5833hNNa2oaJMLxybgMwjaLk5LrLwWQogmaKqui2D0vWhvl9ck2/Xpoy+/IxSsJVhbw8cffdCs16f0zWC5jhelRjJYabVZmhC7wS4cCUtexm6/8wzW3vrtwb3pURBgQt92262pboqha7ENkn81ojNfrCzmL2/+wllPzuPX43ty6uiu6JqG3cZL/5vFS7A7jtrjcfqXvoZeW+yVIRoWmfMewFr9IaG+R+/xOQFwwug1W3B9eeihcrRQRWy/r+bSu3cfqquraN++Pe3atePww4/kyit/zxlnnMjAgYPp0aPnDu9/3HEz+dvf/sKpp86kR49e9O/vbSLdr19/+vcfwCmnzKSwsJChQ+sC9GOOOY7LL/8t7dq15777/hX7/YABAznyyKM5//wzAK/JRf/+Ug7YpFiJYJhA5NvTmrBDtj99L8tCiGRW10XQibQx39sugsl2fZp66MFcdPlVBJXB9GOa9/qkqZZoBr8bwmGH0tK9/9Y3P1vHuq0zlWOv4nfrJrGiuJpnzxodhxEmXn5+Zlyeo2STrvOCBM8tVIV/2WsEB/6qwR4X8dCc8yqtDnPzO7/wwdKtjO6Wx/VHDKBjtkW7h/pTM/QsqsZd1+T9fCvfI/uDKyib/gxOQdNrwPKfOxotXEnJye+Dcmj7n30J9TyUisPq9v7alblt3LiKjh3rmmPoFWvRa4qx2wzAqFgLysZpM2APZt98DEPHSeI9lxo/pwDt2ydHC/i4XZ/yM6n5+mly3/41xSfP5bk12dzy7lJev3AM7bP9cRhpYsh7eOpJ13lB+s5tT65Nu8IoXgK6gZPfm82VQTZXhhhYmI3egk3imvv6ZJQsw3EVC8OF9GyTSabP2K377871KX3r5pyw9/+66ZUISgZLtFa+LIKDTox7cNXc8jMt/n7MYK6b0p+FGys55bHvePPnrdgFfTGKt78Xlv+XFzGqish969dg125zu1n0Pdam76kZcqb3nOgmoR6T8K2aC+5eNBqwg+g1xbiBtmD6cf05aHYtOHu+bkmkr/pNLjIs7yJfE07ewFcIke4UKhIWOK5C17QWDa5agtJ0NCJrsJo5v5S+AZZre/8vJYJCpCxN0zhmSEeePGMkPdtkct3ri/m2uhB9ewGW62Ct+oDy7D6YWxeS/dlftzkk8OP/4VpZDRb9hnpORq8twSz6bo/HqldtBE3DzfTWqimf14QjXmu7RJoxvAYwmhsmMxpgSSdBIUSi1FuD5SpFWrYu0PRYk4vmXvKajk+fJ5LBUtGNhiXAEiJldc0P8NBJw/n1+J58Wt4Oq3IdX/yyNna7qxRfrirh3y+8iBEq4+riqbyfN4PAD4/iW/5m7Ditphj/klcIDpiJ8tWl9UPdJ6B0E3+kbftuC9egB0txA+0g2rzA8KN0H1pQAiyxrcZt2sFbgyWEEIlRbw2Wu/frr5JSgwCreSOs9F1N60ZKBA3La9Oeji8UIVoRU9c4e0x3tvgOgs+e5aHX3mXu0APplh/gpQUbWFNayzUZH+Fg0Hn4FC6ct4U3sufTc+7llLYfipvThYxF/0VzgtQM8ZowVNTafL2mlI3ltUzNGEbGgjmct/QIiqtDnHdQb44bvOPOieCVGRhVG1CagZvZvu4GTUP5c9Fri3GV2+zt2tNBki0Jbl7RNu1OiEBkHUC1BFhCiDhRSu1WsykadRHc2w6CSUkzIFYiuHt33d3rU/oGWA0yWFIiKES6KOw5DD6Ds/pU88cFG1HAiC65XHBgT06c/zOOfzQXTBpOxw4bOf+dS3jNdzUZb15C9fHPEPjxcUJdDmRroDdPfbKCZ+etpypSllVlDeHPxmP0921ltdWev72xmMqqIKfv1227YzFNH9Vlm8gNVuBmdwLdRCnF1uowtWGHbDJop1yqKstQvhwsQ8Nv7t6i2tZCKUVVVTmm2fTeaWmnfgYrwwu+ayXAEkLEgWn6qKoqJysrdzeCLDcWYNlKkZGONYL1M1jsesC0J9en9A2w3LomF46ryJAPNUKkBSevB0q3OLJDKd3Gj0YDerbNRK/cgG/uQioPuBqAY4Z0pDDnMG54ZTl/33Qf4RdOxqhYyzP5F3Djv78kaLtM6t+Ok0Z0oVfbTAqC3eDJx/jbwNVUDD2Mv76zhHs/WoFScMb+TQdZBfntqFz0Bhsy2uO61YRLV1JWEybseFnzrcolSBVVpSsoU1679rZZFj4zcRcuTdOSNlNkmj4KCtrv/MA0oBptNAySwRJCxEdBQXtKSjZTWVm6y/fRK8pRlo2qcqgqD+JYBhtrWzZMaO7rkxasQAtWEFLrKA76CPl3PTbY3etT+gZYjTJYUiIoRJrQTZz83hjFS+jVNjP2a9/q9wEI9Tg09rsxPQpoe/JvePXZn5i2eS4bVBuuX9qLwwa246wx3ejdNit2rBvohV3QF9/KdzGHn8sdM4cRDjvc9/EKoIkgSylyP7+Jjgv+Q9mE2/h3SVf+9dlKsnwmVx3Wl0P7eW/E2a+cila6mrcPfpnfvfAjf5jYh5NHdmmuZ2en0rV1ccqJrtVrsAZLuggKIfaeYZi0a9dpt+7T7uXDqBlyBhUHXsv0pz7mrDHduXjc7rV631vNfX3K+PExcj78M2fV/pPpY4dxYTPOL30DrHpdBKVEUIj0Yrfpj7VpQYPf+Va+h5PdeZt9p/q2y6Lg5HuY//yZ/JA/mWcOG0u3gkCT5w31PIzA/EfQQhWYRiZ/Ocrb1HCbIEspsj65nswF/2HjgLM4f/4gfti4gon92nHVYX1pk1lXRmD3PJScj2cxLr+U/IDFss3b7jCf6rI++yuaXUPlwTcneigpI9bkwgnF9mKRLoJCiIRxwyjDR2XQxlWQl5F+IYKyvC9lC8xQs3+hlX7PXoQW2wdLMlhCpBunoB/+pa+BXQNmAJwg1tpPCPY/vsn9vtoWFMD5r9B5J+cN9ZxM5rwHsVZ/CB1+halr/OWogWiaF2Qt3VKFhuLojfdxVM0rPMZUZs2fTG5GLTcdNZApA9tvU+8e6j4RAN/qD+jbbhRLt6RfgOVb8TZ6sFwCrN0RKxG08UdKRqWLoBAiIZSL5tqgW5TVeAmKvAwrwYOKP2V5pfptzBC1dvO+3+7SQoCPPvqIKVOmMHnyZB566KFtbl+/fj2nn346xx57LNOmTePDDz+M3favf/2LyZMnM2XKFD7++OP4jXxnImuwVKSLoGSwhEgfdpv+aCiMkuUAWOu/Qg9XEeoxaa/OG+44Ctefh3/lu7HfmbrGDUcO5JghhcxdsplDVt3NUTWv8Gb28czvfzm/Ht+Lp88cxRGDOjS5mNjN74Wd3xv/qrn0aZfF8q1Vzd4etkW5Nkb5avSaLWjVWxI9mpRRv027rmkELF3WYAkhEsMJAaAMH2W13ufnvED65WCU5S0LaO8LN/tj7fTZcxyHG2+8kdmzZ1NYWMjMmTOZNGkSffv2jR3zwAMPcOSRR3LKKaewdOlSLrjgAubOncvSpUuZM2cOc+bMoaioiLPPPpu33noLw2iBhhNOXZML21WYhgRYQqQLp6AfAGbJLzjt98G3ai7K8BPqOm7vTqybhHpMwrdqLo5b92HX1DWuO7w/twaeIPOH16kefgGjxl3HqF3szhTqMYnAj48zYIxBTdhlfVktXfObLlNMNXrFWu+bT8Dcuphw5vgEjyhF6HUbDQMELEMyWEKIhNDcuqqvslrv/Tw3LTNYXoD167HtUb26N+tj7TTAWrBgAT169KBbN2/twdSpU3nvvfcaBFiaplFZWQlARUUFHTp4e8e89957TJ06FZ/PR7du3ejRowcLFixgxIgRzTGXhtx6TS4cJz37+QvRSjn5vVCagVG8BADfqvcIdxkLVuZO7rlzoZ6TyfjlJdTc68mqDaNXFaFXFWFUrseoWEP18AuoGnddk6WI2z1nj0lkzn+Y0eoHIJ+lm6vSJsAySlfE/mwWLybcTQKsXVE/gwXRAEuaXAghEqB+BqsmksFKyzVYXoDVJcMhmO1v1sfa6bNXVFREx44dYz8XFhayYEHDxeW/+c1vOPfcc3niiSeoqalh9uzZsfsOHz68wX2Liop2+HiGoZGfv/cfkvSt3jeBObnZKMrJDFhxOW8yMAw9beZSX7rOC9J3bombVya06U2gcjk+dyNm6XKc/S+Iz1iGHoX6IBvjy38SsDIhpxMquyN03x+n6++wRp1D/u5+YZM9EfVGFgOqvwIOZ21lKGGvh3j/nelL1gKgzACZFUvwp+HrvFkYdRsNA2T6DGlyIYRIiOj7EIZFeSSDlRdIwwyWz1uDpYWbfy10XMLTOXPmcNxxx3HOOecwb948rrjiCl577bU9OpfjqLi0aCywQ+hARbVDyHZwwk7atCZO1zbL6TovSN+5JXJeuXl9MYoWEfxhDhZQ2mE8blzGYqKd8SV5+VmUVhvbZqrKavborLldx2Muf5cuudP4aW1Zwp63eP+dZW/4Gb+Vjd1hONqGH5tlXu3b58T9nAnXKIOVYUqJoBAiQWJVX14GSwNy/OmYwfK+ANTClc3+WDttclFYWMjGjRtjPxcVFVFYWNjgmOeff54jjzwSgBEjRhAMBikpKdml+zab2IslugYrDXekFqIVs9v0xyhbiW/Fm9j5fXDzesbt3MqfB/7c3SoD3JlQj4kYFWs5OH8ry9Kok6BRtgInvxd224GYxb+AkjK3XRHdaDh6rcr06RJgCSESItZ52/BRVmuTk2GmZfftaImgFm7+Lzh3GnUMHTqUlStXsmbNGkKhEHPmzGHSpIadujp16sTnn38OwLJlywgGg7Rp04ZJkyYxZ84cQqEQa9asYeXKlQwbNqx5ZtKYE9kHy/DhuErWYAmRZpyCfmjKwbfu873uHtgS7PZDARieuZXVJdWE7PQIRIzSFTj5vXHaDkSza9DLViV6SKmhXpt2kDVYQogEcoKA13m7vDacluuvADB8KN3XIhmsnT6Dpmkya9YszjvvPBzHYcaMGfTr14977rmHIUOGcOihh3LVVVdx7bXX8uijj6JpGrfeeiuaptGvXz+OPPJIjjrqKAzDYNasWS3TQRDqFuxJF0Eh0pLdpn/sz6EehyZwJLvG9ecD0CMQxFGworiaAR2yEzuoveWE0CvW4vQ/DruttymzuXURofxeCR5YCtANFFosgxWwDGnTLoRIiLougj7Kauy07CAYpazM5FmDNWHCBCZMmNDgd5deemnsz3379uXpp59u8r4XX3wxF1988V4McQ+5dW3aZR8sIdKP10lQR5kBwp33S/RwdkplFADQ2e+t4Vq2pSrlAyyjfDWacr0SwTYDUGiYWxcT6nNUooeWGnSrQZv2WgmwhBCJ4NTtHVtWG6YgM40DLF92i5QIpmkOELRIiaCDiYK0rCUVolUzAzhtBmAX9AOjedutxoPy5aA0g7ZaFZahsXRz6q/DMkq9jZ6dvN5gZeLmdscsXpzgUaUOZVixcvaAZVAtXQSFEAmgRUoEo2uwerZJ326wyspCCyVBiWDKinwraEemKBksIdJP6fSnUyK4AkDTUBn5GKEyerbJZGkaNLqI7oHlREoC7bYDMbZKgLXLdLNeiaBOre3iKoUua4aFEC2pQRfBILnpugaLSICVDE0uUlYswPLWfEmAJUT6UYG2sX0tUoHrz0erLaFvu6y06CRolK3A9efHyh/ttgMxylaCvWet7FudeiWCmT7vWlUrjS6EEC0s2kXQwaAq5KTlHlhRXoCVBG3aU1ak7CKseRctKREUQiSayshHD5bSt10WmypDlNeGEz2kveJ1EKxraGG3HYSmXMziJQkcVepQ9TJYGZZ3rZJW7UKIFud6jeEqHO99KC+dm1z4slqkyUUaB1jei8WREkEhRJJwMwrQakvo097bi2PZltTefNooW46T3zv2s9N2kPd7KRPcNYYv1qY9UwIsIUSCaJHPzBVhLyxI2zbtRNdgSYC151wbpenYygusJMASQiSa8uej13oZLCC112GFazAqN+Dk1WWwnLyeKMOPKQHWLvEyWNEmF97lWAIsIUSLi5QIloW8z8p5gXQOsLIlg7VX3DDoFo6rADD19J2qECI1uBkFaMFSOmT7yPYbKb0OyyhfCdCgRBDdwG7TXzoJ7irdin1zHIiswZJOgkKIlqZFSgTLwtEAK41LBFtoH6z0jTqccGyTYZA1WEKIxFMZ+ejhKjQ3TN92WSndqj3Wor1eiSCAI50Ed1mDDJYpTS6EEAkS+aKnNFIimPZdBJ1gLGvXXNI3wHJt0E1sJ5rBkgBLCJFYrj8fAK22lD7tsli2tQqlVGIHtYdiLdrrlQgC2G0GYlRvQqspTsSwUotu1m00HM1gSYmgEKKFRbsIlgUjGay0bnLhdR7W7OZdA52+AZYTBt2HrSSDJYRIDtF25nqkVXtl0KGoIpjgUe0Zo2wFbqD9Nm3y7bYDATC3LkrZ4LHFGL56a7CkyYUQIkEiX/SUBBWGrpEV+cInHSnL20S5uRtdpG2ApTkhlGHiSAZLCJEk3Ix8gFirdkjdToJG6cqG668inEiAtXzxNxzz768org619NBSRv027ZnS5EIIkSDRtaDFQa+DoJbGm50rK5LBauZ1WGkbYHklgpZksIQQSSOawYqWCELqdhI0S5djNxFguZkdCPnyWbroWzrk+MlN41KTvaZbsdKcun2wZA2WEKJlaU4IpZuUBd20Lg8Ebw0W0OybDafvKjY30uTC8S5WpiEBlhAisaJrsPTaEnIyTApz/CkZYGmhCvSazdusvwJYXx5kZagL+xhruGP6YKke2AGlm7F9sGIlgtJFUAjR0iKdt8tqwmndoh28jYYBtLCswdozTqRNu5ISQSFEclCREkEtWApA33ZZKdmq3ShbCbBNiWBFrc1lL/3IL6obA411FKT5hXqv6VasyYWha/hNXUoEhRAtzwmhDB9ltXbaVx3EMlih5s1gpW+A5dqRDJaUCAohkoOyslG6iV5bAkCfdpms2Fody7SniqZatNuOy1WvLmR1SQ3Dho/BsKvRy9ckaogpQelWrMkFeFks6SIohGhpmhMC3edlsNK4RTvULxGUNVh7JpLBsmWjYSFEstA0lD8frbYUgD7tsrBdxaqSmsSOazfFWrTn9gRAKcVt7y3lq9Wl/HlyP7r2GwWAKfth7ZhuxhaXg9foQjJYQoiWprlhlGFRVmun9SbDUD/AkhLBPeOGwbBwZKNhIUQScTMK0OuVCAIpVyZolK3Aye4EVgCAp75dx/9+2MhZ+3fjmCEdsdsMAMAslgBrh4yGGawMy5AmF0KIlueEcHWLoO2m9SbD0HJNLtI3wHIiTS5cWYMlhEgeKiMfLVIi2LNNJoaWep0EjdLlOHleeeCPG8q576PlTOzXjovH9/QO8GXh5PbAkAzWDjUuEcz0GdLkQgjR4jQ3jKP5AFpBBiuyD5aUCO6hSEcUyWAJIZKJ689Hj5QI+kyd7gXeOqxUYpSuwMnvRVXI5to5i2mf7ee6w/uj19s7xW47UEoEd6ZekwuIZrAkwBJCtDAnhK15mat0X4OFpqPMTNloeI85tmSwhBBJR2UUoAVLYj+3zbIorQnv4B7JRastQQ+W4uT14vb3lrKhvJabjhpITqOLst12oNcMw65N0EiTX/2NhgEypcmFECIBNDeETTTASu8MFoDry5YM1p7SGmWwJMASQiSD+hksgGy/SWUwdT5URxtcfFPRhjkLN3HOmO7s2zVvm+PstoPQlINRsa6lh5g6dLNBBitg6dRKgCWEaGlOmHA0wGoF22uEu/7/9u47TKry7v/4+8yZurNl2AV2qUtXBMQuqIiigIKIBZMYkxhjSfIkUaNJfkYTk8dEfWKqxsRETTGxJjZErGDBgr0g0qT3BRZm+7Rzzu+P2R1YdinCbJmzn9d1PdfDzs6cc591s2c+873v730iVvGwNj2He3+KdhI8XlJ240bDClgi0gk4wW4YqYZ0ZccbJBzwUpdI7fuFnYRZlW7R/puPHUb1KuTSseWtPi8xcDLVp/2+xV5ZspNj+ltp064mFyLSvgwrQbwxErh9HyyAmol/bPNzuLaClW5y4dMUQRHpVOzGzYabOgnm+82cqmAZO1Zh4WE9pfxi6iF7/ttq+ogfegEY7r3NHDSPF8NOgZO+T4V8pipYItL+7CQJu4uswWon7r3zWWrTLiKdjx3sBpDZC6upguU0vsnuNKwEWPEWD69dsZANdgnXnj6cPkWhDhiYi3gaPylurGKF/Cb1Cavz/S6IiKsZVoKYYxLwegj6zI4ejiu4N6bauze5cG+WFJHc4QQiAHhiO7BIV7BsB+qTFmF/x/xJNuq34dvyMWblYryVi/FWLsGMrsDxhoiN/CoNh1/K0vow/3p3Pd/ZvoKacDlnDO/ZIWN1E8fT+N+7cd/GkNeDA8RTtt7kiEj7cBw8tRvZHhik6lUWufcnqTbtItIJOY1TBI2mKYKB9J/h2njHBCyjbgvFD52KJ14FgJXfh1TJoSQGnI6nag2hD/6C74O7WZ06mdXGNIb6tpAYOJ6WtS353BorWIadxCG9DxZAQ9JSwBKRduGp24QnXsWqYLnr98BqT+4NWNpoWEQ6oaYpgk2dBHcGrBSlBYF2H09g1XN44lVUnXkPyT4n4gQKgfQGwr99eQU18Ql8L/AsX/C9yoXOyxi2Q6p4ULuP040cs/kUwaZQ1ZC06dZRgxKRLqVpv8IlTj9VsLLIvT9JOwW7NrkwFbBEpOPZjVMEjVh6L6z8QPpNdV2iY5obBJbPJtVtCImBZ0DjRsGbq2Nc/fhCAl4PX59wEsePOJ9oopK8BX/Hv/IZEn1P7JCxuk7jFMFMBasxYGkvLBFpL2ZjwPok2ZfuRapgZYt7A5aVyAQsA/AYClgi0gn48nA8/kwXwaZpgbXx9m/VbjRU4ts4n/qjv5cJVynL5vqnl5C0HP524eGUF+cB4Ph6Ujf2OurGXtfu43Qrp6nJhZXeCyvUGLDUSVBE2ou3cglWuIz1DUEGd4E9sNqLOzs/OA5GU5MLy1H1SkQ6D8PADkZaVLA6ImAFVj6H4djEB0/NPPbH11bxyaZqfjJ5WCZcSRvZpYIFEPKnb8n1HVTNFJGux1u5hFTJoVTFUhR1gT2w2os7A1bTxo2NbdpNVa9EpBNxApGda7CaKlgd8KY6sOIZUkUDsEqGA/DyZ9t48P0NfOGI3kw8pEe7j6fL8fjT/7+pTfsua7BERNqclcTcsZyGokOwbIdCrcHKGlcHrHSTC1sVLBHpVOxgtxZdBOvauYJlxHbg2/AGicFTwDBYH23gpueXclhZAVeNVxOL9uCYTW3adw9YqmCJSNszq1Zh2AmqC4YAqItgFrkyYDVNt2hq0649sESkM3GCETyNUwRDPg+m0f5TBP2rXsSwU8QHTyWesrlu1mI8hsGtZw3H7839v5nz5s1j8uTJTJw4kbvvvrvF9x966CGmTZvG9OnTufDCC1m+fHn7D7KpTbuVAHY2uVDAEpH20NRBcGveYABNEcyi3L+LtqZZBcvRHlgi0qmk12BFATAMg3DAS228fd9UB1bMxiroS6rH4fz+lRUs3VLLz884hN5FwXYdR1uwLIubbrqJe++9l9mzZ/P000+3CFDTpk1j1qxZzJw5k8suu4xbb7213ce5c6NhVbBEpP2ZlUtwDJPN3v4AatOeRa4MWIad/jQQj7+xgqWAJSKdR3oN1o7M12G/SV2i/SpYRrwa/7p5xAdPZf6aHTz28Sa+ekxfxg0uabcxtKUFCxZQXl5Ov3798Pv9TJ06lblz5zZ7Tn5+fubfDQ0NGB2xVneXjYYhXc0EBSwRaR/eyiVYkUFEk+m/PZoimD3ujKpW4xsVVbBEpBOyg90wrDikGsAbIr+dK1j+1S9i2Elqys/gtueX079biG+dOKDdzt/WKioqKCsry3xdWlrKggULWjzvgQce4B//+AfJZJL77rtvn8c1TYNI5OA7K5qmh0gkD6OmAID8PBOn8bg+08D2eLJynvbWdF1u5NZrc+t1gXuvLZvX5Y0uxel1JInGD5j6lxYQyW//De+buOm/mTsDVuOngY6ZDliqYIlIZ+IEIwB4YlHs/BD5fpPadqxgBVY8g5Xfi3tWl7A+up4/zRjlinVXn9dFF13ERRddxKxZs7jrrrv41a9+tdfnW5ZDNFp/0OeNRPKIRuvx1qXoBtRV1ZBoPG6ez2RHTTwr52lvTdflRm69NrdeF7j32lq9LsfJ7GW4v4xELd2ja6g/5Ats3p4+nhNPEk11XAU9F/+b9ehR0OrjrryjGk1t2hubXKiCJSKdiR2IAGT2wmrXNVjxGvxrX2Fb74nc994GJh/ag+PKu7XPudtJaWkpmzdvznxdUVFBaWnpHp8/depU5syZ0x5Da85snI5j7wzXQZ+pKYIiXciqynpmfrLpwF7sOAQX/ouSv43Et27e53qpuX0pAKni9B5YYb+J13RlLOgQ7vxJNlWwPKpgiUjn4wTTgcazS6v29uoiaKx4EcOK86ctI/GbHq4+ZXC7nLc9jRo1itWrV7Nu3ToSiQSzZ89mwoQJzZ6zevXqzL9feeUVysvL23mU4HiaAlYy81ieApZIl5GybP7frEX88oXPeG9t9PO9ONlAwdyrKXj1eoxEDeG3bktXsvZTUwfBVMmhVDUktf4qy1w5RXD3CpYCloh0JnZjwGqqYOX7zawHrE3VMYqCPvL8ZrPHPYufosHfnfs39+baCQPoHvZn9bydgdfr5cYbb+Syyy7DsizOP/98hg4dyu23387IkSM57bTTuP/++5k/fz5er5fCwsJ9Tg9sE5kmF7tWsDwKWCJdxGMfb2JVZT0hn4c7X1vFP758xH413PFEV1H03BWYlUuoO+5a7FAPCl69Dt+6eST7j9+vc5uVS3C8ediF/aiOLVIHwSxz508zU8HypTcaVsASkU7EaZwi6Gls1R4OeKlLWDiOk5Vudttq41zwj/cIej1cfFw/LjiiN0GfCcl6jOVzmGWN45DSQs4f3fugz9VZjR8/nvHjm7/RuOqqqzL//slPftLeQ2ph9zbtAHl+k4aEApaI20Xrk/z1zTUcXx5h0iE9+cULy3j5s21MGNZjr68zlj1Lt5nfBsND1Vn/Ill+Klhx8t6/nbz37qBqPwOWd/sSUiWH4GCwvT6hPbCyzJVTBFtuNKyAJSKdR6aC1TRF0G+Ssh3iKTsrx3/g/Q0kLZthPfO5Y94qpt/7Dg99sAHPqrkYqXqeiB/DdacP1frUjpapYCUyD4V8Jg3J7PweiEjHMCuXEJ73U0jF9vicv7y5mvpEimtOHcyUEaUMLM7jz6+vJmXveZpfcNGDeP97EVbRAHZ84dl0uAIwA9Qf+W38m97Gt/GtfQ/QcfBWLiFVfCh/nLeKxRW1jOzVerMGOTCuDFiZNu2m2rSLSCfkDeKYgcxeWPmBdCWjNguVi6qGJI99vJGJh/Tgzxcczj1fHM2gkjz+9vLHNLz4czY43ek38lQOK9PNtKO1VsEK+UzqNUVQJKflv/kL8j75B+F3f9/q95dtqeWJBZuYcURvBpWE8XoM/uekAazZ0cDTCze3+hqA0IK/Y/c6iuh5j2MX9mv2vdhhF2KHupP37u0AVNYl2Fobb/U4nvoteGI7eGZrCf9+bz0zRvfi8hPafx2qm7kzYDWbIujg9bjzMkUkRxkGdjCC0ThFMBOwsrAO6z8fbqQhafP14/oDcETfIu6aMZIX+91HKZXcFPgB3xo35KDPI1nQ2EXQsHY2uQj5PMQUsERyllm5FP/aV7FDPQh9+Be8Wz9p9n3HcfjdKysoCHi54oTydGOKRB3jh5Qwqlch98xf0+rfAE/NRryVS3AOOwe8wZYn9oaoP+Kb+Ne/xiNPz2TaPW8z7e63+eXzy9hY1byS5mxZDMCjG4v4xpj+/Oi0IXg6YrN1F3Nl8tjZ5MKrNu0i0ik5gcguXQTTjSjqDjJg1SVSPPzhBk4eXMKQHuHM43nv/o6eW98gNv6X/PEHl1Ogxcydg6dlm3ZVsETa2efovLc/QgvuxfEG2XH+E9ih7hTMvRZ2+RDlpc+28f66Kr590gAKfQ6Fs79O8QPjMKwY3z15AFtqE/znw40tjutf+xIA9uDTWz3vjvoEv90xjh1OPsNX/Y2zRpQy44jePLu4gvP+/i43v7CMTdUxGpIWz857BYDxY8bx7RMHZGXtrzTnyoDVsoKlXxwR6VzSFaymLoJNFayDe2P9xILNVMdSfP24nVNH/CufI/ze7TQM/xKxERfpRtqJ7JwiuGsFK70Gy8nymz4RaSmw7AlK/nEE3ooPs3I8o34bwaWPEzvkAuyiAdSOvwVv5SLyPvwzALGkxe2vrmRojzDnjOhJwYtXElgzF7N+C/61r3JU3wgnDizmn++sozqWbHZs/5qXsQr6QvdDmj2+oaqBP7+eXmt738fbeb14Bqd5PuDGIxL8YMIQnrj0OM47vBezF1Vw3t/e5cv/ep9Q1TLq/SWcP3ZkVq5bWnJlwNq1yUXKdvCaekMhIp2LE+y2SxfBdAWrNnHgFax4yuaB99ZzTP8Io3oXAmDuWE7BnKtJ9hxN7cm/BIWrziXT5GKXfbD8JpbtkLQUsETaUuCzpyiYcxWehkpCH92TlWOGFv4Lw4rTMPoyqhqSbOh5CtHyqYTe/QMbV3zM3W+uYVN1nGtPGUTRvOsIrnia2rE/xg4WE1g+C4DvjBtAbTzFfe+s33lgK45v/eskyieAYVBZl+CRDzbwjQc/4px73+Ufb6/jxIHFPHzxMYw9/4fY/gLy3r8DgJ4FAX502hAe/8axTB9VRn3CYkJkK97SEVm5ZmmdO+eJ7LLRsGU7mHpTISKdjB2I4N1tDVbdQVSwZn+6mW11CW6akv5000jUUPjsZeANUH3GPa3P2ZeO5TFxDE+zKYJBXzpsNyQt/F5XfgYq0iGqGpKkbIeSsB//8qcpePF7JMuOxSoeSnDxw9TVVWCHSw/8BKkYwU/uY03xSXzz6SqWbJkPQDHTmBN4FeuZq3gg8XNOG9qDk9feTmjxw9QdcxUNR30Hs2otwWVPUJNqYGiPfM4Y3pNHPtxASTjdDbt39F2+lKzjsZrDeO6+d3lzRSW2A0N7hPnuuIFMOrQHvQrTf+MdoGHUJYTfvwOzcilWSfqeUFYY5LrTh3LdhEF0u3slDYPGHeyPVPbClQHLsHZuNKwKloh0Rk6wW3oNluPsnCJ4gBWslO1w37vrGdmrgGP6RcCxKZj7fczoKqrOfhC7wL37XeU8j695BcuXDlUNSYuikPalETlYqyvreeD99TyzqALLdri271K+te1mUmVHUX3WfXjqtxD69H6Cix6k/tjv7/lAjoOnbhN2fsu/p0sraln/2t/4UqyS66rHY5U4fOekARQGvXhND59uvY5xi37CE0d8zMBwkrz376X+8G9Qf9wPAIgPmUZo0QP417xMYvAUvnliOS9/to3fv7ISgBu8zxE3fdy2vJRuRQ18/bh+TDq0J4O7h1uMBaBh9GXkfXwv4bdvo/rMe5vNXjCrVmNYcVIlhx7ET1X2xZUBK/NpoOklZdmqYIlIp2MHIxhWHFIx8vzpTx4PtIvgi0u3sLEqxjWnDMYwDPLe+T2Blc9Re+KNJPuemM1hS5Y5Hm+zBfChTAVLe2GJfC6Og7fiQwLLnsC/9hWq8/oxp24oD2wdyApzEFNHlHFMbD5fWPNLPnQGcVvsh5y3uoHxQwaS6D+e4Kf3s3rY5WxtcNhWG6dbnp8RZQWZRmmhBX8n//WfUXf096g//kfUJixeWLKFmQsrWLS5mhcCD7E+MJjLz/kyI3sXNl/v6lxMvP4VDl/2eww7RezQL1B30s8zwSfZZwx2qITA8qdJDJ5Cn6IQz317DEnLwWca9PnvjdgFJzD37IlEInlEo/V7/1GEiqk79vvkz7+Z8PxbqDvhhsz3zMp0B0FLAatNuTRg7dbkQhUsEelknEAEIL0XVkFvwn7zgJpc2I7DP99ex+DueYwbXIx/xWzC7/6O2KEX0DD68iyPWrLO421WwWoKWOokKLJ/zOhKAksfJ/DZk3irVpMy/HzgGUXxjuVc6JnHhQGw/IWk4sfiXz+PRM9RvD/gt6xfUMX/m7WY4jwfJztj+IPzKn/8+194zj4uc+yioJcxA7oxoY/DjHd+jR0sJvz+H/l06ad8I3oJtSmTId3D/OHIbQxbvJ7qcX9gVJ+iloM0DGrH30K3R84g3vdEak69DYxdpgB7vMQHTSG49FFqkg3gCxFunNngqVqDL7qc2pFf/Vw/l4Yjv4VZs568D+/CDpfSMPoygHSrd8NDqnjo5/9hy35zZcBq0aZdFSwR6WTsYDcAjHh0l4D1+StYr63YzsrKen4x5VB82xZROOdqkqVHUTP+VjW1yAUef4s27YD2whLZzfKtdUQbkoQDJmG/lwK7mr5v/4S8Vc/iYPCReTgPJq/geetYevcs5aIx5YR6ximoeBvfhjfxr3+TZO8x1JzxF84NFHH2MQ7zVlQyZ+lWDN/pRNfcx0+LXmPi2EsoCfvZWBXjjVXbmb9qO6ctvxPLbOCLnl8yNjmfH9U+wpMFlWw+/a8M69eHyNO/w8rrSXzo2Xscv53fm8qL3wYz2Orf5viQswh9+m/8a+aSGHJW5nH/2pfT3y+f8Pl+YIZB7bib8DRsJf/1n2OHuhMfdg7e7UuwigaAN/T5jiefiysDFnYCAMfjb6xgaaGwiHQuTjACpCtYFhAOeKlNfP431Y9+vJGe+X4m9TMoeuwb2MEIVWfeq6YWOcIxvc3btPsbK1gH8Lsg4lZPf7qZ/31uWebrsZ5P+b3vz3ip5nepGTxsnUrPXv2ZcHR3/jWsO32KQpmpdPGic4kPO7fFMU2PwalDu3Pq0O4AeN//On3e+j9O7VaJVTyMEWUFTDykB57NH1Dy2KvM7/llSoIjKB08jkrjBAa/8gMGvP5V6k74Cf61r1J3/I/A9O/9QvYSapK9x2CHuqenCe4asNa8RKpoIHZk4Of8qQEek+rT76CoYTsFc7+PHSrGrFyC1f2wz38s+VxcGbB2r2BpHywR6WwyFaxd9sJqsdGw4+CpXoN/3ev417+Gb8N87HAZsUNnEBt2LhuSBby9egeXH9eL4he+iSdWSfTcx3HCPdv7cuRAeXw771lA3i5dBEUkvTHvL55fxrH9I1x6bC8GLLqDQ1b9kx2hch4c+DsCkcP42+ASygoP7kOl2GEXEn7nd4QW3kftyTenH3RsCl/7KVZeKUOn38it/nwAbM6nqrAXhc9eTtHsi3G8QRpGfOXgLtRjEh88leCSR6hJ1IE/DKkG/OvfOLhje4NUT/kbkSfOp/DZyzGS9cSHnXdwY5V9cmXAyiwY9nhJ2U5mgaKISGeRWYMVjwKQHzCpiqXfaBuJGsLzb8W/5mXMmnUAWPm9SJSfirljBflv3ET4zZvZXnA8Z3qO5Vv1G/FteofqSX8m1fPwjrgcOUAtm1zs7CIo0tW9tXo7Nzy9mBFlhfxhfIier1yMb+sCGkZ8BevEnzHVl71pbk6ohPjQaQSWPEbdmB/j+PMJLn4E35aPqT79dpzGcNUk2ecEouc9SeGzl5IYdAZOqPigxxAfchahhfcRWDOX+NCz8W+Yj2HF0/tfHQQnUETVtPuJPHYOnmSdOgi2A1cGLMNOpW9ahqEKloh0SnbjFEFjl72wNlTFAAh+ch+hhf8iPnAy9Ud+k2TfcViRQZl5++b2z/Av+S+RDx/mz/43YTnUHX3lXuf/Sye1W5t2dREUSft4QxU/mLmIgSV53D5tAL0eOREwqDrzHhKDzmyTczaM/BrBpY8RWPY48aHTCb/1fyTLjtljxccqHsqOL7+atfMnex2HlVdKYPmsdMBa8xKON0Sy9/EHfWw7XEbV2Q+S9+4f1F22HbgyYGEnwZPenM0BVbBEpPPxhnC8wXQXQdIVrKYmF4FVL5DscTjVU/7W6kut4qG8UPpNfhA7gX+cUM1R+duJjfxauw1dssfx+FptcqEKlnQFRv02nEABmIFmjy+tqOWqxxdSWhDgj+ePomTLq3jiVUTPfpBkv5PbbDyp0qNI9hhF6JP7MHeswGjYTu20+/feMCibzYQ8JvHBUwgtehAjUZveF6vvSVlbU2tFBlEz8Y6sHEv2zqUBK5XeA8t2AFTBEpFOyQ5Edlaw/F7qEhaeugq8FR9Sf/wP9vraJxZsIpIXYNhxZxFTI5/ctVubdp9pYHoMBSxxDcdxeOKTzTz8/gYsJ93Z2eOBQfY6bq//EXOCk/hP8bfJ85mEAyZ5Pi/PLKqgIODlTzNGURL241/7Co43LyuVnL0yDGIjL6bg5R/g3b6UhhFfIdVjVNueczfxIdPI++QfhD76K2b1GuqP/Fa7nl+yw50BCwe8QSwFLBHpxJxgt8warHDAJJ6yMVe+gIFDfOCkPb5uS02cN1Zt56vH9lOX1Fxn+pqtwTIMg5DPoy6C4gr1CYtb53zGc4u3MKpXAWWFQWzHIZCq4caKW8mjgRMTr/Pnmm9Qn7KpjaeoT1iUhP3ccf6oTOMK/9pXSPQ9oUWlqy3Ehk4n/OYvAKgb8//a/Hy7S/U6BitcSt77fwIg0f/Udh+DHDxXBqzYqK/jHz6ZlJ2ew64pgiLSGdnB5hUsAO/K57EKy7GK97wIedanm7EdOGdUWXsMU9qQ4/E2myII6U6CqmBJrltZWcd1Ty1mzY56vnViOZcc3x+PYaQ7883+On5nCw2HXUTJogd46AwvqZ6jWz2OJ7oqXck5op02TveFqJ58F5h+nMZur+3K8BAffBZ5C/5GqvgQ7MK+7T8GOWiu/OjTigzCGTJxlwqWKy9TRHKcE4jssgbLS5gG8ja+ka5e7WFev+04PPXJZo7tH6FvRBtF5jyPv9kUQYCgz1STC+n0qhqS/Pejjby4dCuLK2qabZT+zKIKLr7/Q6piSe6cMYpLx5SnwxWQ985vCax5idpxN1E35v/hGB78q17Y43n8a18BINH/lLa8nGaS/U4m2XtMu51vd/Eh0wBIlKt6latcWcFqsnMNVgcPRESkFXawG95d2rSf7FmAx06S2Mv0wHfW7GBjdZzvjDuATSel03E83mb7YIEqWNL5LdxUzfVPL2ZTdbzZ45GQj+5hP8u31XFk3yJumXoo3fN3Tuvzr3yO8Hu30zD8i8RGfBUMg2SvYwmsep7643/Y6rn8a18hVTQAu2hAW15Sp5IqO5qacTeRGDylo4ciB6iLBCwlLBHpfJxgYwXLccgPeJlovk/CFyHZ69g9vubJTzZTFPRyypDu7ThSaTMeL4adaPZQyOdRwJJOyXEcHvpgA3fMW0Vpvp97vjiacMBkfTTG+mgD66INbIjGuGJsOZeM6d9sDby5/TMK5lxFsufo9Ea+jRWtxMDJ5L9xE56qNdhF5c1PaMXxb3iT2PAvtOdldjzDIHb4Nzp6FHIQXB2wmqYIag2WiHRGdqBbenpYsp4Cr8NYzwds7HEaYU/rf5or6xK8srySLx7ZG79K866we5t2gJDfZEd9cg+vEOkY1bEk//vcMuatqOSUISXcOPkQCoLpv1VDe+Tv9bVGvJrCZy8Fb4jqM+9p1nY8PnAS+W/cRGDVCzTsts7Kt/FdjFSDGj1IznH1HTplqYugiHReTuNmw554lJ5VH1Jk1LOi2573eJn9aQWW7XDOqF7tNEJpc6YPw2oepkI+k5jWYEkn8sH6KF/59we8uWo715w6mNvOPiwTrvbJtih48XuY1WupPuMv2Pm9m3+7aACp4kPwr3q+xUv9a1/G8fhJ9B6bjcsQaTeurmClHFWwRKTzspsCVmwHPTbOJeb4WBY+lsP38PxnFldwRJ9CBpbktdsYpY21VsHymdRriqB0MMdxeGPVdv71zjo+3FBNr8IA935pNCN6FX6u44Tfvo3AmrnUnHzzHhtHxAdOJu+DOzFiO5p17vOvfYVk7+PAHz6oaxFpb66uYFmqYIlIJ9b0RsKI7SB/3Rxes0dRZflafa7tOKzZ3sDoPkXtOURpY+k27a1VsBSwpGOkLJvZn1Zw4b/e5/tPfMrG6jgPDHuD54c+zYjun+9z+cCyJ8n74E80HHYRsZFf2+PzEgMnYTg2/tVzMo95ajfh3b6URL/xB3wtIh2lS1SwvKYCloh0PnYgAoB//Rt4azfwClMx4q2/sd5elyBlO5QVtP1Gm9KOPL4WbdpVwZKsc2ww9v6Zem08xcxPNvPQBxuoqIkzuHse/3vmIUzpGaX7f+7CcGxSG9+g+oy/YBUP2+cpvVsWUPDStSR6HU/tyb/Y49YTAKmeh2OFywisep74oRcA4F/7KgCJ8lP2/zpFOglXV7BSljYaFpHOq6mCFVj6XxwM3vYd32wvmV1trkm3Qy5VwHIVx2xtiqCHpOVk7mEiB8xxCC78NyX3jiB/7rUYiZoWT6moiXPHqys56+63+cOrK+ldFOT3547goa8dzZTDSil899c43jyqJ/0JT6ySbv+dSmDpo3s9rVG/lcJnL8UOlVB9xl/B9O99nIaHxMBJ6VCVagDAt/YVrHDpXjddF+ms3F3BsjVFUEQ6LzuQnu5n1lWQ7HUsyWgxtXuoYFU0BqyyQgUsV/F4W1Sw8vwmAA1JmwLT1Z+DShvy1FWQ/9IPCKx9mVTJYQSX/hf/hjepOf33JHuPYdmWWv47ZzmzPtkEjsNpw3pw0TF9OaysIHMM7+b3Cax8jrrjriU+dDrJ3sdT8MJ3KZxzNQ0b5lM77pfg223DcytO0XNX4IntIHrekzh5+7elRHzgJEIL/4V//Rsk+p+Cf/1rxAeesdfKl0hn5eqApTbtItKpeYM43hBGqoH4gEmEF3upTeyhgtW4oWdZQbDV70tuaq1Ne9DXFLCs/e/UJrIL//KnKXjlOgwrRs24XxAbdTHezR9QMOcqip64gFmhc/jhjumY/hBfOKI3XzqqD72Ldvvb4jiE59+CHepO/egrALDDZVRNf5i8d39P3nt34Nv4NlZk8C4hyMBTvwXflo+pnvRnUj1G7veYk31OwPYX4F/5HHawG554Fcn+p2TnByLSzlz9l1sbDYtIZ2cHu2HWNpAYNJn8FbXU7WWKYJ7PJD9gtvMIpU15vBh2Chwn8yY1rzFgaR1W1/O5PhhONeBp2I5hxSEVw7DiGFac4KKHCC59jGTP0dScfgdWt8E0JC1mbe7NU4lb+Vrqb1zU8AQndPsY7/n3kCwa3Orh/Wtewr/xbWpO/mXzLn4eL/XH/5Bkr+MIv/cHPPVb0o87DpAef+1JPyc+9OzPd/Gmn0T/UwmsnoOd1xPH8JDod9LnO4ZIJ+HqgKUKloh0dnawGMcXxooMIj+wiK21iVafV1ETp7QwgKHpMu7iaVybYicz61RCjQFLnQTdy6xcSvidX1M77hfY+b1IWTaPfbyJe99aS30iRd9IiPLiPPp3C9G/W4hBJXkc0jMfX+OUUU/1Oro9Og1Pw7YWx7bwMKvoqzzoXEDlrCjRhrfY0ZAkaTmMKCuAE3/Ndv8Cur36IzwPT6X6tD+QGHJW84M4NuG3/g+rsJzYYV9u9RqS/ccT7Z/dDn+JgZMILn+K0ML7SJUe2axlu0gucXXA0hosEens6k76GU7jG+v8gEndHqcIxtRB0IUcT+Nt2E7tErDSb6JVwXKv8Du/IbDyOTx1W5k56i/c8cZ61u5o4Jj+EYb3zGftjgZWVdbx2orKzHuZgNfDiLICjuwT5jtrroJUnKVH/pxF220Wbk2wImoRx892byn18b5EPAY9C/wM7ZlPt5CPU4aUcHjvQgzDwOJ0dpS9QPELl1P0/Leorf4xDUf+T6aKGlj2BN7KxVRPvHPfDSqyKFE+AcfjwxOvokHt2SWHuTpgWQpYItLJJfuMzfw7P+Dda5OLQ3rmt9ewpL2Y6X3PDDvZOLmqeZMLcR9P1Wr8K5+jquQoiirep2bDTzALv83vzx3BiQOLm1WpU7bD5uoYy7bW8fGGKj5cX0Xk/dsp9n7MlYnv8tT8YRjAyF6FnDS2mBMHFTOsR3i/Kt1OXnesi54k8fi3yZ9/K2Z0FbXjbwEcwm//hmT3kZ9/mt9BcgKFJPuMxb9uHgmtv5Ic5uqAldIUQRHJIfl+L/VJC8t2mv3diqdsttcn1UHQhTIVLGtnJ8FMk4uEKlhucPurK3lhyRYSlkMiZfP/+DsXejycvuFSrg724hLvbM454SxSg45p8Vqvx6BvJETfSIgJQ7vj2/gWRU/OZF2fs+ldeiH/WxjkhAHFRPJa36B8n3whaibdiRUZSPi92zGr15HsMwazZh01p9y6z72z2kLDqEvA8JDqObrdzy2SLS4PWOlP/1TBEpFcEG5sYFGXSFEY3PmGaUuNOgi6VmPAalbB2qWLoOS+OUu3EvKZjBscocio48Jl81gSmcQlw4/lxEMnkXxmK5FXfsSO7sOxSva855MR20HBi9/DKuxP6MzbuNSfpYq24aH++B9iFQ2k4OUf4t/wBok+J5DsoCl6iYETSQyc2CHnFsmW/QpY8+bN4+abb8a2bS644AKuuOKKZt+/5ZZbePvttwGIxWJUVlby3nvvATB8+HCGDUvv+N2rVy/+8pe/ZHP8e6UpgiKSS/L96T/JtXGrWcDaXBMDtMmwGzmZJhc71941rcFSwMp9lu2wtTbO147rx/+cNJDQB3/CbzfQd+L3uaB7bwCqJ99Ft0fOoPC5K4heMBvHX9DyQI5Dwcs/wlO/jej5T+JkK1ztIn7oDOzCvoTn/x+1J/1c+0+JHIR9BizLsrjpppv4xz/+QWlpKTNmzGDChAkMGTIk85zrr78+8+9///vfLFq0KPN1MBhk5syZWR72/lGTCxHJJU0t2Gt3a9We2QNLUwTdZ5cKVpOQT2uw3GJ7fQLLgZ75AbAShBb8nUTfcVjdD8s8xw6XUn3GXRQ9+UUK5l5D9Rl3twg3wUUPEFj5LLVjb2jTqXPJ3mOInv9kmx1fpKvYZ8BasGAB5eXl9OvXD4CpU6cyd+7cZgFrV7Nnz+Z73/tedkd5gCztgyUiOSQcSP9Jrttt7U1F4xTBnvkKWK7jaaxU7rIGK+D1YKAugm7Q9L/d0oIAgeWzMOsqqD311y2el+w9hrqx15P/5i/o9shk7FB6+4b0/+URXPooiX4n03DkN9v7EkTkAOwzYFVUVFBWVpb5urS0lAULFrT63A0bNrB+/XrGjBmTeSwej3Peeefh9Xq54oorOP300/d6PtM0iETy9nf8ezmOB18gfeMq7pZHYegAF4B2QqbpycrPqLNx63WBe6/NrdcFHXNtvWrTb7Idn9ns3DviFj3yA/TsfvDTgtz83ywXOeYubdobGYZBnt/UPlgukAlY+X5Cr9xDqtvQPXbHazjiCoxkHb7N72Mk6/A0VGIk6zGSdVhFA6k57fcd0nRCRD6/rDa5mD17NpMnT8Y0zcxjL7/8MqWlpaxbt46LL76YYcOG0b9//z0ew7IcotH6gx5LJJJHbV36D1tdbQw7ntzHK3JHJJKXlZ9RZ+PW6wL3Xptbrws65tqcRPrvVMX2umbnXrutjh75/qz9bcy1/2Y9erSyJsUtGtdg7TpFENKdBOvVRTDnNQWsAXUf4tu2kJpTb9tzSDIM6o+7ph1HJyJtZZ8fhZSWlrJ58+bM1xUVFZSWlrb63GeeeYapU6e2eD1Av379OO6445qtz2prmTbtWqgpIjmgaYrg7nthba7RJsNu1Wyj4V3k+TxqcuECFTVxAl4P3Rf9HTtUQmzYuR09JBFpB/sMWKNGjWL16tWsW7eORCLB7NmzmTBhQovnrVixgurqao488sjMY1VVVSQSCQC2b9/OBx98sMe1W20hZTWuwTIVsESk88v3t2xy4TgOFTVxNbhwq1aaXEC6gqUmF7lvS02cY8LbCKyZQ8PIr4E31NFDEpF2sM8pgl6vlxtvvJHLLrsMy7I4//zzGTp0KLfffjsjR47ktNNOA9LVqylTpjTbPXzFihX87Gc/wzAMHMfh8ssvb9+A5TgYgEcVLBHJAQGvB6/HaFbBqo6laEjaatHuUk5Tk4vdAlaez1STCxeort7BNTyEYwZoGHlxRw9HRNrJfq3BGj9+POPHN99w7qqrrmr2dWudA4866ihmzZp1EMM7OCnLUfVKRHKGYRjkB7zUJXZWsDZnNhlWwHIlMx2wDKt5wAr5TGoTqdZeIbkgWU/ok39wT/SPFFFL3TFX4eR17+hRiUg7yWqTi87Gsh3tgSUiOSU/YDabIpjpQlYY7KghSVvKVLCah6mQ32RLbbwDBiQHJdlA6NN/k/fBn/A0VPKKNZrlw7/L9OPP7OiRiUg7cnXAStk2pgKWiOSQfL+32T5YTZsMa4qgO+1scrF7BcujNu05xqjfRrf/nolZu4lE33GsG/E9vj4zxY9Lh3b00ESknbk6YKUrWNozQkRyR7hFBSuGzzQoznPPXn6yi8YK1u5NLkI+k3o1ucgpvi0fYdZuonriH4kPO5dVG6qAj/XhiEgX5Or0kbIdVbBEJKfk+73NmlxU1MQpLQioWY9LOeYepgj6TLVpzzFGLApAsjTdTXnnJsMKWCJdjesDltZgiUgu2X0N1ubquBpcuNke2rTn+UziKRurcT9H6fw88SgATiACwJba9DY1qmCJdD2uDliWKlgikmPyA95m3eM2N1awxJ2cPTS5CPrSt+dYSlWsXGHEojgYOIFCIF3ByvOZ5AfMDh6ZiLQ3VwcsVbBEJNeEA17q4haO45CyHbbWxtVB0M2aKlhWotnDeY2bTjckFLByhSceTYcrI/3Wqml6r6HpvSJdThdocqE/bCKSO/L9Jg5Qn7SoiaWwHe2B5WaOx5/+RytrsAAa1OgiZxixaGZ6IKQDVs8Cf8cNSEQ6jOsrWJoiKCK5JBxIf+5VG7d2LpJXwHIvs6lNe+sBq16NLnKGEa/CDkYyX2/R9F6RLsvVAUsVLBHJNfmNU8Nq46nMHlhlhXqT5lp7aNPetG6nOpZs8RLpnNJTBCMAJC2byrqEApZIF+XqgJWybQUsEckp+ZkKVkoVrK7A8OAYHrCaB6l+kRAAa7Y3dMSo5AAYsWimgrW1NoED9FSLdpEuyeUBSxUsEcktmYCVsNhcE6cg4CXsd/VyWfH4WlSwSgsC5PlMVm+v76BByefliVftbNHe9OGIqs8iXZKrA5batItIrmmaGlYXT7G5OqbpgV2A4/G1WINlGAYDSvJYWamAlRMcGyMexQ4UAaj6LNLFuTpgpStYrr5EEXGZfH/zCpbeoHUBHm+LChbAwJI8VbByhJGoxXBsnMYpggpYIl2bq9OHZTt4TVWwRCR3hHepYKkLWRfRSgULYGBxHltrE9TGW35POhcjXgWA3ThFsKImTthvanqvSBfl6oCVsh1MbfAnIjkkz2fiMdKL5KtiKe2B1QU4prdFkwuAAcV5AKzSNMFOzxOPAmQqWFtq9eGISFfm+oClCpaI5BLDMAj7vazYVgdAWWGwg0ckbc7jb3WK4KCSxoClaYKdnhGLAuDssgZLAUuk63J1wLJUwRKRHJQfMFneFLD0Js31HI+31SmCvYuC+E1DFawc4GkMWLtOEVTAEum6XB2wUpatCpaI5Jz8gJft9emKhto8dwF7aHJhegzKi9XoIhc0rcFyghESKZvt9Ul6KmCJdFnuDliqYIlIDsr3pxtdeAzoEfZ38GikraXbtLcMWJBeh6VW7Z2f0bgGyw4UsaVWHQRFujrXByxVsEQk14QbNxvuHvbjNV39Z1oAzJYbDTcZWJLHpqoYsaTVzoOSz8MT24HjDYI3qBbtIuLugGVpHywRyUHhxgpWaYEaXHQJHh9YrbdiH1ichwOs2d7QvmOSz8WIVzVbfwVQmq+AJdJVuTp9pGwH06MKlojklvzGClaZ1l91CXtqcgHpChaok2Bn54lHm3UQBK2fFOnKXB+wvApYIpJjMgFLU4y6BMcbxEi1XqHq3y2EaShgdXZGLIrdtAdWTZzCoJeQz+zYQYlIh3F1wLJUwRKRHJSfmSKogNUV2Hk98NRVtPo9n+mhbySkVu2dXLqCFQHUol1EXBywHMdRBUtEcpKmCHYtdrgXnoate50muFoBq1Mz4lWZClZFTZyeWn8l0qW5NmDZTvr/q4IlIrmmKOQD0hvNivvZ+WUYjo2nfkur3x9QnMfaaAMpy27nkcn+8sRUwRKRnVwbsJpuRKpgiUiuGT+4hF+dfRhDuoc7eijSDuxwLwA8tZta/f7Akjws22FdNNaew5L9ZcUxUg04gQixpEVVLKWAJdLFuTdgNZawFLBEJNf4vR4mDO2OoY3SuwQrXAaAp25zq9/PdBKsrGu3Mcn+M2JVANjBCFtqE4DWT4p0da4PWJoiKCIinZmdn65gmbWtB6wBxWrV3pl54lEAnEARFTXpKmPPAn8HjkhEOpp7A1ZmiqBrL1FERFzACXbDMQN7rGCFfCa9CgPqJNhJGbEokK5gZfbA0ibhIl2aa9PHzimCHTwQERGRvTEM7HDpHtdgQXqaoAJW57SzghVhS016imDPfFWwRLoy18aPnQHLtZcoIiIuYYV77bGCBelpgmt2NGA1tciVTsOIN67BChRRURMnEvIR1CbDIl2aa9NHJmCZWoMlIiKdm51ftteANagkj3jKZlO1Ogl2Np7GKYJO4xRBVa9ExL0Bq3ENlqkuXCIi0snZ4TLM2k3gtF6hamp0sVqNLjodIx7FMTw4/gLtgSUigIsDlqUKloiI5Ag7XIZhxTEa1/PsbmerdgWszsYTj+IEisDwsKVWAUtEXBywklZjm3ZVsEREpJOzGlu172maYGHQR0nYr4DVCRmxKHagiIakRXUsRU8FLJEuz7UBK2U3tmlXBUtERDo5u2mz4T3shQUwsDikKYKdULqCFaGiuqlFuwKWSFfn2oBlaaNhERHJEU0By6zbW6v2MCsr63H2sE5LOoYRq8o0uAAFLBFxccBqmiLoVcASEZFOzg73BPZewRpQnEddwmJbXaK9hiX7wYhHMy3aQQFLRFwcsCztgyUiIrnC9GOHeuyzVTvASq3D6lQ8sWimgmUAPfMVsES6Otemj6Y1WJoiKCIiucDKL8NTu+cpggMaA9ZqBazOw7Ex4lXYgQiba2IUh/34va59ayUi+8m1fwUyGw0rYImISA6ww2WYe6lgleT5KAx6WVFZ146jkr0xEjUYOOkmFzVxyjQ9UERwc8Cy1ORCRERyh53fa69TBA3D4Oh+EZ5fvJXN1bF2HJnsiRGLAmAHI2yu1h5YIpLm4oDV2KZdAUtERHKAHS7DE9sBqT2Hp++fMggHh/+bs1zdBDsBT+PG0E1NLsoKFbBExM0BS23aRUQkh1hNe2HtpYrVqzDIt08ayBurtvP8kq3tNTTZAyNeBUCNUUAsZauCJSJAFwhYqmCJiEgusPOb9sLac8AC+MIRvRnZq4DfvryCaH2yPYYme+BpnCK4JRUC0BosEQFcHLAsBSwREckhdrgXsPe9sCA9M+OGScOojaf43Ssr2mNosgdG4xTBjfF0wFIFS0TAxQErqTVYIiKSQ5oqWHtr1d5kSPcwXz+uH88u3sKbq7a39dBkD5oqWOtj6WBVWhjswNGISGfh2oCljYZFRCSXOP4CbF94r2uwdnXJ8f0ZWJzHrS9+Rn3CyjxuOw7Lt9Xx5IJNrI82tNVwhfQaLMcbYlOdjddjUJzn6+ghiUgn4O3oAbQVNbkQEZFcY+f32ucarCZ+r4cbJg3l8oc/5raXljO0e5gP11fx0YYqqmIpAMYO6MYd549qyyF3aUYsih1M74FVWhDAY+g9h4i4OGBlpgia+mMnIiK5wQ6X7dcUwSaj+xQx44je/PejjQD0iwQ5eXAJR/Yt4tPNNTyxYBNba+P0yNfaoLbgiUdxAtoDS0Sac23AapoiaOrTJBERyRF2uAzfhjc/12uuHj+IEwZ245Ce+c2C1OG9C3ns4008u2gLXzuuX7aHKqSbXNjBCBVb4hzVr6ijhyMinYRrFyilrMY1WKpgiYhIjrDDZXjqt4Bj7/dr/F4PJw0qaVGlKi/O4/DehTy9qEKbErcRTyyK7S9ia60qWCKyk3sDlu3gMdB8aBGRLmjevHlMnjyZiRMncvfdd7f4/j/+8Q+mTJnCtGnTuPjii9mwYUMHjLIlK78Xhp3CqN+WleNNHVHKqsp6FlXUZuV40pwRj1JvFmI52gNLRHZyccCy1eBCRKQLsiyLm266iXvvvZfZs2fz9NNPs3z58mbPGT58OI899hizZs1i8uTJ/PrXv+6g0TZnh5s2G97/dVh7M3FYDwJeD7M/rcjK8aQ5T7yKasIAlBaoRbuIpLk2YFm2oz2wRES6oAULFlBeXk6/fv3w+/1MnTqVuXPnNnvOmDFjCIXSm8MeccQRbN68f5372trOvbCyM56CoJfxg0t4YckWEqn9n3Yo+yHVgJGKEXUaA1ahKlgikubaJhdJy1EFS0SkC6qoqKCsrCzzdWlpKQsWLNjj8x999FFOPvnkfR7XNA0ikbyDHp9pevZ8HO8gAPLt7dhZOBfAl47vzwtLt/JBRS1njCjb9wsO0F6vK8e1em01VQBEPYUAHNI3QkEwt/bB6nL/zVzArdcF7ro21waslG1rk2EREdmrmTNnsnDhQu6///59PteyHKLR+oM+ZySSt+fjOGG6e7zEt66hLgvnAjisJI8e+X7+885axvQpzMoxW7PX68pxrV2bWbmJYmB9vZ+w38SKJYnGkh0zwAPU1f6buYFbrwty89p69Cho9XHXJhDLVgVLRKQrKi0tbTblr6KigtLS0hbPe/PNN/nLX/7CXXfdhd/vb88h7pnhwc7riWc/NxveH6bH4Mzhpby5ajuVdYmsHber88TTFawNsYA6CIpIM64NWElLa7BERLqiUaNGsXr1atatW0cikWD27NlMmDCh2XMWLVrEjTfeyF133UVJSUkHjbR1drgsqwEL4KwRpVgOPL9kS1aP25UZsSgA62JByrT+SkR24dqApSYXIiJdk9fr5cYbb+Syyy5jypQpnHnmmQwdOpTbb7890+zitttuo76+nquuuorp06fzrW99q4NHvZOd3yvrAWtgSR6HlRXwtLoJZo0RjwKwss6vCpaINOPeNViW2rSLiHRV48ePZ/z48c0eu+qqqzL//uc//9nOI9p/VrgM39pXsn7cs0aUctvc5SzdUsshPfOzfvyupmmK4NpYgPFq0S4iu3BtBSulCpaIiOQgO1yGJ1mHkajJ6nEnHdIDn2moipUlRiyKY5jUElIFS0SacXXAUgVLRERyTbb3wmpSFPJx8uASnlu8hVjSyuqxuyJPPErCVwgYWoMlIs24N2BZtipYIiKSc+z8XgB46jZl/dgXHNGbaEOS/360MevH7mqMWJQGM92iWRUsEdmVewOWpgiKiEgOssJtU8ECOLpfhBMGduOf76yjOsf2bOpsPPEq6oz0Wrae+QpYIrKTApaIiEgnYofTe3aZWe4k2OQ7Jw2kJpbivnfWt8nxuwojHqWKfIrzfPi9rn07JSIHwLV/ESzbwTRde3kiIuJW3hB2sFvWW7U3GdYznzOG9+SRDzdQURNvk3N0BZ5YlEo7TFmhOgiKSHOuTSBJy8ZrqIIlIiK5xw6X4anN/hqsJt86cQC243DP/DVtdg63M+JRtqbUQVBEWnJtwLJsB6+pgCUiIrnHCpe1WQULoHdRkPNH92bWws2sqqxvs/O4lm1hxKvZlAhRpoAlIrtxbcBKWQ6mKlgiIpKD7PwyzDZocrGrbxzfj5DP5M+vr2rT8+SKFz5YzF1PvUDSsvf5XCNRjYHDNitPFSwRacG1AStp26pgiYhITrLDvfA0bAUr0Wbn6Jbn5yvH9OWV5ZUs2FjdZufJBZurYwTf+CVXrL2WX81djuM4e32+Ea8CIOrkaw8sEWnBtQHLUhdBERHJUZnNhuu2tOl5vnx0X4rzfNw5b+U+Q4Wb3TFvFUcYn1Fm7ODlT5bz8Id73yfME4sCUEVYFSwRacG1AStlOZgKWCIikoOsgn4ARJ6cQXj+/2FWLm2T8+T5TS4fW86HG6qZ+UnbTknsrN5fF+XNpWsZZKSbipzfv4E/vLKC+au37/E1RjwKNFawFLBEZDfuDViqYImISI5K9j2R6tPvwOo2mNCHf6b44dPo9vAkQh/chRHbkdVznTOqjCP6FHLzi5/x82eXUBtPZfX4nVnKdvjNSys4KX8TBukK3ndGOAzuHub6pxezeg8NQJoqWLVGPsVhf3sNV0RyhIsDlq0KloiI5CbDQ/yQ86ia9gCVX3+fmnE34XgD5M+/maJZXwHbytqpvKaHuy44nMvG9OfZxVu46N8f8PGGqqwdvzN7/ONNLN9WxzcH77zeUM1qfnvOCPymh2ueXEhVQ7LF65rWYPnyi/GooZaI7Ga/Ata8efOYPHkyEydO5O67727x/VtuuYXp06czffp0Jk+ezDHHHJP53hNPPMGkSZOYNGkSTzzxRPZGvg8py8HrcW1+FBGRLsLJ60Hs8G8QnTGL6tNvx7flY4KLH8rqObymh2+eOIB7vnQEBnDFIx9z1xurSe1HR71cFa1P8tc3V3Ns/wjDnZVYeaVYhf0xq1bRqzDIbWcfxuaaOD9+enGLzoKeximCeQXFHTByEens9plALMvipptu4t5772X27Nk8/fTTLF++vNlzrr/+embOnMnMmTP5yle+wsSJEwGIRqPceeed/Oc//+G///0vd955J1VV7fOpWMrWGiwREXGX+LDzSPQeQ/itX2V9qiDA4b0LeeBrRzHlsFL+/tZaLnv4YzZUNWT9PJ3BXW+spi6e4tpTB+Pb+gmpHiOxIgMxo+m29aP7FHHDxGG8uzbKb19c1uy1RixKHUG6F+Z3xNBFpJPbZ8BasGAB5eXl9OvXD7/fz9SpU5k7d+4enz979mzOOussAF5//XVOPPFEIpEIRUVFnHjiibz22mvZG/1epGxba7BERMRdDIPacTdhxKsJv/2bNjlF2O/lZ2ccwq1nDWfNjnq+8u8PeGnZ1jY5V0dZWlHLEws28YUj+zC4yMCMLifVYxRW0UDM6Epo7Kg4dUQpM0b34m9vrOa1FZU7DxCLEnXy1UFQRFq1z4BVUVFBWVlZ5uvS0lIqKipafe6GDRtYv349Y8aM+dyvzTa1aRcRETeyuh9Gw8ivEfz033i3Lmyz85x+SA/u/+pRlHfL4//NWsxtc5cTT+X+lEHHcfj1S8uJhHxcPrYc77ZFGI5NqufhpCKD8CRrMRq2ZZ5/9SmDOaxXIf/73FI2V8cAsOoqqXLC2gNLRFrlzebBZs+ezeTJkzFN84CPYZoGkUjeQY3DcRySlkM4z3/Qx+qMTNOj68oxbr02t14XuPfa3HpdXU398T8guPwp8uf9hOh5T0AbNVroUxTini+N5k+vreaB99fz8YYqbjlrOOXFufs79OLSrXy8sZobJg6lIOjFu3UBAKkeozA9PgC80ZUk83oAEPB6uOOLo5n+5ze5/ukl3P3FwzF3fMZKp48qWCLSqn0GrNLSUjZv3rk3RkVFBaWlpa0+95lnnuHGG29s9tp33nmn2WuPO+64vZ7Pshyi0dbbou4vy06X9lOJ1EEfqzOKRPJ0XTnGrdfm1usC915bLl5Xjx4FHT2ETscJFFE35joKXv4hgWWPET9kRpudy2d6uPqUQRzdr4j/fW4pX7v/Q756bF9mHNGbSMjXZudtC/GUzZ2vrWJYjzDTRqZn2Pi2LsQOdccOl4GVAMCMriLZ+/jM68pLwlw/cSg3zF7C315dyE/q1rHIPoFxBcEOuQ4R6dz2OUVw1KhRrF69mnXr1pFIJJg9ezYTJkxo8bwVK1ZQXV3NkUcemXnspJNO4vXXX6eqqoqqqipef/11TjrppOxeQStSjQFLTS5ERMStYsO/SLLnaMJv3oKRqGn5BCe70/nGDS7h/q8exdH9ivjrm2uYdvfb/Oal5WysimX1PG3pwffXs6k6zvdPGZx5j+DduoBkj1FgGNgFfXA8PsyqlS1eO+nQnpw/uhcLPpoPwCKnXBUsEWnVPitYXq+XG2+8kcsuuwzLsjj//PMZOnQot99+OyNHjuS0004D0tWrKVOmYOwyTSESifA///M/zJiR/mTtO9/5DpFIpG2uZBcpO31T0RosERFxLcND7ck3E3l0GuH5/0d84ES82xbjrVyEt3Ix5o4V1B/9XeqPuzZrpywrDPK7c0eyfFsd97+3nkc/3sSjH23k9EN6cOm4QZQX+DvtvlDb6hL88+11jB9cwjH9I+kHUw2Y2z8jPnBy+muPF6uoPNNJcHffP2UwT61+AOKwyjuY/MCBL4kQEffarzVY48ePZ/z48c0eu+qqq5p9/b3vfa/V186YMSMTsNqLpQqWiIh0AanSI4gN/yKhhfcRWngfAFZ+L1Ilw8FOEVz8H+qPvSbra7SGdA/z8zMO4dsnDuCh9zfw5CebeH7JVorzfIwdWMy4QcUcX96N/EBWl3oflL+8sZqEZXPl+EGZx9INLixSPUZmHrOKBqU7CbYi4PXwhT7b2baiCLOgtNmHyiIiTTrPX74sapoiqI2GRUTE7epO+hmpsqOwigaQKhmOE+wGQGDxfyh86Rq8Wz4mVXpEm5y7tCDA1acM4rKx/Xl/cy0vLtzEaysqmf1pBabH4Ki+RVw6pj9H94u0yfn317IttTz1yWYuPLoP/buFMo83dWFM9Tg885gVGYh/3avpKZZGy/cRkaolREtH8aMxQ9p+4CKSk1wZsJoqWF5TnyyJiIi7Of4CYod9ucXjiYETcTxeAitmt1nAapIf8DJ9dG/Gl0dI2Q6fbKzm9ZXbeX7JFr71nwWcObwnV44fRPewv03H0RrHcfj9qyspDHq5dEz/Zt/zbl2AHSzGzu+decyKDMSw4nhqN2EX9Gl+MCuBueMz8o64osNDo4h0Xq4s8WQqWCrdi4hIF+UEu5HscyKBFc9kNs5tD16PwZF9i/jeyQN59JJj+MaY/sxZtpUZf3+XRz7YkLlHt5fXVm7nvbVRLh9bTmGweddD35ZPSDU2uGhiFQ0EaHWaoHf7Mgw7Sar7yBbfExFp4sqApQqWiIgIxAdPwaxeg7ltUYecP+gz0+u0vnY0o3oV8puXV3Dx/R/wwpItrNhWR9Jq242Lk5bN7a+uZEBxiPNH92r+zVQMc8eydMDahRVJr9Eyq1o2ujC3fZp+aY8RbTNgEXEFV04RTFmNTS5UwRIRkS4sPugM8l/9MYEVs6nvwFBQXpzHHeeP5OXPtvHbl1dww+wlAJgG9I2EGFiSx6CSPEb2KuTw3oUUZWF/rfqExW9eWs7aHQ38/twReM3mnyl7K5dg2CmSPZsHLDtchuMNtV7B2roQx5uHVTTgoMcnIu7lzoClCpaIiAhOqIRk7zHpgHX8D7PeTfDzMAyDCcN6cNKgElZV1rNqez2rKutYtb2BVZV1vLaiksbPRxlQHGJ07yIO713IoO55dA/76R72twhJe/LOmh388oVl9KlZwEN9PmVw+dgWz/Fu/QSgRQULw8AqGthqq3bvtkWkuh/WavMLEZEmrgxYmTbtqmCJiEgXFx88hYJ5P8Hcvgyr5JCOHg5+r4dDSvM5pDS/2eOxpMWiiho+3lDNgo3VvLx8GzMXbs583wC65fnoHvbTqzDIkX2LOKZ/hKE9wpm9t2rjKe6Yt5InFmymf7cQfy5/jZ6bX6Luw8HUH9N8Oxnv1gXYgSLsgn4txpiKDMLbOB0ww7HxbvuU+KHnZ+cHISKu5cqAldloWBUsERHp4hKDzsCZ91MCK5+hvhMErD0J+kyO6hvhqL4RAGzHYe32BtZXNbC1NsG22gRb6+Jsq02wans9r66oBKAo6OWofhGGl+bz2Meb2Fob5yvH9OWbx5XS41/zccwAee/+lkT5qc32u/JuXZhuz97Kh7FWZCCBlc+ClQQzPV3RU70WT7KWVHetvxKRvXNpwGraB0sBS0REujY7XEaq17HpaYLHfr+jh7PfPIbBgJI8BpTktfr9LTVx3lsX5b21Ud5dG+Xlz7YxoDjEvV86glG9C/GveQkj1UD16XcQfvNmCuZcxY4LZoM3CFYcb+USGkZf1uqxrcggDMfCrFmXaXqR2TNLAUtE9sHVActUwBIRESE+eAr5r/8cM7oyExhyXc+CAFMOK2XKYaU4jsO2ugTdQr7MOi3/6jk43jzig6dgB7sRefqrhN/+NXUn/hRv5dJ0u/VdNhje1c5W7at2Bqxti3AMk1Rx560Cikjn4MpVmpk27R5XXp6IiMjnEh90JgD+Fc908EjahmEY9MgP7GyC4Tj4V88h0W8ceIMky0+lYcRXCX10N74N8zMNLnbvINiktVbt3m0LsboNSVfARET2wpUJRBUsERGRneyCPiR7HpHedLgLMLctwqzdSGLAxMxjtSf+FKuonIK51+DbMB/bX4hdWN7q651gN+xAUbNW7d5tnzZbwyUisieuDlhagyUiIpIWHzwV39YFeKrXdvRQ2lxg9Ys4GMQHnLbzQV8eNaffjqd2A8HPnkyHpT11G969VXvdVsy6Cq2/EpH94sqAZamCJSIi0kx88BQAAiuebf4NK4mnZgM4dgeMqm34V79IqvQInLwezR5PlR1N/VHfSf979/2vdmNFBmUqWEZF455ZClgish9c3eRCFSwREZE0u6icZPeRBBc/hKdhK+aOlZjRFZjVazDsFIm+46g+824cf0FHD/WgeOoq8G35mLrjf9Tq9+uP/T6GlSB26Bf2ehwrMpDgssch1YCxuSlgHZb18YqI+7i6gqWAJSIislN82Ll4dywn9PHfMavXYBUPo+GIb1F37PfxbZxP5PHz8NRu6uhhHhT/mrkAxAec3voTTD91J/50n5su72x0sQaj4hOsgr44wW5ZHauIuJNLK1jpaQ6aIigiIrJTwxGXEx96NnZeT/CYzb6XLDuGwueuIPLY2VSd9W+skkM7aJQHx79qDlZ+H6yS4Qd1nJ2t2ldiVHxCUtMDRWQ/qYIlIiLSVRge7PxeLcIVQLL/eKLnPg62TeTx8/Ctf6MDBniQUg34188jMfD0PTew2E9WJB2wvNs+hcrlWn8lIvvNlQFLa7BEREQ+P6vHCKIznsIOl1E06ysElj7e0UP6XPzr38RIxYjv0p79QDn+AuxQDwIrnsXAUcASkf3mzoBlaaNhERGRA2EX9CF63uMky46mcM6VhOf/H9hWRw9rv/hXv4jjzSPZe0xWjpeKDMK7Y1n63wpYIrKfXJlALEdt2kVERA6UE4xQdfaDNBz2ZfI+uJPCZ76BEa/u6GHtnePgXz2HRP+TwRvMyiGbpgk6wQh2QZ+sHFNE3M+VAStTwTIVsERERA6I6af2lF9RM/4W/OteJfLotMy+ULszYlFIxdp3fLvxbluIWbc5K9MDm2QCVumog17TJSJdhyu7CDZVsLQGS0RE5CAYBrGRX8PqNpTC575J5L9nUTPxj1hF5fg2vYd383v4Nr+Pd8dnOOGemNMe2mf787biX/UiDgaJ8tOydsymVu1O2d43JRYR2ZUrA1ZTBUtTBEVERA5ess9YdlzwDIXPXkrR7Iszj9uBIpJlRxMfMo28xQ8SmfkFomc/hNXeG/LaKQIrnyNVeiROXvesHTZVfCgOBk6fY7J2TBFxP3cGLMfBY4BH5XwREZGssAv7Ej3vSUIL78MOdiNVdnS6wmOkVxv4j7kQz7+mEZn5RaJnP4zVo32aQhjxagpf+DbeykXUnHpbVo9tRway48svUzBgFFQ1ZPXYIuJerl2DpeqViIhIlvlCNBz5LeLDv4jVbUgmXAFQPJjouY/ieENEZn4B79ZPWr4+WY9/1YsEPpuJWbkYrPhBDcdTtZrIY9PxrX+DmlNvI3bYlw/qeK1JX6feU4jI/nNnBcu28ZmuzI4iIiKdll00gOi5jxJ58gsUzfwSVdMewA6V4F8zl8DqOfg2zMfYJVQ5hokVGYhVPAyrsBwjWYfRsB1PbAee2HaM2A6sonLiQ88hPngqTqg481rfxrcofPZycByqzn6QZJ8TOuKSRURacGXAsmxVsERERDqCXdif6DmPEpn5BSKPn4NhpwBIFQ2kYeTXSAw4HTvYDe/2ZZjbl+HdvhRz2+J0kwp/AXaoGCfYDauwP06PUXgrPqDg1R+T/9pPSfQ7mfjQczBS9eTP+ylWYX+qpv4Tu7Hbn4hIZ+DKgGUYBkGv2dHDEBER6ZLswr5Ez3mUvA/uxCoaSGLAaZmOfE32uxGG42BuW0TwsycIfPYUgTVXApDoO47qyXfhBCNZHr2IyMFxZcC68Kg+nHVE744ehoiISJdlF/SmdvwtB38gw8DqMYK6HiOoG3s9vk3v4qlZT3zI2WD6Dv74IiJZ5sqA1bsoSCSSRzRa39FDERERkWwxPCR7Hw8c39EjERHZI3WCEBERERERyRIFLBERERERkSxRwBIREREREckSBSwREREREZEsUcASERERERHJEgUsERERERGRLFHAEhERERERyRIFLBERERERkSxRwBIREREREckSBSwREREREZEsUcASERERERHJEgUsERERERGRLFHAEhERERERyRIFLBERERERkSxRwBIREREREckSBSwREREREZEsUcASERERERHJEgUsERERERGRLFHAEhERERERyRIFLBERERERkSxRwBIREREREckSw3Ecp6MHISIiIiIi4gaqYImIiIiIiGSJApaIiIiIiEiWKGCJiIiIiIhkiQKWiIiIiIhIlihgiYiIiIiIZIkCloiIiIiISJYoYImIiIiIiGSJKwPWvHnzmDx5MhMnTuTuu+/u6OEcsB//+MeMHTuWs846K/NYNBrlkksuYdKkSVxyySVUVVV14AgP3KZNm/jqV7/KlClTmDp1Kvfddx+Q+9cXj8eZMWMGZ599NlOnTuWOO+4AYN26dVxwwQVMnDiRq6++mkQi0cEjPTCWZXHOOefwzW9+E3DPdU2YMIFp06Yxffp0zjvvPCD3fxebVFdXc+WVV3LGGWdw5pln8uGHH7rm2nKNW+5N4N77k1vvTaD7U65el1vvT26/N7kuYFmWxU033cS9997L7Nmzefrpp1m+fHlHD+uAnHfeedx7773NHrv77rsZO3YsL7zwAmPHjs3Zm7Rpmlx33XU888wzPPLIIzz44IMsX74856/P7/dz33338dRTT/Hkk0/y2muv8dFHH/Gb3/yGr3/967z44osUFhby6KOPdvRQD8i//vUvBg8enPnaLdcFcN999zFz5kwef/xxwD3/W7v55psZN24czz33HDNnzmTw4MGuubZc4qZ7E7j3/uTWexPo/pSr1wXuvD+5/d7kuoC1YMECysvL6devH36/n6lTpzJ37tyOHtYBOfbYYykqKmr22Ny5cznnnHMAOOecc5gzZ04HjOzg9ezZkxEjRgCQn5/PoEGDqKioyPnrMwyDcDgMQCqVIpVKYRgGb731FpMnTwbg3HPPzcnfyc2bN/PKK68wY8YMABzHccV17Umu/y4C1NTU8O6772b+m/n9fgoLC11xbbnGTfcmcO/9ya33JtD9KReva09y/fexK9ybXBewKioqKCsry3xdWlpKRUVFB44ouyorK+nZsycAPXr0oLKysoNHdPDWr1/P4sWLGT16tCuuz7Ispk+fzgknnMAJJ5xAv379KCwsxOv1AlBWVpaTv5O33HILP/zhD/F40n82duzY4YrranLppZdy3nnn8cgjjwDu+N/a+vXrKS4u5sc//jHnnHMON9xwA/X19a64tlzj9nsTuON/M7ty270JdH/KVW67P3WFe5PrAlZXYhgGhmF09DAOSl1dHVdeeSXXX389+fn5zb6Xq9dnmiYzZ87k1VdfZcGCBaxcubKjh3TQXn75ZYqLixk5cmRHD6VNPPTQQzzxxBPcc889PPDAA7z77rvNvp+rv4upVIpFixZx4YUX8uSTTxIKhVpMucjVa5POLdd/r9x4bwLdn3KRG+9PXeHe5LqAVVpayubNmzNfV1RUUFpa2oEjyq6SkhK2bNkCwJYtWyguLu7gER24ZDLJlVdeybRp05g0aRLgrusrLCzk+OOP56OPPqK6uppUKgWkpzLk2u/kBx98wEsvvcSECRO45ppreOutt7j55ptz/rqaNI27pKSEiRMnsmDBAlf8LpaVlVFWVsbo0aMBOOOMM1i0aJErri3XuP3eBO75++32exPo/pRL3Hh/6gr3JtcFrFGjRrF69WrWrVtHIpFg9uzZTJgwoaOHlTUTJkzgySefBODJJ5/ktNNO69gBHSDHcbjhhhsYNGgQl1xySebxXL++7du3U11dDUAsFuPNN99k8ODBHH/88Tz//PMAPPHEEzn3O3nttdcyb948XnrpJX73u98xZswYfvvb3+b8dQHU19dTW1ub+fcbb7zB0KFDc/53EdJTLMrKyjKfUs+fP5/Bgwe74tpyjdvvTZD7f7/Bvfcm0P0p164L3Ht/6gr3JsNxHKejB5Ftr776KrfccguWZXH++efz7W9/u6OHdECuueYa3nnnHXbs2EFJSQnf+973OP3007n66qvZtGkTvXv35g9/+AORSKSjh/q5vffee1x00UUMGzYsM2f6mmuu4fDDD8/p61uyZAnXXXcdlmXhOA5nnHEG3/3ud1m3bh3f//73qaqqYvjw4fzmN7/B7/d39HAPyNtvv83f//53/vrXv7riutatW8d3vvMdIL0+4ayzzuLb3/42O3bsyOnfxSaLFy/mhhtuIJlM0q9fP2699VZs23bFteUat9ybwL33J7fem0D3p1y8Ljffn9x+b3JlwBIREREREekIrpsiKCIiIiIi0lEUsERERERERLJEAUtERERERCRLFLBERERERESyRAFLREREREQkSxSwRHLY22+/zTe/+c2OHoaIiEiG7k3S1SlgiYiIiIiIZIm3owcg0hXMnDmTf//73ySTSUaPHs3PfvYzjjnmGC644ALeeOMNunfvzu9//3uKi4tZvHgxP/vZz2hoaKB///7ccsstFBUVsWbNGn72s5+xfft2TNPk9ttvB9K7u1955ZUsW7aMESNG8Jvf/AbDMDr4ikVEpLPTvUmkbaiCJdLGVqxYwbPPPstDDz3EzJkz8Xg8zJo1i/r6ekaOHMns2bM59thjufPOOwH40Y9+xA9+8ANmzZrFsGHDMo//4Ac/4KKLLuKpp57i4YcfpkePHgAsWrSI66+/nmeeeYb169fz/vvvd9i1iohIbtC9SaTtKGCJtLH58+ezcOFCZsyYwfTp05k/fz7r1q3D4/EwZcoUAKZPn877779PTU0NNTU1HHfccQCce+65vPfee9TW1lJRUcHEiRMBCAQChEIhAA4//HDKysrweDwceuihbNiwoWMuVEREcobuTSJtR1MERdqY4zice+65XHvttc0e//Of/9zs6wOdOuH3+zP/Nk0Ty7IO6DgiItJ16N4k0nZUwRJpY2PHjuX555+nsrISgGg0yoYNG7Btm+effx6AWbNmcfTRR1NQUEBhYSHvvfcekJ4ff+yxx5Kfn09ZWRlz5swBIJFI0NDQ0DEXJCIiOU/3JpG2owqWSBsbMmQIV199Nd/4xjewbRufz8eNN95IXl4eCxYs4K677qK4uJg//OEPAPzqV7/KLCTu168ft956KwC33XYbN954I7fffjs+ny+zkFhEROTz0r1JpO0YjuM4HT0Ika7oyCOP5MMPP+zoYYiIiGTo3iRy8DRFUEREREREJEtUwRIREREREckSVbBERERERESyRAFLREREREQkSxSwREREREREskQBS0REREREJEsUsERERERERLLk/wPQ4SG10E+oyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy_inet_decision_function_fv_metric\n",
      "\ttraining         \t (min:    0.680, max:    0.940, cur:    0.790)\n",
      "\tvalidation       \t (min:    0.754, max:    0.947, cur:    0.784)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.166, max:    0.573, cur:    0.340)\n",
      "\tvalidation       \t (min:    0.147, max:    0.515, cur:    0.313)\n",
      "Training Time: 0:49:38\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_lambda_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_representation_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_function_representation_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 449)]             0         \n",
      "_________________________________________________________________\n",
      "hidden1_1056 (Dense)         (None, 1056)              475200    \n",
      "_________________________________________________________________\n",
      "activation1_relu (Activation (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "dropout1_0.2 (Dropout)       (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "hidden2_512 (Dense)          (None, 512)               541184    \n",
      "_________________________________________________________________\n",
      "activation2_relu (Activation (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout2_0.1 (Dropout)       (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output_122 (Dense)           (None, 122)               62586     \n",
      "=================================================================\n",
      "Total params: 1,078,970\n",
      "Trainable params: 1,078,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7071c8a4f1ea46eeb70c94b06dea46d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Target Lambda 0.9447200000000001\n",
      "Binary Crossentropy Target Lambda 0.16287197450032392\n",
      "Accuracy Lambda Decision 0.93568\n",
      "Binary Crossentropy Lambda Decision 0.226880830749004\n",
      "Accuracy Target Decision 0.8957600000000001\n",
      "Binary Crossentropy Target Decision 0.42113506181448734\n"
     ]
    }
   ],
   "source": [
    "acc_target_lambda_list = []\n",
    "bc_target_lambda_list = []\n",
    "\n",
    "acc_lambda_decision_list = []\n",
    "bc_lambda_decision_list = []\n",
    "\n",
    "acc_target_decision_list = []\n",
    "bc_target_decision_list = []\n",
    "\n",
    "decision_function_parameters_list = []\n",
    "decision_functio_list = []\n",
    "\n",
    "for lambda_net in tqdm(lambda_net_dataset_test.lambda_net_list):\n",
    "    \n",
    "    target_function_parameters = lambda_net.target_function_parameters\n",
    "    target_function = lambda_net.target_function\n",
    "    \n",
    "    X_test_lambda = lambda_net.X_test_lambda\n",
    "    y_test_lambda = lambda_net.y_test_lambda\n",
    "    \n",
    "    network = lambda_net.network\n",
    "    network_parameters = lambda_net.network_parameters\n",
    "    \n",
    "    if config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['nas_type']['convolution_layers'] != 'SEQUENTIAL'):\n",
    "        network_parameters, network_parameters_flat = restructure_data_cnn_lstm(np.array([network_parameters]), config, subsequences=None)    \n",
    "      \n",
    "    decision_function_parameters= model.predict(np.array([network_parameters]))[0]\n",
    "    decision_function = generate_decision_tree_from_array(decision_function_parameters, config)\n",
    "    \n",
    "    decision_function_parameters_list.append(decision_function_parameters)\n",
    "    decision_functio_list.append(decision_function)\n",
    "    \n",
    "    y_test_network = network.predict(X_test_lambda)\n",
    "    y_test_decision_function = decision_function.predict_proba(X_test_lambda)\n",
    "    y_test_target_function = target_function.predict_proba(X_test_lambda)  \n",
    "    \n",
    "    acc_target_lambda = accuracy_score(np.round(y_test_target_function), np.round(y_test_network))\n",
    "    bc_target_lambda = log_loss(np.round(y_test_target_function), y_test_network, labels=[0, 1])\n",
    "    \n",
    "    acc_lambda_decision = accuracy_score(np.round(y_test_network), np.round(y_test_decision_function))\n",
    "    bc_lambda_decision = log_loss(np.round(y_test_network), y_test_decision_function, labels=[0, 1])        \n",
    "    \n",
    "    acc_target_decision = accuracy_score(np.round(y_test_target_function), np.round(y_test_decision_function))\n",
    "    bc_target_decision = log_loss(np.round(y_test_target_function), y_test_decision_function, labels=[0, 1])   \n",
    "    \n",
    "    \n",
    "    acc_target_lambda_list.append(acc_target_lambda)\n",
    "    bc_target_lambda_list.append(bc_target_lambda)\n",
    "\n",
    "    acc_lambda_decision_list.append(acc_lambda_decision)\n",
    "    bc_lambda_decision_list.append(bc_lambda_decision)\n",
    "\n",
    "    acc_target_decision_list.append(acc_target_decision)\n",
    "    bc_target_decision_list.append(bc_target_decision)\n",
    "    \n",
    "\n",
    "acc_target_lambda_array = np.array(acc_target_lambda_list)\n",
    "bc_target_lambda_array = np.array(bc_target_lambda_list)\n",
    "\n",
    "acc_lambda_decision_array = np.array(acc_lambda_decision_list)\n",
    "bc_lambda_decision_array = np.array(bc_lambda_decision_list)\n",
    "\n",
    "acc_target_decision_array = np.array(acc_target_decision_list)\n",
    "bc_target_decision_array = np.array(bc_target_decision_list)\n",
    "    \n",
    "    \n",
    "acc_target_lambda = np.mean(acc_target_lambda_array)\n",
    "bc_target_lambda = np.mean(bc_target_lambda_array[~np.isnan(bc_target_lambda_array)])\n",
    "\n",
    "acc_lambda_decision = np.mean(acc_lambda_decision_array)\n",
    "bc_lambda_decision = np.mean(bc_lambda_decision_array[~np.isnan(bc_lambda_decision_array)])\n",
    "\n",
    "acc_target_decision = np.mean(acc_target_decision_array)\n",
    "bc_target_decision = np.mean(bc_target_decision_array[~np.isnan(bc_target_decision_array)])\n",
    "\n",
    "\n",
    "print('Accuracy Target Lambda', acc_target_lambda)\n",
    "print('Binary Crossentropy Target Lambda', bc_target_lambda)\n",
    "print('Accuracy Lambda Decision', acc_lambda_decision)\n",
    "print('Binary Crossentropy Lambda Decision', bc_lambda_decision)\n",
    "print('Accuracy Target Decision', acc_target_decision)\n",
    "print('Binary Crossentropy Target Decision', bc_target_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02786644, 0.91616718, 0.60624075, 0.88591947, 0.03953329],\n",
       "       [0.78132991, 0.57253603, 0.19496315, 0.11438026, 0.57235053],\n",
       "       [0.84345932, 0.11760448, 0.57262199, 0.41297647, 0.93908842],\n",
       "       ...,\n",
       "       [0.8646321 , 0.14561154, 0.40179704, 0.07841369, 0.68174788],\n",
       "       [0.35746751, 0.99758493, 0.38932111, 0.83535797, 0.9646232 ],\n",
       "       [0.11789361, 0.17350065, 0.24059135, 0.84522561, 0.97066957]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29161674,  0.4584652 , -0.43084228, ...,  0.04972852,\n",
       "         0.22788163,  0.01619679],\n",
       "       [ 0.08819469, -0.0678779 , -0.11088647, ...,  0.35068122,\n",
       "         0.10858545,  0.01877911],\n",
       "       [ 0.2642992 ,  0.71580225,  0.37507978, ..., -0.5395654 ,\n",
       "         0.38570604, -0.26972985],\n",
       "       [ 0.377968  ,  0.14853731, -0.23176512, ..., -0.1642764 ,\n",
       "         0.08135869,  0.10158017],\n",
       "       [ 0.58881587,  0.44281018, -0.23438074, ..., -0.23245402,\n",
       "         0.6664687 , -0.12968457]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45681953, -0.44793075,  0.2816615 , -0.0492938 , -0.06176101,\n",
       "        0.4203166 ,  0.5660745 , -0.44356108, -0.08164763,  0.        ,\n",
       "        0.07437179, -0.41223946, -0.5200589 ,  0.        ,  0.5299024 ,\n",
       "        0.58825123,  0.6841029 , -0.36733192, -0.5526599 , -0.41840613,\n",
       "       -0.10449886, -0.41994047, -0.45792708, -0.576029  ,  0.31862995,\n",
       "        0.59918153, -0.06940714,  0.52732533, -0.07208212,  0.5470178 ,\n",
       "       -0.47873873,  0.493543  ,  0.51952726,  0.4665848 , -0.0254315 ,\n",
       "        0.02107218, -0.01309372, -0.02824431,  0.5001492 ,  0.        ,\n",
       "       -0.49418107,  0.55401963,  0.47315872,  0.520664  ,  0.61285615,\n",
       "        0.6254791 ,  0.47207806, -0.04172304,  0.45049247, -0.07797154,\n",
       "        0.393748  ,  0.        ,  0.        , -0.02696179, -0.04680119,\n",
       "        0.49806637, -0.45163694, -0.0701941 , -0.4050519 ,  0.59158844,\n",
       "       -0.06446765,  0.5156172 , -0.41351524,  0.38721523], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77346766],\n",
       "       [-0.57821596],\n",
       "       [ 0.55111617],\n",
       "       [-0.25357288],\n",
       "       [-0.21251057],\n",
       "       [ 0.90730554],\n",
       "       [ 1.0477254 ],\n",
       "       [-0.7378019 ],\n",
       "       [-0.16351095],\n",
       "       [-0.2718169 ],\n",
       "       [ 0.27941492],\n",
       "       [-0.77445775],\n",
       "       [-0.6732928 ],\n",
       "       [-0.15400417],\n",
       "       [ 0.65436   ],\n",
       "       [ 1.0790985 ],\n",
       "       [ 0.8534902 ],\n",
       "       [-0.7536076 ],\n",
       "       [-0.5020092 ],\n",
       "       [-0.8960187 ],\n",
       "       [-0.21985874],\n",
       "       [-0.66866595],\n",
       "       [-0.6072842 ],\n",
       "       [-0.5372026 ],\n",
       "       [ 0.18291588],\n",
       "       [ 0.97887987],\n",
       "       [-0.05379323],\n",
       "       [ 1.1296959 ],\n",
       "       [-0.22503386],\n",
       "       [ 0.7816153 ],\n",
       "       [-0.8748729 ],\n",
       "       [ 1.0477587 ],\n",
       "       [ 1.1145122 ],\n",
       "       [ 0.7316604 ],\n",
       "       [-0.09255359],\n",
       "       [ 0.07171552],\n",
       "       [ 0.00931237],\n",
       "       [ 0.28333545],\n",
       "       [ 0.8939037 ],\n",
       "       [-0.29366696],\n",
       "       [-0.3580425 ],\n",
       "       [ 0.87647027],\n",
       "       [ 0.89232785],\n",
       "       [ 0.9018052 ],\n",
       "       [ 0.97149974],\n",
       "       [ 1.1072325 ],\n",
       "       [ 0.91012734],\n",
       "       [-0.27103508],\n",
       "       [ 0.920401  ],\n",
       "       [-0.23905578],\n",
       "       [ 0.4427272 ],\n",
       "       [-0.18703991],\n",
       "       [-0.2574015 ],\n",
       "       [-0.14070949],\n",
       "       [-0.00701872],\n",
       "       [ 0.8501454 ],\n",
       "       [-0.3472605 ],\n",
       "       [-0.12021901],\n",
       "       [-0.7099311 ],\n",
       "       [ 1.0856591 ],\n",
       "       [-1.0580597 ],\n",
       "       [ 0.94303805],\n",
       "       [-0.8735245 ],\n",
       "       [ 0.49814296]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3486251], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.get_weights()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_test_network).ravel()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_test_decision_function).ravel()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.908, 0.996, 0.992, 0.928, 0.8  , 0.88 , 0.988, 0.916, 0.948,\n",
       "       0.992, 0.892, 0.844, 0.896, 0.94 , 0.948, 0.98 , 0.984, 0.984,\n",
       "       0.992, 0.984, 0.784, 0.976, 0.972, 0.992, 0.88 , 0.984, 1.   ,\n",
       "       0.992, 0.744, 0.988, 0.992, 0.92 , 0.984, 0.996, 0.9  , 0.92 ,\n",
       "       0.984, 0.972, 0.888, 0.964, 0.948, 0.792, 0.992, 0.868, 0.828,\n",
       "       0.932, 0.944, 0.988, 0.892, 0.976])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_lambda_decision_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BENCHMARK RANDOM GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3a710d2a84f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################################################################\n",
    "#################################################################################################### END WORKING CODE ####################################################################################################\n",
    "##########################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not optimize_decision_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if not optimize_decision_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
