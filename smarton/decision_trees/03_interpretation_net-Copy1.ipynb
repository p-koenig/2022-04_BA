{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'fully_grown': True,                      \n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'random_decision_tree_trained', # 'make_classification' 'random_decision_tree'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 1000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [64],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [1056, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.2, 0.1],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 2000, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, \n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 5000,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize1000_numLNets10000_var3_class2_random_decision_tree_trained_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1056-512_drop0.2-0.1e2000b256_adam\n",
      "lNetSize1000_numLNets10000_var3_class2_random_decision_tree_trained_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done 101 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-3)]: Done 1358 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-3)]: Done 7936 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   12.6s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:  5.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   14.7s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if noise_injected_level > 0:\n",
    "    lambda_net_dataset_training = load_lambda_nets(config, no_noise=True, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_evaluation = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_training, test_split=0.1)\n",
    "    _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_evaluation, test_split=test_size)\n",
    "    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8955, 367)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 367)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 367)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>6671.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.570</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.517</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>1.097</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>1.061</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>1.111</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>1.073</td>\n",
       "      <td>-1.239</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-1.164</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.060</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.046</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-1.149</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.306</td>\n",
       "      <td>-1.053</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-1.286</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>3274.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-1.392</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-1.346</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.757</td>\n",
       "      <td>-1.369</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.734</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.302</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.712</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-1.362</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-1.393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-1.834</td>\n",
       "      <td>0.721</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-1.984</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>-1.782</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>1.010</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.758</td>\n",
       "      <td>-1.804</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.833</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-1.705</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-1.806</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.336</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.130</td>\n",
       "      <td>1.505</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.581</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-1.647</td>\n",
       "      <td>1.063</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.747</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.752</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>3095.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.677</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.618</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>0.665</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.878</td>\n",
       "      <td>1.375</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.310</td>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.882</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.878</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>0.380</td>\n",
       "      <td>1.002</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>0.870</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.353</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.433</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.813</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.769</td>\n",
       "      <td>1.387</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.412</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.824</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.695</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>1.209</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.480</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>8379.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-1.157</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.828</td>\n",
       "      <td>-0.668</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.139</td>\n",
       "      <td>1.076</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.706</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-1.094</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.538</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.494</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>0.498</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.485</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.571</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.549</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.624</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.765</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.044</td>\n",
       "      <td>1.013</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>0.706</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.942</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>0.714</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>0.927</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>0.888</td>\n",
       "      <td>-0.216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "6671 6671.000    42  0.502 -0.378 -0.264 -0.008 -0.265  0.279  0.570 -0.147   \n",
       "3274 3274.000    42 -0.496  0.144  0.130 -0.219  0.271 -0.167  0.361  0.125   \n",
       "3095 3095.000    42 -0.098  0.327  0.458 -0.250 -0.521 -0.547  0.483 -0.043   \n",
       "8379 8379.000    42 -0.127 -0.364  0.387  0.083  0.305 -0.370 -0.362 -0.083   \n",
       "3043 3043.000    42 -0.214 -0.220 -0.451  0.410 -0.059  0.014 -0.464  0.360   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "6671  0.004  0.189  0.151 -0.510  0.033  0.414  0.258  0.517 -0.245  0.558   \n",
       "3274  0.172 -0.518  0.038 -0.159  0.378 -0.553 -0.339 -0.011 -0.498  0.567   \n",
       "3095  0.471  0.272 -0.133 -0.243 -0.397 -0.419  0.074  0.237 -0.522 -0.468   \n",
       "8379 -0.307 -0.541  0.348 -0.278 -0.199 -0.373 -0.200 -0.465  0.322 -0.287   \n",
       "3043 -0.289  0.027 -0.050 -0.094  0.461  0.044 -0.484  0.009 -0.571 -0.127   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "6671 -0.145 -0.534 -0.484  0.306 -0.361 -0.302  0.274  0.006  0.265 -0.190   \n",
       "3274  0.479  0.102  0.261 -0.037  0.176 -0.537  0.264 -0.489  0.187 -0.373   \n",
       "3095 -0.138  0.441  0.547 -0.051  0.316  0.389  0.004  0.349  0.178 -0.163   \n",
       "8379 -0.245  0.379  0.326 -0.079 -0.555  0.098 -0.366  0.109 -0.155  0.283   \n",
       "3043  0.167 -0.120 -0.495 -0.534  0.413  0.376 -0.156 -0.111 -0.042 -0.508   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "6671  0.121 -0.250  0.341  0.299 -0.007 -0.313 -0.256 -0.102  0.158  0.160   \n",
       "3274  0.281 -0.235  0.325 -0.223 -0.180 -0.052 -0.046  0.115 -0.129  0.207   \n",
       "3095  0.252  0.270 -0.272  0.185 -0.064  0.323 -0.081 -0.236 -0.132  0.280   \n",
       "8379  0.324  0.010 -0.076 -0.095  0.111 -0.309 -0.283  0.266 -0.184 -0.079   \n",
       "3043  0.179 -0.209 -0.030 -0.027  0.135  0.046 -0.240  0.129  0.306 -0.301   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3  \\\n",
       "6671 -0.292 -0.145  0.144 -0.186 -0.260 -0.022 -0.118  1.097 -0.695 -0.292   \n",
       "3274  0.248  0.186  0.115 -0.285  0.021  0.102 -1.343 -0.756 -0.312 -1.392   \n",
       "3095 -0.109 -0.316  0.200  0.192  0.339 -0.186  0.504  0.677 -0.525 -0.139   \n",
       "8379 -0.000 -0.104  0.281  0.054 -0.061 -0.095  0.291 -0.304 -0.086  0.225   \n",
       "3043  0.182 -0.223  0.031  0.276  0.012  0.312 -0.254  0.531  0.438 -0.287   \n",
       "\n",
       "       wb_4   wb_5  wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "6671 -0.252  0.002 0.091  0.197 -0.253 -0.268 -0.069 -0.169  1.061 -0.152   \n",
       "3274 -0.252 -0.625 0.098  0.730 -1.346 -0.268 -0.153 -0.172 -0.023 -0.152   \n",
       "3095 -0.252 -0.710 0.585  0.224  0.399 -0.268 -0.009 -0.170 -0.021 -0.152   \n",
       "8379 -0.252 -0.226 0.092  0.084  0.159 -0.268  0.068 -0.175 -0.021 -0.152   \n",
       "3043 -0.252  0.511 0.538 -0.326 -0.333 -0.268  0.494 -0.171 -0.023 -0.152   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "6671 -0.565  0.097  0.010 -0.197  1.111 -0.140 -0.301 -0.106  1.073 -1.239   \n",
       "3274 -0.476  0.113  0.015 -0.240 -0.757 -1.369 -0.302 -0.141 -0.734 -0.731   \n",
       "3095  0.014  0.618 -0.343 -0.237 -0.986  0.363 -0.301 -0.138 -0.909  0.665   \n",
       "8379  0.030  0.094  0.013 -0.238 -0.301  0.189  0.231 -0.140 -0.298  0.461   \n",
       "3043  0.017  0.595  0.507 -0.274  0.530 -0.277 -0.383 -0.405  0.498 -0.264   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "6671  0.097 -1.164  0.756  0.098 -0.224  0.026  0.962  0.127  0.176  0.060   \n",
       "3274 -0.285 -0.648  0.861  0.110 -0.224  0.024 -1.302  0.124  0.162  0.052   \n",
       "3095 -0.075 -0.917  0.396  0.355 -0.224  0.677  0.400  0.520  0.591  0.292   \n",
       "8379 -0.033 -0.233  0.228  0.093 -0.224  0.025  0.232  0.122  0.162  0.064   \n",
       "3043  0.492  0.485 -0.251  0.571 -0.224  0.024 -0.270  0.494  0.588  0.036   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "6671  1.040  0.032  0.003  0.145  0.054 -0.289 -1.122  0.050  0.113  0.100   \n",
       "3274 -1.373 -0.599 -0.712  0.079  0.068 -0.289 -0.631  0.045  0.103  0.091   \n",
       "3095  0.543 -0.561 -0.443 -0.021  0.296 -0.289  0.507  0.449  0.435  0.363   \n",
       "8379  0.349 -0.198 -0.258  0.058  0.054 -0.289  0.201  0.049  0.113  0.097   \n",
       "3043 -0.201  0.518  0.500  0.600  0.549 -0.289 -0.204  0.034  0.102  0.086   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "6671  0.143  0.114  0.136 -0.294  0.092 -0.212 -0.365 -0.184 -0.254  0.917   \n",
       "3274  0.347  0.117  0.021 -0.294  0.098 -1.362 -0.262 -0.184 -0.254 -1.393   \n",
       "3095 -0.085  0.597  0.347 -0.294  0.648  0.181 -0.161 -0.184 -0.254  0.457   \n",
       "8379 -0.033  0.113  0.128 -0.294  0.096  0.052 -0.024 -0.184 -0.254  0.262   \n",
       "3043  0.594  0.547  0.423 -0.294  0.136 -0.328  0.467 -0.184 -0.254 -0.226   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "6671  ...  -0.032   0.362  -0.077  -0.089  -0.096   0.497  -0.069  -0.026   \n",
       "3274  ...  -0.032   0.043  -0.088  -0.105  -0.112   0.042   0.479   0.575   \n",
       "3095  ...   0.238  -0.173   0.126   0.205  -0.022  -0.125   0.351   0.481   \n",
       "8379  ...  -0.032   0.140  -0.080  -0.100  -0.085   0.225   0.222   0.303   \n",
       "3043  ...  -0.031   0.331  -0.118  -0.056  -0.092   0.420  -0.089  -0.048   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "6671  -0.149  -0.031   0.000  -0.006  -0.065  -0.099  -0.095  -0.095  -0.068   \n",
       "3274  -0.490  -0.016   0.000   0.409  -0.072  -0.115  -0.112  -0.502  -0.067   \n",
       "3095   0.305   0.213   0.000  -0.376   0.055   0.023   0.015   0.397   0.232   \n",
       "8379   0.092  -0.029   0.000  -0.076  -0.061  -0.092  -0.090   0.193  -0.067   \n",
       "3043  -0.118  -0.018   0.000   0.203  -0.064  -0.092  -0.090  -0.055  -0.059   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "6671  -0.101   0.000  -0.063  -0.107  -0.054   0.000   0.000   0.474   0.374   \n",
       "3274  -0.295   0.000  -0.061   0.021  -0.292   0.000   0.000   0.034   0.116   \n",
       "3095   0.022   0.000   0.239  -0.067   0.136   0.000   0.000  -0.119  -0.155   \n",
       "8379  -0.099   0.000  -0.059   0.154   0.012   0.000   0.000   0.200   0.172   \n",
       "3043  -0.116   0.000  -0.036   0.268  -0.161   0.000   0.000   0.363   0.381   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "6671  -0.080   0.015   0.475   0.168  -0.074   0.000  -0.110  -0.005  -0.107   \n",
       "3274  -0.364   0.426   0.096  -0.103  -0.087   0.000  -0.134  -0.004  -0.305   \n",
       "3095   0.031   0.069  -0.093  -0.045   0.125   0.000  -0.004  -0.007   0.042   \n",
       "8379  -0.043   0.069   0.202   0.132  -0.080   0.000  -0.101   0.186  -0.114   \n",
       "3043  -0.146  -0.157   0.339   0.371  -0.116   0.000  -0.105   0.378  -0.129   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "6671  -0.113  -0.829   0.720  -0.287  -0.256   0.019   0.107  -0.189  -0.195   \n",
       "3274  -1.834   0.721   1.030  -1.984  -0.256   0.897   0.118  -0.648  -1.782   \n",
       "3095  -0.864  -0.362   0.518  -0.986  -0.256   0.878   1.375  -0.521  -0.814   \n",
       "8379  -0.895   0.803   0.533  -1.222  -0.256   0.934   0.112  -0.599  -0.901   \n",
       "3043  -0.793   0.655   0.624  -0.292  -0.256   0.740   1.012  -0.611  -0.800   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "6671  -0.272   0.264  -0.156  -1.017  -0.154   0.611   0.116   0.010  -0.161   \n",
       "3274  -0.272   1.010  -0.153  -0.003  -0.154   0.794   0.127   0.017  -0.158   \n",
       "3095  -0.272   0.256  -0.154  -0.006  -0.154   0.051   1.310   0.848  -0.156   \n",
       "8379  -0.272   0.554  -0.151  -0.008  -0.154   0.060   0.117   0.015  -0.155   \n",
       "3043  -0.272   0.765  -0.156  -0.004  -0.154   0.044   1.013   0.984  -0.130   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "6671  -0.892  -0.112  -0.260  -0.082  -0.976   1.074   0.134   1.046  -0.909   \n",
       "3274   0.758  -1.804  -0.257  -0.081   0.861   0.907   0.666   0.833  -0.862   \n",
       "3095   0.882  -0.771  -0.258  -0.081   0.878  -0.694   0.380   1.002  -0.617   \n",
       "8379   0.829  -0.859  -1.157  -0.081   0.829  -0.821   0.794   0.828  -0.668   \n",
       "3043   0.672  -0.748  -0.929  -0.849   0.706  -0.687   0.840   0.753  -0.608   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "6671   0.118  -0.227   0.031  -1.149   0.160   0.202   0.091  -1.300   0.050   \n",
       "3274   0.126  -0.227   0.031  -1.705   0.164   0.210   0.087  -1.806   0.955   \n",
       "3095   0.870  -0.227   1.353  -0.677   1.202   1.433   0.289  -0.843   0.812   \n",
       "8379   0.112  -0.227   0.033  -0.733   0.161   0.202   0.096  -0.910   0.969   \n",
       "3043   0.914  -0.227   0.033  -0.677   0.908   0.983   0.094  -0.804   0.749   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "6671  -0.007   0.196   0.063  -0.294   0.812   0.070   0.150   0.138   0.180   \n",
       "3274   0.993   1.336   0.071  -0.294   0.663   0.071   0.151   0.130   1.505   \n",
       "3095   0.706   0.577   0.813  -0.294  -0.387   0.757   0.654   0.509   0.769   \n",
       "8379   0.919   0.947   0.065  -0.294  -0.219   0.070   0.152   0.139   1.076   \n",
       "3043   0.732   0.868   0.942  -0.294  -0.253   0.071   0.149   0.134   0.846   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "6671   0.132   0.182  -0.298   0.113  -0.163   0.508  -0.187  -0.257  -1.306   \n",
       "3274   0.143   0.581  -0.298   0.120  -1.647   1.063  -0.187  -0.257  -1.747   \n",
       "3095   1.387   0.530  -0.298   1.412  -0.703   0.286  -0.187  -0.257  -0.824   \n",
       "8379   0.137   0.181  -0.298   0.118  -0.899   0.592  -0.187  -0.257  -0.903   \n",
       "3043   0.993   0.582  -0.298   0.221  -0.789   0.714  -0.187  -0.257  -0.792   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "6671  -1.053   0.508   0.935  -1.286  -0.185   0.157  -0.130   0.160  -0.211   \n",
       "3274  -0.853   0.918   0.752  -0.961  -0.156   0.161  -0.130   0.156  -0.214   \n",
       "3095  -0.670   0.073   0.695  -0.761  -0.176   1.209  -0.130   0.480  -0.211   \n",
       "8379  -0.706   0.196   0.563  -0.872  -0.206   0.157  -0.130   0.163  -1.094   \n",
       "3043  -0.644   0.318   0.584  -0.761  -0.785   0.927  -0.130   0.156  -0.899   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "6671   0.235  -0.129  \n",
       "3274   0.535   0.027  \n",
       "3095   0.749   0.079  \n",
       "8379   0.227  -0.100  \n",
       "3043   0.888  -0.216  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.407</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.485</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.494</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-1.008</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.858</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.619</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.707</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.016</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.619</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.903</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.971</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.785</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.886</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.782</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>689.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.998</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.814</td>\n",
       "      <td>1.069</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-1.018</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.786</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>1.134</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.853</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.201</td>\n",
       "      <td>1.031</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.956</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>1.032</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.906</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>4148.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>1.082</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>1.074</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.006</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.019</td>\n",
       "      <td>1.082</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>0.933</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.822</td>\n",
       "      <td>1.029</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.195</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.207</td>\n",
       "      <td>1.102</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-1.118</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.842</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.214</td>\n",
       "      <td>1.179</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.135</td>\n",
       "      <td>1.080</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.162</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.603</td>\n",
       "      <td>-0.152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2815.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.799</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.788</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.802</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.576</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.107</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-1.072</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>1.111</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.909</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.178</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.075</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>5185.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.562</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.712</td>\n",
       "      <td>-1.164</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-1.112</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.826</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-1.049</td>\n",
       "      <td>0.677</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.177</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.725</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.873</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.839</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-1.052</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.873</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>0.864</td>\n",
       "      <td>-0.323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "3466 3466.000    42 -0.165 -0.392  0.210 -0.089 -0.369 -0.041  0.111 -0.381   \n",
       "689   689.000    42  0.127 -0.542  0.507 -0.476  0.274  0.481 -0.116  0.012   \n",
       "4148 4148.000    42 -0.297  0.318  0.085 -0.001 -0.066 -0.340  0.215 -0.236   \n",
       "2815 2815.000    42 -0.338  0.312 -0.402 -0.424  0.081  0.288  0.403 -0.394   \n",
       "5185 5185.000    42  0.055  0.078  0.093  0.103  0.116  0.188 -0.000  0.000   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "3466 -0.025 -0.503 -0.008  0.230 -0.051  0.011  0.385  0.400 -0.257 -0.418   \n",
       "689   0.492 -0.112 -0.399 -0.265  0.511 -0.066  0.488  0.055  0.503 -0.330   \n",
       "4148  0.171 -0.000 -0.000  0.000 -0.022 -0.079 -0.349 -0.224  0.222 -0.179   \n",
       "2815  0.263  0.012  0.500  0.551  0.264 -0.512  0.123  0.192 -0.174  0.541   \n",
       "5185 -0.000 -0.089 -0.110 -0.155 -0.003 -0.000  0.014  0.000 -0.000 -0.000   \n",
       "\n",
       "       f6v0   f6v1  f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "3466 -0.512  0.569 0.418 -0.160 -0.063  0.232 -0.521  0.398  0.365  0.266   \n",
       "689   0.084  0.421 0.292 -0.289  0.379 -0.035 -0.135 -0.165 -0.207  0.489   \n",
       "4148 -0.000 -0.000 0.000 -0.001  0.014  0.035  0.000 -0.020 -0.057  0.000   \n",
       "2815  0.017 -0.150 0.543 -0.322  0.514 -0.421  0.177  0.468  0.499 -0.239   \n",
       "5185 -0.000 -0.000 0.000  0.056 -0.196 -0.000  0.035 -0.159 -0.000  0.000   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "3466 -0.019  0.033 -0.070 -0.110  0.249  0.259  0.347  0.025 -0.257 -0.074   \n",
       "689   0.156 -0.154  0.093 -0.084  0.018  0.059 -0.128  0.020 -0.242  0.062   \n",
       "4148  0.000 -0.000  0.000 -0.000  0.000 -0.000 -0.521  0.205 -0.000 -0.000   \n",
       "2815  0.044  0.209 -0.009  0.107  0.118  0.169  0.240 -0.106 -0.151 -0.227   \n",
       "5185 -0.000 -0.000 -0.229  0.191  0.000 -0.000  0.388 -0.479 -0.000 -0.000   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0  wb_1   wb_2   wb_3  \\\n",
       "3466  0.096  0.019  0.327 -0.248 -0.233  0.209 -0.139 0.170  0.244 -0.206   \n",
       "689   0.268 -0.276 -0.216  0.030  0.011  0.024 -0.142 0.227  0.081 -0.294   \n",
       "4148  0.391 -0.476 -0.000  0.000  0.000  0.000 -0.145 1.082 -0.615 -0.287   \n",
       "2815  0.244  0.073 -0.189  0.323  0.312  0.329 -0.141 0.799 -0.240 -0.289   \n",
       "5185 -0.000  0.000 -0.000 -0.000  0.009  0.001 -0.085 0.299  0.256 -0.160   \n",
       "\n",
       "       wb_4   wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "3466 -0.252  0.353  0.366 -0.221 -0.206 -0.268  0.372 -0.174 -0.020 -0.152   \n",
       "689  -0.252  0.002  0.190 -0.028 -0.013 -0.268  0.155  0.059  0.204 -0.152   \n",
       "4148 -0.252 -0.699  0.088 -0.271 -0.253 -0.268 -0.363 -0.173 -0.015 -0.152   \n",
       "2815 -0.252  0.011 -0.513  0.422 -0.253 -0.268 -0.044  0.626  0.788 -0.152   \n",
       "5185 -0.252  0.296  0.090 -0.355 -0.255 -0.268  0.334 -0.111 -0.038 -0.152   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "3466  0.270  0.407  0.292 -0.244  0.212 -0.166 -0.232 -0.150  0.164 -0.064   \n",
       "689   0.078  0.253  0.095  0.009  0.225 -0.213 -0.048  0.073  0.204  0.193   \n",
       "4148 -0.604  0.099  0.012 -0.232  1.074 -0.220 -0.302 -0.130  1.006 -0.787   \n",
       "2815 -0.475  0.080  0.014  0.495  0.802 -0.215 -0.304  0.595  0.751 -0.689   \n",
       "5185  0.281  0.102  0.010 -0.140  0.297 -0.132 -0.275 -0.119 -0.027  0.277   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "3466  0.353  0.287 -0.134  0.407 -0.224  0.533 -0.149  0.425  0.462  0.435   \n",
       "689   0.078  0.184 -0.155  0.242 -0.224  0.153  0.067  0.158  0.248  0.120   \n",
       "4148 -0.483 -0.773 -0.159  0.096 -0.224 -0.775 -0.149  0.061  0.160 -0.590   \n",
       "2815  0.090  0.810 -0.157  0.086 -0.224 -0.576 -0.148 -0.355  0.273 -0.464   \n",
       "5185  0.309  0.268 -0.126  0.100 -0.224  0.024 -0.152  0.110  0.160  0.317   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "3466 -0.108  0.371  0.358  0.497  0.347 -0.289 -0.041  0.531  0.546  0.493   \n",
       "689  -0.119  0.026  0.002  0.110  0.164 -0.289  0.160  0.114  0.180  0.135   \n",
       "4148 -0.123  0.019  1.082 -0.030  0.056 -0.289 -0.743 -0.667 -0.524 -0.538   \n",
       "2815 -0.125  0.033 -0.000  0.146  0.047 -0.289 -0.651 -0.532 -0.397 -0.412   \n",
       "5185 -0.047  0.307  0.265  0.423  0.052 -0.289  0.231  0.290  0.331  0.329   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "3466  0.484  0.395  0.485 -0.294  0.494 -0.239  0.312 -0.184 -0.254 -0.123   \n",
       "689   0.139  0.217  0.142 -0.294  0.219 -0.365  0.095 -0.184 -0.254 -0.185   \n",
       "4148  0.137  0.107 -0.469 -0.294  0.080 -0.359 -0.507 -0.184 -0.254 -0.195   \n",
       "2815  0.130 -0.460 -0.316 -0.294 -0.470 -0.356 -0.074 -0.184 -0.254 -0.184   \n",
       "5185  0.342  0.112  0.343 -0.294  0.091 -0.284  0.283 -0.184 -0.254 -0.056   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "3466  ...   0.314  -0.046   0.233   0.323   0.110  -0.001   0.370   0.432   \n",
       "689   ...   0.417  -0.088   0.355   0.393   0.343  -0.014  -0.071  -0.023   \n",
       "4148  ...   0.158  -0.040  -0.105  -0.103   0.065  -0.013  -0.079   0.535   \n",
       "2815  ...   0.514  -0.044   0.353   0.205   0.399  -0.016  -0.071  -0.026   \n",
       "5185  ...  -0.033   0.574  -0.090  -0.096  -0.376   0.568  -0.412  -0.246   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "3466   0.299   0.381   0.000  -0.135   0.233   0.196   0.165   0.339   0.339   \n",
       "689   -0.160   0.465   0.000  -0.172   0.375   0.348   0.326  -0.085   0.416   \n",
       "4148  -0.137  -0.029   0.000   0.056   0.100   0.051   0.048  -0.105  -0.074   \n",
       "2815  -0.157  -0.027   0.000   0.407   0.445   0.385   0.367  -0.103   0.462   \n",
       "5185  -0.345  -0.035   0.000  -0.475  -0.379  -0.320  -0.363  -0.333  -0.067   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "3466   0.169   0.000   0.287   0.022   0.178   0.000   0.000   0.011  -0.046   \n",
       "689    0.273   0.000   0.403  -0.113  -0.055   0.000   0.000  -0.043  -0.039   \n",
       "4148   0.028   0.000  -0.075  -0.088   0.025   0.000   0.000  -0.045  -0.045   \n",
       "2815   0.284   0.000   0.497  -0.088   0.022   0.000   0.000  -0.047  -0.044   \n",
       "5185  -0.373   0.000  -0.063   0.543  -0.405   0.000   0.000   0.560   0.586   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "3466   0.011  -0.040   0.033  -0.012   0.292   0.000   0.139  -0.014   0.171   \n",
       "689    0.041  -0.152  -0.060  -0.010   0.343   0.000   0.325  -0.006   0.256   \n",
       "4148   0.001   0.078  -0.075  -0.118  -0.100   0.000   0.034  -0.006   0.009   \n",
       "2815   0.185   0.266  -0.072  -0.019   0.224   0.000   0.360  -0.006  -0.016   \n",
       "5185  -0.421  -0.449   0.561   0.355  -0.374   0.000  -0.325   0.562  -0.360   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "3466  -0.789   0.670   0.371  -1.008  -0.256   0.714   0.858  -0.633  -0.779   \n",
       "689   -0.116  -0.554  -0.234  -0.284  -0.256   0.023   0.998  -0.742  -0.965   \n",
       "4148  -0.119  -0.943   0.933  -0.292  -0.256   1.053   0.109  -0.128  -0.197   \n",
       "2815  -0.116  -0.643   0.133  -0.286  -0.256   0.024   1.107  -0.516  -0.196   \n",
       "5185  -0.934   0.654   0.712  -1.164  -0.256   0.796   0.114  -0.577  -1.112   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "3466  -0.272   0.619  -0.156  -0.006  -0.154   0.184   0.896   0.838  -0.159   \n",
       "689   -0.272   0.069  -0.929  -0.763  -0.154   0.814   1.069   0.914  -0.871   \n",
       "4148  -0.272   0.701  -0.153  -0.008  -0.154   0.953   0.119   0.014  -0.158   \n",
       "2815  -0.272   0.128  -1.072  -0.840  -0.154   0.808   0.094   0.015  -0.969   \n",
       "5185  -0.272   0.826  -1.040  -0.832  -0.154   0.744   0.119   0.011  -1.049   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "3466   0.679  -0.741  -0.877  -0.078   0.707  -0.605   0.797   0.735  -0.612   \n",
       "689   -0.580  -0.143  -1.018  -0.795  -0.620  -0.786   0.129  -0.630  -0.051   \n",
       "4148  -0.963  -0.146  -0.260  -0.081  -0.976   0.994   0.822   1.029  -0.054   \n",
       "2815  -0.673  -0.142  -0.267  -0.906  -0.705   0.860   0.123  -0.751  -0.055   \n",
       "5185   0.677  -1.000  -1.177  -0.989  -0.782   0.764   0.867   0.725  -0.903   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "3466   0.857  -0.227   1.016  -0.670   0.939   0.896   0.619  -0.803   0.722   \n",
       "689    1.134  -0.227   0.859  -0.898   0.993   1.101   0.853  -0.100   0.053   \n",
       "4148   0.112  -0.227   1.195  -0.090   0.450   0.207   1.102  -0.100   0.037   \n",
       "2815   0.106  -0.227   0.940  -0.089   1.111   0.966   0.909  -0.102   0.054   \n",
       "5185   0.117  -0.227   0.033  -0.997   0.145   0.205   0.723  -0.873   0.839   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "3466   0.702   0.809   0.838  -0.294  -0.387   0.966   0.939   0.830   0.807   \n",
       "689   -0.004   0.201   1.031  -0.294  -0.517   0.860   0.904   0.910   0.175   \n",
       "4148  -1.118   0.164   0.064  -0.294   0.842   1.161   1.214   1.179   0.182   \n",
       "2815   0.002   0.209   0.060  -0.294   0.660   0.945   0.985   0.950   0.178   \n",
       "5185   0.741   0.847   0.059  -0.294   0.639   0.783   0.665   0.756   0.825   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "3466   0.865   0.903  -0.298   0.971  -0.796   0.557  -0.187  -0.257  -0.785   \n",
       "689    1.035   0.984  -0.298   0.956  -0.255  -0.084  -0.187  -0.257  -0.120   \n",
       "4148   0.135   1.080  -0.298   0.106  -0.233   0.859  -0.187  -0.257  -0.130   \n",
       "2815   1.164   0.835  -0.298   1.075  -0.231   0.068  -0.187  -0.257  -0.124   \n",
       "5185   0.141   0.839  -0.298   0.118  -1.052   0.763  -0.187  -0.257  -0.900   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "3466  -0.644   0.077  -0.021  -0.754  -0.682   0.886  -0.130   0.782  -0.859   \n",
       "689   -0.015   0.206  -0.540  -0.117  -0.783   1.032  -0.130   0.906  -0.211   \n",
       "4148  -0.009   0.913   0.952  -0.114  -0.150   0.142  -0.130   1.162  -0.211   \n",
       "2815  -0.013   0.499   0.469  -0.116  -0.883   0.935  -0.130   0.940  -0.210   \n",
       "5185  -0.772   0.748   0.692  -0.851  -0.591   0.873  -0.130   0.689  -1.114   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "3466   0.895   0.040  \n",
       "689    1.000   0.038  \n",
       "4148   0.603  -0.152  \n",
       "2815   0.385   0.081  \n",
       "5185   0.864  -0.323  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.721</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.680</td>\n",
       "      <td>-1.154</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>-0.991</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-1.364</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-1.053</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-1.349</td>\n",
       "      <td>-1.009</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-0.721</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-1.049</td>\n",
       "      <td>0.704</td>\n",
       "      <td>-0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.702</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.707</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.653</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.695</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-1.355</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.853</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.036</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>1.165</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.034</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.508</td>\n",
       "      <td>1.053</td>\n",
       "      <td>1.094</td>\n",
       "      <td>1.064</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.148</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.532</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.067</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-1.069</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.897</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.622</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.870</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.745</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.765</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>0.889</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.903</td>\n",
       "      <td>-0.712</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.877</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.928</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.562</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>0.884</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.882</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.855</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.883</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.834</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.728</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.548</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.253</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.888</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.023</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.961</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.183</td>\n",
       "      <td>1.337</td>\n",
       "      <td>0.954</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.815</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.022</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.505</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.703</td>\n",
       "      <td>-0.722</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.921</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.767</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>0.823</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0  f1v1   f1v2   f2v0   f2v1  \\\n",
       "7217 7217.000    42 -0.022  0.056 -0.206  0.152 0.126 -0.177 -0.183 -0.082   \n",
       "8291 8291.000    42 -0.425  0.249 -0.116  0.085 0.347  0.573  0.251  0.111   \n",
       "4607 4607.000    42 -0.073  0.234  0.091 -0.009 0.009 -0.067  0.093 -0.216   \n",
       "5114 5114.000    42 -0.364  0.381 -0.540  0.432 0.375  0.496 -0.362  0.034   \n",
       "1859 1859.000    42 -0.023 -0.259  0.265  0.202 0.408  0.285 -0.123 -0.033   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2  f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "7217 -0.004  0.159  0.129 -0.175 0.016 -0.020 -0.042 -0.000 -0.000  0.000   \n",
       "8291 -0.386 -0.009  0.027 -0.354 0.356  0.108 -0.550 -0.138  0.232 -0.223   \n",
       "4607 -0.010  0.000  0.000  0.000 0.018 -0.000  0.075 -0.095  0.206  0.003   \n",
       "5114 -0.290  0.510 -0.564 -0.127 0.332 -0.358 -0.400  0.318 -0.278 -0.560   \n",
       "1859  0.426  0.505 -0.036  0.325 0.054  0.177 -0.313  0.216 -0.560  0.130   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "7217  0.189  0.087  0.006  0.099  0.056  0.039  0.063 -0.025  0.000 -0.029   \n",
       "8291 -0.199 -0.179  0.450 -0.542  0.554  0.478  0.288  0.011  0.038 -0.168   \n",
       "4607  0.000  0.000  0.000 -0.153 -0.022  0.155  0.000  0.039 -0.177 -0.000   \n",
       "5114 -0.409 -0.086 -0.306  0.326  0.568 -0.396 -0.274  0.012 -0.562 -0.024   \n",
       "1859 -0.574 -0.086 -0.333 -0.423  0.550 -0.320  0.280 -0.509 -0.215  0.248   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "7217  0.403 -0.268  0.005 -0.002  0.125 -0.125  0.038 -0.038  0.001 -0.015   \n",
       "8291  0.255  0.022  0.273 -0.105  0.059 -0.351  0.349  0.095 -0.045  0.152   \n",
       "4607  0.000 -0.000  0.002 -0.002  0.308 -0.309  0.002 -0.008  0.070 -0.070   \n",
       "5114  0.268  0.116 -0.058  0.162 -0.095  0.286  0.240  0.216  0.002  0.045   \n",
       "1859  0.349  0.276  0.080 -0.012 -0.177 -0.319 -0.301 -0.223 -0.190 -0.226   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0  wb_1   wb_2   wb_3  \\\n",
       "7217  0.001 -0.002 -0.210  0.210  0.000 -0.000  0.057 0.124  0.156 -0.029   \n",
       "8291  0.049 -0.280  0.342  0.008  0.287 -0.269 -0.143 0.702 -0.029 -0.291   \n",
       "4607 -0.563  0.392  0.000 -0.000 -0.055  0.058 -0.031 0.234  0.173 -0.165   \n",
       "5114 -0.201 -0.247 -0.326 -0.253  0.062 -0.350 -0.144 0.882 -0.480 -0.287   \n",
       "1859  0.120 -0.073 -0.156  0.051 -0.334  0.177 -0.144 0.180  0.014 -0.292   \n",
       "\n",
       "       wb_4  wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "7217 -0.252 0.147  0.092 -0.087 -0.054 -0.268  0.231 -0.167  0.017 -0.152   \n",
       "8291 -0.252 0.012  0.085  0.305 -0.252 -0.268  0.052  0.471  0.684 -0.152   \n",
       "4607 -0.252 0.233  0.265 -0.133 -0.118 -0.268  0.309 -0.171 -0.020 -0.152   \n",
       "5114 -0.252 0.003 -0.581 -0.244 -0.252 -0.268 -0.150 -0.170  0.855 -0.152   \n",
       "1859 -0.252 0.003  0.267 -0.072 -0.057 -0.268  0.100  0.014  0.161 -0.152   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "7217  0.149  0.103  0.014 -0.721  0.124  0.010 -0.225 -0.648  0.132  0.165   \n",
       "8291 -0.383  0.102  0.012  0.388  0.707 -0.217  0.477  0.488  0.653 -0.580   \n",
       "4607  0.159  0.315  0.180 -0.233  0.225 -0.078 -0.141 -0.140  0.226  0.096   \n",
       "5114 -0.521  0.087  0.013 -0.166  0.883 -0.216 -0.302 -0.089  0.834 -0.703   \n",
       "1859  0.161  0.341  0.175 -0.034  0.176 -0.081 -0.092  0.028  0.155  0.105   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "7217  0.202  0.144  0.058  0.102 -0.224  0.025  0.036  0.117  0.160  0.056   \n",
       "8291  0.092  0.695 -0.156  0.094 -0.224 -0.500 -0.146 -0.293  0.158 -0.376   \n",
       "4607  0.264  0.153 -0.038  0.315 -0.224  0.217 -0.047  0.297  0.376  0.200   \n",
       "5114 -0.266 -0.728 -0.158  0.092 -0.224 -0.623 -0.147 -0.445  0.155 -0.508   \n",
       "1859  0.080  0.090 -0.101  0.312 -0.224  0.238  0.028  0.231  0.311  0.219   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "7217  0.110  0.158  0.150  0.296  0.060 -0.289  0.157  0.044  0.106  0.091   \n",
       "8291 -0.122  0.034  0.001  0.150  0.057 -0.289 -0.411 -0.447 -0.305 -0.312   \n",
       "4607  0.026  0.261  0.230  0.415  0.245 -0.289  0.112  0.205  0.295  0.265   \n",
       "5114 -0.120  0.031  0.907  0.084  0.045 -0.289 -0.654 -0.579 -0.451 -0.459   \n",
       "1859 -0.121  0.026  0.000  0.121  0.252 -0.289  0.079  0.205  0.263  0.226   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "7217  0.265  0.112  0.205 -0.294  0.096 -0.083  0.185 -0.184 -0.254  0.068   \n",
       "8291  0.138  0.108 -0.203 -0.294 -0.403 -0.359  0.036 -0.184 -0.254 -0.192   \n",
       "4607  0.377  0.296  0.294 -0.294  0.288 -0.171  0.238 -0.184 -0.254 -0.028   \n",
       "5114  0.134 -0.470 -0.383 -0.294 -0.539 -0.355 -0.269 -0.184 -0.254 -0.186   \n",
       "1859  0.139  0.289  0.220 -0.294  0.298 -0.680  0.041 -0.184 -0.254 -0.187   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "7217  ...  -0.032   0.355  -0.081  -0.099  -0.092   0.423  -0.238  -0.225   \n",
       "8291  ...   0.420  -0.048   0.249  -0.107   0.291  -0.013  -0.065  -0.023   \n",
       "4607  ...   0.518  -0.200   0.411   0.408   0.233  -0.201   0.478   0.535   \n",
       "5114  ...   0.557  -0.040   0.443  -0.108   0.457  -0.009  -0.074  -0.033   \n",
       "1859  ...   0.320   0.084   0.249   0.277   0.221  -0.014  -0.070  -0.025   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "7217  -0.225  -0.025   0.000  -0.326  -0.063  -0.095  -0.094  -0.224  -0.068   \n",
       "8291  -0.138  -0.028   0.000   0.157   0.345   0.285   0.255  -0.096  -0.075   \n",
       "4607   0.409   0.491   0.000  -0.301   0.451   0.391   0.341   0.415   0.427   \n",
       "5114  -0.115  -0.018   0.000   0.465   0.497   0.458   0.439  -0.108   0.498   \n",
       "1859  -0.151   0.367   0.000  -0.043   0.263   0.236   0.205  -0.091   0.311   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "7217  -0.218   0.000  -0.059   0.352  -0.266   0.000   0.000   0.397   0.394   \n",
       "8291   0.162   0.000   0.393  -0.100  -0.146   0.000   0.000  -0.044  -0.045   \n",
       "4607   0.344   0.000   0.461  -0.105   0.314   0.000   0.000  -0.144  -0.206   \n",
       "5114   0.379   0.000   0.548  -0.082   0.191   0.000   0.000  -0.036  -0.043   \n",
       "1859   0.138   0.000   0.305  -0.074  -0.144   0.000   0.000  -0.044  -0.045   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "7217  -0.285  -0.282   0.394   0.064  -0.082   0.000  -0.107   0.412  -0.222   \n",
       "8291   0.060   0.002  -0.079   0.102  -0.087   0.000   0.250  -0.005  -0.127   \n",
       "4607   0.147   0.158  -0.109  -0.160   0.404   0.000   0.296  -0.006   0.362   \n",
       "5114   0.299   0.495  -0.067  -0.086   0.047   0.000   0.426  -0.006   0.265   \n",
       "1859  -0.112  -0.027  -0.073   0.080   0.229   0.000   0.203  -0.007   0.094   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "7217  -0.888   0.576   0.680  -1.154  -0.256   0.707   0.115  -0.679  -0.991   \n",
       "8291  -0.116  -0.661   0.081  -0.287  -0.256   0.023   0.106  -0.738  -0.182   \n",
       "4607  -0.825   0.673   0.405  -1.069  -0.256   0.746   0.897  -0.672  -0.800   \n",
       "5114  -0.120  -0.752   0.565  -0.292  -0.256   0.005   1.253  -0.120  -0.195   \n",
       "1859  -0.118  -0.416   0.012  -0.288  -0.256   0.021   0.794  -0.566  -0.730   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "7217  -0.272   0.790  -1.364  -1.081  -0.154   0.376   0.128   0.015  -1.053   \n",
       "8291  -0.272   0.145  -1.062  -0.853  -0.154   0.919   0.123   0.014  -1.030   \n",
       "4607  -0.272   0.622  -0.152  -0.005  -0.154  -0.083   0.931   0.870  -0.148   \n",
       "5114  -0.272   0.193  -0.158  -0.908  -0.154   0.842   0.105   0.017  -0.158   \n",
       "1859  -0.272   0.159  -0.741  -0.536  -0.154   0.567   0.858   0.703  -0.722   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "7217   0.595  -0.900  -1.349  -1.009   0.647   0.689   0.808   0.676  -0.721   \n",
       "8291  -0.694  -0.143  -1.355  -0.943  -0.740   0.972   0.136  -0.853  -0.053   \n",
       "4607   0.694  -0.767  -0.895  -0.074   0.745  -0.639   0.814   0.765  -0.652   \n",
       "5114  -0.773  -0.149  -0.259  -0.082  -0.796   0.900   0.371   0.888  -0.058   \n",
       "1859  -0.435  -0.856  -0.820  -0.617  -0.477  -0.550   0.131  -0.406  -0.540   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "7217   0.122  -0.227   0.032  -0.822   0.161   0.203   0.095  -0.869   0.751   \n",
       "8291   0.113  -0.227   1.036  -0.084   1.165   0.195   1.034  -0.102   0.053   \n",
       "4607   0.889  -0.227   0.903  -0.712   0.910   0.910   0.301  -0.887   0.747   \n",
       "5114   0.112  -0.227   1.023  -0.091   1.260   0.193   0.961  -0.105   0.052   \n",
       "1859   0.921  -0.227   0.673  -0.598   0.812   0.915   0.709  -0.103   0.051   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "7217   0.741   0.861   0.068  -0.294   0.564   0.072   0.153   0.137   0.897   \n",
       "8291  -0.002   0.211   0.063  -0.294   0.508   1.053   1.094   1.064   0.181   \n",
       "4607   0.749   0.796   0.877  -0.294  -0.441   0.892   0.748   0.649   0.812   \n",
       "5114  -0.911   0.118   0.043  -0.294   0.730   1.005   1.068   1.035   0.183   \n",
       "1859   0.001   0.200   0.810  -0.294  -0.299   0.702   0.758   0.754   0.185   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "7217   0.139   0.558  -0.298   0.120  -0.943   0.734  -0.187  -0.257  -0.886   \n",
       "8291   0.136   0.952  -0.298   1.148  -0.229   0.072  -0.187  -0.257  -0.125   \n",
       "4607   0.899   0.763  -0.298   0.928  -0.801   0.562  -0.187  -0.257  -0.806   \n",
       "5114   1.337   0.954  -0.298   1.211  -0.237   0.258  -0.187  -0.257  -0.132   \n",
       "1859   0.829   0.747  -0.298   0.767  -0.878   0.088  -0.187  -0.257  -0.122   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "7217  -0.714   0.603   0.648  -0.856  -0.485   0.154  -0.130   0.162  -1.049   \n",
       "8291  -0.004   0.532  -0.649  -0.108  -0.938   0.146  -0.130   1.067  -0.211   \n",
       "4607  -0.686   0.108   0.098  -0.765  -0.733   0.884  -0.130   0.512  -0.213   \n",
       "5114  -0.020   0.673   0.815  -0.125  -0.907   0.499  -0.130   1.022  -0.211   \n",
       "1859  -0.014   0.138  -0.297  -0.119  -0.651   0.823  -0.130   0.763  -0.210   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "7217   0.704  -0.245  \n",
       "8291   0.226  -0.065  \n",
       "4607   0.880   0.195  \n",
       "5114   0.550   0.161  \n",
       "1859   0.720  -0.066  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy_inet_decision_function_fv_metric\n",
      "\ttraining         \t (min:    0.724, max:    0.961, cur:    0.961)\n",
      "\tvalidation       \t (min:    0.880, max:    0.964, cur:    0.964)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.112, max:    0.507, cur:    0.112)\n",
      "\tvalidation       \t (min:    0.100, max:    0.304, cur:    0.100)\n",
      "Epoch 29/2000\n",
      "22/35 [=================>............] - ETA: 9s - loss: 0.1119 - binary_accuracy_inet_decision_function_fv_metric: 0.9608"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_target_lambda_list = []\n",
    "bc_target_lambda_list = []\n",
    "\n",
    "acc_lambda_decision_list = []\n",
    "bc_lambda_decision_list = []\n",
    "\n",
    "acc_target_decision_list = []\n",
    "bc_target_decision_list = []\n",
    "\n",
    "decision_function_parameters_list = []\n",
    "decision_functio_list = []\n",
    "\n",
    "for lambda_net in tqdm(lambda_net_dataset_test.lambda_net_list):\n",
    "    \n",
    "    target_function_parameters = lambda_net.target_function_parameters\n",
    "    target_function = lambda_net.target_function\n",
    "    \n",
    "    X_test_lambda = lambda_net.X_test_lambda\n",
    "    y_test_lambda = lambda_net.y_test_lambda\n",
    "    \n",
    "    network = lambda_net.network\n",
    "    network_parameters = lambda_net.network_parameters\n",
    "    \n",
    "    if config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['nas_type']['convolution_layers'] != 'SEQUENTIAL'):\n",
    "        network_parameters, network_parameters_flat = restructure_data_cnn_lstm(np.array([network_parameters]), config, subsequences=None)    \n",
    "      \n",
    "    decision_function_parameters= model.predict(np.array([network_parameters]))[0]\n",
    "    decision_function = generate_decision_tree_from_array(decision_function_parameters, config)\n",
    "    \n",
    "    decision_function_parameters_list.append(decision_function_parameters)\n",
    "    decision_functio_list.append(decision_function)\n",
    "    \n",
    "    y_test_network = network.predict(X_test_lambda)\n",
    "    y_test_decision_function = decision_function.predict_proba(X_test_lambda)\n",
    "    y_test_target_function = target_function.predict_proba(X_test_lambda)  \n",
    "    \n",
    "    acc_target_lambda = accuracy_score(np.round(y_test_target_function), np.round(y_test_network))\n",
    "    bc_target_lambda = log_loss(np.round(y_test_target_function), y_test_network, labels=[0, 1])\n",
    "    \n",
    "    acc_lambda_decision = accuracy_score(np.round(y_test_network), np.round(y_test_decision_function))\n",
    "    bc_lambda_decision = log_loss(np.round(y_test_network), y_test_decision_function, labels=[0, 1])        \n",
    "    \n",
    "    acc_target_decision = accuracy_score(np.round(y_test_target_function), np.round(y_test_decision_function))\n",
    "    bc_target_decision = log_loss(np.round(y_test_target_function), y_test_decision_function, labels=[0, 1])   \n",
    "    \n",
    "    \n",
    "    acc_target_lambda_list.append(acc_target_lambda)\n",
    "    bc_target_lambda_list.append(bc_target_lambda)\n",
    "\n",
    "    acc_lambda_decision_list.append(acc_lambda_decision)\n",
    "    bc_lambda_decision_list.append(bc_lambda_decision)\n",
    "\n",
    "    acc_target_decision_list.append(acc_target_decision)\n",
    "    bc_target_decision_list.append(bc_target_decision)\n",
    "    \n",
    "\n",
    "acc_target_lambda_array = np.array(acc_target_lambda_list)\n",
    "bc_target_lambda_array = np.array(bc_target_lambda_list)\n",
    "\n",
    "acc_lambda_decision_array = np.array(acc_lambda_decision_list)\n",
    "bc_lambda_decision_array = np.array(bc_lambda_decision_list)\n",
    "\n",
    "acc_target_decision_array = np.array(acc_target_decision_list)\n",
    "bc_target_decision_array = np.array(bc_target_decision_list)\n",
    "    \n",
    "    \n",
    "acc_target_lambda = np.mean(acc_target_lambda_array)\n",
    "bc_target_lambda = np.mean(bc_target_lambda_array[~np.isnan(bc_target_lambda_array)])\n",
    "\n",
    "acc_lambda_decision = np.mean(acc_lambda_decision_array)\n",
    "bc_lambda_decision = np.mean(bc_lambda_decision_array[~np.isnan(bc_lambda_decision_array)])\n",
    "\n",
    "acc_target_decision = np.mean(acc_target_decision_array)\n",
    "bc_target_decision = np.mean(bc_target_decision_array[~np.isnan(bc_target_decision_array)])\n",
    "\n",
    "\n",
    "print('Accuracy Target Lambda', acc_target_lambda)\n",
    "print('Binary Crossentropy Target Lambda', bc_target_lambda)\n",
    "print('Accuracy Lambda Decision', acc_lambda_decision)\n",
    "print('Binary Crossentropy Lambda Decision', bc_lambda_decision)\n",
    "print('Accuracy Target Decision', acc_target_decision)\n",
    "print('Binary Crossentropy Target Decision', bc_target_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BENCHMARK RANDOM GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################################################################\n",
    "#################################################################################################### END WORKING CODE ####################################################################################################\n",
    "##########################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not optimize_decision_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if not optimize_decision_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
