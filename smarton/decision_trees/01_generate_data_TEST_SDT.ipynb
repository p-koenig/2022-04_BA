{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Generation for the Training of Î»-Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:49.061308Z",
     "start_time": "2020-09-16T12:26:49.055692Z"
    }
   },
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utilities.DecisionTree_BASIC import SDT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1])\n",
    "print(int(max(y_train))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=X_train.shape[1], \n",
    "          output_dim=int(max(y_train))+1, \n",
    "          depth=2,\n",
    "          use_cuda=False,\n",
    "          verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3823,  0.4150, -0.1171,  0.4593],\n",
       "        [-0.1096,  0.1009, -0.2434,  0.2936],\n",
       "        [ 0.4408, -0.3668,  0.4346,  0.0936]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3694, 0.0677, 0.2411], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0706,  0.3854,  0.0739, -0.2334],\n",
       "        [ 0.1274, -0.2304, -0.0586, -0.2031],\n",
       "        [ 0.3317, -0.3947, -0.2305, -0.1412]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.leaf_nodes.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33413871d96a44a2bb8462ab3e9d68eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6984, 0.5194, 0.6359],\n",
      "        [0.7621, 0.5057, 0.7277],\n",
      "        [0.7066, 0.5243, 0.6364],\n",
      "        [0.6737, 0.5255, 0.5357],\n",
      "        [0.7538, 0.4988, 0.7367],\n",
      "        [0.6814, 0.5084, 0.6413],\n",
      "        [0.6722, 0.5237, 0.5426],\n",
      "        [0.6760, 0.5304, 0.5318],\n",
      "        [0.6911, 0.5296, 0.5354],\n",
      "        [0.6379, 0.5133, 0.6332],\n",
      "        [0.7213, 0.5208, 0.6754],\n",
      "        [0.6606, 0.5216, 0.5396],\n",
      "        [0.6511, 0.5190, 0.5553],\n",
      "        [0.6774, 0.5295, 0.5517],\n",
      "        [0.6888, 0.5181, 0.6401],\n",
      "        [0.6694, 0.5234, 0.5414],\n",
      "        [0.6980, 0.5174, 0.6314],\n",
      "        [0.7909, 0.5154, 0.7077],\n",
      "        [0.6502, 0.5262, 0.5319],\n",
      "        [0.7091, 0.5146, 0.6516],\n",
      "        [0.7086, 0.5284, 0.6693],\n",
      "        [0.7086, 0.5298, 0.5213],\n",
      "        [0.7257, 0.5273, 0.6553],\n",
      "        [0.7136, 0.5256, 0.6660],\n",
      "        [0.6676, 0.5027, 0.6626]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6984],\n",
      "         [0.5194],\n",
      "         [0.6359]],\n",
      "\n",
      "        [[0.7621],\n",
      "         [0.5057],\n",
      "         [0.7277]],\n",
      "\n",
      "        [[0.7066],\n",
      "         [0.5243],\n",
      "         [0.6364]],\n",
      "\n",
      "        [[0.6737],\n",
      "         [0.5255],\n",
      "         [0.5357]],\n",
      "\n",
      "        [[0.7538],\n",
      "         [0.4988],\n",
      "         [0.7367]],\n",
      "\n",
      "        [[0.6814],\n",
      "         [0.5084],\n",
      "         [0.6413]],\n",
      "\n",
      "        [[0.6722],\n",
      "         [0.5237],\n",
      "         [0.5426]],\n",
      "\n",
      "        [[0.6760],\n",
      "         [0.5304],\n",
      "         [0.5318]],\n",
      "\n",
      "        [[0.6911],\n",
      "         [0.5296],\n",
      "         [0.5354]],\n",
      "\n",
      "        [[0.6379],\n",
      "         [0.5133],\n",
      "         [0.6332]],\n",
      "\n",
      "        [[0.7213],\n",
      "         [0.5208],\n",
      "         [0.6754]],\n",
      "\n",
      "        [[0.6606],\n",
      "         [0.5216],\n",
      "         [0.5396]],\n",
      "\n",
      "        [[0.6511],\n",
      "         [0.5190],\n",
      "         [0.5553]],\n",
      "\n",
      "        [[0.6774],\n",
      "         [0.5295],\n",
      "         [0.5517]],\n",
      "\n",
      "        [[0.6888],\n",
      "         [0.5181],\n",
      "         [0.6401]],\n",
      "\n",
      "        [[0.6694],\n",
      "         [0.5234],\n",
      "         [0.5414]],\n",
      "\n",
      "        [[0.6980],\n",
      "         [0.5174],\n",
      "         [0.6314]],\n",
      "\n",
      "        [[0.7909],\n",
      "         [0.5154],\n",
      "         [0.7077]],\n",
      "\n",
      "        [[0.6502],\n",
      "         [0.5262],\n",
      "         [0.5319]],\n",
      "\n",
      "        [[0.7091],\n",
      "         [0.5146],\n",
      "         [0.6516]],\n",
      "\n",
      "        [[0.7086],\n",
      "         [0.5284],\n",
      "         [0.6693]],\n",
      "\n",
      "        [[0.7086],\n",
      "         [0.5298],\n",
      "         [0.5213]],\n",
      "\n",
      "        [[0.7257],\n",
      "         [0.5273],\n",
      "         [0.6553]],\n",
      "\n",
      "        [[0.7136],\n",
      "         [0.5256],\n",
      "         [0.6660]],\n",
      "\n",
      "        [[0.6676],\n",
      "         [0.5027],\n",
      "         [0.6626]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6984, 0.3016],\n",
      "         [0.5194, 0.4806],\n",
      "         [0.6359, 0.3641]],\n",
      "\n",
      "        [[0.7621, 0.2379],\n",
      "         [0.5057, 0.4943],\n",
      "         [0.7277, 0.2723]],\n",
      "\n",
      "        [[0.7066, 0.2934],\n",
      "         [0.5243, 0.4757],\n",
      "         [0.6364, 0.3636]],\n",
      "\n",
      "        [[0.6737, 0.3263],\n",
      "         [0.5255, 0.4745],\n",
      "         [0.5357, 0.4643]],\n",
      "\n",
      "        [[0.7538, 0.2462],\n",
      "         [0.4988, 0.5012],\n",
      "         [0.7367, 0.2633]],\n",
      "\n",
      "        [[0.6814, 0.3186],\n",
      "         [0.5084, 0.4916],\n",
      "         [0.6413, 0.3587]],\n",
      "\n",
      "        [[0.6722, 0.3278],\n",
      "         [0.5237, 0.4763],\n",
      "         [0.5426, 0.4574]],\n",
      "\n",
      "        [[0.6760, 0.3240],\n",
      "         [0.5304, 0.4696],\n",
      "         [0.5318, 0.4682]],\n",
      "\n",
      "        [[0.6911, 0.3089],\n",
      "         [0.5296, 0.4704],\n",
      "         [0.5354, 0.4646]],\n",
      "\n",
      "        [[0.6379, 0.3621],\n",
      "         [0.5133, 0.4867],\n",
      "         [0.6332, 0.3668]],\n",
      "\n",
      "        [[0.7213, 0.2787],\n",
      "         [0.5208, 0.4792],\n",
      "         [0.6754, 0.3246]],\n",
      "\n",
      "        [[0.6606, 0.3394],\n",
      "         [0.5216, 0.4784],\n",
      "         [0.5396, 0.4604]],\n",
      "\n",
      "        [[0.6511, 0.3489],\n",
      "         [0.5190, 0.4810],\n",
      "         [0.5553, 0.4447]],\n",
      "\n",
      "        [[0.6774, 0.3226],\n",
      "         [0.5295, 0.4705],\n",
      "         [0.5517, 0.4483]],\n",
      "\n",
      "        [[0.6888, 0.3112],\n",
      "         [0.5181, 0.4819],\n",
      "         [0.6401, 0.3599]],\n",
      "\n",
      "        [[0.6694, 0.3306],\n",
      "         [0.5234, 0.4766],\n",
      "         [0.5414, 0.4586]],\n",
      "\n",
      "        [[0.6980, 0.3020],\n",
      "         [0.5174, 0.4826],\n",
      "         [0.6314, 0.3686]],\n",
      "\n",
      "        [[0.7909, 0.2091],\n",
      "         [0.5154, 0.4846],\n",
      "         [0.7077, 0.2923]],\n",
      "\n",
      "        [[0.6502, 0.3498],\n",
      "         [0.5262, 0.4738],\n",
      "         [0.5319, 0.4681]],\n",
      "\n",
      "        [[0.7091, 0.2909],\n",
      "         [0.5146, 0.4854],\n",
      "         [0.6516, 0.3484]],\n",
      "\n",
      "        [[0.7086, 0.2914],\n",
      "         [0.5284, 0.4716],\n",
      "         [0.6693, 0.3307]],\n",
      "\n",
      "        [[0.7086, 0.2914],\n",
      "         [0.5298, 0.4702],\n",
      "         [0.5213, 0.4787]],\n",
      "\n",
      "        [[0.7257, 0.2743],\n",
      "         [0.5273, 0.4727],\n",
      "         [0.6553, 0.3447]],\n",
      "\n",
      "        [[0.7136, 0.2864],\n",
      "         [0.5256, 0.4744],\n",
      "         [0.6660, 0.3340]],\n",
      "\n",
      "        [[0.6676, 0.3324],\n",
      "         [0.5027, 0.4973],\n",
      "         [0.6626, 0.3374]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3628, 0.3356, 0.1918, 0.1098],\n",
      "        [0.3854, 0.3767, 0.1731, 0.0648],\n",
      "        [0.3704, 0.3362, 0.1867, 0.1067],\n",
      "        [0.3540, 0.3197, 0.1748, 0.1515],\n",
      "        [0.3760, 0.3778, 0.1814, 0.0648],\n",
      "        [0.3465, 0.3350, 0.2043, 0.1143],\n",
      "        [0.3521, 0.3202, 0.1778, 0.1499],\n",
      "        [0.3585, 0.3175, 0.1723, 0.1517],\n",
      "        [0.3660, 0.3251, 0.1654, 0.1435],\n",
      "        [0.3275, 0.3104, 0.2293, 0.1328],\n",
      "        [0.3757, 0.3456, 0.1882, 0.0905],\n",
      "        [0.3446, 0.3160, 0.1831, 0.1563],\n",
      "        [0.3379, 0.3132, 0.1937, 0.1552],\n",
      "        [0.3587, 0.3187, 0.1780, 0.1446],\n",
      "        [0.3569, 0.3320, 0.1992, 0.1120],\n",
      "        [0.3504, 0.3190, 0.1790, 0.1516],\n",
      "        [0.3612, 0.3369, 0.1907, 0.1113],\n",
      "        [0.4076, 0.3833, 0.1480, 0.0611],\n",
      "        [0.3421, 0.3081, 0.1861, 0.1637],\n",
      "        [0.3649, 0.3442, 0.1896, 0.1014],\n",
      "        [0.3744, 0.3342, 0.1950, 0.0964],\n",
      "        [0.3754, 0.3332, 0.1519, 0.1395],\n",
      "        [0.3826, 0.3430, 0.1798, 0.0946],\n",
      "        [0.3751, 0.3385, 0.1907, 0.0957],\n",
      "        [0.3356, 0.3320, 0.2203, 0.1122]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3628, 0.3356, 0.1918, 0.1098],\n",
      "        [0.3854, 0.3767, 0.1731, 0.0648],\n",
      "        [0.3704, 0.3362, 0.1867, 0.1067],\n",
      "        [0.3540, 0.3197, 0.1748, 0.1515],\n",
      "        [0.3760, 0.3778, 0.1814, 0.0648],\n",
      "        [0.3465, 0.3350, 0.2043, 0.1143],\n",
      "        [0.3521, 0.3202, 0.1778, 0.1499],\n",
      "        [0.3585, 0.3175, 0.1723, 0.1517],\n",
      "        [0.3660, 0.3251, 0.1654, 0.1435],\n",
      "        [0.3275, 0.3104, 0.2293, 0.1328],\n",
      "        [0.3757, 0.3456, 0.1882, 0.0905],\n",
      "        [0.3446, 0.3160, 0.1831, 0.1563],\n",
      "        [0.3379, 0.3132, 0.1937, 0.1552],\n",
      "        [0.3587, 0.3187, 0.1780, 0.1446],\n",
      "        [0.3569, 0.3320, 0.1992, 0.1120],\n",
      "        [0.3504, 0.3190, 0.1790, 0.1516],\n",
      "        [0.3612, 0.3369, 0.1907, 0.1113],\n",
      "        [0.4076, 0.3833, 0.1480, 0.0611],\n",
      "        [0.3421, 0.3081, 0.1861, 0.1637],\n",
      "        [0.3649, 0.3442, 0.1896, 0.1014],\n",
      "        [0.3744, 0.3342, 0.1950, 0.0964],\n",
      "        [0.3754, 0.3332, 0.1519, 0.1395],\n",
      "        [0.3826, 0.3430, 0.1798, 0.0946],\n",
      "        [0.3751, 0.3385, 0.1907, 0.0957],\n",
      "        [0.3356, 0.3320, 0.2203, 0.1122]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0256,  0.0462,  0.1203],\n",
      "        [-0.0272,  0.0491,  0.1278],\n",
      "        [-0.0262,  0.0472,  0.1229],\n",
      "        [-0.0250,  0.0451,  0.1174],\n",
      "        [ 0.1456, -0.0870, -0.1491],\n",
      "        [-0.0245,  0.0442,  0.1149],\n",
      "        [-0.0249,  0.0449,  0.1168],\n",
      "        [-0.0253,  0.0457,  0.1189],\n",
      "        [-0.0258,  0.0466,  0.1214],\n",
      "        [-0.0231,  0.0417,  0.1086],\n",
      "        [-0.0265,  0.0479,  0.1246],\n",
      "        [-0.0243,  0.0439,  0.1143],\n",
      "        [-0.0239,  0.0431,  0.1121],\n",
      "        [-0.0253,  0.0457,  0.1190],\n",
      "        [-0.0252,  0.0455,  0.1184],\n",
      "        [-0.0247,  0.0447,  0.1162],\n",
      "        [-0.0255,  0.0460,  0.1198],\n",
      "        [-0.0288,  0.0520,  0.1352],\n",
      "        [-0.0242,  0.0436,  0.1135],\n",
      "        [-0.0258,  0.0465,  0.1210],\n",
      "        [-0.0264,  0.0477,  0.1242],\n",
      "        [-0.0265,  0.0478,  0.1245],\n",
      "        [-0.0270,  0.0488,  0.1269],\n",
      "        [-0.0265,  0.0478,  0.1244],\n",
      "        [-0.0237,  0.0428,  0.1113]], grad_fn=<MmBackward>)\n",
      "Epoch: 00 | Loss: 1.11831 | Correct: 006/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6969, 0.5196, 0.6311],\n",
      "        [0.7611, 0.5090, 0.7211],\n",
      "        [0.7053, 0.5246, 0.6315],\n",
      "        [0.6699, 0.5223, 0.5340],\n",
      "        [0.7529, 0.5022, 0.7301],\n",
      "        [0.6797, 0.5085, 0.6367],\n",
      "        [0.6684, 0.5207, 0.5406],\n",
      "        [0.6723, 0.5271, 0.5301],\n",
      "        [0.6874, 0.5265, 0.5336],\n",
      "        [0.6370, 0.5133, 0.6286],\n",
      "        [0.7203, 0.5224, 0.6696],\n",
      "        [0.6571, 0.5185, 0.5377],\n",
      "        [0.6478, 0.5163, 0.5530],\n",
      "        [0.6741, 0.5269, 0.5493],\n",
      "        [0.6875, 0.5184, 0.6352],\n",
      "        [0.6657, 0.5204, 0.5395],\n",
      "        [0.6963, 0.5174, 0.6268],\n",
      "        [0.7895, 0.5180, 0.7013],\n",
      "        [0.6468, 0.5229, 0.5302],\n",
      "        [0.7073, 0.5151, 0.6466],\n",
      "        [0.7084, 0.5300, 0.6634],\n",
      "        [0.7041, 0.5261, 0.5200],\n",
      "        [0.7246, 0.5283, 0.6499],\n",
      "        [0.7130, 0.5270, 0.6602],\n",
      "        [0.6661, 0.5034, 0.6575]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6969],\n",
      "         [0.5196],\n",
      "         [0.6311]],\n",
      "\n",
      "        [[0.7611],\n",
      "         [0.5090],\n",
      "         [0.7211]],\n",
      "\n",
      "        [[0.7053],\n",
      "         [0.5246],\n",
      "         [0.6315]],\n",
      "\n",
      "        [[0.6699],\n",
      "         [0.5223],\n",
      "         [0.5340]],\n",
      "\n",
      "        [[0.7529],\n",
      "         [0.5022],\n",
      "         [0.7301]],\n",
      "\n",
      "        [[0.6797],\n",
      "         [0.5085],\n",
      "         [0.6367]],\n",
      "\n",
      "        [[0.6684],\n",
      "         [0.5207],\n",
      "         [0.5406]],\n",
      "\n",
      "        [[0.6723],\n",
      "         [0.5271],\n",
      "         [0.5301]],\n",
      "\n",
      "        [[0.6874],\n",
      "         [0.5265],\n",
      "         [0.5336]],\n",
      "\n",
      "        [[0.6370],\n",
      "         [0.5133],\n",
      "         [0.6286]],\n",
      "\n",
      "        [[0.7203],\n",
      "         [0.5224],\n",
      "         [0.6696]],\n",
      "\n",
      "        [[0.6571],\n",
      "         [0.5185],\n",
      "         [0.5377]],\n",
      "\n",
      "        [[0.6478],\n",
      "         [0.5163],\n",
      "         [0.5530]],\n",
      "\n",
      "        [[0.6741],\n",
      "         [0.5269],\n",
      "         [0.5493]],\n",
      "\n",
      "        [[0.6875],\n",
      "         [0.5184],\n",
      "         [0.6352]],\n",
      "\n",
      "        [[0.6657],\n",
      "         [0.5204],\n",
      "         [0.5395]],\n",
      "\n",
      "        [[0.6963],\n",
      "         [0.5174],\n",
      "         [0.6268]],\n",
      "\n",
      "        [[0.7895],\n",
      "         [0.5180],\n",
      "         [0.7013]],\n",
      "\n",
      "        [[0.6468],\n",
      "         [0.5229],\n",
      "         [0.5302]],\n",
      "\n",
      "        [[0.7073],\n",
      "         [0.5151],\n",
      "         [0.6466]],\n",
      "\n",
      "        [[0.7084],\n",
      "         [0.5300],\n",
      "         [0.6634]],\n",
      "\n",
      "        [[0.7041],\n",
      "         [0.5261],\n",
      "         [0.5200]],\n",
      "\n",
      "        [[0.7246],\n",
      "         [0.5283],\n",
      "         [0.6499]],\n",
      "\n",
      "        [[0.7130],\n",
      "         [0.5270],\n",
      "         [0.6602]],\n",
      "\n",
      "        [[0.6661],\n",
      "         [0.5034],\n",
      "         [0.6575]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6969, 0.3031],\n",
      "         [0.5196, 0.4804],\n",
      "         [0.6311, 0.3689]],\n",
      "\n",
      "        [[0.7611, 0.2389],\n",
      "         [0.5090, 0.4910],\n",
      "         [0.7211, 0.2789]],\n",
      "\n",
      "        [[0.7053, 0.2947],\n",
      "         [0.5246, 0.4754],\n",
      "         [0.6315, 0.3685]],\n",
      "\n",
      "        [[0.6699, 0.3301],\n",
      "         [0.5223, 0.4777],\n",
      "         [0.5340, 0.4660]],\n",
      "\n",
      "        [[0.7529, 0.2471],\n",
      "         [0.5022, 0.4978],\n",
      "         [0.7301, 0.2699]],\n",
      "\n",
      "        [[0.6797, 0.3203],\n",
      "         [0.5085, 0.4915],\n",
      "         [0.6367, 0.3633]],\n",
      "\n",
      "        [[0.6684, 0.3316],\n",
      "         [0.5207, 0.4793],\n",
      "         [0.5406, 0.4594]],\n",
      "\n",
      "        [[0.6723, 0.3277],\n",
      "         [0.5271, 0.4729],\n",
      "         [0.5301, 0.4699]],\n",
      "\n",
      "        [[0.6874, 0.3126],\n",
      "         [0.5265, 0.4735],\n",
      "         [0.5336, 0.4664]],\n",
      "\n",
      "        [[0.6370, 0.3630],\n",
      "         [0.5133, 0.4867],\n",
      "         [0.6286, 0.3714]],\n",
      "\n",
      "        [[0.7203, 0.2797],\n",
      "         [0.5224, 0.4776],\n",
      "         [0.6696, 0.3304]],\n",
      "\n",
      "        [[0.6571, 0.3429],\n",
      "         [0.5185, 0.4815],\n",
      "         [0.5377, 0.4623]],\n",
      "\n",
      "        [[0.6478, 0.3522],\n",
      "         [0.5163, 0.4837],\n",
      "         [0.5530, 0.4470]],\n",
      "\n",
      "        [[0.6741, 0.3259],\n",
      "         [0.5269, 0.4731],\n",
      "         [0.5493, 0.4507]],\n",
      "\n",
      "        [[0.6875, 0.3125],\n",
      "         [0.5184, 0.4816],\n",
      "         [0.6352, 0.3648]],\n",
      "\n",
      "        [[0.6657, 0.3343],\n",
      "         [0.5204, 0.4796],\n",
      "         [0.5395, 0.4605]],\n",
      "\n",
      "        [[0.6963, 0.3037],\n",
      "         [0.5174, 0.4826],\n",
      "         [0.6268, 0.3732]],\n",
      "\n",
      "        [[0.7895, 0.2105],\n",
      "         [0.5180, 0.4820],\n",
      "         [0.7013, 0.2987]],\n",
      "\n",
      "        [[0.6468, 0.3532],\n",
      "         [0.5229, 0.4771],\n",
      "         [0.5302, 0.4698]],\n",
      "\n",
      "        [[0.7073, 0.2927],\n",
      "         [0.5151, 0.4849],\n",
      "         [0.6466, 0.3534]],\n",
      "\n",
      "        [[0.7084, 0.2916],\n",
      "         [0.5300, 0.4700],\n",
      "         [0.6634, 0.3366]],\n",
      "\n",
      "        [[0.7041, 0.2959],\n",
      "         [0.5261, 0.4739],\n",
      "         [0.5200, 0.4800]],\n",
      "\n",
      "        [[0.7246, 0.2754],\n",
      "         [0.5283, 0.4717],\n",
      "         [0.6499, 0.3501]],\n",
      "\n",
      "        [[0.7130, 0.2870],\n",
      "         [0.5270, 0.4730],\n",
      "         [0.6602, 0.3398]],\n",
      "\n",
      "        [[0.6661, 0.3339],\n",
      "         [0.5034, 0.4966],\n",
      "         [0.6575, 0.3425]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3621, 0.3348, 0.1913, 0.1118],\n",
      "        [0.3874, 0.3737, 0.1723, 0.0666],\n",
      "        [0.3700, 0.3353, 0.1861, 0.1086],\n",
      "        [0.3499, 0.3200, 0.1763, 0.1539],\n",
      "        [0.3781, 0.3748, 0.1804, 0.0667],\n",
      "        [0.3456, 0.3341, 0.2039, 0.1164],\n",
      "        [0.3480, 0.3204, 0.1793, 0.1523],\n",
      "        [0.3544, 0.3179, 0.1737, 0.1540],\n",
      "        [0.3619, 0.3255, 0.1668, 0.1458],\n",
      "        [0.3270, 0.3100, 0.2282, 0.1348],\n",
      "        [0.3763, 0.3440, 0.1873, 0.0924],\n",
      "        [0.3407, 0.3164, 0.1844, 0.1585],\n",
      "        [0.3344, 0.3133, 0.1948, 0.1574],\n",
      "        [0.3552, 0.3189, 0.1790, 0.1469],\n",
      "        [0.3564, 0.3311, 0.1985, 0.1140],\n",
      "        [0.3464, 0.3193, 0.1803, 0.1540],\n",
      "        [0.3603, 0.3361, 0.1903, 0.1133],\n",
      "        [0.4090, 0.3805, 0.1476, 0.0629],\n",
      "        [0.3382, 0.3086, 0.1872, 0.1659],\n",
      "        [0.3644, 0.3430, 0.1892, 0.1034],\n",
      "        [0.3754, 0.3329, 0.1935, 0.0982],\n",
      "        [0.3705, 0.3337, 0.1538, 0.1420],\n",
      "        [0.3828, 0.3418, 0.1790, 0.0964],\n",
      "        [0.3758, 0.3372, 0.1895, 0.0975],\n",
      "        [0.3353, 0.3308, 0.2195, 0.1144]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3621, 0.3348, 0.1913, 0.1118],\n",
      "        [0.3874, 0.3737, 0.1723, 0.0666],\n",
      "        [0.3700, 0.3353, 0.1861, 0.1086],\n",
      "        [0.3499, 0.3200, 0.1763, 0.1539],\n",
      "        [0.3781, 0.3748, 0.1804, 0.0667],\n",
      "        [0.3456, 0.3341, 0.2039, 0.1164],\n",
      "        [0.3480, 0.3204, 0.1793, 0.1523],\n",
      "        [0.3544, 0.3179, 0.1737, 0.1540],\n",
      "        [0.3619, 0.3255, 0.1668, 0.1458],\n",
      "        [0.3270, 0.3100, 0.2282, 0.1348],\n",
      "        [0.3763, 0.3440, 0.1873, 0.0924],\n",
      "        [0.3407, 0.3164, 0.1844, 0.1585],\n",
      "        [0.3344, 0.3133, 0.1948, 0.1574],\n",
      "        [0.3552, 0.3189, 0.1790, 0.1469],\n",
      "        [0.3564, 0.3311, 0.1985, 0.1140],\n",
      "        [0.3464, 0.3193, 0.1803, 0.1540],\n",
      "        [0.3603, 0.3361, 0.1903, 0.1133],\n",
      "        [0.4090, 0.3805, 0.1476, 0.0629],\n",
      "        [0.3382, 0.3086, 0.1872, 0.1659],\n",
      "        [0.3644, 0.3430, 0.1892, 0.1034],\n",
      "        [0.3754, 0.3329, 0.1935, 0.0982],\n",
      "        [0.3705, 0.3337, 0.1538, 0.1420],\n",
      "        [0.3828, 0.3418, 0.1790, 0.0964],\n",
      "        [0.3758, 0.3372, 0.1895, 0.0975],\n",
      "        [0.3353, 0.3308, 0.2195, 0.1144]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0219,  0.0425,  0.1165],\n",
      "        [-0.0235,  0.0455,  0.1246],\n",
      "        [-0.0224,  0.0435,  0.1190],\n",
      "        [-0.0212,  0.0411,  0.1125],\n",
      "        [-0.0229,  0.0444,  0.1216],\n",
      "        [-0.0209,  0.0406,  0.1112],\n",
      "        [-0.0211,  0.0409,  0.1120],\n",
      "        [-0.0215,  0.0416,  0.1140],\n",
      "        [-0.0219,  0.0425,  0.1164],\n",
      "        [-0.0198,  0.0384,  0.1052],\n",
      "        [-0.0228,  0.0442,  0.1210],\n",
      "        [-0.0206,  0.0400,  0.1096],\n",
      "        [-0.0203,  0.0393,  0.1076],\n",
      "        [-0.0215,  0.0417,  0.1143],\n",
      "        [-0.0216,  0.0419,  0.1146],\n",
      "        [-0.0210,  0.0407,  0.1114],\n",
      "        [-0.0218,  0.0423,  0.1159],\n",
      "        [-0.0248,  0.0480,  0.1316],\n",
      "        [-0.0205,  0.0397,  0.1088],\n",
      "        [-0.0221,  0.0428,  0.1172],\n",
      "        [-0.0227,  0.0441,  0.1208],\n",
      "        [-0.0224,  0.0435,  0.1192],\n",
      "        [-0.0232,  0.0450,  0.1231],\n",
      "        [-0.0228,  0.0441,  0.1209],\n",
      "        [-0.0203,  0.0394,  0.1079]], grad_fn=<MmBackward>)\n",
      "Epoch: 01 | Loss: 1.10930 | Correct: 007/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6961, 0.5197, 0.6263],\n",
      "        [0.7617, 0.5121, 0.7145],\n",
      "        [0.7048, 0.5248, 0.6265],\n",
      "        [0.6666, 0.5190, 0.5322],\n",
      "        [0.7535, 0.5055, 0.7235],\n",
      "        [0.6788, 0.5085, 0.6320],\n",
      "        [0.6652, 0.5176, 0.5387],\n",
      "        [0.6691, 0.5239, 0.5283],\n",
      "        [0.6843, 0.5234, 0.5317],\n",
      "        [0.6365, 0.5133, 0.6239],\n",
      "        [0.7204, 0.5239, 0.6637],\n",
      "        [0.6541, 0.5153, 0.5358],\n",
      "        [0.6449, 0.5136, 0.5507],\n",
      "        [0.6714, 0.5243, 0.5469],\n",
      "        [0.6869, 0.5186, 0.6303],\n",
      "        [0.6625, 0.5173, 0.5376],\n",
      "        [0.6954, 0.5172, 0.6222],\n",
      "        [0.7895, 0.5205, 0.6949],\n",
      "        [0.6437, 0.5196, 0.5285],\n",
      "        [0.7066, 0.5156, 0.6415],\n",
      "        [0.7088, 0.5315, 0.6574],\n",
      "        [0.7004, 0.5224, 0.5186],\n",
      "        [0.7244, 0.5293, 0.6444],\n",
      "        [0.7132, 0.5284, 0.6544],\n",
      "        [0.6656, 0.5040, 0.6523]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6961],\n",
      "         [0.5197],\n",
      "         [0.6263]],\n",
      "\n",
      "        [[0.7617],\n",
      "         [0.5121],\n",
      "         [0.7145]],\n",
      "\n",
      "        [[0.7048],\n",
      "         [0.5248],\n",
      "         [0.6265]],\n",
      "\n",
      "        [[0.6666],\n",
      "         [0.5190],\n",
      "         [0.5322]],\n",
      "\n",
      "        [[0.7535],\n",
      "         [0.5055],\n",
      "         [0.7235]],\n",
      "\n",
      "        [[0.6788],\n",
      "         [0.5085],\n",
      "         [0.6320]],\n",
      "\n",
      "        [[0.6652],\n",
      "         [0.5176],\n",
      "         [0.5387]],\n",
      "\n",
      "        [[0.6691],\n",
      "         [0.5239],\n",
      "         [0.5283]],\n",
      "\n",
      "        [[0.6843],\n",
      "         [0.5234],\n",
      "         [0.5317]],\n",
      "\n",
      "        [[0.6365],\n",
      "         [0.5133],\n",
      "         [0.6239]],\n",
      "\n",
      "        [[0.7204],\n",
      "         [0.5239],\n",
      "         [0.6637]],\n",
      "\n",
      "        [[0.6541],\n",
      "         [0.5153],\n",
      "         [0.5358]],\n",
      "\n",
      "        [[0.6449],\n",
      "         [0.5136],\n",
      "         [0.5507]],\n",
      "\n",
      "        [[0.6714],\n",
      "         [0.5243],\n",
      "         [0.5469]],\n",
      "\n",
      "        [[0.6869],\n",
      "         [0.5186],\n",
      "         [0.6303]],\n",
      "\n",
      "        [[0.6625],\n",
      "         [0.5173],\n",
      "         [0.5376]],\n",
      "\n",
      "        [[0.6954],\n",
      "         [0.5172],\n",
      "         [0.6222]],\n",
      "\n",
      "        [[0.7895],\n",
      "         [0.5205],\n",
      "         [0.6949]],\n",
      "\n",
      "        [[0.6437],\n",
      "         [0.5196],\n",
      "         [0.5285]],\n",
      "\n",
      "        [[0.7066],\n",
      "         [0.5156],\n",
      "         [0.6415]],\n",
      "\n",
      "        [[0.7088],\n",
      "         [0.5315],\n",
      "         [0.6574]],\n",
      "\n",
      "        [[0.7004],\n",
      "         [0.5224],\n",
      "         [0.5186]],\n",
      "\n",
      "        [[0.7244],\n",
      "         [0.5293],\n",
      "         [0.6444]],\n",
      "\n",
      "        [[0.7132],\n",
      "         [0.5284],\n",
      "         [0.6544]],\n",
      "\n",
      "        [[0.6656],\n",
      "         [0.5040],\n",
      "         [0.6523]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6961, 0.3039],\n",
      "         [0.5197, 0.4803],\n",
      "         [0.6263, 0.3737]],\n",
      "\n",
      "        [[0.7617, 0.2383],\n",
      "         [0.5121, 0.4879],\n",
      "         [0.7145, 0.2855]],\n",
      "\n",
      "        [[0.7048, 0.2952],\n",
      "         [0.5248, 0.4752],\n",
      "         [0.6265, 0.3735]],\n",
      "\n",
      "        [[0.6666, 0.3334],\n",
      "         [0.5190, 0.4810],\n",
      "         [0.5322, 0.4678]],\n",
      "\n",
      "        [[0.7535, 0.2465],\n",
      "         [0.5055, 0.4945],\n",
      "         [0.7235, 0.2765]],\n",
      "\n",
      "        [[0.6788, 0.3212],\n",
      "         [0.5085, 0.4915],\n",
      "         [0.6320, 0.3680]],\n",
      "\n",
      "        [[0.6652, 0.3348],\n",
      "         [0.5176, 0.4824],\n",
      "         [0.5387, 0.4613]],\n",
      "\n",
      "        [[0.6691, 0.3309],\n",
      "         [0.5239, 0.4761],\n",
      "         [0.5283, 0.4717]],\n",
      "\n",
      "        [[0.6843, 0.3157],\n",
      "         [0.5234, 0.4766],\n",
      "         [0.5317, 0.4683]],\n",
      "\n",
      "        [[0.6365, 0.3635],\n",
      "         [0.5133, 0.4867],\n",
      "         [0.6239, 0.3761]],\n",
      "\n",
      "        [[0.7204, 0.2796],\n",
      "         [0.5239, 0.4761],\n",
      "         [0.6637, 0.3363]],\n",
      "\n",
      "        [[0.6541, 0.3459],\n",
      "         [0.5153, 0.4847],\n",
      "         [0.5358, 0.4642]],\n",
      "\n",
      "        [[0.6449, 0.3551],\n",
      "         [0.5136, 0.4864],\n",
      "         [0.5507, 0.4493]],\n",
      "\n",
      "        [[0.6714, 0.3286],\n",
      "         [0.5243, 0.4757],\n",
      "         [0.5469, 0.4531]],\n",
      "\n",
      "        [[0.6869, 0.3131],\n",
      "         [0.5186, 0.4814],\n",
      "         [0.6303, 0.3697]],\n",
      "\n",
      "        [[0.6625, 0.3375],\n",
      "         [0.5173, 0.4827],\n",
      "         [0.5376, 0.4624]],\n",
      "\n",
      "        [[0.6954, 0.3046],\n",
      "         [0.5172, 0.4828],\n",
      "         [0.6222, 0.3778]],\n",
      "\n",
      "        [[0.7895, 0.2105],\n",
      "         [0.5205, 0.4795],\n",
      "         [0.6949, 0.3051]],\n",
      "\n",
      "        [[0.6437, 0.3563],\n",
      "         [0.5196, 0.4804],\n",
      "         [0.5285, 0.4715]],\n",
      "\n",
      "        [[0.7066, 0.2934],\n",
      "         [0.5156, 0.4844],\n",
      "         [0.6415, 0.3585]],\n",
      "\n",
      "        [[0.7088, 0.2912],\n",
      "         [0.5315, 0.4685],\n",
      "         [0.6574, 0.3426]],\n",
      "\n",
      "        [[0.7004, 0.2996],\n",
      "         [0.5224, 0.4776],\n",
      "         [0.5186, 0.4814]],\n",
      "\n",
      "        [[0.7244, 0.2756],\n",
      "         [0.5293, 0.4707],\n",
      "         [0.6444, 0.3556]],\n",
      "\n",
      "        [[0.7132, 0.2868],\n",
      "         [0.5284, 0.4716],\n",
      "         [0.6544, 0.3456]],\n",
      "\n",
      "        [[0.6656, 0.3344],\n",
      "         [0.5040, 0.4960],\n",
      "         [0.6523, 0.3477]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3618, 0.3344, 0.1903, 0.1136],\n",
      "        [0.3900, 0.3716, 0.1703, 0.0681],\n",
      "        [0.3699, 0.3349, 0.1850, 0.1103],\n",
      "        [0.3460, 0.3206, 0.1775, 0.1560],\n",
      "        [0.3809, 0.3726, 0.1783, 0.0682],\n",
      "        [0.3452, 0.3336, 0.2030, 0.1182],\n",
      "        [0.3443, 0.3209, 0.1804, 0.1544],\n",
      "        [0.3505, 0.3186, 0.1748, 0.1561],\n",
      "        [0.3581, 0.3262, 0.1678, 0.1478],\n",
      "        [0.3267, 0.3098, 0.2268, 0.1367],\n",
      "        [0.3774, 0.3430, 0.1856, 0.0940],\n",
      "        [0.3371, 0.3170, 0.1854, 0.1606],\n",
      "        [0.3312, 0.3137, 0.1956, 0.1595],\n",
      "        [0.3520, 0.3194, 0.1797, 0.1489],\n",
      "        [0.3562, 0.3307, 0.1973, 0.1157],\n",
      "        [0.3427, 0.3198, 0.1814, 0.1561],\n",
      "        [0.3597, 0.3357, 0.1895, 0.1151],\n",
      "        [0.4109, 0.3786, 0.1463, 0.0642],\n",
      "        [0.3345, 0.3092, 0.1883, 0.1680],\n",
      "        [0.3643, 0.3423, 0.1882, 0.1052],\n",
      "        [0.3768, 0.3320, 0.1914, 0.0998],\n",
      "        [0.3659, 0.3345, 0.1554, 0.1442],\n",
      "        [0.3834, 0.3410, 0.1776, 0.0980],\n",
      "        [0.3769, 0.3364, 0.1877, 0.0991],\n",
      "        [0.3354, 0.3301, 0.2182, 0.1163]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3618, 0.3344, 0.1903, 0.1136],\n",
      "        [0.3900, 0.3716, 0.1703, 0.0681],\n",
      "        [0.3699, 0.3349, 0.1850, 0.1103],\n",
      "        [0.3460, 0.3206, 0.1775, 0.1560],\n",
      "        [0.3809, 0.3726, 0.1783, 0.0682],\n",
      "        [0.3452, 0.3336, 0.2030, 0.1182],\n",
      "        [0.3443, 0.3209, 0.1804, 0.1544],\n",
      "        [0.3505, 0.3186, 0.1748, 0.1561],\n",
      "        [0.3581, 0.3262, 0.1678, 0.1478],\n",
      "        [0.3267, 0.3098, 0.2268, 0.1367],\n",
      "        [0.3774, 0.3430, 0.1856, 0.0940],\n",
      "        [0.3371, 0.3170, 0.1854, 0.1606],\n",
      "        [0.3312, 0.3137, 0.1956, 0.1595],\n",
      "        [0.3520, 0.3194, 0.1797, 0.1489],\n",
      "        [0.3562, 0.3307, 0.1973, 0.1157],\n",
      "        [0.3427, 0.3198, 0.1814, 0.1561],\n",
      "        [0.3597, 0.3357, 0.1895, 0.1151],\n",
      "        [0.4109, 0.3786, 0.1463, 0.0642],\n",
      "        [0.3345, 0.3092, 0.1883, 0.1680],\n",
      "        [0.3643, 0.3423, 0.1882, 0.1052],\n",
      "        [0.3768, 0.3320, 0.1914, 0.0998],\n",
      "        [0.3659, 0.3345, 0.1554, 0.1442],\n",
      "        [0.3834, 0.3410, 0.1776, 0.0980],\n",
      "        [0.3769, 0.3364, 0.1877, 0.0991],\n",
      "        [0.3354, 0.3301, 0.2182, 0.1163]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0183,  0.0392,  0.1129],\n",
      "        [-0.0198,  0.0422,  0.1217],\n",
      "        [-0.0188,  0.0400,  0.1154],\n",
      "        [-0.0175,  0.0374,  0.1079],\n",
      "        [-0.0193,  0.0412,  0.1188],\n",
      "        [-0.0175,  0.0374,  0.1077],\n",
      "        [-0.0175,  0.0373,  0.1074],\n",
      "        [-0.0178,  0.0379,  0.1094],\n",
      "        [-0.0182,  0.0388,  0.1117],\n",
      "        [-0.0166,  0.0354,  0.1019],\n",
      "        [-0.0191,  0.0409,  0.1177],\n",
      "        [-0.0171,  0.0365,  0.1052],\n",
      "        [-0.0168,  0.0359,  0.1033],\n",
      "        [-0.0178,  0.0381,  0.1098],\n",
      "        [-0.0181,  0.0386,  0.1111],\n",
      "        [-0.0174,  0.0371,  0.1069],\n",
      "        [-0.0182,  0.0389,  0.1122],\n",
      "        [-0.0208,  0.0445,  0.1282],\n",
      "        [-0.0170,  0.0362,  0.1044],\n",
      "        [-0.0185,  0.0394,  0.1137],\n",
      "        [-0.0191,  0.0408,  0.1175],\n",
      "        [-0.0186,  0.0396,  0.1142],\n",
      "        [-0.0194,  0.0415,  0.1196],\n",
      "        [-0.0191,  0.0408,  0.1176],\n",
      "        [-0.0170,  0.0363,  0.1047]], grad_fn=<MmBackward>)\n",
      "Epoch: 02 | Loss: 1.10840 | Correct: 007/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6958, 0.5197, 0.6215],\n",
      "        [0.7629, 0.5151, 0.7078],\n",
      "        [0.7045, 0.5250, 0.6216],\n",
      "        [0.6635, 0.5158, 0.5305],\n",
      "        [0.7549, 0.5088, 0.7167],\n",
      "        [0.6783, 0.5085, 0.6273],\n",
      "        [0.6622, 0.5145, 0.5368],\n",
      "        [0.6660, 0.5206, 0.5266],\n",
      "        [0.6814, 0.5202, 0.5298],\n",
      "        [0.6362, 0.5132, 0.6192],\n",
      "        [0.7209, 0.5254, 0.6578],\n",
      "        [0.6511, 0.5122, 0.5340],\n",
      "        [0.6422, 0.5109, 0.5485],\n",
      "        [0.6689, 0.5217, 0.5445],\n",
      "        [0.6867, 0.5188, 0.6254],\n",
      "        [0.6595, 0.5142, 0.5357],\n",
      "        [0.6948, 0.5171, 0.6176],\n",
      "        [0.7902, 0.5229, 0.6885],\n",
      "        [0.6407, 0.5164, 0.5267],\n",
      "        [0.7064, 0.5161, 0.6364],\n",
      "        [0.7096, 0.5331, 0.6515],\n",
      "        [0.6970, 0.5188, 0.5173],\n",
      "        [0.7246, 0.5302, 0.6389],\n",
      "        [0.7139, 0.5297, 0.6486],\n",
      "        [0.6655, 0.5046, 0.6472]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6958],\n",
      "         [0.5197],\n",
      "         [0.6215]],\n",
      "\n",
      "        [[0.7629],\n",
      "         [0.5151],\n",
      "         [0.7078]],\n",
      "\n",
      "        [[0.7045],\n",
      "         [0.5250],\n",
      "         [0.6216]],\n",
      "\n",
      "        [[0.6635],\n",
      "         [0.5158],\n",
      "         [0.5305]],\n",
      "\n",
      "        [[0.7549],\n",
      "         [0.5088],\n",
      "         [0.7167]],\n",
      "\n",
      "        [[0.6783],\n",
      "         [0.5085],\n",
      "         [0.6273]],\n",
      "\n",
      "        [[0.6622],\n",
      "         [0.5145],\n",
      "         [0.5368]],\n",
      "\n",
      "        [[0.6660],\n",
      "         [0.5206],\n",
      "         [0.5266]],\n",
      "\n",
      "        [[0.6814],\n",
      "         [0.5202],\n",
      "         [0.5298]],\n",
      "\n",
      "        [[0.6362],\n",
      "         [0.5132],\n",
      "         [0.6192]],\n",
      "\n",
      "        [[0.7209],\n",
      "         [0.5254],\n",
      "         [0.6578]],\n",
      "\n",
      "        [[0.6511],\n",
      "         [0.5122],\n",
      "         [0.5340]],\n",
      "\n",
      "        [[0.6422],\n",
      "         [0.5109],\n",
      "         [0.5485]],\n",
      "\n",
      "        [[0.6689],\n",
      "         [0.5217],\n",
      "         [0.5445]],\n",
      "\n",
      "        [[0.6867],\n",
      "         [0.5188],\n",
      "         [0.6254]],\n",
      "\n",
      "        [[0.6595],\n",
      "         [0.5142],\n",
      "         [0.5357]],\n",
      "\n",
      "        [[0.6948],\n",
      "         [0.5171],\n",
      "         [0.6176]],\n",
      "\n",
      "        [[0.7902],\n",
      "         [0.5229],\n",
      "         [0.6885]],\n",
      "\n",
      "        [[0.6407],\n",
      "         [0.5164],\n",
      "         [0.5267]],\n",
      "\n",
      "        [[0.7064],\n",
      "         [0.5161],\n",
      "         [0.6364]],\n",
      "\n",
      "        [[0.7096],\n",
      "         [0.5331],\n",
      "         [0.6515]],\n",
      "\n",
      "        [[0.6970],\n",
      "         [0.5188],\n",
      "         [0.5173]],\n",
      "\n",
      "        [[0.7246],\n",
      "         [0.5302],\n",
      "         [0.6389]],\n",
      "\n",
      "        [[0.7139],\n",
      "         [0.5297],\n",
      "         [0.6486]],\n",
      "\n",
      "        [[0.6655],\n",
      "         [0.5046],\n",
      "         [0.6472]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6958, 0.3042],\n",
      "         [0.5197, 0.4803],\n",
      "         [0.6215, 0.3785]],\n",
      "\n",
      "        [[0.7629, 0.2371],\n",
      "         [0.5151, 0.4849],\n",
      "         [0.7078, 0.2922]],\n",
      "\n",
      "        [[0.7045, 0.2955],\n",
      "         [0.5250, 0.4750],\n",
      "         [0.6216, 0.3784]],\n",
      "\n",
      "        [[0.6635, 0.3365],\n",
      "         [0.5158, 0.4842],\n",
      "         [0.5305, 0.4695]],\n",
      "\n",
      "        [[0.7549, 0.2451],\n",
      "         [0.5088, 0.4912],\n",
      "         [0.7167, 0.2833]],\n",
      "\n",
      "        [[0.6783, 0.3217],\n",
      "         [0.5085, 0.4915],\n",
      "         [0.6273, 0.3727]],\n",
      "\n",
      "        [[0.6622, 0.3378],\n",
      "         [0.5145, 0.4855],\n",
      "         [0.5368, 0.4632]],\n",
      "\n",
      "        [[0.6660, 0.3340],\n",
      "         [0.5206, 0.4794],\n",
      "         [0.5266, 0.4734]],\n",
      "\n",
      "        [[0.6814, 0.3186],\n",
      "         [0.5202, 0.4798],\n",
      "         [0.5298, 0.4702]],\n",
      "\n",
      "        [[0.6362, 0.3638],\n",
      "         [0.5132, 0.4868],\n",
      "         [0.6192, 0.3808]],\n",
      "\n",
      "        [[0.7209, 0.2791],\n",
      "         [0.5254, 0.4746],\n",
      "         [0.6578, 0.3422]],\n",
      "\n",
      "        [[0.6511, 0.3489],\n",
      "         [0.5122, 0.4878],\n",
      "         [0.5340, 0.4660]],\n",
      "\n",
      "        [[0.6422, 0.3578],\n",
      "         [0.5109, 0.4891],\n",
      "         [0.5485, 0.4515]],\n",
      "\n",
      "        [[0.6689, 0.3311],\n",
      "         [0.5217, 0.4783],\n",
      "         [0.5445, 0.4555]],\n",
      "\n",
      "        [[0.6867, 0.3133],\n",
      "         [0.5188, 0.4812],\n",
      "         [0.6254, 0.3746]],\n",
      "\n",
      "        [[0.6595, 0.3405],\n",
      "         [0.5142, 0.4858],\n",
      "         [0.5357, 0.4643]],\n",
      "\n",
      "        [[0.6948, 0.3052],\n",
      "         [0.5171, 0.4829],\n",
      "         [0.6176, 0.3824]],\n",
      "\n",
      "        [[0.7902, 0.2098],\n",
      "         [0.5229, 0.4771],\n",
      "         [0.6885, 0.3115]],\n",
      "\n",
      "        [[0.6407, 0.3593],\n",
      "         [0.5164, 0.4836],\n",
      "         [0.5267, 0.4733]],\n",
      "\n",
      "        [[0.7064, 0.2936],\n",
      "         [0.5161, 0.4839],\n",
      "         [0.6364, 0.3636]],\n",
      "\n",
      "        [[0.7096, 0.2904],\n",
      "         [0.5331, 0.4669],\n",
      "         [0.6515, 0.3485]],\n",
      "\n",
      "        [[0.6970, 0.3030],\n",
      "         [0.5188, 0.4812],\n",
      "         [0.5173, 0.4827]],\n",
      "\n",
      "        [[0.7246, 0.2754],\n",
      "         [0.5302, 0.4698],\n",
      "         [0.6389, 0.3611]],\n",
      "\n",
      "        [[0.7139, 0.2861],\n",
      "         [0.5297, 0.4703],\n",
      "         [0.6486, 0.3514]],\n",
      "\n",
      "        [[0.6655, 0.3345],\n",
      "         [0.5046, 0.4954],\n",
      "         [0.6472, 0.3528]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3616, 0.3341, 0.1891, 0.1152],\n",
      "        [0.3930, 0.3699, 0.1678, 0.0693],\n",
      "        [0.3699, 0.3346, 0.1837, 0.1118],\n",
      "        [0.3422, 0.3212, 0.1785, 0.1580],\n",
      "        [0.3841, 0.3708, 0.1757, 0.0694],\n",
      "        [0.3449, 0.3334, 0.2018, 0.1199],\n",
      "        [0.3407, 0.3215, 0.1813, 0.1565],\n",
      "        [0.3467, 0.3193, 0.1759, 0.1581],\n",
      "        [0.3545, 0.3269, 0.1688, 0.1498],\n",
      "        [0.3265, 0.3097, 0.2253, 0.1385],\n",
      "        [0.3788, 0.3421, 0.1836, 0.0955],\n",
      "        [0.3335, 0.3176, 0.1863, 0.1626],\n",
      "        [0.3281, 0.3141, 0.1962, 0.1615],\n",
      "        [0.3490, 0.3199, 0.1803, 0.1508],\n",
      "        [0.3563, 0.3304, 0.1960, 0.1174],\n",
      "        [0.3391, 0.3204, 0.1824, 0.1581],\n",
      "        [0.3593, 0.3355, 0.1885, 0.1167],\n",
      "        [0.4132, 0.3770, 0.1444, 0.0653],\n",
      "        [0.3308, 0.3099, 0.1893, 0.1700],\n",
      "        [0.3645, 0.3418, 0.1869, 0.1068],\n",
      "        [0.3783, 0.3313, 0.1892, 0.1012],\n",
      "        [0.3616, 0.3354, 0.1568, 0.1463],\n",
      "        [0.3842, 0.3404, 0.1760, 0.0995],\n",
      "        [0.3782, 0.3357, 0.1856, 0.1005],\n",
      "        [0.3358, 0.3297, 0.2165, 0.1180]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3616, 0.3341, 0.1891, 0.1152],\n",
      "        [0.3930, 0.3699, 0.1678, 0.0693],\n",
      "        [0.3699, 0.3346, 0.1837, 0.1118],\n",
      "        [0.3422, 0.3212, 0.1785, 0.1580],\n",
      "        [0.3841, 0.3708, 0.1757, 0.0694],\n",
      "        [0.3449, 0.3334, 0.2018, 0.1199],\n",
      "        [0.3407, 0.3215, 0.1813, 0.1565],\n",
      "        [0.3467, 0.3193, 0.1759, 0.1581],\n",
      "        [0.3545, 0.3269, 0.1688, 0.1498],\n",
      "        [0.3265, 0.3097, 0.2253, 0.1385],\n",
      "        [0.3788, 0.3421, 0.1836, 0.0955],\n",
      "        [0.3335, 0.3176, 0.1863, 0.1626],\n",
      "        [0.3281, 0.3141, 0.1962, 0.1615],\n",
      "        [0.3490, 0.3199, 0.1803, 0.1508],\n",
      "        [0.3563, 0.3304, 0.1960, 0.1174],\n",
      "        [0.3391, 0.3204, 0.1824, 0.1581],\n",
      "        [0.3593, 0.3355, 0.1885, 0.1167],\n",
      "        [0.4132, 0.3770, 0.1444, 0.0653],\n",
      "        [0.3308, 0.3099, 0.1893, 0.1700],\n",
      "        [0.3645, 0.3418, 0.1869, 0.1068],\n",
      "        [0.3783, 0.3313, 0.1892, 0.1012],\n",
      "        [0.3616, 0.3354, 0.1568, 0.1463],\n",
      "        [0.3842, 0.3404, 0.1760, 0.0995],\n",
      "        [0.3782, 0.3357, 0.1856, 0.1005],\n",
      "        [0.3358, 0.3297, 0.2165, 0.1180]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0148,  0.0357,  0.1093],\n",
      "        [-0.0161,  0.0388,  0.1188],\n",
      "        [-0.0151,  0.0365,  0.1119],\n",
      "        [-0.0140,  0.0338,  0.1035],\n",
      "        [-0.0157,  0.0379,  0.1162],\n",
      "        [-0.0141,  0.0340,  0.1043],\n",
      "        [-0.0139,  0.0336,  0.1030],\n",
      "        [-0.0142,  0.0342,  0.1048],\n",
      "        [-0.0145,  0.0350,  0.1072],\n",
      "        [-0.0133,  0.0322,  0.0987],\n",
      "        [-0.0155,  0.0374,  0.1145],\n",
      "        [-0.0136,  0.0329,  0.1008],\n",
      "        [-0.0134,  0.0324,  0.0992],\n",
      "        [-0.0143,  0.0344,  0.1055],\n",
      "        [-0.0146,  0.0352,  0.1077],\n",
      "        [-0.0139,  0.0335,  0.1025],\n",
      "        [-0.0147,  0.0355,  0.1086],\n",
      "        [-0.0169,  0.0408,  0.1250],\n",
      "        [-0.0135,  0.0326,  0.1000],\n",
      "        [-0.0149,  0.0360,  0.1102],\n",
      "        [-0.0155,  0.0373,  0.1144],\n",
      "        [-0.0148,  0.0357,  0.1093],\n",
      "        [-0.0157,  0.0379,  0.1162],\n",
      "        [-0.0155,  0.0373,  0.1143],\n",
      "        [-0.0137,  0.0331,  0.1015]], grad_fn=<MmBackward>)\n",
      "Epoch: 03 | Loss: 1.10755 | Correct: 007/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6956, 0.5198, 0.6167],\n",
      "        [0.7644, 0.5182, 0.7010],\n",
      "        [0.7045, 0.5253, 0.6166],\n",
      "        [0.6605, 0.5126, 0.5287],\n",
      "        [0.7566, 0.5121, 0.7099],\n",
      "        [0.6780, 0.5085, 0.6226],\n",
      "        [0.6593, 0.5115, 0.5349],\n",
      "        [0.6630, 0.5174, 0.5248],\n",
      "        [0.6786, 0.5171, 0.5280],\n",
      "        [0.6361, 0.5131, 0.6146],\n",
      "        [0.7217, 0.5269, 0.6519],\n",
      "        [0.6482, 0.5091, 0.5321],\n",
      "        [0.6397, 0.5082, 0.5462],\n",
      "        [0.6665, 0.5191, 0.5422],\n",
      "        [0.6866, 0.5190, 0.6205],\n",
      "        [0.6566, 0.5111, 0.5338],\n",
      "        [0.6945, 0.5170, 0.6129],\n",
      "        [0.7912, 0.5254, 0.6820],\n",
      "        [0.6377, 0.5131, 0.5250],\n",
      "        [0.7064, 0.5165, 0.6313],\n",
      "        [0.7106, 0.5346, 0.6455],\n",
      "        [0.6937, 0.5151, 0.5160],\n",
      "        [0.7250, 0.5311, 0.6334],\n",
      "        [0.7147, 0.5311, 0.6428],\n",
      "        [0.6657, 0.5052, 0.6420]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6956],\n",
      "         [0.5198],\n",
      "         [0.6167]],\n",
      "\n",
      "        [[0.7644],\n",
      "         [0.5182],\n",
      "         [0.7010]],\n",
      "\n",
      "        [[0.7045],\n",
      "         [0.5253],\n",
      "         [0.6166]],\n",
      "\n",
      "        [[0.6605],\n",
      "         [0.5126],\n",
      "         [0.5287]],\n",
      "\n",
      "        [[0.7566],\n",
      "         [0.5121],\n",
      "         [0.7099]],\n",
      "\n",
      "        [[0.6780],\n",
      "         [0.5085],\n",
      "         [0.6226]],\n",
      "\n",
      "        [[0.6593],\n",
      "         [0.5115],\n",
      "         [0.5349]],\n",
      "\n",
      "        [[0.6630],\n",
      "         [0.5174],\n",
      "         [0.5248]],\n",
      "\n",
      "        [[0.6786],\n",
      "         [0.5171],\n",
      "         [0.5280]],\n",
      "\n",
      "        [[0.6361],\n",
      "         [0.5131],\n",
      "         [0.6146]],\n",
      "\n",
      "        [[0.7217],\n",
      "         [0.5269],\n",
      "         [0.6519]],\n",
      "\n",
      "        [[0.6482],\n",
      "         [0.5091],\n",
      "         [0.5321]],\n",
      "\n",
      "        [[0.6397],\n",
      "         [0.5082],\n",
      "         [0.5462]],\n",
      "\n",
      "        [[0.6665],\n",
      "         [0.5191],\n",
      "         [0.5422]],\n",
      "\n",
      "        [[0.6866],\n",
      "         [0.5190],\n",
      "         [0.6205]],\n",
      "\n",
      "        [[0.6566],\n",
      "         [0.5111],\n",
      "         [0.5338]],\n",
      "\n",
      "        [[0.6945],\n",
      "         [0.5170],\n",
      "         [0.6129]],\n",
      "\n",
      "        [[0.7912],\n",
      "         [0.5254],\n",
      "         [0.6820]],\n",
      "\n",
      "        [[0.6377],\n",
      "         [0.5131],\n",
      "         [0.5250]],\n",
      "\n",
      "        [[0.7064],\n",
      "         [0.5165],\n",
      "         [0.6313]],\n",
      "\n",
      "        [[0.7106],\n",
      "         [0.5346],\n",
      "         [0.6455]],\n",
      "\n",
      "        [[0.6937],\n",
      "         [0.5151],\n",
      "         [0.5160]],\n",
      "\n",
      "        [[0.7250],\n",
      "         [0.5311],\n",
      "         [0.6334]],\n",
      "\n",
      "        [[0.7147],\n",
      "         [0.5311],\n",
      "         [0.6428]],\n",
      "\n",
      "        [[0.6657],\n",
      "         [0.5052],\n",
      "         [0.6420]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6956, 0.3044],\n",
      "         [0.5198, 0.4802],\n",
      "         [0.6167, 0.3833]],\n",
      "\n",
      "        [[0.7644, 0.2356],\n",
      "         [0.5182, 0.4818],\n",
      "         [0.7010, 0.2990]],\n",
      "\n",
      "        [[0.7045, 0.2955],\n",
      "         [0.5253, 0.4747],\n",
      "         [0.6166, 0.3834]],\n",
      "\n",
      "        [[0.6605, 0.3395],\n",
      "         [0.5126, 0.4874],\n",
      "         [0.5287, 0.4713]],\n",
      "\n",
      "        [[0.7566, 0.2434],\n",
      "         [0.5121, 0.4879],\n",
      "         [0.7099, 0.2901]],\n",
      "\n",
      "        [[0.6780, 0.3220],\n",
      "         [0.5085, 0.4915],\n",
      "         [0.6226, 0.3774]],\n",
      "\n",
      "        [[0.6593, 0.3407],\n",
      "         [0.5115, 0.4885],\n",
      "         [0.5349, 0.4651]],\n",
      "\n",
      "        [[0.6630, 0.3370],\n",
      "         [0.5174, 0.4826],\n",
      "         [0.5248, 0.4752]],\n",
      "\n",
      "        [[0.6786, 0.3214],\n",
      "         [0.5171, 0.4829],\n",
      "         [0.5280, 0.4720]],\n",
      "\n",
      "        [[0.6361, 0.3639],\n",
      "         [0.5131, 0.4869],\n",
      "         [0.6146, 0.3854]],\n",
      "\n",
      "        [[0.7217, 0.2783],\n",
      "         [0.5269, 0.4731],\n",
      "         [0.6519, 0.3481]],\n",
      "\n",
      "        [[0.6482, 0.3518],\n",
      "         [0.5091, 0.4909],\n",
      "         [0.5321, 0.4679]],\n",
      "\n",
      "        [[0.6397, 0.3603],\n",
      "         [0.5082, 0.4918],\n",
      "         [0.5462, 0.4538]],\n",
      "\n",
      "        [[0.6665, 0.3335],\n",
      "         [0.5191, 0.4809],\n",
      "         [0.5422, 0.4578]],\n",
      "\n",
      "        [[0.6866, 0.3134],\n",
      "         [0.5190, 0.4810],\n",
      "         [0.6205, 0.3795]],\n",
      "\n",
      "        [[0.6566, 0.3434],\n",
      "         [0.5111, 0.4889],\n",
      "         [0.5338, 0.4662]],\n",
      "\n",
      "        [[0.6945, 0.3055],\n",
      "         [0.5170, 0.4830],\n",
      "         [0.6129, 0.3871]],\n",
      "\n",
      "        [[0.7912, 0.2088],\n",
      "         [0.5254, 0.4746],\n",
      "         [0.6820, 0.3180]],\n",
      "\n",
      "        [[0.6377, 0.3623],\n",
      "         [0.5131, 0.4869],\n",
      "         [0.5250, 0.4750]],\n",
      "\n",
      "        [[0.7064, 0.2936],\n",
      "         [0.5165, 0.4835],\n",
      "         [0.6313, 0.3687]],\n",
      "\n",
      "        [[0.7106, 0.2894],\n",
      "         [0.5346, 0.4654],\n",
      "         [0.6455, 0.3545]],\n",
      "\n",
      "        [[0.6937, 0.3063],\n",
      "         [0.5151, 0.4849],\n",
      "         [0.5160, 0.4840]],\n",
      "\n",
      "        [[0.7250, 0.2750],\n",
      "         [0.5311, 0.4689],\n",
      "         [0.6334, 0.3666]],\n",
      "\n",
      "        [[0.7147, 0.2853],\n",
      "         [0.5311, 0.4689],\n",
      "         [0.6428, 0.3572]],\n",
      "\n",
      "        [[0.6657, 0.3343],\n",
      "         [0.5052, 0.4948],\n",
      "         [0.6420, 0.3580]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3616, 0.3340, 0.1877, 0.1167],\n",
      "        [0.3961, 0.3683, 0.1651, 0.0704],\n",
      "        [0.3701, 0.3344, 0.1822, 0.1133],\n",
      "        [0.3385, 0.3219, 0.1795, 0.1600],\n",
      "        [0.3875, 0.3692, 0.1727, 0.0706],\n",
      "        [0.3448, 0.3332, 0.2005, 0.1215],\n",
      "        [0.3372, 0.3221, 0.1822, 0.1585],\n",
      "        [0.3431, 0.3200, 0.1768, 0.1601],\n",
      "        [0.3509, 0.3277, 0.1697, 0.1517],\n",
      "        [0.3264, 0.3097, 0.2237, 0.1403],\n",
      "        [0.3803, 0.3414, 0.1814, 0.0969],\n",
      "        [0.3300, 0.3182, 0.1872, 0.1646],\n",
      "        [0.3251, 0.3146, 0.1968, 0.1635],\n",
      "        [0.3460, 0.3205, 0.1808, 0.1527],\n",
      "        [0.3564, 0.3302, 0.1945, 0.1189],\n",
      "        [0.3356, 0.3210, 0.1833, 0.1601],\n",
      "        [0.3590, 0.3354, 0.1873, 0.1183],\n",
      "        [0.4157, 0.3755, 0.1424, 0.0664],\n",
      "        [0.3272, 0.3105, 0.1902, 0.1721],\n",
      "        [0.3648, 0.3415, 0.1854, 0.1083],\n",
      "        [0.3799, 0.3307, 0.1868, 0.1026],\n",
      "        [0.3573, 0.3364, 0.1581, 0.1483],\n",
      "        [0.3851, 0.3399, 0.1742, 0.1008],\n",
      "        [0.3795, 0.3351, 0.1834, 0.1019],\n",
      "        [0.3363, 0.3294, 0.2146, 0.1197]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3616, 0.3340, 0.1877, 0.1167],\n",
      "        [0.3961, 0.3683, 0.1651, 0.0704],\n",
      "        [0.3701, 0.3344, 0.1822, 0.1133],\n",
      "        [0.3385, 0.3219, 0.1795, 0.1600],\n",
      "        [0.3875, 0.3692, 0.1727, 0.0706],\n",
      "        [0.3448, 0.3332, 0.2005, 0.1215],\n",
      "        [0.3372, 0.3221, 0.1822, 0.1585],\n",
      "        [0.3431, 0.3200, 0.1768, 0.1601],\n",
      "        [0.3509, 0.3277, 0.1697, 0.1517],\n",
      "        [0.3264, 0.3097, 0.2237, 0.1403],\n",
      "        [0.3803, 0.3414, 0.1814, 0.0969],\n",
      "        [0.3300, 0.3182, 0.1872, 0.1646],\n",
      "        [0.3251, 0.3146, 0.1968, 0.1635],\n",
      "        [0.3460, 0.3205, 0.1808, 0.1527],\n",
      "        [0.3564, 0.3302, 0.1945, 0.1189],\n",
      "        [0.3356, 0.3210, 0.1833, 0.1601],\n",
      "        [0.3590, 0.3354, 0.1873, 0.1183],\n",
      "        [0.4157, 0.3755, 0.1424, 0.0664],\n",
      "        [0.3272, 0.3105, 0.1902, 0.1721],\n",
      "        [0.3648, 0.3415, 0.1854, 0.1083],\n",
      "        [0.3799, 0.3307, 0.1868, 0.1026],\n",
      "        [0.3573, 0.3364, 0.1581, 0.1483],\n",
      "        [0.3851, 0.3399, 0.1742, 0.1008],\n",
      "        [0.3795, 0.3351, 0.1834, 0.1019],\n",
      "        [0.3363, 0.3294, 0.2146, 0.1197]], grad_fn=<ViewBackward>) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0112,  0.0322,  0.1059],\n",
      "        [-0.0123,  0.0352,  0.1160],\n",
      "        [-0.0115,  0.0329,  0.1084],\n",
      "        [-0.0105,  0.0301,  0.0991],\n",
      "        [-0.0120,  0.0345,  0.1135],\n",
      "        [-0.0107,  0.0307,  0.1010],\n",
      "        [-0.0105,  0.0300,  0.0988],\n",
      "        [-0.0107,  0.0305,  0.1005],\n",
      "        [-0.0109,  0.0312,  0.1028],\n",
      "        [-0.0101,  0.0290,  0.0956],\n",
      "        [-0.0118,  0.0338,  0.1114],\n",
      "        [-0.0103,  0.0294,  0.0966],\n",
      "        [-0.0101,  0.0289,  0.0952],\n",
      "        [-0.0108,  0.0308,  0.1013],\n",
      "        [-0.0111,  0.0317,  0.1044],\n",
      "        [-0.0104,  0.0299,  0.0983],\n",
      "        [-0.0112,  0.0319,  0.1051],\n",
      "        [-0.0129,  0.0370,  0.1217],\n",
      "        [-0.0102,  0.0291,  0.0958],\n",
      "        [-0.0113,  0.0325,  0.1068],\n",
      "        [-0.0118,  0.0338,  0.1112],\n",
      "        [-0.0111,  0.0318,  0.1046],\n",
      "        [-0.0120,  0.0343,  0.1128],\n",
      "        [-0.0118,  0.0338,  0.1111],\n",
      "        [-0.0105,  0.0299,  0.0985]], grad_fn=<MmBackward>)\n",
      "Epoch: 04 | Loss: 1.10677 | Correct: 007/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6955, 0.5199, 0.6119],\n",
      "        [0.7662, 0.5212, 0.6941],\n",
      "        [0.7046, 0.5255, 0.6117],\n",
      "        [0.6575, 0.5094, 0.5270],\n",
      "        [0.7586, 0.5154, 0.7030],\n",
      "        [0.6778, 0.5085, 0.6179],\n",
      "        [0.6565, 0.5084, 0.5330],\n",
      "        [0.6601, 0.5142, 0.5231],\n",
      "        [0.6758, 0.5140, 0.5261],\n",
      "        [0.6360, 0.5131, 0.6099],\n",
      "        [0.7226, 0.5284, 0.6460],\n",
      "        [0.6454, 0.5060, 0.5303],\n",
      "        [0.6371, 0.5055, 0.5439],\n",
      "        [0.6642, 0.5166, 0.5398],\n",
      "        [0.6867, 0.5193, 0.6156],\n",
      "        [0.6538, 0.5080, 0.5319],\n",
      "        [0.6942, 0.5169, 0.6083],\n",
      "        [0.7924, 0.5278, 0.6755],\n",
      "        [0.6348, 0.5099, 0.5233],\n",
      "        [0.7065, 0.5170, 0.6262],\n",
      "        [0.7117, 0.5361, 0.6395],\n",
      "        [0.6904, 0.5114, 0.5147],\n",
      "        [0.7256, 0.5321, 0.6278],\n",
      "        [0.7156, 0.5324, 0.6370],\n",
      "        [0.6660, 0.5058, 0.6368]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6955],\n",
      "         [0.5199],\n",
      "         [0.6119]],\n",
      "\n",
      "        [[0.7662],\n",
      "         [0.5212],\n",
      "         [0.6941]],\n",
      "\n",
      "        [[0.7046],\n",
      "         [0.5255],\n",
      "         [0.6117]],\n",
      "\n",
      "        [[0.6575],\n",
      "         [0.5094],\n",
      "         [0.5270]],\n",
      "\n",
      "        [[0.7586],\n",
      "         [0.5154],\n",
      "         [0.7030]],\n",
      "\n",
      "        [[0.6778],\n",
      "         [0.5085],\n",
      "         [0.6179]],\n",
      "\n",
      "        [[0.6565],\n",
      "         [0.5084],\n",
      "         [0.5330]],\n",
      "\n",
      "        [[0.6601],\n",
      "         [0.5142],\n",
      "         [0.5231]],\n",
      "\n",
      "        [[0.6758],\n",
      "         [0.5140],\n",
      "         [0.5261]],\n",
      "\n",
      "        [[0.6360],\n",
      "         [0.5131],\n",
      "         [0.6099]],\n",
      "\n",
      "        [[0.7226],\n",
      "         [0.5284],\n",
      "         [0.6460]],\n",
      "\n",
      "        [[0.6454],\n",
      "         [0.5060],\n",
      "         [0.5303]],\n",
      "\n",
      "        [[0.6371],\n",
      "         [0.5055],\n",
      "         [0.5439]],\n",
      "\n",
      "        [[0.6642],\n",
      "         [0.5166],\n",
      "         [0.5398]],\n",
      "\n",
      "        [[0.6867],\n",
      "         [0.5193],\n",
      "         [0.6156]],\n",
      "\n",
      "        [[0.6538],\n",
      "         [0.5080],\n",
      "         [0.5319]],\n",
      "\n",
      "        [[0.6942],\n",
      "         [0.5169],\n",
      "         [0.6083]],\n",
      "\n",
      "        [[0.7924],\n",
      "         [0.5278],\n",
      "         [0.6755]],\n",
      "\n",
      "        [[0.6348],\n",
      "         [0.5099],\n",
      "         [0.5233]],\n",
      "\n",
      "        [[0.7065],\n",
      "         [0.5170],\n",
      "         [0.6262]],\n",
      "\n",
      "        [[0.7117],\n",
      "         [0.5361],\n",
      "         [0.6395]],\n",
      "\n",
      "        [[0.6904],\n",
      "         [0.5114],\n",
      "         [0.5147]],\n",
      "\n",
      "        [[0.7256],\n",
      "         [0.5321],\n",
      "         [0.6278]],\n",
      "\n",
      "        [[0.7156],\n",
      "         [0.5324],\n",
      "         [0.6370]],\n",
      "\n",
      "        [[0.6660],\n",
      "         [0.5058],\n",
      "         [0.6368]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6955, 0.3045],\n",
      "         [0.5199, 0.4801],\n",
      "         [0.6119, 0.3881]],\n",
      "\n",
      "        [[0.7662, 0.2338],\n",
      "         [0.5212, 0.4788],\n",
      "         [0.6941, 0.3059]],\n",
      "\n",
      "        [[0.7046, 0.2954],\n",
      "         [0.5255, 0.4745],\n",
      "         [0.6117, 0.3883]],\n",
      "\n",
      "        [[0.6575, 0.3425],\n",
      "         [0.5094, 0.4906],\n",
      "         [0.5270, 0.4730]],\n",
      "\n",
      "        [[0.7586, 0.2414],\n",
      "         [0.5154, 0.4846],\n",
      "         [0.7030, 0.2970]],\n",
      "\n",
      "        [[0.6778, 0.3222],\n",
      "         [0.5085, 0.4915],\n",
      "         [0.6179, 0.3821]],\n",
      "\n",
      "        [[0.6565, 0.3435],\n",
      "         [0.5084, 0.4916],\n",
      "         [0.5330, 0.4670]],\n",
      "\n",
      "        [[0.6601, 0.3399],\n",
      "         [0.5142, 0.4858],\n",
      "         [0.5231, 0.4769]],\n",
      "\n",
      "        [[0.6758, 0.3242],\n",
      "         [0.5140, 0.4860],\n",
      "         [0.5261, 0.4739]],\n",
      "\n",
      "        [[0.6360, 0.3640],\n",
      "         [0.5131, 0.4869],\n",
      "         [0.6099, 0.3901]],\n",
      "\n",
      "        [[0.7226, 0.2774],\n",
      "         [0.5284, 0.4716],\n",
      "         [0.6460, 0.3540]],\n",
      "\n",
      "        [[0.6454, 0.3546],\n",
      "         [0.5060, 0.4940],\n",
      "         [0.5303, 0.4697]],\n",
      "\n",
      "        [[0.6371, 0.3629],\n",
      "         [0.5055, 0.4945],\n",
      "         [0.5439, 0.4561]],\n",
      "\n",
      "        [[0.6642, 0.3358],\n",
      "         [0.5166, 0.4834],\n",
      "         [0.5398, 0.4602]],\n",
      "\n",
      "        [[0.6867, 0.3133],\n",
      "         [0.5193, 0.4807],\n",
      "         [0.6156, 0.3844]],\n",
      "\n",
      "        [[0.6538, 0.3462],\n",
      "         [0.5080, 0.4920],\n",
      "         [0.5319, 0.4681]],\n",
      "\n",
      "        [[0.6942, 0.3058],\n",
      "         [0.5169, 0.4831],\n",
      "         [0.6083, 0.3917]],\n",
      "\n",
      "        [[0.7924, 0.2076],\n",
      "         [0.5278, 0.4722],\n",
      "         [0.6755, 0.3245]],\n",
      "\n",
      "        [[0.6348, 0.3652],\n",
      "         [0.5099, 0.4901],\n",
      "         [0.5233, 0.4767]],\n",
      "\n",
      "        [[0.7065, 0.2935],\n",
      "         [0.5170, 0.4830],\n",
      "         [0.6262, 0.3738]],\n",
      "\n",
      "        [[0.7117, 0.2883],\n",
      "         [0.5361, 0.4639],\n",
      "         [0.6395, 0.3605]],\n",
      "\n",
      "        [[0.6904, 0.3096],\n",
      "         [0.5114, 0.4886],\n",
      "         [0.5147, 0.4853]],\n",
      "\n",
      "        [[0.7256, 0.2744],\n",
      "         [0.5321, 0.4679],\n",
      "         [0.6278, 0.3722]],\n",
      "\n",
      "        [[0.7156, 0.2844],\n",
      "         [0.5324, 0.4676],\n",
      "         [0.6370, 0.3630]],\n",
      "\n",
      "        [[0.6660, 0.3340],\n",
      "         [0.5058, 0.4942],\n",
      "         [0.6368, 0.3632]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3616, 0.3339, 0.1863, 0.1182],\n",
      "        [0.3994, 0.3668, 0.1623, 0.0715],\n",
      "        [0.3703, 0.3343, 0.1807, 0.1147],\n",
      "        [0.3349, 0.3226, 0.1805, 0.1620],\n",
      "        [0.3909, 0.3676, 0.1697, 0.0717],\n",
      "        [0.3447, 0.3331, 0.1991, 0.1231],\n",
      "        [0.3338, 0.3227, 0.1831, 0.1604],\n",
      "        [0.3394, 0.3207, 0.1778, 0.1621],\n",
      "        [0.3474, 0.3284, 0.1706, 0.1536],\n",
      "        [0.3263, 0.3097, 0.2220, 0.1420],\n",
      "        [0.3818, 0.3408, 0.1792, 0.0982],\n",
      "        [0.3266, 0.3188, 0.1880, 0.1666],\n",
      "        [0.3221, 0.3151, 0.1974, 0.1655],\n",
      "        [0.3431, 0.3211, 0.1813, 0.1546],\n",
      "        [0.3565, 0.3301, 0.1929, 0.1205],\n",
      "        [0.3322, 0.3217, 0.1841, 0.1620],\n",
      "        [0.3588, 0.3354, 0.1860, 0.1198],\n",
      "        [0.4183, 0.3742, 0.1402, 0.0674],\n",
      "        [0.3237, 0.3111, 0.1911, 0.1741],\n",
      "        [0.3652, 0.3413, 0.1838, 0.1097],\n",
      "        [0.3816, 0.3301, 0.1843, 0.1039],\n",
      "        [0.3531, 0.3373, 0.1594, 0.1502],\n",
      "        [0.3860, 0.3395, 0.1723, 0.1021],\n",
      "        [0.3810, 0.3346, 0.1812, 0.1033],\n",
      "        [0.3369, 0.3291, 0.2127, 0.1213]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3616, 0.3339, 0.1863, 0.1182],\n",
      "        [0.3994, 0.3668, 0.1623, 0.0715],\n",
      "        [0.3703, 0.3343, 0.1807, 0.1147],\n",
      "        [0.3349, 0.3226, 0.1805, 0.1620],\n",
      "        [0.3909, 0.3676, 0.1697, 0.0717],\n",
      "        [0.3447, 0.3331, 0.1991, 0.1231],\n",
      "        [0.3338, 0.3227, 0.1831, 0.1604],\n",
      "        [0.3394, 0.3207, 0.1778, 0.1621],\n",
      "        [0.3474, 0.3284, 0.1706, 0.1536],\n",
      "        [0.3263, 0.3097, 0.2220, 0.1420],\n",
      "        [0.3818, 0.3408, 0.1792, 0.0982],\n",
      "        [0.3266, 0.3188, 0.1880, 0.1666],\n",
      "        [0.3221, 0.3151, 0.1974, 0.1655],\n",
      "        [0.3431, 0.3211, 0.1813, 0.1546],\n",
      "        [0.3565, 0.3301, 0.1929, 0.1205],\n",
      "        [0.3322, 0.3217, 0.1841, 0.1620],\n",
      "        [0.3588, 0.3354, 0.1860, 0.1198],\n",
      "        [0.4183, 0.3742, 0.1402, 0.0674],\n",
      "        [0.3237, 0.3111, 0.1911, 0.1741],\n",
      "        [0.3652, 0.3413, 0.1838, 0.1097],\n",
      "        [0.3816, 0.3301, 0.1843, 0.1039],\n",
      "        [0.3531, 0.3373, 0.1594, 0.1502],\n",
      "        [0.3860, 0.3395, 0.1723, 0.1021],\n",
      "        [0.3810, 0.3346, 0.1812, 0.1033],\n",
      "        [0.3369, 0.3291, 0.2127, 0.1213]], grad_fn=<ViewBackward>) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0077,  0.0287,  0.1025],\n",
      "        [-0.0085,  0.0316,  0.1132],\n",
      "        [-0.0079,  0.0293,  0.1049],\n",
      "        [-0.0072,  0.0265,  0.0949],\n",
      "        [-0.0084,  0.0310,  0.1108],\n",
      "        [-0.0074,  0.0273,  0.0977],\n",
      "        [-0.0071,  0.0265,  0.0946],\n",
      "        [-0.0073,  0.0269,  0.0962],\n",
      "        [-0.0074,  0.0275,  0.0984],\n",
      "        [-0.0070,  0.0259,  0.0925],\n",
      "        [-0.0082,  0.0303,  0.1082],\n",
      "        [-0.0070,  0.0259,  0.0925],\n",
      "        [-0.0069,  0.0255,  0.0913],\n",
      "        [-0.0073,  0.0272,  0.0972],\n",
      "        [-0.0076,  0.0283,  0.1010],\n",
      "        [-0.0071,  0.0263,  0.0941],\n",
      "        [-0.0077,  0.0284,  0.1017],\n",
      "        [-0.0089,  0.0331,  0.1185],\n",
      "        [-0.0069,  0.0256,  0.0917],\n",
      "        [-0.0078,  0.0289,  0.1035],\n",
      "        [-0.0082,  0.0302,  0.1081],\n",
      "        [-0.0076,  0.0280,  0.1001],\n",
      "        [-0.0083,  0.0306,  0.1094],\n",
      "        [-0.0081,  0.0302,  0.1080],\n",
      "        [-0.0072,  0.0267,  0.0955]], grad_fn=<MmBackward>)\n",
      "Epoch: 05 | Loss: 1.10604 | Correct: 007/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6955, 0.5200, 0.6071],\n",
      "        [0.7681, 0.5243, 0.6873],\n",
      "        [0.7047, 0.5258, 0.6067],\n",
      "        [0.6547, 0.5062, 0.5253],\n",
      "        [0.7606, 0.5186, 0.6960],\n",
      "        [0.6777, 0.5086, 0.6131],\n",
      "        [0.6538, 0.5055, 0.5311],\n",
      "        [0.6573, 0.5111, 0.5214],\n",
      "        [0.6731, 0.5110, 0.5243],\n",
      "        [0.6359, 0.5130, 0.6052],\n",
      "        [0.7237, 0.5299, 0.6401],\n",
      "        [0.6427, 0.5030, 0.5284],\n",
      "        [0.6347, 0.5029, 0.5417],\n",
      "        [0.6619, 0.5140, 0.5374],\n",
      "        [0.6868, 0.5195, 0.6107],\n",
      "        [0.6511, 0.5050, 0.5300],\n",
      "        [0.6940, 0.5168, 0.6037],\n",
      "        [0.7938, 0.5303, 0.6689],\n",
      "        [0.6319, 0.5067, 0.5216],\n",
      "        [0.7067, 0.5175, 0.6211],\n",
      "        [0.7129, 0.5377, 0.6335],\n",
      "        [0.6873, 0.5079, 0.5135],\n",
      "        [0.7262, 0.5330, 0.6223],\n",
      "        [0.7166, 0.5337, 0.6311],\n",
      "        [0.6664, 0.5065, 0.6317]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6955],\n",
      "         [0.5200],\n",
      "         [0.6071]],\n",
      "\n",
      "        [[0.7681],\n",
      "         [0.5243],\n",
      "         [0.6873]],\n",
      "\n",
      "        [[0.7047],\n",
      "         [0.5258],\n",
      "         [0.6067]],\n",
      "\n",
      "        [[0.6547],\n",
      "         [0.5062],\n",
      "         [0.5253]],\n",
      "\n",
      "        [[0.7606],\n",
      "         [0.5186],\n",
      "         [0.6960]],\n",
      "\n",
      "        [[0.6777],\n",
      "         [0.5086],\n",
      "         [0.6131]],\n",
      "\n",
      "        [[0.6538],\n",
      "         [0.5055],\n",
      "         [0.5311]],\n",
      "\n",
      "        [[0.6573],\n",
      "         [0.5111],\n",
      "         [0.5214]],\n",
      "\n",
      "        [[0.6731],\n",
      "         [0.5110],\n",
      "         [0.5243]],\n",
      "\n",
      "        [[0.6359],\n",
      "         [0.5130],\n",
      "         [0.6052]],\n",
      "\n",
      "        [[0.7237],\n",
      "         [0.5299],\n",
      "         [0.6401]],\n",
      "\n",
      "        [[0.6427],\n",
      "         [0.5030],\n",
      "         [0.5284]],\n",
      "\n",
      "        [[0.6347],\n",
      "         [0.5029],\n",
      "         [0.5417]],\n",
      "\n",
      "        [[0.6619],\n",
      "         [0.5140],\n",
      "         [0.5374]],\n",
      "\n",
      "        [[0.6868],\n",
      "         [0.5195],\n",
      "         [0.6107]],\n",
      "\n",
      "        [[0.6511],\n",
      "         [0.5050],\n",
      "         [0.5300]],\n",
      "\n",
      "        [[0.6940],\n",
      "         [0.5168],\n",
      "         [0.6037]],\n",
      "\n",
      "        [[0.7938],\n",
      "         [0.5303],\n",
      "         [0.6689]],\n",
      "\n",
      "        [[0.6319],\n",
      "         [0.5067],\n",
      "         [0.5216]],\n",
      "\n",
      "        [[0.7067],\n",
      "         [0.5175],\n",
      "         [0.6211]],\n",
      "\n",
      "        [[0.7129],\n",
      "         [0.5377],\n",
      "         [0.6335]],\n",
      "\n",
      "        [[0.6873],\n",
      "         [0.5079],\n",
      "         [0.5135]],\n",
      "\n",
      "        [[0.7262],\n",
      "         [0.5330],\n",
      "         [0.6223]],\n",
      "\n",
      "        [[0.7166],\n",
      "         [0.5337],\n",
      "         [0.6311]],\n",
      "\n",
      "        [[0.6664],\n",
      "         [0.5065],\n",
      "         [0.6317]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6955, 0.3045],\n",
      "         [0.5200, 0.4800],\n",
      "         [0.6071, 0.3929]],\n",
      "\n",
      "        [[0.7681, 0.2319],\n",
      "         [0.5243, 0.4757],\n",
      "         [0.6873, 0.3127]],\n",
      "\n",
      "        [[0.7047, 0.2953],\n",
      "         [0.5258, 0.4742],\n",
      "         [0.6067, 0.3933]],\n",
      "\n",
      "        [[0.6547, 0.3453],\n",
      "         [0.5062, 0.4938],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.7606, 0.2394],\n",
      "         [0.5186, 0.4814],\n",
      "         [0.6960, 0.3040]],\n",
      "\n",
      "        [[0.6777, 0.3223],\n",
      "         [0.5086, 0.4914],\n",
      "         [0.6131, 0.3869]],\n",
      "\n",
      "        [[0.6538, 0.3462],\n",
      "         [0.5055, 0.4945],\n",
      "         [0.5311, 0.4689]],\n",
      "\n",
      "        [[0.6573, 0.3427],\n",
      "         [0.5111, 0.4889],\n",
      "         [0.5214, 0.4786]],\n",
      "\n",
      "        [[0.6731, 0.3269],\n",
      "         [0.5110, 0.4890],\n",
      "         [0.5243, 0.4757]],\n",
      "\n",
      "        [[0.6359, 0.3641],\n",
      "         [0.5130, 0.4870],\n",
      "         [0.6052, 0.3948]],\n",
      "\n",
      "        [[0.7237, 0.2763],\n",
      "         [0.5299, 0.4701],\n",
      "         [0.6401, 0.3599]],\n",
      "\n",
      "        [[0.6427, 0.3573],\n",
      "         [0.5030, 0.4970],\n",
      "         [0.5284, 0.4716]],\n",
      "\n",
      "        [[0.6347, 0.3653],\n",
      "         [0.5029, 0.4971],\n",
      "         [0.5417, 0.4583]],\n",
      "\n",
      "        [[0.6619, 0.3381],\n",
      "         [0.5140, 0.4860],\n",
      "         [0.5374, 0.4626]],\n",
      "\n",
      "        [[0.6868, 0.3132],\n",
      "         [0.5195, 0.4805],\n",
      "         [0.6107, 0.3893]],\n",
      "\n",
      "        [[0.6511, 0.3489],\n",
      "         [0.5050, 0.4950],\n",
      "         [0.5300, 0.4700]],\n",
      "\n",
      "        [[0.6940, 0.3060],\n",
      "         [0.5168, 0.4832],\n",
      "         [0.6037, 0.3963]],\n",
      "\n",
      "        [[0.7938, 0.2062],\n",
      "         [0.5303, 0.4697],\n",
      "         [0.6689, 0.3311]],\n",
      "\n",
      "        [[0.6319, 0.3681],\n",
      "         [0.5067, 0.4933],\n",
      "         [0.5216, 0.4784]],\n",
      "\n",
      "        [[0.7067, 0.2933],\n",
      "         [0.5175, 0.4825],\n",
      "         [0.6211, 0.3789]],\n",
      "\n",
      "        [[0.7129, 0.2871],\n",
      "         [0.5377, 0.4623],\n",
      "         [0.6335, 0.3665]],\n",
      "\n",
      "        [[0.6873, 0.3127],\n",
      "         [0.5079, 0.4921],\n",
      "         [0.5135, 0.4865]],\n",
      "\n",
      "        [[0.7262, 0.2738],\n",
      "         [0.5330, 0.4670],\n",
      "         [0.6223, 0.3777]],\n",
      "\n",
      "        [[0.7166, 0.2834],\n",
      "         [0.5337, 0.4663],\n",
      "         [0.6311, 0.3689]],\n",
      "\n",
      "        [[0.6664, 0.3336],\n",
      "         [0.5065, 0.4935],\n",
      "         [0.6317, 0.3683]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3616, 0.3338, 0.1849, 0.1197],\n",
      "        [0.4027, 0.3654, 0.1594, 0.0725],\n",
      "        [0.3705, 0.3342, 0.1792, 0.1161],\n",
      "        [0.3314, 0.3232, 0.1814, 0.1639],\n",
      "        [0.3945, 0.3661, 0.1666, 0.0728],\n",
      "        [0.3446, 0.3331, 0.1976, 0.1247],\n",
      "        [0.3305, 0.3233, 0.1839, 0.1624],\n",
      "        [0.3359, 0.3214, 0.1787, 0.1640],\n",
      "        [0.3439, 0.3292, 0.1714, 0.1555],\n",
      "        [0.3263, 0.3097, 0.2204, 0.1437],\n",
      "        [0.3834, 0.3402, 0.1769, 0.0995],\n",
      "        [0.3232, 0.3194, 0.1888, 0.1685],\n",
      "        [0.3192, 0.3155, 0.1979, 0.1674],\n",
      "        [0.3402, 0.3216, 0.1817, 0.1564],\n",
      "        [0.3568, 0.3300, 0.1913, 0.1220],\n",
      "        [0.3288, 0.3223, 0.1849, 0.1640],\n",
      "        [0.3586, 0.3354, 0.1847, 0.1213],\n",
      "        [0.4209, 0.3728, 0.1380, 0.0683],\n",
      "        [0.3202, 0.3117, 0.1920, 0.1761],\n",
      "        [0.3657, 0.3410, 0.1822, 0.1111],\n",
      "        [0.3833, 0.3296, 0.1819, 0.1052],\n",
      "        [0.3490, 0.3382, 0.1606, 0.1522],\n",
      "        [0.3871, 0.3391, 0.1704, 0.1034],\n",
      "        [0.3825, 0.3341, 0.1789, 0.1045],\n",
      "        [0.3375, 0.3289, 0.2107, 0.1229]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3616, 0.3338, 0.1849, 0.1197],\n",
      "        [0.4027, 0.3654, 0.1594, 0.0725],\n",
      "        [0.3705, 0.3342, 0.1792, 0.1161],\n",
      "        [0.3314, 0.3232, 0.1814, 0.1639],\n",
      "        [0.3945, 0.3661, 0.1666, 0.0728],\n",
      "        [0.3446, 0.3331, 0.1976, 0.1247],\n",
      "        [0.3305, 0.3233, 0.1839, 0.1624],\n",
      "        [0.3359, 0.3214, 0.1787, 0.1640],\n",
      "        [0.3439, 0.3292, 0.1714, 0.1555],\n",
      "        [0.3263, 0.3097, 0.2204, 0.1437],\n",
      "        [0.3834, 0.3402, 0.1769, 0.0995],\n",
      "        [0.3232, 0.3194, 0.1888, 0.1685],\n",
      "        [0.3192, 0.3155, 0.1979, 0.1674],\n",
      "        [0.3402, 0.3216, 0.1817, 0.1564],\n",
      "        [0.3568, 0.3300, 0.1913, 0.1220],\n",
      "        [0.3288, 0.3223, 0.1849, 0.1640],\n",
      "        [0.3586, 0.3354, 0.1847, 0.1213],\n",
      "        [0.4209, 0.3728, 0.1380, 0.0683],\n",
      "        [0.3202, 0.3117, 0.1920, 0.1761],\n",
      "        [0.3657, 0.3410, 0.1822, 0.1111],\n",
      "        [0.3833, 0.3296, 0.1819, 0.1052],\n",
      "        [0.3490, 0.3382, 0.1606, 0.1522],\n",
      "        [0.3871, 0.3391, 0.1704, 0.1034],\n",
      "        [0.3825, 0.3341, 0.1789, 0.1045],\n",
      "        [0.3375, 0.3289, 0.2107, 0.1229]], grad_fn=<ViewBackward>) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0043,  0.0252,  0.0991],\n",
      "        [-0.0047,  0.0280,  0.1103],\n",
      "        [-0.0044,  0.0258,  0.1015],\n",
      "        [-0.0039,  0.0231,  0.0908],\n",
      "        [-0.0046,  0.0274,  0.1081],\n",
      "        [-0.0041,  0.0240,  0.0944],\n",
      "        [-0.0039,  0.0230,  0.0905],\n",
      "        [-0.0040,  0.0234,  0.0920],\n",
      "        [-0.0041,  0.0239,  0.0942],\n",
      "        [-0.0038,  0.0227,  0.0894],\n",
      "        [-0.0045,  0.0267,  0.1051],\n",
      "        [-0.0038,  0.0225,  0.0886],\n",
      "        [-0.0038,  0.0222,  0.0874],\n",
      "        [-0.0040,  0.0237,  0.0932],\n",
      "        [-0.0042,  0.0248,  0.0978],\n",
      "        [-0.0039,  0.0229,  0.0901],\n",
      "        [-0.0042,  0.0249,  0.0983],\n",
      "        [-0.0050,  0.0293,  0.1153],\n",
      "        [-0.0038,  0.0223,  0.0877],\n",
      "        [-0.0043,  0.0254,  0.1002],\n",
      "        [-0.0045,  0.0267,  0.1050],\n",
      "        [-0.0041,  0.0243,  0.0956],\n",
      "        [-0.0046,  0.0269,  0.1061],\n",
      "        [-0.0045,  0.0266,  0.1048],\n",
      "        [-0.0040,  0.0235,  0.0925]], grad_fn=<MmBackward>)\n",
      "Epoch: 06 | Loss: 1.10537 | Correct: 007/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6955, 0.5201, 0.6023],\n",
      "        [0.7700, 0.5274, 0.6804],\n",
      "        [0.7049, 0.5261, 0.6018],\n",
      "        [0.6518, 0.5032, 0.5235],\n",
      "        [0.7627, 0.5219, 0.6890],\n",
      "        [0.6776, 0.5086, 0.6085],\n",
      "        [0.6511, 0.5025, 0.5292],\n",
      "        [0.6545, 0.5080, 0.5197],\n",
      "        [0.6705, 0.5080, 0.5225],\n",
      "        [0.6359, 0.5130, 0.6006],\n",
      "        [0.7247, 0.5314, 0.6342],\n",
      "        [0.6399, 0.5000, 0.5266],\n",
      "        [0.6323, 0.5003, 0.5395],\n",
      "        [0.6596, 0.5116, 0.5351],\n",
      "        [0.6869, 0.5198, 0.6058],\n",
      "        [0.6484, 0.5021, 0.5282],\n",
      "        [0.6939, 0.5167, 0.5991],\n",
      "        [0.7951, 0.5328, 0.6624],\n",
      "        [0.6290, 0.5036, 0.5199],\n",
      "        [0.7070, 0.5180, 0.6160],\n",
      "        [0.7141, 0.5392, 0.6275],\n",
      "        [0.6841, 0.5043, 0.5122],\n",
      "        [0.7269, 0.5340, 0.6168],\n",
      "        [0.7176, 0.5351, 0.6253],\n",
      "        [0.6668, 0.5071, 0.6265]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6955],\n",
      "         [0.5201],\n",
      "         [0.6023]],\n",
      "\n",
      "        [[0.7700],\n",
      "         [0.5274],\n",
      "         [0.6804]],\n",
      "\n",
      "        [[0.7049],\n",
      "         [0.5261],\n",
      "         [0.6018]],\n",
      "\n",
      "        [[0.6518],\n",
      "         [0.5032],\n",
      "         [0.5235]],\n",
      "\n",
      "        [[0.7627],\n",
      "         [0.5219],\n",
      "         [0.6890]],\n",
      "\n",
      "        [[0.6776],\n",
      "         [0.5086],\n",
      "         [0.6085]],\n",
      "\n",
      "        [[0.6511],\n",
      "         [0.5025],\n",
      "         [0.5292]],\n",
      "\n",
      "        [[0.6545],\n",
      "         [0.5080],\n",
      "         [0.5197]],\n",
      "\n",
      "        [[0.6705],\n",
      "         [0.5080],\n",
      "         [0.5225]],\n",
      "\n",
      "        [[0.6359],\n",
      "         [0.5130],\n",
      "         [0.6006]],\n",
      "\n",
      "        [[0.7247],\n",
      "         [0.5314],\n",
      "         [0.6342]],\n",
      "\n",
      "        [[0.6399],\n",
      "         [0.5000],\n",
      "         [0.5266]],\n",
      "\n",
      "        [[0.6323],\n",
      "         [0.5003],\n",
      "         [0.5395]],\n",
      "\n",
      "        [[0.6596],\n",
      "         [0.5116],\n",
      "         [0.5351]],\n",
      "\n",
      "        [[0.6869],\n",
      "         [0.5198],\n",
      "         [0.6058]],\n",
      "\n",
      "        [[0.6484],\n",
      "         [0.5021],\n",
      "         [0.5282]],\n",
      "\n",
      "        [[0.6939],\n",
      "         [0.5167],\n",
      "         [0.5991]],\n",
      "\n",
      "        [[0.7951],\n",
      "         [0.5328],\n",
      "         [0.6624]],\n",
      "\n",
      "        [[0.6290],\n",
      "         [0.5036],\n",
      "         [0.5199]],\n",
      "\n",
      "        [[0.7070],\n",
      "         [0.5180],\n",
      "         [0.6160]],\n",
      "\n",
      "        [[0.7141],\n",
      "         [0.5392],\n",
      "         [0.6275]],\n",
      "\n",
      "        [[0.6841],\n",
      "         [0.5043],\n",
      "         [0.5122]],\n",
      "\n",
      "        [[0.7269],\n",
      "         [0.5340],\n",
      "         [0.6168]],\n",
      "\n",
      "        [[0.7176],\n",
      "         [0.5351],\n",
      "         [0.6253]],\n",
      "\n",
      "        [[0.6668],\n",
      "         [0.5071],\n",
      "         [0.6265]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6955, 0.3045],\n",
      "         [0.5201, 0.4799],\n",
      "         [0.6023, 0.3977]],\n",
      "\n",
      "        [[0.7700, 0.2300],\n",
      "         [0.5274, 0.4726],\n",
      "         [0.6804, 0.3196]],\n",
      "\n",
      "        [[0.7049, 0.2951],\n",
      "         [0.5261, 0.4739],\n",
      "         [0.6018, 0.3982]],\n",
      "\n",
      "        [[0.6518, 0.3482],\n",
      "         [0.5032, 0.4968],\n",
      "         [0.5235, 0.4765]],\n",
      "\n",
      "        [[0.7627, 0.2373],\n",
      "         [0.5219, 0.4781],\n",
      "         [0.6890, 0.3110]],\n",
      "\n",
      "        [[0.6776, 0.3224],\n",
      "         [0.5086, 0.4914],\n",
      "         [0.6085, 0.3915]],\n",
      "\n",
      "        [[0.6511, 0.3489],\n",
      "         [0.5025, 0.4975],\n",
      "         [0.5292, 0.4708]],\n",
      "\n",
      "        [[0.6545, 0.3455],\n",
      "         [0.5080, 0.4920],\n",
      "         [0.5197, 0.4803]],\n",
      "\n",
      "        [[0.6705, 0.3295],\n",
      "         [0.5080, 0.4920],\n",
      "         [0.5225, 0.4775]],\n",
      "\n",
      "        [[0.6359, 0.3641],\n",
      "         [0.5130, 0.4870],\n",
      "         [0.6006, 0.3994]],\n",
      "\n",
      "        [[0.7247, 0.2753],\n",
      "         [0.5314, 0.4686],\n",
      "         [0.6342, 0.3658]],\n",
      "\n",
      "        [[0.6399, 0.3601],\n",
      "         [0.5000, 0.5000],\n",
      "         [0.5266, 0.4734]],\n",
      "\n",
      "        [[0.6323, 0.3677],\n",
      "         [0.5003, 0.4997],\n",
      "         [0.5395, 0.4605]],\n",
      "\n",
      "        [[0.6596, 0.3404],\n",
      "         [0.5116, 0.4884],\n",
      "         [0.5351, 0.4649]],\n",
      "\n",
      "        [[0.6869, 0.3131],\n",
      "         [0.5198, 0.4802],\n",
      "         [0.6058, 0.3942]],\n",
      "\n",
      "        [[0.6484, 0.3516],\n",
      "         [0.5021, 0.4979],\n",
      "         [0.5282, 0.4718]],\n",
      "\n",
      "        [[0.6939, 0.3061],\n",
      "         [0.5167, 0.4833],\n",
      "         [0.5991, 0.4009]],\n",
      "\n",
      "        [[0.7951, 0.2049],\n",
      "         [0.5328, 0.4672],\n",
      "         [0.6624, 0.3376]],\n",
      "\n",
      "        [[0.6290, 0.3710],\n",
      "         [0.5036, 0.4964],\n",
      "         [0.5199, 0.4801]],\n",
      "\n",
      "        [[0.7070, 0.2930],\n",
      "         [0.5180, 0.4820],\n",
      "         [0.6160, 0.3840]],\n",
      "\n",
      "        [[0.7141, 0.2859],\n",
      "         [0.5392, 0.4608],\n",
      "         [0.6275, 0.3725]],\n",
      "\n",
      "        [[0.6841, 0.3159],\n",
      "         [0.5043, 0.4957],\n",
      "         [0.5122, 0.4878]],\n",
      "\n",
      "        [[0.7269, 0.2731],\n",
      "         [0.5340, 0.4660],\n",
      "         [0.6168, 0.3832]],\n",
      "\n",
      "        [[0.7176, 0.2824],\n",
      "         [0.5351, 0.4649],\n",
      "         [0.6253, 0.3747]],\n",
      "\n",
      "        [[0.6668, 0.3332],\n",
      "         [0.5071, 0.4929],\n",
      "         [0.6265, 0.3735]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3617, 0.3337, 0.1834, 0.1211],\n",
      "        [0.4060, 0.3639, 0.1565, 0.0735],\n",
      "        [0.3708, 0.3341, 0.1776, 0.1175],\n",
      "        [0.3280, 0.3239, 0.1823, 0.1659],\n",
      "        [0.3981, 0.3647, 0.1635, 0.0738],\n",
      "        [0.3447, 0.3330, 0.1961, 0.1262],\n",
      "        [0.3272, 0.3239, 0.1846, 0.1643],\n",
      "        [0.3324, 0.3220, 0.1796, 0.1660],\n",
      "        [0.3406, 0.3299, 0.1722, 0.1573],\n",
      "        [0.3262, 0.3097, 0.2187, 0.1454],\n",
      "        [0.3851, 0.3396, 0.1746, 0.1007],\n",
      "        [0.3199, 0.3200, 0.1896, 0.1704],\n",
      "        [0.3163, 0.3160, 0.1984, 0.1693],\n",
      "        [0.3375, 0.3222, 0.1821, 0.1582],\n",
      "        [0.3570, 0.3299, 0.1896, 0.1234],\n",
      "        [0.3255, 0.3228, 0.1857, 0.1659],\n",
      "        [0.3585, 0.3354, 0.1834, 0.1227],\n",
      "        [0.4236, 0.3715, 0.1357, 0.0692],\n",
      "        [0.3168, 0.3122, 0.1929, 0.1781],\n",
      "        [0.3662, 0.3408, 0.1805, 0.1125],\n",
      "        [0.3850, 0.3291, 0.1794, 0.1065],\n",
      "        [0.3451, 0.3391, 0.1618, 0.1541],\n",
      "        [0.3881, 0.3387, 0.1685, 0.1047],\n",
      "        [0.3840, 0.3336, 0.1766, 0.1058],\n",
      "        [0.3381, 0.3287, 0.2087, 0.1244]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3617, 0.3337, 0.1834, 0.1211],\n",
      "        [0.4060, 0.3639, 0.1565, 0.0735],\n",
      "        [0.3708, 0.3341, 0.1776, 0.1175],\n",
      "        [0.3280, 0.3239, 0.1823, 0.1659],\n",
      "        [0.3981, 0.3647, 0.1635, 0.0738],\n",
      "        [0.3447, 0.3330, 0.1961, 0.1262],\n",
      "        [0.3272, 0.3239, 0.1846, 0.1643],\n",
      "        [0.3324, 0.3220, 0.1796, 0.1660],\n",
      "        [0.3406, 0.3299, 0.1722, 0.1573],\n",
      "        [0.3262, 0.3097, 0.2187, 0.1454],\n",
      "        [0.3851, 0.3396, 0.1746, 0.1007],\n",
      "        [0.3199, 0.3200, 0.1896, 0.1704],\n",
      "        [0.3163, 0.3160, 0.1984, 0.1693],\n",
      "        [0.3375, 0.3222, 0.1821, 0.1582],\n",
      "        [0.3570, 0.3299, 0.1896, 0.1234],\n",
      "        [0.3255, 0.3228, 0.1857, 0.1659],\n",
      "        [0.3585, 0.3354, 0.1834, 0.1227],\n",
      "        [0.4236, 0.3715, 0.1357, 0.0692],\n",
      "        [0.3168, 0.3122, 0.1929, 0.1781],\n",
      "        [0.3662, 0.3408, 0.1805, 0.1125],\n",
      "        [0.3850, 0.3291, 0.1794, 0.1065],\n",
      "        [0.3451, 0.3391, 0.1618, 0.1541],\n",
      "        [0.3881, 0.3387, 0.1685, 0.1047],\n",
      "        [0.3840, 0.3336, 0.1766, 0.1058],\n",
      "        [0.3381, 0.3287, 0.2087, 0.1244]], grad_fn=<ViewBackward>) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0008,  0.0217,  0.0958],\n",
      "        [-0.0009,  0.0244,  0.1075],\n",
      "        [-0.0009,  0.0222,  0.0982],\n",
      "        [-0.0008,  0.0197,  0.0868],\n",
      "        [-0.0009,  0.0239,  0.1054],\n",
      "        [-0.0008,  0.0207,  0.0912],\n",
      "        [-0.0008,  0.0196,  0.0866],\n",
      "        [-0.0008,  0.0199,  0.0880],\n",
      "        [-0.0008,  0.0204,  0.0902],\n",
      "        [-0.0007,  0.0196,  0.0864],\n",
      "        [-0.0009,  0.0231,  0.1019],\n",
      "        [ 0.1110, -0.0842, -0.1144],\n",
      "        [-0.0007,  0.0190,  0.0837],\n",
      "        [-0.0008,  0.0202,  0.0893],\n",
      "        [-0.0008,  0.0214,  0.0945],\n",
      "        [-0.0007,  0.0195,  0.0862],\n",
      "        [-0.0008,  0.0215,  0.0949],\n",
      "        [-0.0010,  0.0254,  0.1121],\n",
      "        [-0.0007,  0.0190,  0.0838],\n",
      "        [-0.0008,  0.0220,  0.0969],\n",
      "        [-0.0009,  0.0231,  0.1019],\n",
      "        [-0.0008,  0.0207,  0.0913],\n",
      "        [-0.0009,  0.0233,  0.1027],\n",
      "        [-0.0009,  0.0230,  0.1016],\n",
      "        [-0.0008,  0.0203,  0.0895]], grad_fn=<MmBackward>)\n",
      "Epoch: 07 | Loss: 1.09793 | Correct: 008/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6958, 0.5202, 0.5976],\n",
      "        [0.7722, 0.5303, 0.6734],\n",
      "        [0.7053, 0.5263, 0.5969],\n",
      "        [0.6493, 0.5000, 0.5219],\n",
      "        [0.7651, 0.5250, 0.6820],\n",
      "        [0.6779, 0.5086, 0.6038],\n",
      "        [0.6487, 0.4996, 0.5273],\n",
      "        [0.6520, 0.5048, 0.5180],\n",
      "        [0.6681, 0.5050, 0.5207],\n",
      "        [0.6361, 0.5129, 0.5960],\n",
      "        [0.7260, 0.5328, 0.6283],\n",
      "        [0.6375, 0.4969, 0.5248],\n",
      "        [0.6301, 0.4977, 0.5373],\n",
      "        [0.6577, 0.5090, 0.5328],\n",
      "        [0.6873, 0.5199, 0.6009],\n",
      "        [0.6459, 0.4991, 0.5263],\n",
      "        [0.6940, 0.5165, 0.5945],\n",
      "        [0.7968, 0.5351, 0.6558],\n",
      "        [0.6264, 0.5004, 0.5183],\n",
      "        [0.7075, 0.5184, 0.6110],\n",
      "        [0.7155, 0.5406, 0.6215],\n",
      "        [0.6814, 0.5008, 0.5109],\n",
      "        [0.7278, 0.5348, 0.6113],\n",
      "        [0.7189, 0.5363, 0.6195],\n",
      "        [0.6675, 0.5076, 0.6213]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6958],\n",
      "         [0.5202],\n",
      "         [0.5976]],\n",
      "\n",
      "        [[0.7722],\n",
      "         [0.5303],\n",
      "         [0.6734]],\n",
      "\n",
      "        [[0.7053],\n",
      "         [0.5263],\n",
      "         [0.5969]],\n",
      "\n",
      "        [[0.6493],\n",
      "         [0.5000],\n",
      "         [0.5219]],\n",
      "\n",
      "        [[0.7651],\n",
      "         [0.5250],\n",
      "         [0.6820]],\n",
      "\n",
      "        [[0.6779],\n",
      "         [0.5086],\n",
      "         [0.6038]],\n",
      "\n",
      "        [[0.6487],\n",
      "         [0.4996],\n",
      "         [0.5273]],\n",
      "\n",
      "        [[0.6520],\n",
      "         [0.5048],\n",
      "         [0.5180]],\n",
      "\n",
      "        [[0.6681],\n",
      "         [0.5050],\n",
      "         [0.5207]],\n",
      "\n",
      "        [[0.6361],\n",
      "         [0.5129],\n",
      "         [0.5960]],\n",
      "\n",
      "        [[0.7260],\n",
      "         [0.5328],\n",
      "         [0.6283]],\n",
      "\n",
      "        [[0.6375],\n",
      "         [0.4969],\n",
      "         [0.5248]],\n",
      "\n",
      "        [[0.6301],\n",
      "         [0.4977],\n",
      "         [0.5373]],\n",
      "\n",
      "        [[0.6577],\n",
      "         [0.5090],\n",
      "         [0.5328]],\n",
      "\n",
      "        [[0.6873],\n",
      "         [0.5199],\n",
      "         [0.6009]],\n",
      "\n",
      "        [[0.6459],\n",
      "         [0.4991],\n",
      "         [0.5263]],\n",
      "\n",
      "        [[0.6940],\n",
      "         [0.5165],\n",
      "         [0.5945]],\n",
      "\n",
      "        [[0.7968],\n",
      "         [0.5351],\n",
      "         [0.6558]],\n",
      "\n",
      "        [[0.6264],\n",
      "         [0.5004],\n",
      "         [0.5183]],\n",
      "\n",
      "        [[0.7075],\n",
      "         [0.5184],\n",
      "         [0.6110]],\n",
      "\n",
      "        [[0.7155],\n",
      "         [0.5406],\n",
      "         [0.6215]],\n",
      "\n",
      "        [[0.6814],\n",
      "         [0.5008],\n",
      "         [0.5109]],\n",
      "\n",
      "        [[0.7278],\n",
      "         [0.5348],\n",
      "         [0.6113]],\n",
      "\n",
      "        [[0.7189],\n",
      "         [0.5363],\n",
      "         [0.6195]],\n",
      "\n",
      "        [[0.6675],\n",
      "         [0.5076],\n",
      "         [0.6213]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6958, 0.3042],\n",
      "         [0.5202, 0.4798],\n",
      "         [0.5976, 0.4024]],\n",
      "\n",
      "        [[0.7722, 0.2278],\n",
      "         [0.5303, 0.4697],\n",
      "         [0.6734, 0.3266]],\n",
      "\n",
      "        [[0.7053, 0.2947],\n",
      "         [0.5263, 0.4737],\n",
      "         [0.5969, 0.4031]],\n",
      "\n",
      "        [[0.6493, 0.3507],\n",
      "         [0.5000, 0.5000],\n",
      "         [0.5219, 0.4781]],\n",
      "\n",
      "        [[0.7651, 0.2349],\n",
      "         [0.5250, 0.4750],\n",
      "         [0.6820, 0.3180]],\n",
      "\n",
      "        [[0.6779, 0.3221],\n",
      "         [0.5086, 0.4914],\n",
      "         [0.6038, 0.3962]],\n",
      "\n",
      "        [[0.6487, 0.3513],\n",
      "         [0.4996, 0.5004],\n",
      "         [0.5273, 0.4727]],\n",
      "\n",
      "        [[0.6520, 0.3480],\n",
      "         [0.5048, 0.4952],\n",
      "         [0.5180, 0.4820]],\n",
      "\n",
      "        [[0.6681, 0.3319],\n",
      "         [0.5050, 0.4950],\n",
      "         [0.5207, 0.4793]],\n",
      "\n",
      "        [[0.6361, 0.3639],\n",
      "         [0.5129, 0.4871],\n",
      "         [0.5960, 0.4040]],\n",
      "\n",
      "        [[0.7260, 0.2740],\n",
      "         [0.5328, 0.4672],\n",
      "         [0.6283, 0.3717]],\n",
      "\n",
      "        [[0.6375, 0.3625],\n",
      "         [0.4969, 0.5031],\n",
      "         [0.5248, 0.4752]],\n",
      "\n",
      "        [[0.6301, 0.3699],\n",
      "         [0.4977, 0.5023],\n",
      "         [0.5373, 0.4627]],\n",
      "\n",
      "        [[0.6577, 0.3423],\n",
      "         [0.5090, 0.4910],\n",
      "         [0.5328, 0.4672]],\n",
      "\n",
      "        [[0.6873, 0.3127],\n",
      "         [0.5199, 0.4801],\n",
      "         [0.6009, 0.3991]],\n",
      "\n",
      "        [[0.6459, 0.3541],\n",
      "         [0.4991, 0.5009],\n",
      "         [0.5263, 0.4737]],\n",
      "\n",
      "        [[0.6940, 0.3060],\n",
      "         [0.5165, 0.4835],\n",
      "         [0.5945, 0.4055]],\n",
      "\n",
      "        [[0.7968, 0.2032],\n",
      "         [0.5351, 0.4649],\n",
      "         [0.6558, 0.3442]],\n",
      "\n",
      "        [[0.6264, 0.3736],\n",
      "         [0.5004, 0.4996],\n",
      "         [0.5183, 0.4817]],\n",
      "\n",
      "        [[0.7075, 0.2925],\n",
      "         [0.5184, 0.4816],\n",
      "         [0.6110, 0.3890]],\n",
      "\n",
      "        [[0.7155, 0.2845],\n",
      "         [0.5406, 0.4594],\n",
      "         [0.6215, 0.3785]],\n",
      "\n",
      "        [[0.6814, 0.3186],\n",
      "         [0.5008, 0.4992],\n",
      "         [0.5109, 0.4891]],\n",
      "\n",
      "        [[0.7278, 0.2722],\n",
      "         [0.5348, 0.4652],\n",
      "         [0.6113, 0.3887]],\n",
      "\n",
      "        [[0.7189, 0.2811],\n",
      "         [0.5363, 0.4637],\n",
      "         [0.6195, 0.3805]],\n",
      "\n",
      "        [[0.6675, 0.3325],\n",
      "         [0.5076, 0.4924],\n",
      "         [0.6213, 0.3787]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3619, 0.3339, 0.1818, 0.1224],\n",
      "        [0.4095, 0.3627, 0.1534, 0.0744],\n",
      "        [0.3712, 0.3341, 0.1759, 0.1188],\n",
      "        [0.3247, 0.3246, 0.1830, 0.1677],\n",
      "        [0.4017, 0.3634, 0.1602, 0.0747],\n",
      "        [0.3447, 0.3331, 0.1945, 0.1276],\n",
      "        [0.3240, 0.3246, 0.1853, 0.1661],\n",
      "        [0.3291, 0.3228, 0.1803, 0.1678],\n",
      "        [0.3374, 0.3307, 0.1728, 0.1591],\n",
      "        [0.3263, 0.3098, 0.2169, 0.1470],\n",
      "        [0.3868, 0.3392, 0.1721, 0.1018],\n",
      "        [0.3168, 0.3207, 0.1903, 0.1723],\n",
      "        [0.3136, 0.3165, 0.1987, 0.1712],\n",
      "        [0.3348, 0.3229, 0.1824, 0.1599],\n",
      "        [0.3574, 0.3300, 0.1879, 0.1248],\n",
      "        [0.3224, 0.3236, 0.1864, 0.1677],\n",
      "        [0.3585, 0.3355, 0.1819, 0.1241],\n",
      "        [0.4264, 0.3704, 0.1333, 0.0699],\n",
      "        [0.3135, 0.3129, 0.1936, 0.1800],\n",
      "        [0.3668, 0.3408, 0.1787, 0.1138],\n",
      "        [0.3868, 0.3287, 0.1768, 0.1077],\n",
      "        [0.3412, 0.3402, 0.1628, 0.1558],\n",
      "        [0.3893, 0.3385, 0.1664, 0.1058],\n",
      "        [0.3856, 0.3333, 0.1742, 0.1070],\n",
      "        [0.3389, 0.3286, 0.2066, 0.1259]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3619, 0.3339, 0.1818, 0.1224],\n",
      "        [0.4095, 0.3627, 0.1534, 0.0744],\n",
      "        [0.3712, 0.3341, 0.1759, 0.1188],\n",
      "        [0.3247, 0.3246, 0.1830, 0.1677],\n",
      "        [0.4017, 0.3634, 0.1602, 0.0747],\n",
      "        [0.3447, 0.3331, 0.1945, 0.1276],\n",
      "        [0.3240, 0.3246, 0.1853, 0.1661],\n",
      "        [0.3291, 0.3228, 0.1803, 0.1678],\n",
      "        [0.3374, 0.3307, 0.1728, 0.1591],\n",
      "        [0.3263, 0.3098, 0.2169, 0.1470],\n",
      "        [0.3868, 0.3392, 0.1721, 0.1018],\n",
      "        [0.3168, 0.3207, 0.1903, 0.1723],\n",
      "        [0.3136, 0.3165, 0.1987, 0.1712],\n",
      "        [0.3348, 0.3229, 0.1824, 0.1599],\n",
      "        [0.3574, 0.3300, 0.1879, 0.1248],\n",
      "        [0.3224, 0.3236, 0.1864, 0.1677],\n",
      "        [0.3585, 0.3355, 0.1819, 0.1241],\n",
      "        [0.4264, 0.3704, 0.1333, 0.0699],\n",
      "        [0.3135, 0.3129, 0.1936, 0.1800],\n",
      "        [0.3668, 0.3408, 0.1787, 0.1138],\n",
      "        [0.3868, 0.3287, 0.1768, 0.1077],\n",
      "        [0.3412, 0.3402, 0.1628, 0.1558],\n",
      "        [0.3893, 0.3385, 0.1664, 0.1058],\n",
      "        [0.3856, 0.3333, 0.1742, 0.1070],\n",
      "        [0.3389, 0.3286, 0.2066, 0.1259]], grad_fn=<ViewBackward>) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0024,  0.0188,  0.0926],\n",
      "        [ 0.0027,  0.0212,  0.1047],\n",
      "        [ 0.0025,  0.0192,  0.0949],\n",
      "        [ 0.0022,  0.0168,  0.0830],\n",
      "        [ 0.0027,  0.0208,  0.1027],\n",
      "        [ 0.0023,  0.0179,  0.0882],\n",
      "        [ 0.1133, -0.0869, -0.1157],\n",
      "        [ 0.0022,  0.0171,  0.0842],\n",
      "        [ 0.0022,  0.0175,  0.0863],\n",
      "        [ 0.0022,  0.0169,  0.0834],\n",
      "        [ 0.0026,  0.0200,  0.0989],\n",
      "        [ 0.1119, -0.0859, -0.1143],\n",
      "        [ 0.1105, -0.0847, -0.1129],\n",
      "        [ 0.0022,  0.0173,  0.0856],\n",
      "        [ 0.0024,  0.0185,  0.0914],\n",
      "        [ 0.1129, -0.0866, -0.1154],\n",
      "        [ 0.0024,  0.0186,  0.0917],\n",
      "        [ 0.0028,  0.0221,  0.1090],\n",
      "        [ 0.0021,  0.0162,  0.0802],\n",
      "        [ 0.0024,  0.0190,  0.0938],\n",
      "        [ 0.0026,  0.0200,  0.0989],\n",
      "        [ 0.0023,  0.0177,  0.0873],\n",
      "        [ 0.0026,  0.0202,  0.0996],\n",
      "        [ 0.0026,  0.0200,  0.0986],\n",
      "        [ 0.0023,  0.0176,  0.0867]], grad_fn=<MmBackward>)\n",
      "Epoch: 08 | Loss: 1.07729 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6972, 0.5201, 0.5929],\n",
      "        [0.7753, 0.5330, 0.6665],\n",
      "        [0.7069, 0.5263, 0.5921],\n",
      "        [0.6480, 0.4968, 0.5202],\n",
      "        [0.7684, 0.5279, 0.6750],\n",
      "        [0.6791, 0.5084, 0.5992],\n",
      "        [0.6475, 0.4965, 0.5255],\n",
      "        [0.6507, 0.5016, 0.5164],\n",
      "        [0.6671, 0.5018, 0.5190],\n",
      "        [0.6372, 0.5128, 0.5914],\n",
      "        [0.7284, 0.5340, 0.6224],\n",
      "        [0.6363, 0.4939, 0.5231],\n",
      "        [0.6292, 0.4950, 0.5351],\n",
      "        [0.6569, 0.5065, 0.5305],\n",
      "        [0.6888, 0.5200, 0.5961],\n",
      "        [0.6448, 0.4960, 0.5245],\n",
      "        [0.6953, 0.5163, 0.5900],\n",
      "        [0.7995, 0.5372, 0.6493],\n",
      "        [0.6250, 0.4972, 0.5167],\n",
      "        [0.7092, 0.5186, 0.6060],\n",
      "        [0.7179, 0.5419, 0.6156],\n",
      "        [0.6800, 0.4971, 0.5097],\n",
      "        [0.7298, 0.5356, 0.6059],\n",
      "        [0.7211, 0.5375, 0.6138],\n",
      "        [0.6692, 0.5081, 0.6162]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6972],\n",
      "         [0.5201],\n",
      "         [0.5929]],\n",
      "\n",
      "        [[0.7753],\n",
      "         [0.5330],\n",
      "         [0.6665]],\n",
      "\n",
      "        [[0.7069],\n",
      "         [0.5263],\n",
      "         [0.5921]],\n",
      "\n",
      "        [[0.6480],\n",
      "         [0.4968],\n",
      "         [0.5202]],\n",
      "\n",
      "        [[0.7684],\n",
      "         [0.5279],\n",
      "         [0.6750]],\n",
      "\n",
      "        [[0.6791],\n",
      "         [0.5084],\n",
      "         [0.5992]],\n",
      "\n",
      "        [[0.6475],\n",
      "         [0.4965],\n",
      "         [0.5255]],\n",
      "\n",
      "        [[0.6507],\n",
      "         [0.5016],\n",
      "         [0.5164]],\n",
      "\n",
      "        [[0.6671],\n",
      "         [0.5018],\n",
      "         [0.5190]],\n",
      "\n",
      "        [[0.6372],\n",
      "         [0.5128],\n",
      "         [0.5914]],\n",
      "\n",
      "        [[0.7284],\n",
      "         [0.5340],\n",
      "         [0.6224]],\n",
      "\n",
      "        [[0.6363],\n",
      "         [0.4939],\n",
      "         [0.5231]],\n",
      "\n",
      "        [[0.6292],\n",
      "         [0.4950],\n",
      "         [0.5351]],\n",
      "\n",
      "        [[0.6569],\n",
      "         [0.5065],\n",
      "         [0.5305]],\n",
      "\n",
      "        [[0.6888],\n",
      "         [0.5200],\n",
      "         [0.5961]],\n",
      "\n",
      "        [[0.6448],\n",
      "         [0.4960],\n",
      "         [0.5245]],\n",
      "\n",
      "        [[0.6953],\n",
      "         [0.5163],\n",
      "         [0.5900]],\n",
      "\n",
      "        [[0.7995],\n",
      "         [0.5372],\n",
      "         [0.6493]],\n",
      "\n",
      "        [[0.6250],\n",
      "         [0.4972],\n",
      "         [0.5167]],\n",
      "\n",
      "        [[0.7092],\n",
      "         [0.5186],\n",
      "         [0.6060]],\n",
      "\n",
      "        [[0.7179],\n",
      "         [0.5419],\n",
      "         [0.6156]],\n",
      "\n",
      "        [[0.6800],\n",
      "         [0.4971],\n",
      "         [0.5097]],\n",
      "\n",
      "        [[0.7298],\n",
      "         [0.5356],\n",
      "         [0.6059]],\n",
      "\n",
      "        [[0.7211],\n",
      "         [0.5375],\n",
      "         [0.6138]],\n",
      "\n",
      "        [[0.6692],\n",
      "         [0.5081],\n",
      "         [0.6162]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6972, 0.3028],\n",
      "         [0.5201, 0.4799],\n",
      "         [0.5929, 0.4071]],\n",
      "\n",
      "        [[0.7753, 0.2247],\n",
      "         [0.5330, 0.4670],\n",
      "         [0.6665, 0.3335]],\n",
      "\n",
      "        [[0.7069, 0.2931],\n",
      "         [0.5263, 0.4737],\n",
      "         [0.5921, 0.4079]],\n",
      "\n",
      "        [[0.6480, 0.3520],\n",
      "         [0.4968, 0.5032],\n",
      "         [0.5202, 0.4798]],\n",
      "\n",
      "        [[0.7684, 0.2316],\n",
      "         [0.5279, 0.4721],\n",
      "         [0.6750, 0.3250]],\n",
      "\n",
      "        [[0.6791, 0.3209],\n",
      "         [0.5084, 0.4916],\n",
      "         [0.5992, 0.4008]],\n",
      "\n",
      "        [[0.6475, 0.3525],\n",
      "         [0.4965, 0.5035],\n",
      "         [0.5255, 0.4745]],\n",
      "\n",
      "        [[0.6507, 0.3493],\n",
      "         [0.5016, 0.4984],\n",
      "         [0.5164, 0.4836]],\n",
      "\n",
      "        [[0.6671, 0.3329],\n",
      "         [0.5018, 0.4982],\n",
      "         [0.5190, 0.4810]],\n",
      "\n",
      "        [[0.6372, 0.3628],\n",
      "         [0.5128, 0.4872],\n",
      "         [0.5914, 0.4086]],\n",
      "\n",
      "        [[0.7284, 0.2716],\n",
      "         [0.5340, 0.4660],\n",
      "         [0.6224, 0.3776]],\n",
      "\n",
      "        [[0.6363, 0.3637],\n",
      "         [0.4939, 0.5061],\n",
      "         [0.5231, 0.4769]],\n",
      "\n",
      "        [[0.6292, 0.3708],\n",
      "         [0.4950, 0.5050],\n",
      "         [0.5351, 0.4649]],\n",
      "\n",
      "        [[0.6569, 0.3431],\n",
      "         [0.5065, 0.4935],\n",
      "         [0.5305, 0.4695]],\n",
      "\n",
      "        [[0.6888, 0.3112],\n",
      "         [0.5200, 0.4800],\n",
      "         [0.5961, 0.4039]],\n",
      "\n",
      "        [[0.6448, 0.3552],\n",
      "         [0.4960, 0.5040],\n",
      "         [0.5245, 0.4755]],\n",
      "\n",
      "        [[0.6953, 0.3047],\n",
      "         [0.5163, 0.4837],\n",
      "         [0.5900, 0.4100]],\n",
      "\n",
      "        [[0.7995, 0.2005],\n",
      "         [0.5372, 0.4628],\n",
      "         [0.6493, 0.3507]],\n",
      "\n",
      "        [[0.6250, 0.3750],\n",
      "         [0.4972, 0.5028],\n",
      "         [0.5167, 0.4833]],\n",
      "\n",
      "        [[0.7092, 0.2908],\n",
      "         [0.5186, 0.4814],\n",
      "         [0.6060, 0.3940]],\n",
      "\n",
      "        [[0.7179, 0.2821],\n",
      "         [0.5419, 0.4581],\n",
      "         [0.6156, 0.3844]],\n",
      "\n",
      "        [[0.6800, 0.3200],\n",
      "         [0.4971, 0.5029],\n",
      "         [0.5097, 0.4903]],\n",
      "\n",
      "        [[0.7298, 0.2702],\n",
      "         [0.5356, 0.4644],\n",
      "         [0.6059, 0.3941]],\n",
      "\n",
      "        [[0.7211, 0.2789],\n",
      "         [0.5375, 0.4625],\n",
      "         [0.6138, 0.3862]],\n",
      "\n",
      "        [[0.6692, 0.3308],\n",
      "         [0.5081, 0.4919],\n",
      "         [0.6162, 0.3838]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3626, 0.3346, 0.1795, 0.1233],\n",
      "        [0.4132, 0.3621, 0.1498, 0.0749],\n",
      "        [0.3720, 0.3348, 0.1736, 0.1196],\n",
      "        [0.3220, 0.3261, 0.1831, 0.1689],\n",
      "        [0.4056, 0.3627, 0.1564, 0.0753],\n",
      "        [0.3453, 0.3338, 0.1922, 0.1286],\n",
      "        [0.3215, 0.3260, 0.1852, 0.1672],\n",
      "        [0.3264, 0.3243, 0.1804, 0.1689],\n",
      "        [0.3348, 0.3323, 0.1728, 0.1601],\n",
      "        [0.3267, 0.3105, 0.2146, 0.1483],\n",
      "        [0.3890, 0.3394, 0.1691, 0.1026],\n",
      "        [0.3142, 0.3221, 0.1902, 0.1735],\n",
      "        [0.3114, 0.3177, 0.1984, 0.1724],\n",
      "        [0.3327, 0.3242, 0.1820, 0.1611],\n",
      "        [0.3582, 0.3306, 0.1855, 0.1257],\n",
      "        [0.3198, 0.3249, 0.1863, 0.1689],\n",
      "        [0.3590, 0.3363, 0.1798, 0.1249],\n",
      "        [0.4294, 0.3700, 0.1302, 0.0703],\n",
      "        [0.3108, 0.3142, 0.1937, 0.1812],\n",
      "        [0.3678, 0.3414, 0.1762, 0.1146],\n",
      "        [0.3891, 0.3288, 0.1737, 0.1084],\n",
      "        [0.3380, 0.3420, 0.1631, 0.1569],\n",
      "        [0.3908, 0.3389, 0.1637, 0.1065],\n",
      "        [0.3876, 0.3336, 0.1712, 0.1077],\n",
      "        [0.3400, 0.3291, 0.2039, 0.1270]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3626, 0.3346, 0.1795, 0.1233],\n",
      "        [0.4132, 0.3621, 0.1498, 0.0749],\n",
      "        [0.3720, 0.3348, 0.1736, 0.1196],\n",
      "        [0.3220, 0.3261, 0.1831, 0.1689],\n",
      "        [0.4056, 0.3627, 0.1564, 0.0753],\n",
      "        [0.3453, 0.3338, 0.1922, 0.1286],\n",
      "        [0.3215, 0.3260, 0.1852, 0.1672],\n",
      "        [0.3264, 0.3243, 0.1804, 0.1689],\n",
      "        [0.3348, 0.3323, 0.1728, 0.1601],\n",
      "        [0.3267, 0.3105, 0.2146, 0.1483],\n",
      "        [0.3890, 0.3394, 0.1691, 0.1026],\n",
      "        [0.3142, 0.3221, 0.1902, 0.1735],\n",
      "        [0.3114, 0.3177, 0.1984, 0.1724],\n",
      "        [0.3327, 0.3242, 0.1820, 0.1611],\n",
      "        [0.3582, 0.3306, 0.1855, 0.1257],\n",
      "        [0.3198, 0.3249, 0.1863, 0.1689],\n",
      "        [0.3590, 0.3363, 0.1798, 0.1249],\n",
      "        [0.4294, 0.3700, 0.1302, 0.0703],\n",
      "        [0.3108, 0.3142, 0.1937, 0.1812],\n",
      "        [0.3678, 0.3414, 0.1762, 0.1146],\n",
      "        [0.3891, 0.3288, 0.1737, 0.1084],\n",
      "        [0.3380, 0.3420, 0.1631, 0.1569],\n",
      "        [0.3908, 0.3389, 0.1637, 0.1065],\n",
      "        [0.3876, 0.3336, 0.1712, 0.1077],\n",
      "        [0.3400, 0.3291, 0.2039, 0.1270]], grad_fn=<ViewBackward>) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0047,  0.0182,  0.0900],\n",
      "        [ 0.0054,  0.0208,  0.1026],\n",
      "        [ 0.0049,  0.0187,  0.0924],\n",
      "        [ 0.1155, -0.0893, -0.1174],\n",
      "        [ 0.0053,  0.0204,  0.1007],\n",
      "        [ 0.0045,  0.0174,  0.0857],\n",
      "        [ 0.1155, -0.0892, -0.1173],\n",
      "        [ 0.0043,  0.0164,  0.0810],\n",
      "        [ 0.0044,  0.0168,  0.0831],\n",
      "        [ 0.0043,  0.0164,  0.0811],\n",
      "        [ 0.0051,  0.0195,  0.0966],\n",
      "        [ 0.1141, -0.0882, -0.1159],\n",
      "        [ 0.1126, -0.0870, -0.1144],\n",
      "        [ 0.0043,  0.0167,  0.0826],\n",
      "        [ 0.0047,  0.0180,  0.0889],\n",
      "        [ 0.1151, -0.0889, -0.1170],\n",
      "        [ 0.0047,  0.0180,  0.0891],\n",
      "        [ 0.0056,  0.0216,  0.1066],\n",
      "        [ 0.1113, -0.0860, -0.1131],\n",
      "        [ 0.0048,  0.0185,  0.0913],\n",
      "        [ 0.0051,  0.0196,  0.0966],\n",
      "        [ 0.1211, -0.0936, -0.1231],\n",
      "        [ 0.0051,  0.0196,  0.0970],\n",
      "        [ 0.0051,  0.0195,  0.0962],\n",
      "        [ 0.0044,  0.0171,  0.0844]], grad_fn=<MmBackward>)\n",
      "Epoch: 09 | Loss: 1.05641 | Correct: 014/025\n"
     ]
    }
   ],
   "source": [
    "tree.fit(X_train[:25], y_train[:25], batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0.5000, 0.3333, 0.6271, 0.4583],\n",
      "        [0.3889, 0.7500, 0.1186, 0.0833],\n",
      "        [0.9444, 0.2500, 1.0000, 0.9167],\n",
      "        [0.4722, 0.3750, 0.5932, 0.5833],\n",
      "        [0.6944, 0.3333, 0.6441, 0.5417],\n",
      "        [0.3056, 0.5833, 0.0847, 0.1250],\n",
      "        [0.3611, 0.3750, 0.4407, 0.5000],\n",
      "        [0.7222, 0.4583, 0.6949, 0.9167],\n",
      "        [0.5278, 0.0833, 0.5932, 0.5833],\n",
      "        [0.4167, 0.2917, 0.4915, 0.4583],\n",
      "        [0.6111, 0.5000, 0.6949, 0.7917],\n",
      "        [0.1389, 0.4167, 0.0678, 0.0000],\n",
      "        [0.3333, 0.6250, 0.0508, 0.0417],\n",
      "        [0.1667, 0.4583, 0.0847, 0.0000],\n",
      "        [0.2222, 0.7500, 0.0847, 0.0833],\n",
      "        [0.5556, 0.5417, 0.6271, 0.6250],\n",
      "        [0.6111, 0.4167, 0.8136, 0.8750],\n",
      "        [0.3611, 0.2083, 0.4915, 0.4167],\n",
      "        [0.3889, 0.3333, 0.5932, 0.5000],\n",
      "        [0.5833, 0.3333, 0.7797, 0.8750],\n",
      "        [0.1111, 0.5000, 0.1017, 0.0417],\n",
      "        [0.5000, 0.4167, 0.6610, 0.7083],\n",
      "        [0.1944, 0.5833, 0.1017, 0.1250],\n",
      "        [0.5833, 0.3333, 0.7797, 0.8333],\n",
      "        [1.0000, 0.7500, 0.9153, 0.7917],\n",
      "        [0.6667, 0.4167, 0.7119, 0.9167],\n",
      "        [0.6667, 0.2083, 0.8136, 0.7083],\n",
      "        [0.6944, 0.5000, 0.8305, 0.9167],\n",
      "        [0.1389, 0.4167, 0.0678, 0.0833],\n",
      "        [0.1389, 0.4583, 0.1017, 0.0417],\n",
      "        [0.0833, 0.6667, 0.0000, 0.0417],\n",
      "        [0.3889, 1.0000, 0.0847, 0.1250],\n",
      "        [0.6667, 0.4583, 0.5763, 0.5417],\n",
      "        [0.1389, 0.5833, 0.1017, 0.0417],\n",
      "        [0.0278, 0.5000, 0.0508, 0.0417],\n",
      "        [0.5556, 0.2083, 0.6780, 0.7500],\n",
      "        [0.5833, 0.5000, 0.5932, 0.5833],\n",
      "        [0.2500, 0.6250, 0.0847, 0.0417],\n",
      "        [0.1944, 0.6667, 0.0678, 0.0417],\n",
      "        [0.2500, 0.8750, 0.0847, 0.0000],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.5833, 0.5932, 0.6250],\n",
      "        [0.6667, 0.4583, 0.6271, 0.5833],\n",
      "        [0.3056, 0.7917, 0.0508, 0.1250],\n",
      "        [0.3056, 0.7083, 0.0847, 0.0417],\n",
      "        [0.3333, 0.1667, 0.4576, 0.3750],\n",
      "        [0.5556, 0.3333, 0.6949, 0.5833],\n",
      "        [0.5833, 0.4583, 0.7627, 0.7083],\n",
      "        [0.6389, 0.4167, 0.5763, 0.5417],\n",
      "        [0.8056, 0.6667, 0.8644, 1.0000]])\n",
      "path_prob tensor([[0.7018, 0.5123, 0.6069],\n",
      "        [0.6786, 0.4953, 0.5286],\n",
      "        [0.7800, 0.5413, 0.6754],\n",
      "        [0.7167, 0.5258, 0.5992],\n",
      "        [0.7281, 0.5190, 0.6239],\n",
      "        [0.6639, 0.5008, 0.5298],\n",
      "        [0.6979, 0.5238, 0.5776],\n",
      "        [0.7766, 0.5535, 0.6222],\n",
      "        [0.7017, 0.5254, 0.6222],\n",
      "        [0.6920, 0.5176, 0.5918],\n",
      "        [0.7582, 0.5419, 0.6106],\n",
      "        [0.6190, 0.4897, 0.5254],\n",
      "        [0.6594, 0.4938, 0.5266],\n",
      "        [0.6249, 0.4890, 0.5264],\n",
      "        [0.6631, 0.4972, 0.5116],\n",
      "        [0.7391, 0.5284, 0.5980],\n",
      "        [0.7612, 0.5455, 0.6252],\n",
      "        [0.6759, 0.5137, 0.5926],\n",
      "        [0.6969, 0.5180, 0.5950],\n",
      "        [0.7541, 0.5468, 0.6256],\n",
      "        [0.6280, 0.4927, 0.5202],\n",
      "        [0.7354, 0.5354, 0.6042],\n",
      "        [0.6532, 0.5006, 0.5218],\n",
      "        [0.7498, 0.5428, 0.6256],\n",
      "        [0.8001, 0.5324, 0.6433],\n",
      "        [0.7700, 0.5530, 0.6217],\n",
      "        [0.7352, 0.5290, 0.6426],\n",
      "        [0.7765, 0.5486, 0.6280],\n",
      "        [0.6299, 0.4979, 0.5253],\n",
      "        [0.6275, 0.4926, 0.5254],\n",
      "        [0.6385, 0.4967, 0.4982],\n",
      "        [0.7014, 0.5008, 0.5091],\n",
      "        [0.7341, 0.5217, 0.6084],\n",
      "        [0.6371, 0.4927, 0.5170],\n",
      "        [0.6199, 0.4949, 0.5089],\n",
      "        [0.7310, 0.5384, 0.6232],\n",
      "        [0.7344, 0.5255, 0.6003],\n",
      "        [0.6512, 0.4929, 0.5223],\n",
      "        [0.6490, 0.4938, 0.5134],\n",
      "        [0.6646, 0.4890, 0.5056],\n",
      "        [0.7247, 0.5384, 0.6081],\n",
      "        [0.7350, 0.5300, 0.5857],\n",
      "        [0.7385, 0.5239, 0.6124],\n",
      "        [0.6792, 0.5022, 0.5130],\n",
      "        [0.6628, 0.4927, 0.5215],\n",
      "        [0.6652, 0.5109, 0.5903],\n",
      "        [0.7207, 0.5217, 0.6167],\n",
      "        [0.7445, 0.5314, 0.6164],\n",
      "        [0.7291, 0.5218, 0.6088],\n",
      "        [0.8011, 0.5551, 0.6291]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7018],\n",
      "         [0.5123],\n",
      "         [0.6069]],\n",
      "\n",
      "        [[0.6786],\n",
      "         [0.4953],\n",
      "         [0.5286]],\n",
      "\n",
      "        [[0.7800],\n",
      "         [0.5413],\n",
      "         [0.6754]],\n",
      "\n",
      "        [[0.7167],\n",
      "         [0.5258],\n",
      "         [0.5992]],\n",
      "\n",
      "        [[0.7281],\n",
      "         [0.5190],\n",
      "         [0.6239]],\n",
      "\n",
      "        [[0.6639],\n",
      "         [0.5008],\n",
      "         [0.5298]],\n",
      "\n",
      "        [[0.6979],\n",
      "         [0.5238],\n",
      "         [0.5776]],\n",
      "\n",
      "        [[0.7766],\n",
      "         [0.5535],\n",
      "         [0.6222]],\n",
      "\n",
      "        [[0.7017],\n",
      "         [0.5254],\n",
      "         [0.6222]],\n",
      "\n",
      "        [[0.6920],\n",
      "         [0.5176],\n",
      "         [0.5918]],\n",
      "\n",
      "        [[0.7582],\n",
      "         [0.5419],\n",
      "         [0.6106]],\n",
      "\n",
      "        [[0.6190],\n",
      "         [0.4897],\n",
      "         [0.5254]],\n",
      "\n",
      "        [[0.6594],\n",
      "         [0.4938],\n",
      "         [0.5266]],\n",
      "\n",
      "        [[0.6249],\n",
      "         [0.4890],\n",
      "         [0.5264]],\n",
      "\n",
      "        [[0.6631],\n",
      "         [0.4972],\n",
      "         [0.5116]],\n",
      "\n",
      "        [[0.7391],\n",
      "         [0.5284],\n",
      "         [0.5980]],\n",
      "\n",
      "        [[0.7612],\n",
      "         [0.5455],\n",
      "         [0.6252]],\n",
      "\n",
      "        [[0.6759],\n",
      "         [0.5137],\n",
      "         [0.5926]],\n",
      "\n",
      "        [[0.6969],\n",
      "         [0.5180],\n",
      "         [0.5950]],\n",
      "\n",
      "        [[0.7541],\n",
      "         [0.5468],\n",
      "         [0.6256]],\n",
      "\n",
      "        [[0.6280],\n",
      "         [0.4927],\n",
      "         [0.5202]],\n",
      "\n",
      "        [[0.7354],\n",
      "         [0.5354],\n",
      "         [0.6042]],\n",
      "\n",
      "        [[0.6532],\n",
      "         [0.5006],\n",
      "         [0.5218]],\n",
      "\n",
      "        [[0.7498],\n",
      "         [0.5428],\n",
      "         [0.6256]],\n",
      "\n",
      "        [[0.8001],\n",
      "         [0.5324],\n",
      "         [0.6433]],\n",
      "\n",
      "        [[0.7700],\n",
      "         [0.5530],\n",
      "         [0.6217]],\n",
      "\n",
      "        [[0.7352],\n",
      "         [0.5290],\n",
      "         [0.6426]],\n",
      "\n",
      "        [[0.7765],\n",
      "         [0.5486],\n",
      "         [0.6280]],\n",
      "\n",
      "        [[0.6299],\n",
      "         [0.4979],\n",
      "         [0.5253]],\n",
      "\n",
      "        [[0.6275],\n",
      "         [0.4926],\n",
      "         [0.5254]],\n",
      "\n",
      "        [[0.6385],\n",
      "         [0.4967],\n",
      "         [0.4982]],\n",
      "\n",
      "        [[0.7014],\n",
      "         [0.5008],\n",
      "         [0.5091]],\n",
      "\n",
      "        [[0.7341],\n",
      "         [0.5217],\n",
      "         [0.6084]],\n",
      "\n",
      "        [[0.6371],\n",
      "         [0.4927],\n",
      "         [0.5170]],\n",
      "\n",
      "        [[0.6199],\n",
      "         [0.4949],\n",
      "         [0.5089]],\n",
      "\n",
      "        [[0.7310],\n",
      "         [0.5384],\n",
      "         [0.6232]],\n",
      "\n",
      "        [[0.7344],\n",
      "         [0.5255],\n",
      "         [0.6003]],\n",
      "\n",
      "        [[0.6512],\n",
      "         [0.4929],\n",
      "         [0.5223]],\n",
      "\n",
      "        [[0.6490],\n",
      "         [0.4938],\n",
      "         [0.5134]],\n",
      "\n",
      "        [[0.6646],\n",
      "         [0.4890],\n",
      "         [0.5056]],\n",
      "\n",
      "        [[0.7247],\n",
      "         [0.5384],\n",
      "         [0.6081]],\n",
      "\n",
      "        [[0.7350],\n",
      "         [0.5300],\n",
      "         [0.5857]],\n",
      "\n",
      "        [[0.7385],\n",
      "         [0.5239],\n",
      "         [0.6124]],\n",
      "\n",
      "        [[0.6792],\n",
      "         [0.5022],\n",
      "         [0.5130]],\n",
      "\n",
      "        [[0.6628],\n",
      "         [0.4927],\n",
      "         [0.5215]],\n",
      "\n",
      "        [[0.6652],\n",
      "         [0.5109],\n",
      "         [0.5903]],\n",
      "\n",
      "        [[0.7207],\n",
      "         [0.5217],\n",
      "         [0.6167]],\n",
      "\n",
      "        [[0.7445],\n",
      "         [0.5314],\n",
      "         [0.6164]],\n",
      "\n",
      "        [[0.7291],\n",
      "         [0.5218],\n",
      "         [0.6088]],\n",
      "\n",
      "        [[0.8011],\n",
      "         [0.5551],\n",
      "         [0.6291]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7018, 0.2982],\n",
      "         [0.5123, 0.4877],\n",
      "         [0.6069, 0.3931]],\n",
      "\n",
      "        [[0.6786, 0.3214],\n",
      "         [0.4953, 0.5047],\n",
      "         [0.5286, 0.4714]],\n",
      "\n",
      "        [[0.7800, 0.2200],\n",
      "         [0.5413, 0.4587],\n",
      "         [0.6754, 0.3246]],\n",
      "\n",
      "        [[0.7167, 0.2833],\n",
      "         [0.5258, 0.4742],\n",
      "         [0.5992, 0.4008]],\n",
      "\n",
      "        [[0.7281, 0.2719],\n",
      "         [0.5190, 0.4810],\n",
      "         [0.6239, 0.3761]],\n",
      "\n",
      "        [[0.6639, 0.3361],\n",
      "         [0.5008, 0.4992],\n",
      "         [0.5298, 0.4702]],\n",
      "\n",
      "        [[0.6979, 0.3021],\n",
      "         [0.5238, 0.4762],\n",
      "         [0.5776, 0.4224]],\n",
      "\n",
      "        [[0.7766, 0.2234],\n",
      "         [0.5535, 0.4465],\n",
      "         [0.6222, 0.3778]],\n",
      "\n",
      "        [[0.7017, 0.2983],\n",
      "         [0.5254, 0.4746],\n",
      "         [0.6222, 0.3778]],\n",
      "\n",
      "        [[0.6920, 0.3080],\n",
      "         [0.5176, 0.4824],\n",
      "         [0.5918, 0.4082]],\n",
      "\n",
      "        [[0.7582, 0.2418],\n",
      "         [0.5419, 0.4581],\n",
      "         [0.6106, 0.3894]],\n",
      "\n",
      "        [[0.6190, 0.3810],\n",
      "         [0.4897, 0.5103],\n",
      "         [0.5254, 0.4746]],\n",
      "\n",
      "        [[0.6594, 0.3406],\n",
      "         [0.4938, 0.5062],\n",
      "         [0.5266, 0.4734]],\n",
      "\n",
      "        [[0.6249, 0.3751],\n",
      "         [0.4890, 0.5110],\n",
      "         [0.5264, 0.4736]],\n",
      "\n",
      "        [[0.6631, 0.3369],\n",
      "         [0.4972, 0.5028],\n",
      "         [0.5116, 0.4884]],\n",
      "\n",
      "        [[0.7391, 0.2609],\n",
      "         [0.5284, 0.4716],\n",
      "         [0.5980, 0.4020]],\n",
      "\n",
      "        [[0.7612, 0.2388],\n",
      "         [0.5455, 0.4545],\n",
      "         [0.6252, 0.3748]],\n",
      "\n",
      "        [[0.6759, 0.3241],\n",
      "         [0.5137, 0.4863],\n",
      "         [0.5926, 0.4074]],\n",
      "\n",
      "        [[0.6969, 0.3031],\n",
      "         [0.5180, 0.4820],\n",
      "         [0.5950, 0.4050]],\n",
      "\n",
      "        [[0.7541, 0.2459],\n",
      "         [0.5468, 0.4532],\n",
      "         [0.6256, 0.3744]],\n",
      "\n",
      "        [[0.6280, 0.3720],\n",
      "         [0.4927, 0.5073],\n",
      "         [0.5202, 0.4798]],\n",
      "\n",
      "        [[0.7354, 0.2646],\n",
      "         [0.5354, 0.4646],\n",
      "         [0.6042, 0.3958]],\n",
      "\n",
      "        [[0.6532, 0.3468],\n",
      "         [0.5006, 0.4994],\n",
      "         [0.5218, 0.4782]],\n",
      "\n",
      "        [[0.7498, 0.2502],\n",
      "         [0.5428, 0.4572],\n",
      "         [0.6256, 0.3744]],\n",
      "\n",
      "        [[0.8001, 0.1999],\n",
      "         [0.5324, 0.4676],\n",
      "         [0.6433, 0.3567]],\n",
      "\n",
      "        [[0.7700, 0.2300],\n",
      "         [0.5530, 0.4470],\n",
      "         [0.6217, 0.3783]],\n",
      "\n",
      "        [[0.7352, 0.2648],\n",
      "         [0.5290, 0.4710],\n",
      "         [0.6426, 0.3574]],\n",
      "\n",
      "        [[0.7765, 0.2235],\n",
      "         [0.5486, 0.4514],\n",
      "         [0.6280, 0.3720]],\n",
      "\n",
      "        [[0.6299, 0.3701],\n",
      "         [0.4979, 0.5021],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.6275, 0.3725],\n",
      "         [0.4926, 0.5074],\n",
      "         [0.5254, 0.4746]],\n",
      "\n",
      "        [[0.6385, 0.3615],\n",
      "         [0.4967, 0.5033],\n",
      "         [0.4982, 0.5018]],\n",
      "\n",
      "        [[0.7014, 0.2986],\n",
      "         [0.5008, 0.4992],\n",
      "         [0.5091, 0.4909]],\n",
      "\n",
      "        [[0.7341, 0.2659],\n",
      "         [0.5217, 0.4783],\n",
      "         [0.6084, 0.3916]],\n",
      "\n",
      "        [[0.6371, 0.3629],\n",
      "         [0.4927, 0.5073],\n",
      "         [0.5170, 0.4830]],\n",
      "\n",
      "        [[0.6199, 0.3801],\n",
      "         [0.4949, 0.5051],\n",
      "         [0.5089, 0.4911]],\n",
      "\n",
      "        [[0.7310, 0.2690],\n",
      "         [0.5384, 0.4616],\n",
      "         [0.6232, 0.3768]],\n",
      "\n",
      "        [[0.7344, 0.2656],\n",
      "         [0.5255, 0.4745],\n",
      "         [0.6003, 0.3997]],\n",
      "\n",
      "        [[0.6512, 0.3488],\n",
      "         [0.4929, 0.5071],\n",
      "         [0.5223, 0.4777]],\n",
      "\n",
      "        [[0.6490, 0.3510],\n",
      "         [0.4938, 0.5062],\n",
      "         [0.5134, 0.4866]],\n",
      "\n",
      "        [[0.6646, 0.3354],\n",
      "         [0.4890, 0.5110],\n",
      "         [0.5056, 0.4944]],\n",
      "\n",
      "        [[0.7247, 0.2753],\n",
      "         [0.5384, 0.4616],\n",
      "         [0.6081, 0.3919]],\n",
      "\n",
      "        [[0.7350, 0.2650],\n",
      "         [0.5300, 0.4700],\n",
      "         [0.5857, 0.4143]],\n",
      "\n",
      "        [[0.7385, 0.2615],\n",
      "         [0.5239, 0.4761],\n",
      "         [0.6124, 0.3876]],\n",
      "\n",
      "        [[0.6792, 0.3208],\n",
      "         [0.5022, 0.4978],\n",
      "         [0.5130, 0.4870]],\n",
      "\n",
      "        [[0.6628, 0.3372],\n",
      "         [0.4927, 0.5073],\n",
      "         [0.5215, 0.4785]],\n",
      "\n",
      "        [[0.6652, 0.3348],\n",
      "         [0.5109, 0.4891],\n",
      "         [0.5903, 0.4097]],\n",
      "\n",
      "        [[0.7207, 0.2793],\n",
      "         [0.5217, 0.4783],\n",
      "         [0.6167, 0.3833]],\n",
      "\n",
      "        [[0.7445, 0.2555],\n",
      "         [0.5314, 0.4686],\n",
      "         [0.6164, 0.3836]],\n",
      "\n",
      "        [[0.7291, 0.2709],\n",
      "         [0.5218, 0.4782],\n",
      "         [0.6088, 0.3912]],\n",
      "\n",
      "        [[0.8011, 0.1989],\n",
      "         [0.5551, 0.4449],\n",
      "         [0.6291, 0.3709]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3595, 0.3423, 0.1810, 0.1172],\n",
      "        [0.3361, 0.3425, 0.1699, 0.1515],\n",
      "        [0.4222, 0.3578, 0.1486, 0.0714],\n",
      "        [0.3769, 0.3398, 0.1697, 0.1136],\n",
      "        [0.3779, 0.3502, 0.1697, 0.1023],\n",
      "        [0.3325, 0.3314, 0.1781, 0.1580],\n",
      "        [0.3656, 0.3324, 0.1745, 0.1276],\n",
      "        [0.4298, 0.3468, 0.1390, 0.0844],\n",
      "        [0.3686, 0.3330, 0.1856, 0.1127],\n",
      "        [0.3581, 0.3338, 0.1823, 0.1257],\n",
      "        [0.4108, 0.3474, 0.1476, 0.0942],\n",
      "        [0.3031, 0.3159, 0.2002, 0.1808],\n",
      "        [0.3256, 0.3338, 0.1794, 0.1612],\n",
      "        [0.3056, 0.3193, 0.1974, 0.1776],\n",
      "        [0.3297, 0.3334, 0.1724, 0.1646],\n",
      "        [0.3906, 0.3485, 0.1560, 0.1049],\n",
      "        [0.4152, 0.3460, 0.1493, 0.0895],\n",
      "        [0.3472, 0.3287, 0.1921, 0.1320],\n",
      "        [0.3610, 0.3359, 0.1804, 0.1228],\n",
      "        [0.4123, 0.3418, 0.1538, 0.0921],\n",
      "        [0.3094, 0.3186, 0.1935, 0.1785],\n",
      "        [0.3937, 0.3417, 0.1599, 0.1047],\n",
      "        [0.3270, 0.3262, 0.1810, 0.1659],\n",
      "        [0.4069, 0.3428, 0.1565, 0.0937],\n",
      "        [0.4260, 0.3741, 0.1286, 0.0713],\n",
      "        [0.4258, 0.3442, 0.1430, 0.0870],\n",
      "        [0.3889, 0.3463, 0.1702, 0.0946],\n",
      "        [0.4260, 0.3505, 0.1404, 0.0832],\n",
      "        [0.3136, 0.3163, 0.1944, 0.1757],\n",
      "        [0.3091, 0.3184, 0.1957, 0.1768],\n",
      "        [0.3171, 0.3213, 0.1801, 0.1814],\n",
      "        [0.3513, 0.3502, 0.1520, 0.1466],\n",
      "        [0.3830, 0.3511, 0.1617, 0.1041],\n",
      "        [0.3139, 0.3232, 0.1876, 0.1752],\n",
      "        [0.3068, 0.3131, 0.1934, 0.1867],\n",
      "        [0.3936, 0.3374, 0.1676, 0.1013],\n",
      "        [0.3859, 0.3485, 0.1594, 0.1062],\n",
      "        [0.3210, 0.3302, 0.1822, 0.1666],\n",
      "        [0.3205, 0.3286, 0.1802, 0.1708],\n",
      "        [0.3250, 0.3396, 0.1696, 0.1658],\n",
      "        [0.3902, 0.3345, 0.1674, 0.1079],\n",
      "        [0.3896, 0.3454, 0.1552, 0.1098],\n",
      "        [0.3869, 0.3516, 0.1602, 0.1014],\n",
      "        [0.3411, 0.3381, 0.1646, 0.1562],\n",
      "        [0.3266, 0.3362, 0.1759, 0.1614],\n",
      "        [0.3399, 0.3254, 0.1976, 0.1372],\n",
      "        [0.3760, 0.3447, 0.1723, 0.1070],\n",
      "        [0.3956, 0.3489, 0.1575, 0.0980],\n",
      "        [0.3804, 0.3487, 0.1649, 0.1060],\n",
      "        [0.4448, 0.3564, 0.1251, 0.0738]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3595, 0.3423, 0.1810, 0.1172],\n",
      "        [0.3361, 0.3425, 0.1699, 0.1515],\n",
      "        [0.4222, 0.3578, 0.1486, 0.0714],\n",
      "        [0.3769, 0.3398, 0.1697, 0.1136],\n",
      "        [0.3779, 0.3502, 0.1697, 0.1023],\n",
      "        [0.3325, 0.3314, 0.1781, 0.1580],\n",
      "        [0.3656, 0.3324, 0.1745, 0.1276],\n",
      "        [0.4298, 0.3468, 0.1390, 0.0844],\n",
      "        [0.3686, 0.3330, 0.1856, 0.1127],\n",
      "        [0.3581, 0.3338, 0.1823, 0.1257],\n",
      "        [0.4108, 0.3474, 0.1476, 0.0942],\n",
      "        [0.3031, 0.3159, 0.2002, 0.1808],\n",
      "        [0.3256, 0.3338, 0.1794, 0.1612],\n",
      "        [0.3056, 0.3193, 0.1974, 0.1776],\n",
      "        [0.3297, 0.3334, 0.1724, 0.1646],\n",
      "        [0.3906, 0.3485, 0.1560, 0.1049],\n",
      "        [0.4152, 0.3460, 0.1493, 0.0895],\n",
      "        [0.3472, 0.3287, 0.1921, 0.1320],\n",
      "        [0.3610, 0.3359, 0.1804, 0.1228],\n",
      "        [0.4123, 0.3418, 0.1538, 0.0921],\n",
      "        [0.3094, 0.3186, 0.1935, 0.1785],\n",
      "        [0.3937, 0.3417, 0.1599, 0.1047],\n",
      "        [0.3270, 0.3262, 0.1810, 0.1659],\n",
      "        [0.4069, 0.3428, 0.1565, 0.0937],\n",
      "        [0.4260, 0.3741, 0.1286, 0.0713],\n",
      "        [0.4258, 0.3442, 0.1430, 0.0870],\n",
      "        [0.3889, 0.3463, 0.1702, 0.0946],\n",
      "        [0.4260, 0.3505, 0.1404, 0.0832],\n",
      "        [0.3136, 0.3163, 0.1944, 0.1757],\n",
      "        [0.3091, 0.3184, 0.1957, 0.1768],\n",
      "        [0.3171, 0.3213, 0.1801, 0.1814],\n",
      "        [0.3513, 0.3502, 0.1520, 0.1466],\n",
      "        [0.3830, 0.3511, 0.1617, 0.1041],\n",
      "        [0.3139, 0.3232, 0.1876, 0.1752],\n",
      "        [0.3068, 0.3131, 0.1934, 0.1867],\n",
      "        [0.3936, 0.3374, 0.1676, 0.1013],\n",
      "        [0.3859, 0.3485, 0.1594, 0.1062],\n",
      "        [0.3210, 0.3302, 0.1822, 0.1666],\n",
      "        [0.3205, 0.3286, 0.1802, 0.1708],\n",
      "        [0.3250, 0.3396, 0.1696, 0.1658],\n",
      "        [0.3902, 0.3345, 0.1674, 0.1079],\n",
      "        [0.3896, 0.3454, 0.1552, 0.1098],\n",
      "        [0.3869, 0.3516, 0.1602, 0.1014],\n",
      "        [0.3411, 0.3381, 0.1646, 0.1562],\n",
      "        [0.3266, 0.3362, 0.1759, 0.1614],\n",
      "        [0.3399, 0.3254, 0.1976, 0.1372],\n",
      "        [0.3760, 0.3447, 0.1723, 0.1070],\n",
      "        [0.3956, 0.3489, 0.1575, 0.0980],\n",
      "        [0.3804, 0.3487, 0.1649, 0.1060],\n",
      "        [0.4448, 0.3564, 0.1251, 0.0738]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0055,  0.0192,  0.0875],\n",
      "        [ 0.1236, -0.0961, -0.1252],\n",
      "        [ 0.0064,  0.0225,  0.1028],\n",
      "        [ 0.0057,  0.0201,  0.0917],\n",
      "        [ 0.0058,  0.0202,  0.0920],\n",
      "        [ 0.0051,  0.0177,  0.0809],\n",
      "        [ 0.0056,  0.0195,  0.0890],\n",
      "        [ 0.0065,  0.0229,  0.1046],\n",
      "        [ 0.0056,  0.0197,  0.0897],\n",
      "        [ 0.0055,  0.0191,  0.0872],\n",
      "        [ 0.0063,  0.0219,  0.1000],\n",
      "        [ 0.1140, -0.0886, -0.1155],\n",
      "        [ 0.1204, -0.0937, -0.1220],\n",
      "        [ 0.1152, -0.0896, -0.1168],\n",
      "        [ 0.1203, -0.0936, -0.1219],\n",
      "        [ 0.0059,  0.0208,  0.0951],\n",
      "        [ 0.0063,  0.0222,  0.1011],\n",
      "        [ 0.0053,  0.0185,  0.0845],\n",
      "        [ 0.0055,  0.0193,  0.0879],\n",
      "        [ 0.0063,  0.0220,  0.1004],\n",
      "        [ 0.1149, -0.0894, -0.1165],\n",
      "        [ 0.0060,  0.0210,  0.0958],\n",
      "        [ 0.0050,  0.0174,  0.0796],\n",
      "        [ 0.0062,  0.0217,  0.0990],\n",
      "        [ 0.0065,  0.0227,  0.1037],\n",
      "        [ 0.0065,  0.0227,  0.1036],\n",
      "        [ 0.0059,  0.0208,  0.0947],\n",
      "        [ 0.0065,  0.0227,  0.1037],\n",
      "        [ 0.1141, -0.0888, -0.1156],\n",
      "        [ 0.1149, -0.0894, -0.1164],\n",
      "        [ 0.1159, -0.0902, -0.1175],\n",
      "        [ 0.0053,  0.0187,  0.0855],\n",
      "        [ 0.0058,  0.0204,  0.0932],\n",
      "        [ 0.1166, -0.0907, -0.1182],\n",
      "        [ 0.1130, -0.0879, -0.1145],\n",
      "        [ 0.0060,  0.0210,  0.0958],\n",
      "        [ 0.0059,  0.0206,  0.0939],\n",
      "        [ 0.1191, -0.0927, -0.1207],\n",
      "        [ 0.1185, -0.0922, -0.1201],\n",
      "        [ 0.1225, -0.0953, -0.1242],\n",
      "        [ 0.0059,  0.0208,  0.0950],\n",
      "        [ 0.0059,  0.0208,  0.0948],\n",
      "        [ 0.0059,  0.0206,  0.0942],\n",
      "        [ 0.0052,  0.0182,  0.0830],\n",
      "        [ 0.1213, -0.0944, -0.1229],\n",
      "        [ 0.0052,  0.0181,  0.0827],\n",
      "        [ 0.0057,  0.0201,  0.0915],\n",
      "        [ 0.0060,  0.0211,  0.0963],\n",
      "        [ 0.0058,  0.0203,  0.0926],\n",
      "        [ 0.0068,  0.0237,  0.1083]], grad_fn=<MmBackward>)\n",
      "\n",
      "Testing Accuracy: 31.0/50 (62.000%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "tree.plot_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[ 0.,  0.,  0., -2.]])\n",
      "path_prob tensor([[0.3048, 0.3079, 0.5365]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.3048],\n",
      "         [0.3079],\n",
      "         [0.5365]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.3048, 0.6952],\n",
      "         [0.3079, 0.6921],\n",
      "         [0.5365, 0.4635]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.0938, 0.2110, 0.3730, 0.3223]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.0938, 0.2110, 0.3730, 0.3223]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0046,  0.0072, -0.0494]], grad_fn=<MmBackward>)\n",
      "output tensor([[0.3369, 0.3409, 0.3221]], grad_fn=<SoftmaxBackward>)\n",
      "pred tensor([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0,0,0,-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "value=2\n",
    "feature_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.6006495060573165>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy())*tf.math.sigmoid(value*tree.inner_nodes[0].weight[1][feature_index].detach().numpy()+tree.inner_nodes[0].bias[1].detach().numpy())\n",
    "prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.2302840874230937>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2 = tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy())*(1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[1][feature_index].detach().numpy()+tree.inner_nodes[0].bias[1].detach().numpy()))\n",
    "prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.09042762086644066>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob3 = (1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy()))*tf.math.sigmoid(value*tree.inner_nodes[0].weight[2][feature_index].detach().numpy()+tree.inner_nodes[0].bias[2].detach().numpy())\n",
    "prob3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.07863878565314916>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob4 = (1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy()))*(1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[2][feature_index].detach().numpy()+tree.inner_nodes[0].bias[2].detach().numpy()))\n",
    "prob4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([-0.10283867,  0.01608272,  0.25969243])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1 = prob1*tree.leaf_nodes.weight.T[0].detach().numpy()\n",
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 0.06597296, -0.03048799, -0.06809449])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2 = prob2*tree.leaf_nodes.weight.T[1].detach().numpy()\n",
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([-0.00111432,  0.00175781, -0.01198421])>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3 = prob3*tree.leaf_nodes.weight.T[2].detach().numpy()\n",
    "o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([-0.01064835, -0.008296  , -0.00356906])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o4 = prob4*tree.leaf_nodes.weight.T[3].detach().numpy()\n",
    "o4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.30487944, 0.31343792, 0.38168264])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(tf.reduce_sum([o1, o2, o3, o4], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.2806432 , 0.31608321, 0.40327359])>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(o1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4836,  0.5165, -0.0162,  0.5607],\n",
       "        [-0.0084,  0.2021, -0.1423,  0.3949],\n",
       "        [ 0.3418, -0.2677,  0.3356, -0.0016]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4708, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1712,  0.0268,  0.4324], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.sigmoid(2*1.115+0.014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.sigmoid(2*1.01-0.142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "parameters = tree.to_array()\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'fully_grown': True,       \n",
    "        'balance': 0.5\n",
    "        'balancing_tolerance': 0.05               \n",
    "    }\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'make_classification', #'random'\n",
    "        'objective': 'classification' # 'multiclass_classification', 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "        \n",
    "        'lambda_dataset_size': 5000, #number of samples per polynomial\n",
    "        'number_of_generated_datasets': 10000,\n",
    "    }, \n",
    "    'computation':{\n",
    "        'n_jobs': 5,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 0,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:58.879427Z",
     "start_time": "2020-09-16T12:26:58.874894Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T21:12:40.476681Z",
     "start_time": "2021-01-13T21:12:38.298249Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product       # forms cartesian products\n",
    "from more_itertools import random_product \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import random \n",
    "from random import sample \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sympy import Symbol, sympify\n",
    "\n",
    "        \n",
    "import seaborn as sns\n",
    "        \n",
    "import random \n",
    "\n",
    "import warnings\n",
    "\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='data_creation'))\n",
    "generate_directory_structure()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_identifier_polynomial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:28:46.853042Z",
     "start_time": "2020-09-16T12:28:46.848346Z"
    }
   },
   "source": [
    "# Function Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_decision_tree():\n",
    "    \n",
    "    \n",
    "    \n",
    "number_of_variables,\n",
    "maximum_depth,\n",
    "num_classes,\n",
    "fully_grown\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decision_tree_data(n_samples, noise, noise_dist, seed):\n",
    "    \n",
    "    decision_tree = generate_random_decision_tree()\n",
    "    \n",
    "    return decision_tree, X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if same_training_all_lambda_nets:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(generate_decision_tree_data)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))  \n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(gen_regression_symbolic)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED+i, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))\n",
    "\n",
    "X_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[1], columns=list(variables[:n]))] for result in result_list]\n",
    "y_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[2], columns=['result'])] for result in result_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T12:59:22.156778Z",
     "start_time": "2021-01-14T12:57:34.187753Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample_' + path_identifier_polynomial_data + '.csv'\n",
    "polynomials_list_df.to_csv(path_polynomials, index=False)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_X_data, 'wb') as f:\n",
    "    pickle.dump(X_data_list, f)#, protocol=2)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_y_data, 'wb') as f:\n",
    "    pickle.dump(y_data_list, f)#, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
