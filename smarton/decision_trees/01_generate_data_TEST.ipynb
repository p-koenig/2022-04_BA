{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Generation for the Training of Î»-Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:49.061308Z",
     "start_time": "2020-09-16T12:26:49.055692Z"
    }
   },
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utilities.DecisionTree import SoftBinaryDecisionTree, brand_new_tfsession, draw_tree, draw_tree_image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "y = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "y = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features = img_rows * img_cols * img_chans\n",
    "tree_depth = 1\n",
    "penalty_strength = 0#1e+1\n",
    "penalty_decay = 0#0.25\n",
    "ema_win_size = 1000\n",
    "inv_temp = 0#0.01\n",
    "learning_rate = 0.01#5e-03\n",
    "batch_size = 1#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smarton/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built tree has 2 leaves out of 3 nodes\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "sess = brand_new_tfsession()\n",
    "\n",
    "tree = SoftBinaryDecisionTree(tree_depth, \n",
    "                              n_features=X_train.shape[1], \n",
    "                              n_classes=y_train.shape[1],\n",
    "                              penalty_strength=penalty_strength, \n",
    "                              penalty_decay=penalty_decay,\n",
    "                              inv_temp=inv_temp, \n",
    "                              ema_win_size=ema_win_size, \n",
    "                              learning_rate=learning_rate)\n",
    "\n",
    "tree.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from assets/non-distilled/tree-model.\n",
      "assets/non-distilled/tree-model is not a valid checkpoint. Training from scratch.\n",
      "Train on 100 samples, validate on 50 samples\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0959 - acc: 0.3500 - val_loss: 0.0954 - val_acc: 0.3000\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0950 - acc: 0.3500 - val_loss: 0.0955 - val_acc: 0.3000\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0947 - acc: 0.3500 - val_loss: 0.0956 - val_acc: 0.3000\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0942 - acc: 0.3500 - val_loss: 0.0957 - val_acc: 0.3000\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0936 - acc: 0.3500 - val_loss: 0.0954 - val_acc: 0.3200\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0929 - acc: 0.3500 - val_loss: 0.0953 - val_acc: 0.3200\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0923 - acc: 0.3500 - val_loss: 0.0951 - val_acc: 0.3000\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0908 - acc: 0.3600 - val_loss: 0.0939 - val_acc: 0.3200\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0887 - acc: 0.3600 - val_loss: 0.0915 - val_acc: 0.3200\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0851 - acc: 0.3600 - val_loss: 0.0881 - val_acc: 0.3200\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0799 - acc: 0.3700 - val_loss: 0.0825 - val_acc: 0.3600\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0732 - acc: 0.4500 - val_loss: 0.0750 - val_acc: 0.4800\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0646 - acc: 0.5300 - val_loss: 0.0649 - val_acc: 0.6400\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0533 - acc: 0.6100 - val_loss: 0.0516 - val_acc: 0.7000\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0393 - acc: 0.6500 - val_loss: 0.0321 - val_acc: 0.7000\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0202 - acc: 0.6500 - val_loss: 0.0070 - val_acc: 0.7000\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.0040 - acc: 0.6100 - val_loss: -0.0243 - val_acc: 0.7000\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.0341 - acc: 0.6500 - val_loss: -0.0622 - val_acc: 0.7000\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.0684 - acc: 0.6500 - val_loss: -0.1070 - val_acc: 0.7000\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.1067 - acc: 0.6500 - val_loss: -0.1517 - val_acc: 0.7000\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.1469 - acc: 0.6500 - val_loss: -0.1990 - val_acc: 0.7000\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.1894 - acc: 0.6500 - val_loss: -0.2532 - val_acc: 0.7000\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.2334 - acc: 0.6500 - val_loss: -0.3033 - val_acc: 0.7000\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.2775 - acc: 0.6500 - val_loss: -0.3566 - val_acc: 0.7000\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.3217 - acc: 0.6500 - val_loss: -0.4099 - val_acc: 0.7000\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.3668 - acc: 0.6500 - val_loss: -0.4635 - val_acc: 0.7000\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.4124 - acc: 0.6500 - val_loss: -0.5156 - val_acc: 0.7000\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.4570 - acc: 0.6500 - val_loss: -0.5686 - val_acc: 0.7000\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.5021 - acc: 0.6500 - val_loss: -0.6214 - val_acc: 0.7000\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.5470 - acc: 0.6500 - val_loss: -0.6741 - val_acc: 0.7000\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.5911 - acc: 0.6500 - val_loss: -0.7303 - val_acc: 0.7000\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.6353 - acc: 0.6500 - val_loss: -0.7830 - val_acc: 0.7000\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.6801 - acc: 0.6500 - val_loss: -0.8322 - val_acc: 0.7000\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: -0.7236 - acc: 0.6500 - val_loss: -0.8819 - val_acc: 0.7000\n",
      "Epoch 00034: early stopping\n",
      "Saving trained model to assets/non-distilled/tree-model.\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', patience=20, verbose=1)\n",
    "\n",
    "'''If you wish to train your own model instead of loading one from checkpoint, remove the checkpoint.'''\n",
    "os.remove('assets/non-distilled/checkpoint')\n",
    "for f in glob.glob('assets/non-distilled/tree-model*'):\n",
    "    os.remove(f)\n",
    "\n",
    "tree.maybe_train(\n",
    "    sess=sess, data_train=(X_train, y_train), data_valid=(X_test, y_test),\n",
    "    batch_size=batch_size, epochs=epochs, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = dict([(l.name.split('_')[-1],\n",
    "                 np.squeeze(l.get_weights()[0]))\n",
    "                for l in tree.model.layers if 'dense' in l.name])\n",
    "biases = dict([(l.name.split('_')[-1], np.squeeze(l.get_weights()[1][0]))\n",
    "               for l in tree.model.layers if 'dense' in l.name])\n",
    "leaves = dict([(l.name.split('_')[-1], sess.run(l.output))\n",
    "               for l in tree.model.layers if 'pdist' in l.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': array([-0.57247144,  1.7576524 , -2.406543  , -2.512611  ], dtype=float32)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1.0149852}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': array([[0.87572837, 0.06968661, 0.05458508]], dtype=float32),\n",
       " '01': array([[0.09978398, 0.41757396, 0.48264208]], dtype=float32)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree= SDT(input_dim=X_train.shape[1], \n",
    "          output_dim=int(max(y_train))+1, \n",
    "          depth=1,\n",
    "          use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "m1 = torch.FloatTensor(X_test[index:index+1])\n",
    "print(m1.shape)\n",
    "print(m1)\n",
    "\n",
    "m1_new = torch.reshape(torch.cat((torch.FloatTensor([1]), m1[0])), (1, torch.cat((torch.FloatTensor([1]), m1[0])).shape[0]))\n",
    "print(m1_new.shape)\n",
    "print(m1_new)\n",
    "\n",
    "m2=tree.inner_nodes[0].weight\n",
    "print(m2.shape)\n",
    "print(m2)\n",
    "\n",
    "m3 = tree.leaf_nodes.weight\n",
    "print(m3.shape)\n",
    "print(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_index = 5\n",
    "\n",
    "m2_custom = torch.FloatTensor([[0 for i in range(31)]])\n",
    "m2_custom[0][1+adjust_index] = 1\n",
    "print(m2_custom.shape)\n",
    "print(m2_custom)\n",
    "\n",
    "m2 = m2_custom\n",
    "\n",
    "print(m1[0][adjust_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_index = 5\n",
    "\n",
    "m1_custom = torch.FloatTensor([[0 for i in range(31)]])\n",
    "m1_custom[0][0] = 0#1\n",
    "m1_custom[0][1+adjust_index] = -3\n",
    "print(m1_custom.shape)\n",
    "print(m1_custom)\n",
    "\n",
    "m1_new = m1_custom\n",
    "\n",
    "\n",
    "print(m1[0][adjust_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = torch.FloatTensor([[1,0], [1,0]]) #niedrige werte in erstem array (stehen fÃ¼r klasse 0) stehen fÃ¼r geringe wahrscheinlichkeit dieser klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(m2[0], m1_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.3168 * -1.6799 + 0.6832 * 1.0056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.3168 * 1.1978 + 0.6832 * -0.6925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = torch.stack([torch.stack([torch.sigmoid(torch.matmul(m1[0], m2[0][1:])+m2[0][0])])])\n",
    "print(m2[0]) # input\n",
    "print(m1_new[0]) # filter\n",
    "output = torch.stack([torch.stack([torch.sigmoid(torch.matmul(m2[0], m1_new[0]))])])\n",
    "print(output)\n",
    "output = torch.unsqueeze(output, dim=2)\n",
    "print(output)\n",
    "output = torch.cat((output, 1 - output), dim=2)\n",
    "print(output)\n",
    "print(m3) #output distrib\n",
    "output = torch.stack([torch.matmul(m3, output[0][0])])\n",
    "print(output)\n",
    "output = torch.softmax(output, dim=1)\n",
    "print(output)\n",
    "output = output.data.max(1)[1]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "tree_copy = copy.deepcopy(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_index = 5\n",
    "\n",
    "m2_custom = torch.FloatTensor([[0 for i in range(31)]])\n",
    "m2_custom[0][1+adjust_index] = -2\n",
    "print(m2_custom.shape)\n",
    "print(m2_custom)\n",
    "\n",
    "m2 = m2_custom\n",
    "\n",
    "print(m1[0][adjust_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_copy.inner_nodes[0].weight = torch.nn.Parameter(m2_custom)\n",
    "print(tree_copy.inner_nodes[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_copy.leaf_nodes.weight = torch.nn.Parameter(torch.FloatTensor([[-1,1], [1,-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(0.2562*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "index = 1\n",
    "tree_copy.predict(X_test[index:index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([-0.4566, 0.3367])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.FloatTensor([[-0.4566, 0.3367]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(t1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.inner_nodes[0].weight.shape)\n",
    "print(tree.inner_nodes[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.leaf_nodes.weight.shape)\n",
    "print(tree.leaf_nodes.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "normal_tree = DecisionTreeClassifier(max_depth=1)\n",
    "normal_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = normal_tree.predict(X_test)\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plot_tree(normal_tree, fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'fully_grown': True,       \n",
    "        'balance': 0.5\n",
    "        'balancing_tolerance': 0.05               \n",
    "    }\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'make_classification', #'random'\n",
    "        'objective': 'classification' # 'multiclass_classification', 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "        \n",
    "        'lambda_dataset_size': 5000, #number of samples per polynomial\n",
    "        'number_of_generated_datasets': 10000,\n",
    "    }, \n",
    "    'computation':{\n",
    "        'n_jobs': 5,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 0,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:58.879427Z",
     "start_time": "2020-09-16T12:26:58.874894Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T21:12:40.476681Z",
     "start_time": "2021-01-13T21:12:38.298249Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product       # forms cartesian products\n",
    "from more_itertools import random_product \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import random \n",
    "from random import sample \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sympy import Symbol, sympify\n",
    "\n",
    "        \n",
    "import seaborn as sns\n",
    "        \n",
    "import random \n",
    "\n",
    "import warnings\n",
    "\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='data_creation'))\n",
    "generate_directory_structure()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_identifier_polynomial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:28:46.853042Z",
     "start_time": "2020-09-16T12:28:46.848346Z"
    }
   },
   "source": [
    "# Function Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_decision_tree():\n",
    "    \n",
    "    \n",
    "    \n",
    "number_of_variables,\n",
    "maximum_depth,\n",
    "num_classes,\n",
    "fully_grown\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decision_tree_data(n_samples, noise, noise_dist, seed):\n",
    "    \n",
    "    decision_tree = generate_random_decision_tree()\n",
    "    \n",
    "    return decision_tree, X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if same_training_all_lambda_nets:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(generate_decision_tree_data)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))  \n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(gen_regression_symbolic)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED+i, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))\n",
    "\n",
    "X_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[1], columns=list(variables[:n]))] for result in result_list]\n",
    "y_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[2], columns=['result'])] for result in result_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T12:59:22.156778Z",
     "start_time": "2021-01-14T12:57:34.187753Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample_' + path_identifier_polynomial_data + '.csv'\n",
    "polynomials_list_df.to_csv(path_polynomials, index=False)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_X_data, 'wb') as f:\n",
    "    pickle.dump(X_data_list, f)#, protocol=2)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_y_data, 'wb') as f:\n",
    "    pickle.dump(y_data_list, f)#, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
