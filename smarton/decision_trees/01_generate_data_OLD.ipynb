{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Generation for the Training of Î»-Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:49.061308Z",
     "start_time": "2020-09-16T12:26:49.055692Z"
    }
   },
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SDT(nn.Module):\n",
    "    \"\"\"Fast implementation of soft decision tree in PyTorch.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "      The number of input dimensions.\n",
    "    output_dim : int\n",
    "      The number of output dimensions. For example, for a multi-class\n",
    "      classification problem with `K` classes, it is set to `K`.\n",
    "    depth : int, default=5\n",
    "      The depth of the soft decision tree. Since the soft decision tree is\n",
    "      a full binary tree, setting `depth` to a large value will drastically\n",
    "      increases the training and evaluating cost.\n",
    "    lamda : float, default=1e-3\n",
    "      The coefficient of the regularization term in the training loss. Please\n",
    "      refer to the paper on the formulation of the regularization term.\n",
    "    use_cuda : bool, default=False\n",
    "      When set to `True`, use GPU to fit the model. Training a soft decision\n",
    "      tree using CPU could be faster considering the inherent data forwarding\n",
    "      process.\n",
    "    Attributes\n",
    "    ----------\n",
    "    internal_node_num_ : int\n",
    "      The number of internal nodes in the tree. Given the tree depth `d`, it\n",
    "      equals to :math:`2^d - 1`.\n",
    "    leaf_node_num_ : int\n",
    "      The number of leaf nodes in the tree. Given the tree depth `d`, it equals\n",
    "      to :math:`2^d`.\n",
    "    penalty_list : list\n",
    "      A list storing the layer-wise coefficients of the regularization term.\n",
    "    inner_nodes : torch.nn.Sequential\n",
    "      A container that simulates all internal nodes in the soft decision tree.\n",
    "      The sigmoid activation function is concatenated to simulate the\n",
    "      probabilistic routing mechanism.\n",
    "    leaf_nodes : torch.nn.Linear\n",
    "      A `nn.Linear` module that simulates all leaf nodes in the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            depth=5,\n",
    "            lamda=1e-3,\n",
    "            use_cuda=False):\n",
    "        super(SDT, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.depth = depth\n",
    "        self.lamda = lamda\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "        self._validate_parameters()\n",
    "\n",
    "        self.internal_node_num_ = 2 ** self.depth - 1\n",
    "        self.leaf_node_num_ = 2 ** self.depth\n",
    "\n",
    "        # Different penalty coefficients for nodes in different layers\n",
    "        self.penalty_list = [\n",
    "            self.lamda * (2 ** (-depth)) for depth in range(0, self.depth)\n",
    "        ]\n",
    "\n",
    "        # Initialize internal nodes and leaf nodes, the input dimension on\n",
    "        # internal nodes is added by 1, serving as the bias.\n",
    "        self.inner_nodes = nn.Sequential(\n",
    "            nn.Linear(self.input_dim + 1, self.internal_node_num_, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.leaf_nodes = nn.Linear(self.leaf_node_num_,\n",
    "                                    self.output_dim,\n",
    "                                    bias=False)\n",
    "\n",
    "    def forward(self, X, is_training_data=False):\n",
    "        _mu, _penalty = self._forward(X)\n",
    "        y_pred = self.leaf_nodes(_mu)\n",
    "\n",
    "        # When `X` is the training data, the model also returns the penalty\n",
    "        # to compute the training loss.\n",
    "        if is_training_data:\n",
    "            return y_pred, _penalty\n",
    "        else:\n",
    "            return y_pred\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"Implementation on the data forwarding process.\"\"\"\n",
    "\n",
    "        batch_size = X.size()[0]\n",
    "        X = self._data_augment(X)\n",
    "\n",
    "        path_prob = self.inner_nodes(X)\n",
    "        path_prob = torch.unsqueeze(path_prob, dim=2)\n",
    "        path_prob = torch.cat((path_prob, 1 - path_prob), dim=2)\n",
    "\n",
    "        _mu = X.data.new(batch_size, 1, 1).fill_(1.0)\n",
    "        _penalty = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "        # Iterate through internal odes in each layer to compute the final path\n",
    "        # probabilities and the regularization term.\n",
    "        begin_idx = 0\n",
    "        end_idx = 1\n",
    "\n",
    "        for layer_idx in range(0, self.depth):\n",
    "            _path_prob = path_prob[:, begin_idx:end_idx, :]\n",
    "\n",
    "            # Extract internal nodes in the current layer to compute the\n",
    "            # regularization term\n",
    "            _penalty = _penalty + self._cal_penalty(layer_idx, _mu, _path_prob)\n",
    "            _mu = _mu.view(batch_size, -1, 1).repeat(1, 1, 2)\n",
    "\n",
    "            _mu = _mu * _path_prob  # update path probabilities\n",
    "\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (layer_idx + 1)\n",
    "\n",
    "        mu = _mu.view(batch_size, self.leaf_node_num_)\n",
    "\n",
    "        return mu, _penalty\n",
    "\n",
    "    def _cal_penalty(self, layer_idx, _mu, _path_prob):\n",
    "        \"\"\"\n",
    "        Compute the regularization term for internal nodes in different layers.\n",
    "        \"\"\"\n",
    "\n",
    "        penalty = torch.tensor(0.0).to(self.device)\n",
    "\n",
    "        batch_size = _mu.size()[0]\n",
    "        _mu = _mu.view(batch_size, 2 ** layer_idx)\n",
    "        _path_prob = _path_prob.view(batch_size, 2 ** (layer_idx + 1))\n",
    "\n",
    "        for node in range(0, 2 ** (layer_idx + 1)):\n",
    "            alpha = torch.sum(\n",
    "                _path_prob[:, node] * _mu[:, node // 2], dim=0\n",
    "            ) / torch.sum(_mu[:, node // 2], dim=0)\n",
    "\n",
    "            coeff = self.penalty_list[layer_idx]\n",
    "\n",
    "            penalty -= 0.5 * coeff * (torch.log(alpha) + torch.log(1 - alpha))\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def _data_augment(self, X):\n",
    "        \"\"\"Add a constant input `1` onto the front of each sample.\"\"\"\n",
    "        batch_size = X.size()[0]\n",
    "        X = X.view(batch_size, -1)\n",
    "        bias = torch.ones(batch_size, 1).to(self.device)\n",
    "        X = torch.cat((bias, X), 1)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _validate_parameters(self):\n",
    "\n",
    "        if not self.depth > 0:\n",
    "            msg = (\"The tree depth should be strictly positive, but got {}\"\n",
    "                   \"instead.\")\n",
    "            raise ValueError(msg.format(self.depth))\n",
    "\n",
    "        if not self.lamda >= 0:\n",
    "            msg = (\n",
    "                \"The coefficient of the regularization term should not be\"\n",
    "                \" negative, but got {} instead.\"\n",
    "            )\n",
    "            raise ValueError(msg.format(self.lamda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-403-5ac94247d00d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_breast_cancer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 30)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15.46    ,   11.89    ,  102.5     ,  736.9     ,    0.1257  ,\n",
       "           0.1555  ,    0.2032  ,    0.1097  ,    0.1966  ,    0.07069 ,\n",
       "           0.4209  ,    0.6583  ,    2.805   ,   44.64    ,    0.005393,\n",
       "           0.02321 ,    0.04303 ,    0.0132  ,    0.01792 ,    0.004168,\n",
       "          18.79    ,   17.04    ,  125.      , 1102.      ,    0.1531  ,\n",
       "           0.3583  ,    0.583   ,    0.1827  ,    0.3216  ,    0.101   ],\n",
       "       [  12.85    ,   21.37    ,   82.63    ,  514.5     ,    0.07551 ,\n",
       "           0.08316 ,    0.06126 ,    0.01867 ,    0.158   ,    0.06114 ,\n",
       "           0.4993  ,    1.798   ,    2.552   ,   41.24    ,    0.006011,\n",
       "           0.0448  ,    0.05175 ,    0.01341 ,    0.02669 ,    0.007731,\n",
       "          14.4     ,   27.01    ,   91.63    ,  645.8     ,    0.09402 ,\n",
       "           0.1936  ,    0.1838  ,    0.05601 ,    0.2488  ,    0.08151 ],\n",
       "       [  19.21    ,   18.57    ,  125.5     , 1152.      ,    0.1053  ,\n",
       "           0.1267  ,    0.1323  ,    0.08994 ,    0.1917  ,    0.05961 ,\n",
       "           0.7275  ,    1.193   ,    4.837   ,  102.5     ,    0.006458,\n",
       "           0.02306 ,    0.02945 ,    0.01538 ,    0.01852 ,    0.002608,\n",
       "          26.14    ,   28.14    ,  170.1     , 2145.      ,    0.1624  ,\n",
       "           0.3511  ,    0.3879  ,    0.2091  ,    0.3537  ,    0.08294 ],\n",
       "       [  12.47    ,   17.31    ,   80.45    ,  480.1     ,    0.08928 ,\n",
       "           0.0763  ,    0.03609 ,    0.02369 ,    0.1526  ,    0.06046 ,\n",
       "           0.1532  ,    0.781   ,    1.253   ,   11.91    ,    0.003796,\n",
       "           0.01371 ,    0.01346 ,    0.007096,    0.01536 ,    0.001541,\n",
       "          14.06    ,   24.34    ,   92.82    ,  607.3     ,    0.1276  ,\n",
       "           0.2506  ,    0.2028  ,    0.1053  ,    0.3035  ,    0.07661 ],\n",
       "       [  12.46    ,   19.89    ,   80.43    ,  471.3     ,    0.08451 ,\n",
       "           0.1014  ,    0.0683  ,    0.03099 ,    0.1781  ,    0.06249 ,\n",
       "           0.3642  ,    1.04    ,    2.579   ,   28.32    ,    0.00653 ,\n",
       "           0.03369 ,    0.04712 ,    0.01403 ,    0.0274  ,    0.004651,\n",
       "          13.46    ,   23.07    ,   88.13    ,  551.3     ,    0.105   ,\n",
       "           0.2158  ,    0.1904  ,    0.07625 ,    0.2685  ,    0.07764 ],\n",
       "       [  10.86    ,   21.48    ,   68.51    ,  360.5     ,    0.07431 ,\n",
       "           0.04227 ,    0.      ,    0.      ,    0.1661  ,    0.05948 ,\n",
       "           0.3163  ,    1.304   ,    2.115   ,   20.67    ,    0.009579,\n",
       "           0.01104 ,    0.      ,    0.      ,    0.03004 ,    0.002228,\n",
       "          11.66    ,   24.77    ,   74.08    ,  412.3     ,    0.1001  ,\n",
       "           0.07348 ,    0.      ,    0.      ,    0.2458  ,    0.06592 ],\n",
       "       [  11.37    ,   18.89    ,   72.17    ,  396.      ,    0.08713 ,\n",
       "           0.05008 ,    0.02399 ,    0.02173 ,    0.2013  ,    0.05955 ,\n",
       "           0.2656  ,    1.974   ,    1.954   ,   17.49    ,    0.006538,\n",
       "           0.01395 ,    0.01376 ,    0.009924,    0.03416 ,    0.002928,\n",
       "          12.36    ,   26.14    ,   79.29    ,  459.3     ,    0.1118  ,\n",
       "           0.09708 ,    0.07529 ,    0.06203 ,    0.3267  ,    0.06994 ],\n",
       "       [  18.81    ,   19.98    ,  120.9     , 1102.      ,    0.08923 ,\n",
       "           0.05884 ,    0.0802  ,    0.05843 ,    0.155   ,    0.04996 ,\n",
       "           0.3283  ,    0.828   ,    2.363   ,   36.74    ,    0.007571,\n",
       "           0.01114 ,    0.02623 ,    0.01463 ,    0.0193  ,    0.001676,\n",
       "          19.96    ,   24.3     ,  129.      , 1236.      ,    0.1243  ,\n",
       "           0.116   ,    0.221   ,    0.1294  ,    0.2567  ,    0.05737 ],\n",
       "       [  13.49    ,   22.3     ,   86.91    ,  561.      ,    0.08752 ,\n",
       "           0.07698 ,    0.04751 ,    0.03384 ,    0.1809  ,    0.05718 ,\n",
       "           0.2338  ,    1.353   ,    1.735   ,   20.2     ,    0.004455,\n",
       "           0.01382 ,    0.02095 ,    0.01184 ,    0.01641 ,    0.001956,\n",
       "          15.15    ,   31.82    ,   99.      ,  698.8     ,    0.1162  ,\n",
       "           0.1711  ,    0.2282  ,    0.1282  ,    0.2871  ,    0.06917 ],\n",
       "       [   9.567   ,   15.91    ,   60.21    ,  279.6     ,    0.08464 ,\n",
       "           0.04087 ,    0.01652 ,    0.01667 ,    0.1551  ,    0.06403 ,\n",
       "           0.2152  ,    0.8301  ,    1.215   ,   12.64    ,    0.01164 ,\n",
       "           0.0104  ,    0.01186 ,    0.009623,    0.02383 ,    0.00354 ,\n",
       "          10.51    ,   19.16    ,   65.74    ,  335.9     ,    0.1504  ,\n",
       "           0.09515 ,    0.07161 ,    0.07222 ,    0.2757  ,    0.08178 ]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188,)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor([[-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0717,  0.1469],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0736,  0.1487],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0554,  0.1306],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0741,  0.1493],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0652,  0.1404],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.0147,  0.0606],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0724,  0.1476],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0738,  0.1489],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0507,  0.1259],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0740,  0.1492],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.0732,  0.0022],\n",
      "        [-0.0742,  0.1494],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0740,  0.1491],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0742,  0.1494],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1716, -0.0960],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0742,  0.1493],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0740,  0.1491],\n",
      "        [-0.0735,  0.1487],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1725, -0.0969],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0036,  0.0789],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0737,  0.1489],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0499,  0.1251],\n",
      "        [ 0.1676, -0.0920],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0742,  0.1494],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1727, -0.0971],\n",
      "        [-0.0740,  0.1492],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.1702, -0.0947],\n",
      "        [-0.0740,  0.1492],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0727,  0.1479],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.0429,  0.0324],\n",
      "        [ 0.1220, -0.0465],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [ 0.0842, -0.0088],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0717,  0.1469],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0736,  0.1488],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0742,  0.1494],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0743,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0744,  0.1495],\n",
      "        [-0.0742,  0.1494],\n",
      "        [-0.0744,  0.1495]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6628, grad_fn=<NllLossBackward>)\n",
      "Epoch: 00 | Batch: 000 | Loss: nan | Correct: 252/381\n",
      "\n",
      "Epoch: 00 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 01 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 01 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 02 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 02 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 03 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 03 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 04 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 04 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 05 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 05 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 06 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 06 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 07 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 07 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 08 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 08 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n",
      "output tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<MmBackward>)\n",
      "penalty tensor(nan, grad_fn=<AddBackward0>)\n",
      "loss tensor(nan, grad_fn=<NllLossBackward>)\n",
      "Epoch: 09 | Batch: 000 | Loss: nan | Correct: 145/381\n",
      "\n",
      "Epoch: 09 | Testing Accuracy: 67.0/188 (35.638%) | Historical Best: 35.638%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "\n",
    "def onehot_coding(target, device, output_dim):\n",
    "    \"\"\"Convert the class labels into one-hot encoded vectors.\"\"\"\n",
    "    target_onehot = torch.FloatTensor(target.size()[0], output_dim).to(device)\n",
    "    target_onehot.data.zero_()\n",
    "    #print(target_onehot)\n",
    "    target_onehot.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return target_onehot\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parameters\n",
    "    input_dim = X_train.shape[1]#28 * 28    # the number of input dimensions\n",
    "    output_dim = 2#10        # the number of outputs (i.e., # classes on MNIST)\n",
    "    depth = 5              # tree depth\n",
    "    lamda = 1e-3           # coefficient of the regularization term\n",
    "    lr = 1e-2              # learning rate\n",
    "    weight_decaly = 5e-4   # weight decay\n",
    "    batch_size = 128       # batch size\n",
    "    epochs = 10          # the number of training epochs\n",
    "    log_interval = 100     # the number of batches to wait before printing logs\n",
    "    use_cuda = False       # whether to use GPU\n",
    "\n",
    "    # Model and Optimizer\n",
    "    tree = SDT(input_dim, output_dim, depth, lamda, use_cuda)\n",
    "\n",
    "    optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decaly)\n",
    "\n",
    "    # Load data\n",
    "    data_dir = \"../Dataset/mnist\"\n",
    "\n",
    "    transformer = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    \n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.LongTensor(y_train)#torch.LongTensor(y.reshape(-1,1))\n",
    "    \n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.LongTensor(y_test)    \n",
    "    #print('X', X.dtype)\n",
    "    #print('y', y.dtype)    \n",
    "    #print('X', X)\n",
    "    #print('y', y)\n",
    "    \n",
    "    \n",
    "    # Utils\n",
    "    best_testing_acc = 0.0\n",
    "    testing_acc_list = []\n",
    "    training_loss_list = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training\n",
    "        tree.train()\n",
    "        \n",
    "        batch_size = X_train.shape[0]#data.size()[0]\n",
    "        data, target = X_train, y_train#data.to(device), target.to(device)\n",
    "        target_onehot = onehot_coding(target, device, output_dim)\n",
    "\n",
    "        #print(batch_size)\n",
    "        #print(data.shape)\n",
    "        #print(target.shape)\n",
    "        #print(target_onehot.shape)\n",
    "\n",
    "        \n",
    "        output, penalty = tree.forward(data, is_training_data=True)\n",
    "        print('output', output)\n",
    "        print('penalty', penalty)\n",
    "        \n",
    "        loss = criterion(output, target.view(-1))\n",
    "        print('loss', loss)\n",
    "        \n",
    "        loss += penalty\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct = pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "            msg = (\n",
    "                \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                \" Correct: {:03d}/{:03d}\"\n",
    "            )\n",
    "            print(msg.format(epoch, batch_idx, loss, correct, batch_size))\n",
    "            training_loss_list.append(loss.cpu().data.numpy())\n",
    "\n",
    "        # Evaluating\n",
    "        tree.eval()\n",
    "        correct = 0.\n",
    "\n",
    "\n",
    "        batch_size = X_test.shape[0]#data.size()[0]\n",
    "        data, target = X_test, y_test#data.to(device), target.to(device)\n",
    "\n",
    "        output = F.softmax(tree.forward(data), dim=1)\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "        accuracy = 100.0 * float(correct) / target.shape[0]\n",
    "\n",
    "        if accuracy > best_testing_acc:\n",
    "            best_testing_acc = accuracy\n",
    "\n",
    "        msg = (\n",
    "            \"\\nEpoch: {:02d} | Testing Accuracy: {}/{} ({:.3f}%) |\"\n",
    "            \" Historical Best: {:.3f}%\\n\"\n",
    "        )\n",
    "        print(\n",
    "            msg.format(\n",
    "                epoch, correct,\n",
    "                target.shape[0],\n",
    "                accuracy,\n",
    "                best_testing_acc\n",
    "            )\n",
    "        )\n",
    "        testing_acc_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Batch: 000 | Loss: 1.10822 | Correct: 035/100\n",
      "\n",
      "Epoch: 00 | Testing Accuracy: 15.0/50 (30.000%) | Historical Best: 30.000%\n",
      "\n",
      "Epoch: 01 | Batch: 000 | Loss: 1.10411 | Correct: 035/100\n",
      "\n",
      "Epoch: 01 | Testing Accuracy: 17.0/50 (34.000%) | Historical Best: 34.000%\n",
      "\n",
      "Epoch: 02 | Batch: 000 | Loss: 1.10004 | Correct: 041/100\n",
      "\n",
      "Epoch: 02 | Testing Accuracy: 26.0/50 (52.000%) | Historical Best: 52.000%\n",
      "\n",
      "Epoch: 03 | Batch: 000 | Loss: 1.09585 | Correct: 055/100\n",
      "\n",
      "Epoch: 03 | Testing Accuracy: 19.0/50 (38.000%) | Historical Best: 52.000%\n",
      "\n",
      "Epoch: 04 | Batch: 000 | Loss: 1.09144 | Correct: 043/100\n",
      "\n",
      "Epoch: 04 | Testing Accuracy: 17.0/50 (34.000%) | Historical Best: 52.000%\n",
      "\n",
      "Epoch: 05 | Batch: 000 | Loss: 1.08678 | Correct: 037/100\n",
      "\n",
      "Epoch: 05 | Testing Accuracy: 16.0/50 (32.000%) | Historical Best: 52.000%\n",
      "\n",
      "Epoch: 06 | Batch: 000 | Loss: 1.08184 | Correct: 036/100\n",
      "\n",
      "Epoch: 06 | Testing Accuracy: 17.0/50 (34.000%) | Historical Best: 52.000%\n",
      "\n",
      "Epoch: 07 | Batch: 000 | Loss: 1.07664 | Correct: 036/100\n",
      "\n",
      "Epoch: 07 | Testing Accuracy: 23.0/50 (46.000%) | Historical Best: 52.000%\n",
      "\n",
      "Epoch: 08 | Batch: 000 | Loss: 1.07117 | Correct: 050/100\n",
      "\n",
      "Epoch: 08 | Testing Accuracy: 34.0/50 (68.000%) | Historical Best: 68.000%\n",
      "\n",
      "Epoch: 09 | Batch: 000 | Loss: 1.06546 | Correct: 060/100\n",
      "\n",
      "Epoch: 09 | Testing Accuracy: 35.0/50 (70.000%) | Historical Best: 70.000%\n",
      "\n",
      "Epoch: 10 | Batch: 000 | Loss: 1.05954 | Correct: 064/100\n",
      "\n",
      "Epoch: 10 | Testing Accuracy: 35.0/50 (70.000%) | Historical Best: 70.000%\n",
      "\n",
      "Epoch: 11 | Batch: 000 | Loss: 1.05341 | Correct: 066/100\n",
      "\n",
      "Epoch: 11 | Testing Accuracy: 35.0/50 (70.000%) | Historical Best: 70.000%\n",
      "\n",
      "Epoch: 12 | Batch: 000 | Loss: 1.04706 | Correct: 066/100\n",
      "\n",
      "Epoch: 12 | Testing Accuracy: 35.0/50 (70.000%) | Historical Best: 70.000%\n",
      "\n",
      "Epoch: 13 | Batch: 000 | Loss: 1.04047 | Correct: 067/100\n",
      "\n",
      "Epoch: 13 | Testing Accuracy: 35.0/50 (70.000%) | Historical Best: 70.000%\n",
      "\n",
      "Epoch: 14 | Batch: 000 | Loss: 1.03363 | Correct: 068/100\n",
      "\n",
      "Epoch: 14 | Testing Accuracy: 35.0/50 (70.000%) | Historical Best: 70.000%\n",
      "\n",
      "Epoch: 15 | Batch: 000 | Loss: 1.02652 | Correct: 069/100\n",
      "\n",
      "Epoch: 15 | Testing Accuracy: 35.0/50 (70.000%) | Historical Best: 70.000%\n",
      "\n",
      "Epoch: 16 | Batch: 000 | Loss: 1.01915 | Correct: 070/100\n",
      "\n",
      "Epoch: 16 | Testing Accuracy: 37.0/50 (74.000%) | Historical Best: 74.000%\n",
      "\n",
      "Epoch: 17 | Batch: 000 | Loss: 1.01151 | Correct: 070/100\n",
      "\n",
      "Epoch: 17 | Testing Accuracy: 37.0/50 (74.000%) | Historical Best: 74.000%\n",
      "\n",
      "Epoch: 18 | Batch: 000 | Loss: 1.00363 | Correct: 071/100\n",
      "\n",
      "Epoch: 18 | Testing Accuracy: 39.0/50 (78.000%) | Historical Best: 78.000%\n",
      "\n",
      "Epoch: 19 | Batch: 000 | Loss: 0.99555 | Correct: 073/100\n",
      "\n",
      "Epoch: 19 | Testing Accuracy: 39.0/50 (78.000%) | Historical Best: 78.000%\n",
      "\n",
      "Epoch: 20 | Batch: 000 | Loss: 0.98728 | Correct: 078/100\n",
      "\n",
      "Epoch: 20 | Testing Accuracy: 39.0/50 (78.000%) | Historical Best: 78.000%\n",
      "\n",
      "Epoch: 21 | Batch: 000 | Loss: 0.97886 | Correct: 082/100\n",
      "\n",
      "Epoch: 21 | Testing Accuracy: 39.0/50 (78.000%) | Historical Best: 78.000%\n",
      "\n",
      "Epoch: 22 | Batch: 000 | Loss: 0.97033 | Correct: 084/100\n",
      "\n",
      "Epoch: 22 | Testing Accuracy: 39.0/50 (78.000%) | Historical Best: 78.000%\n",
      "\n",
      "Epoch: 23 | Batch: 000 | Loss: 0.96172 | Correct: 087/100\n",
      "\n",
      "Epoch: 23 | Testing Accuracy: 43.0/50 (86.000%) | Historical Best: 86.000%\n",
      "\n",
      "Epoch: 24 | Batch: 000 | Loss: 0.95305 | Correct: 087/100\n",
      "\n",
      "Epoch: 24 | Testing Accuracy: 44.0/50 (88.000%) | Historical Best: 88.000%\n",
      "\n",
      "Epoch: 25 | Batch: 000 | Loss: 0.94436 | Correct: 089/100\n",
      "\n",
      "Epoch: 25 | Testing Accuracy: 46.0/50 (92.000%) | Historical Best: 92.000%\n",
      "\n",
      "Epoch: 26 | Batch: 000 | Loss: 0.93563 | Correct: 092/100\n",
      "\n",
      "Epoch: 26 | Testing Accuracy: 46.0/50 (92.000%) | Historical Best: 92.000%\n",
      "\n",
      "Epoch: 27 | Batch: 000 | Loss: 0.92688 | Correct: 093/100\n",
      "\n",
      "Epoch: 27 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 96.000%\n",
      "\n",
      "Epoch: 28 | Batch: 000 | Loss: 0.91808 | Correct: 093/100\n",
      "\n",
      "Epoch: 28 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 96.000%\n",
      "\n",
      "Epoch: 29 | Batch: 000 | Loss: 0.90925 | Correct: 095/100\n",
      "\n",
      "Epoch: 29 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 96.000%\n",
      "\n",
      "Epoch: 30 | Batch: 000 | Loss: 0.90040 | Correct: 095/100\n",
      "\n",
      "Epoch: 30 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 96.000%\n",
      "\n",
      "Epoch: 31 | Batch: 000 | Loss: 0.89153 | Correct: 095/100\n",
      "\n",
      "Epoch: 31 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 32 | Batch: 000 | Loss: 0.88268 | Correct: 096/100\n",
      "\n",
      "Epoch: 32 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 33 | Batch: 000 | Loss: 0.87387 | Correct: 096/100\n",
      "\n",
      "Epoch: 33 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 34 | Batch: 000 | Loss: 0.86511 | Correct: 096/100\n",
      "\n",
      "Epoch: 34 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 35 | Batch: 000 | Loss: 0.85644 | Correct: 096/100\n",
      "\n",
      "Epoch: 35 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 36 | Batch: 000 | Loss: 0.84784 | Correct: 096/100\n",
      "\n",
      "Epoch: 36 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 37 | Batch: 000 | Loss: 0.83933 | Correct: 096/100\n",
      "\n",
      "Epoch: 37 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 38 | Batch: 000 | Loss: 0.83091 | Correct: 096/100\n",
      "\n",
      "Epoch: 38 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 39 | Batch: 000 | Loss: 0.82260 | Correct: 097/100\n",
      "\n",
      "Epoch: 39 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 98.000%\n",
      "\n",
      "Epoch: 40 | Batch: 000 | Loss: 0.81438 | Correct: 097/100\n",
      "\n",
      "Epoch: 40 | Testing Accuracy: 50.0/50 (100.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 41 | Batch: 000 | Loss: 0.80626 | Correct: 097/100\n",
      "\n",
      "Epoch: 41 | Testing Accuracy: 50.0/50 (100.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 42 | Batch: 000 | Loss: 0.79824 | Correct: 097/100\n",
      "\n",
      "Epoch: 42 | Testing Accuracy: 50.0/50 (100.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 43 | Batch: 000 | Loss: 0.79032 | Correct: 097/100\n",
      "\n",
      "Epoch: 43 | Testing Accuracy: 50.0/50 (100.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 44 | Batch: 000 | Loss: 0.78251 | Correct: 098/100\n",
      "\n",
      "Epoch: 44 | Testing Accuracy: 50.0/50 (100.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 45 | Batch: 000 | Loss: 0.77479 | Correct: 097/100\n",
      "\n",
      "Epoch: 45 | Testing Accuracy: 50.0/50 (100.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 46 | Batch: 000 | Loss: 0.76718 | Correct: 097/100\n",
      "\n",
      "Epoch: 46 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 47 | Batch: 000 | Loss: 0.75967 | Correct: 097/100\n",
      "\n",
      "Epoch: 47 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 48 | Batch: 000 | Loss: 0.75226 | Correct: 097/100\n",
      "\n",
      "Epoch: 48 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 49 | Batch: 000 | Loss: 0.74494 | Correct: 097/100\n",
      "\n",
      "Epoch: 49 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 50 | Batch: 000 | Loss: 0.73770 | Correct: 097/100\n",
      "\n",
      "Epoch: 50 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 51 | Batch: 000 | Loss: 0.73054 | Correct: 097/100\n",
      "\n",
      "Epoch: 51 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 52 | Batch: 000 | Loss: 0.72346 | Correct: 097/100\n",
      "\n",
      "Epoch: 52 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 53 | Batch: 000 | Loss: 0.71644 | Correct: 097/100\n",
      "\n",
      "Epoch: 53 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 54 | Batch: 000 | Loss: 0.70948 | Correct: 097/100\n",
      "\n",
      "Epoch: 54 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 55 | Batch: 000 | Loss: 0.70258 | Correct: 097/100\n",
      "\n",
      "Epoch: 55 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 56 | Batch: 000 | Loss: 0.69572 | Correct: 097/100\n",
      "\n",
      "Epoch: 56 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 57 | Batch: 000 | Loss: 0.68891 | Correct: 097/100\n",
      "\n",
      "Epoch: 57 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 58 | Batch: 000 | Loss: 0.68214 | Correct: 097/100\n",
      "\n",
      "Epoch: 58 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 59 | Batch: 000 | Loss: 0.67540 | Correct: 097/100\n",
      "\n",
      "Epoch: 59 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 60 | Batch: 000 | Loss: 0.66869 | Correct: 097/100\n",
      "\n",
      "Epoch: 60 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 61 | Batch: 000 | Loss: 0.66201 | Correct: 096/100\n",
      "\n",
      "Epoch: 61 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 62 | Batch: 000 | Loss: 0.65536 | Correct: 096/100\n",
      "\n",
      "Epoch: 62 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 63 | Batch: 000 | Loss: 0.64873 | Correct: 096/100\n",
      "\n",
      "Epoch: 63 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 64 | Batch: 000 | Loss: 0.64212 | Correct: 096/100\n",
      "\n",
      "Epoch: 64 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 65 | Batch: 000 | Loss: 0.63553 | Correct: 096/100\n",
      "\n",
      "Epoch: 65 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 66 | Batch: 000 | Loss: 0.62896 | Correct: 096/100\n",
      "\n",
      "Epoch: 66 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 67 | Batch: 000 | Loss: 0.62241 | Correct: 096/100\n",
      "\n",
      "Epoch: 67 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 68 | Batch: 000 | Loss: 0.61587 | Correct: 096/100\n",
      "\n",
      "Epoch: 68 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 69 | Batch: 000 | Loss: 0.60934 | Correct: 096/100\n",
      "\n",
      "Epoch: 69 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 70 | Batch: 000 | Loss: 0.60282 | Correct: 096/100\n",
      "\n",
      "Epoch: 70 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 71 | Batch: 000 | Loss: 0.59630 | Correct: 097/100\n",
      "\n",
      "Epoch: 71 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 72 | Batch: 000 | Loss: 0.58980 | Correct: 097/100\n",
      "\n",
      "Epoch: 72 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 73 | Batch: 000 | Loss: 0.58330 | Correct: 097/100\n",
      "\n",
      "Epoch: 73 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 74 | Batch: 000 | Loss: 0.57681 | Correct: 097/100\n",
      "\n",
      "Epoch: 74 | Testing Accuracy: 47.0/50 (94.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 75 | Batch: 000 | Loss: 0.57033 | Correct: 097/100\n",
      "\n",
      "Epoch: 75 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 76 | Batch: 000 | Loss: 0.56385 | Correct: 097/100\n",
      "\n",
      "Epoch: 76 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 77 | Batch: 000 | Loss: 0.55739 | Correct: 097/100\n",
      "\n",
      "Epoch: 77 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 78 | Batch: 000 | Loss: 0.55092 | Correct: 097/100\n",
      "\n",
      "Epoch: 78 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 79 | Batch: 000 | Loss: 0.54447 | Correct: 097/100\n",
      "\n",
      "Epoch: 79 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 80 | Batch: 000 | Loss: 0.53802 | Correct: 097/100\n",
      "\n",
      "Epoch: 80 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 81 | Batch: 000 | Loss: 0.53158 | Correct: 097/100\n",
      "\n",
      "Epoch: 81 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 82 | Batch: 000 | Loss: 0.52514 | Correct: 097/100\n",
      "\n",
      "Epoch: 82 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 83 | Batch: 000 | Loss: 0.51871 | Correct: 097/100\n",
      "\n",
      "Epoch: 83 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 84 | Batch: 000 | Loss: 0.51230 | Correct: 097/100\n",
      "\n",
      "Epoch: 84 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 85 | Batch: 000 | Loss: 0.50589 | Correct: 097/100\n",
      "\n",
      "Epoch: 85 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 86 | Batch: 000 | Loss: 0.49950 | Correct: 097/100\n",
      "\n",
      "Epoch: 86 | Testing Accuracy: 48.0/50 (96.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 87 | Batch: 000 | Loss: 0.49312 | Correct: 097/100\n",
      "\n",
      "Epoch: 87 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 88 | Batch: 000 | Loss: 0.48675 | Correct: 097/100\n",
      "\n",
      "Epoch: 88 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 89 | Batch: 000 | Loss: 0.48040 | Correct: 097/100\n",
      "\n",
      "Epoch: 89 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 90 | Batch: 000 | Loss: 0.47407 | Correct: 097/100\n",
      "\n",
      "Epoch: 90 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 91 | Batch: 000 | Loss: 0.46774 | Correct: 097/100\n",
      "\n",
      "Epoch: 91 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 92 | Batch: 000 | Loss: 0.46144 | Correct: 098/100\n",
      "\n",
      "Epoch: 92 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 93 | Batch: 000 | Loss: 0.45514 | Correct: 098/100\n",
      "\n",
      "Epoch: 93 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 94 | Batch: 000 | Loss: 0.44885 | Correct: 098/100\n",
      "\n",
      "Epoch: 94 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 95 | Batch: 000 | Loss: 0.44257 | Correct: 098/100\n",
      "\n",
      "Epoch: 95 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 96 | Batch: 000 | Loss: 0.43629 | Correct: 098/100\n",
      "\n",
      "Epoch: 96 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 97 | Batch: 000 | Loss: 0.43000 | Correct: 098/100\n",
      "\n",
      "Epoch: 97 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 98 | Batch: 000 | Loss: 0.42371 | Correct: 099/100\n",
      "\n",
      "Epoch: 98 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 99 | Batch: 000 | Loss: 0.41740 | Correct: 099/100\n",
      "\n",
      "Epoch: 99 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 100 | Batch: 000 | Loss: 0.41108 | Correct: 099/100\n",
      "\n",
      "Epoch: 100 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 101 | Batch: 000 | Loss: 0.40474 | Correct: 099/100\n",
      "\n",
      "Epoch: 101 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 102 | Batch: 000 | Loss: 0.39839 | Correct: 099/100\n",
      "\n",
      "Epoch: 102 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 103 | Batch: 000 | Loss: 0.39204 | Correct: 099/100\n",
      "\n",
      "Epoch: 103 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 104 | Batch: 000 | Loss: 0.38569 | Correct: 099/100\n",
      "\n",
      "Epoch: 104 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 105 | Batch: 000 | Loss: 0.37937 | Correct: 099/100\n",
      "\n",
      "Epoch: 105 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 106 | Batch: 000 | Loss: 0.37308 | Correct: 099/100\n",
      "\n",
      "Epoch: 106 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 107 | Batch: 000 | Loss: 0.36685 | Correct: 099/100\n",
      "\n",
      "Epoch: 107 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 108 | Batch: 000 | Loss: 0.36069 | Correct: 099/100\n",
      "\n",
      "Epoch: 108 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 109 | Batch: 000 | Loss: 0.35462 | Correct: 099/100\n",
      "\n",
      "Epoch: 109 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 110 | Batch: 000 | Loss: 0.34864 | Correct: 099/100\n",
      "\n",
      "Epoch: 110 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 111 | Batch: 000 | Loss: 0.34277 | Correct: 099/100\n",
      "\n",
      "Epoch: 111 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 112 | Batch: 000 | Loss: 0.33702 | Correct: 099/100\n",
      "\n",
      "Epoch: 112 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 113 | Batch: 000 | Loss: 0.33137 | Correct: 099/100\n",
      "\n",
      "Epoch: 113 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 114 | Batch: 000 | Loss: 0.32584 | Correct: 099/100\n",
      "\n",
      "Epoch: 114 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 115 | Batch: 000 | Loss: 0.32042 | Correct: 099/100\n",
      "\n",
      "Epoch: 115 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 116 | Batch: 000 | Loss: 0.31510 | Correct: 099/100\n",
      "\n",
      "Epoch: 116 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 117 | Batch: 000 | Loss: 0.30989 | Correct: 099/100\n",
      "\n",
      "Epoch: 117 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 118 | Batch: 000 | Loss: 0.30478 | Correct: 099/100\n",
      "\n",
      "Epoch: 118 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 119 | Batch: 000 | Loss: 0.29976 | Correct: 099/100\n",
      "\n",
      "Epoch: 119 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 120 | Batch: 000 | Loss: 0.29482 | Correct: 099/100\n",
      "\n",
      "Epoch: 120 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 121 | Batch: 000 | Loss: 0.28996 | Correct: 099/100\n",
      "\n",
      "Epoch: 121 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 122 | Batch: 000 | Loss: 0.28517 | Correct: 099/100\n",
      "\n",
      "Epoch: 122 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 123 | Batch: 000 | Loss: 0.28045 | Correct: 099/100\n",
      "\n",
      "Epoch: 123 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 124 | Batch: 000 | Loss: 0.27578 | Correct: 099/100\n",
      "\n",
      "Epoch: 124 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 125 | Batch: 000 | Loss: 0.27117 | Correct: 099/100\n",
      "\n",
      "Epoch: 125 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 126 | Batch: 000 | Loss: 0.26660 | Correct: 099/100\n",
      "\n",
      "Epoch: 126 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 127 | Batch: 000 | Loss: 0.26208 | Correct: 099/100\n",
      "\n",
      "Epoch: 127 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 128 | Batch: 000 | Loss: 0.25761 | Correct: 099/100\n",
      "\n",
      "Epoch: 128 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 129 | Batch: 000 | Loss: 0.25318 | Correct: 099/100\n",
      "\n",
      "Epoch: 129 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 130 | Batch: 000 | Loss: 0.24881 | Correct: 099/100\n",
      "\n",
      "Epoch: 130 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 131 | Batch: 000 | Loss: 0.24449 | Correct: 099/100\n",
      "\n",
      "Epoch: 131 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 132 | Batch: 000 | Loss: 0.24024 | Correct: 099/100\n",
      "\n",
      "Epoch: 132 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 133 | Batch: 000 | Loss: 0.23606 | Correct: 099/100\n",
      "\n",
      "Epoch: 133 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 134 | Batch: 000 | Loss: 0.23197 | Correct: 099/100\n",
      "\n",
      "Epoch: 134 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 135 | Batch: 000 | Loss: 0.22796 | Correct: 099/100\n",
      "\n",
      "Epoch: 135 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 136 | Batch: 000 | Loss: 0.22406 | Correct: 099/100\n",
      "\n",
      "Epoch: 136 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 137 | Batch: 000 | Loss: 0.22025 | Correct: 099/100\n",
      "\n",
      "Epoch: 137 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 138 | Batch: 000 | Loss: 0.21655 | Correct: 099/100\n",
      "\n",
      "Epoch: 138 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 139 | Batch: 000 | Loss: 0.21296 | Correct: 099/100\n",
      "\n",
      "Epoch: 139 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 140 | Batch: 000 | Loss: 0.20947 | Correct: 099/100\n",
      "\n",
      "Epoch: 140 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 141 | Batch: 000 | Loss: 0.20609 | Correct: 099/100\n",
      "\n",
      "Epoch: 141 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 142 | Batch: 000 | Loss: 0.20281 | Correct: 099/100\n",
      "\n",
      "Epoch: 142 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 143 | Batch: 000 | Loss: 0.19964 | Correct: 099/100\n",
      "\n",
      "Epoch: 143 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 144 | Batch: 000 | Loss: 0.19657 | Correct: 099/100\n",
      "\n",
      "Epoch: 144 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 145 | Batch: 000 | Loss: 0.19360 | Correct: 099/100\n",
      "\n",
      "Epoch: 145 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 146 | Batch: 000 | Loss: 0.19073 | Correct: 099/100\n",
      "\n",
      "Epoch: 146 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 147 | Batch: 000 | Loss: 0.18795 | Correct: 099/100\n",
      "\n",
      "Epoch: 147 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 148 | Batch: 000 | Loss: 0.18526 | Correct: 099/100\n",
      "\n",
      "Epoch: 148 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 149 | Batch: 000 | Loss: 0.18266 | Correct: 099/100\n",
      "\n",
      "Epoch: 149 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 150 | Batch: 000 | Loss: 0.18014 | Correct: 098/100\n",
      "\n",
      "Epoch: 150 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 151 | Batch: 000 | Loss: 0.17771 | Correct: 098/100\n",
      "\n",
      "Epoch: 151 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 152 | Batch: 000 | Loss: 0.17536 | Correct: 098/100\n",
      "\n",
      "Epoch: 152 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 153 | Batch: 000 | Loss: 0.17309 | Correct: 098/100\n",
      "\n",
      "Epoch: 153 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 154 | Batch: 000 | Loss: 0.17089 | Correct: 098/100\n",
      "\n",
      "Epoch: 154 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 155 | Batch: 000 | Loss: 0.16876 | Correct: 098/100\n",
      "\n",
      "Epoch: 155 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 156 | Batch: 000 | Loss: 0.16670 | Correct: 098/100\n",
      "\n",
      "Epoch: 156 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 157 | Batch: 000 | Loss: 0.16471 | Correct: 098/100\n",
      "\n",
      "Epoch: 157 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 158 | Batch: 000 | Loss: 0.16279 | Correct: 098/100\n",
      "\n",
      "Epoch: 158 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 159 | Batch: 000 | Loss: 0.16093 | Correct: 098/100\n",
      "\n",
      "Epoch: 159 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 160 | Batch: 000 | Loss: 0.15913 | Correct: 098/100\n",
      "\n",
      "Epoch: 160 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 161 | Batch: 000 | Loss: 0.15738 | Correct: 098/100\n",
      "\n",
      "Epoch: 161 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 162 | Batch: 000 | Loss: 0.15570 | Correct: 098/100\n",
      "\n",
      "Epoch: 162 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 163 | Batch: 000 | Loss: 0.15406 | Correct: 098/100\n",
      "\n",
      "Epoch: 163 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 164 | Batch: 000 | Loss: 0.15248 | Correct: 098/100\n",
      "\n",
      "Epoch: 164 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 165 | Batch: 000 | Loss: 0.15095 | Correct: 098/100\n",
      "\n",
      "Epoch: 165 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 166 | Batch: 000 | Loss: 0.14947 | Correct: 098/100\n",
      "\n",
      "Epoch: 166 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 167 | Batch: 000 | Loss: 0.14803 | Correct: 098/100\n",
      "\n",
      "Epoch: 167 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 168 | Batch: 000 | Loss: 0.14664 | Correct: 098/100\n",
      "\n",
      "Epoch: 168 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 169 | Batch: 000 | Loss: 0.14529 | Correct: 098/100\n",
      "\n",
      "Epoch: 169 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 170 | Batch: 000 | Loss: 0.14398 | Correct: 098/100\n",
      "\n",
      "Epoch: 170 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 171 | Batch: 000 | Loss: 0.14271 | Correct: 098/100\n",
      "\n",
      "Epoch: 171 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 172 | Batch: 000 | Loss: 0.14148 | Correct: 098/100\n",
      "\n",
      "Epoch: 172 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 173 | Batch: 000 | Loss: 0.14029 | Correct: 098/100\n",
      "\n",
      "Epoch: 173 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 174 | Batch: 000 | Loss: 0.13913 | Correct: 098/100\n",
      "\n",
      "Epoch: 174 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 175 | Batch: 000 | Loss: 0.13800 | Correct: 098/100\n",
      "\n",
      "Epoch: 175 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 176 | Batch: 000 | Loss: 0.13691 | Correct: 098/100\n",
      "\n",
      "Epoch: 176 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 177 | Batch: 000 | Loss: 0.13585 | Correct: 098/100\n",
      "\n",
      "Epoch: 177 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 178 | Batch: 000 | Loss: 0.13482 | Correct: 098/100\n",
      "\n",
      "Epoch: 178 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 179 | Batch: 000 | Loss: 0.13382 | Correct: 098/100\n",
      "\n",
      "Epoch: 179 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 180 | Batch: 000 | Loss: 0.13284 | Correct: 098/100\n",
      "\n",
      "Epoch: 180 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 181 | Batch: 000 | Loss: 0.13190 | Correct: 098/100\n",
      "\n",
      "Epoch: 181 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 182 | Batch: 000 | Loss: 0.13098 | Correct: 098/100\n",
      "\n",
      "Epoch: 182 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 183 | Batch: 000 | Loss: 0.13008 | Correct: 098/100\n",
      "\n",
      "Epoch: 183 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 184 | Batch: 000 | Loss: 0.12921 | Correct: 098/100\n",
      "\n",
      "Epoch: 184 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 185 | Batch: 000 | Loss: 0.12836 | Correct: 098/100\n",
      "\n",
      "Epoch: 185 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 186 | Batch: 000 | Loss: 0.12753 | Correct: 098/100\n",
      "\n",
      "Epoch: 186 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 187 | Batch: 000 | Loss: 0.12673 | Correct: 098/100\n",
      "\n",
      "Epoch: 187 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 188 | Batch: 000 | Loss: 0.12594 | Correct: 098/100\n",
      "\n",
      "Epoch: 188 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 189 | Batch: 000 | Loss: 0.12518 | Correct: 098/100\n",
      "\n",
      "Epoch: 189 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 190 | Batch: 000 | Loss: 0.12443 | Correct: 098/100\n",
      "\n",
      "Epoch: 190 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 191 | Batch: 000 | Loss: 0.12371 | Correct: 098/100\n",
      "\n",
      "Epoch: 191 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 192 | Batch: 000 | Loss: 0.12300 | Correct: 098/100\n",
      "\n",
      "Epoch: 192 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 193 | Batch: 000 | Loss: 0.12231 | Correct: 098/100\n",
      "\n",
      "Epoch: 193 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 194 | Batch: 000 | Loss: 0.12163 | Correct: 098/100\n",
      "\n",
      "Epoch: 194 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 195 | Batch: 000 | Loss: 0.12097 | Correct: 098/100\n",
      "\n",
      "Epoch: 195 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 196 | Batch: 000 | Loss: 0.12033 | Correct: 098/100\n",
      "\n",
      "Epoch: 196 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 197 | Batch: 000 | Loss: 0.11970 | Correct: 098/100\n",
      "\n",
      "Epoch: 197 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 198 | Batch: 000 | Loss: 0.11909 | Correct: 098/100\n",
      "\n",
      "Epoch: 198 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 199 | Batch: 000 | Loss: 0.11849 | Correct: 098/100\n",
      "\n",
      "Epoch: 199 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 200 | Batch: 000 | Loss: 0.11790 | Correct: 098/100\n",
      "\n",
      "Epoch: 200 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 201 | Batch: 000 | Loss: 0.11733 | Correct: 098/100\n",
      "\n",
      "Epoch: 201 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 202 | Batch: 000 | Loss: 0.11677 | Correct: 098/100\n",
      "\n",
      "Epoch: 202 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 203 | Batch: 000 | Loss: 0.11622 | Correct: 098/100\n",
      "\n",
      "Epoch: 203 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 204 | Batch: 000 | Loss: 0.11568 | Correct: 098/100\n",
      "\n",
      "Epoch: 204 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 205 | Batch: 000 | Loss: 0.11515 | Correct: 098/100\n",
      "\n",
      "Epoch: 205 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 206 | Batch: 000 | Loss: 0.11464 | Correct: 098/100\n",
      "\n",
      "Epoch: 206 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 207 | Batch: 000 | Loss: 0.11414 | Correct: 098/100\n",
      "\n",
      "Epoch: 207 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 208 | Batch: 000 | Loss: 0.11364 | Correct: 098/100\n",
      "\n",
      "Epoch: 208 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 209 | Batch: 000 | Loss: 0.11316 | Correct: 098/100\n",
      "\n",
      "Epoch: 209 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 210 | Batch: 000 | Loss: 0.11269 | Correct: 098/100\n",
      "\n",
      "Epoch: 210 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 211 | Batch: 000 | Loss: 0.11222 | Correct: 098/100\n",
      "\n",
      "Epoch: 211 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 212 | Batch: 000 | Loss: 0.11177 | Correct: 098/100\n",
      "\n",
      "Epoch: 212 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 213 | Batch: 000 | Loss: 0.11132 | Correct: 098/100\n",
      "\n",
      "Epoch: 213 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 214 | Batch: 000 | Loss: 0.11088 | Correct: 098/100\n",
      "\n",
      "Epoch: 214 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 215 | Batch: 000 | Loss: 0.11045 | Correct: 098/100\n",
      "\n",
      "Epoch: 215 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 216 | Batch: 000 | Loss: 0.11003 | Correct: 098/100\n",
      "\n",
      "Epoch: 216 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 217 | Batch: 000 | Loss: 0.10962 | Correct: 098/100\n",
      "\n",
      "Epoch: 217 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 218 | Batch: 000 | Loss: 0.10921 | Correct: 098/100\n",
      "\n",
      "Epoch: 218 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 219 | Batch: 000 | Loss: 0.10881 | Correct: 098/100\n",
      "\n",
      "Epoch: 219 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 220 | Batch: 000 | Loss: 0.10842 | Correct: 098/100\n",
      "\n",
      "Epoch: 220 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 221 | Batch: 000 | Loss: 0.10804 | Correct: 098/100\n",
      "\n",
      "Epoch: 221 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 222 | Batch: 000 | Loss: 0.10766 | Correct: 098/100\n",
      "\n",
      "Epoch: 222 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 223 | Batch: 000 | Loss: 0.10729 | Correct: 098/100\n",
      "\n",
      "Epoch: 223 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 224 | Batch: 000 | Loss: 0.10693 | Correct: 098/100\n",
      "\n",
      "Epoch: 224 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 225 | Batch: 000 | Loss: 0.10657 | Correct: 098/100\n",
      "\n",
      "Epoch: 225 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 226 | Batch: 000 | Loss: 0.10622 | Correct: 098/100\n",
      "\n",
      "Epoch: 226 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 227 | Batch: 000 | Loss: 0.10587 | Correct: 098/100\n",
      "\n",
      "Epoch: 227 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 228 | Batch: 000 | Loss: 0.10553 | Correct: 098/100\n",
      "\n",
      "Epoch: 228 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 229 | Batch: 000 | Loss: 0.10519 | Correct: 098/100\n",
      "\n",
      "Epoch: 229 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 230 | Batch: 000 | Loss: 0.10486 | Correct: 098/100\n",
      "\n",
      "Epoch: 230 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 231 | Batch: 000 | Loss: 0.10454 | Correct: 098/100\n",
      "\n",
      "Epoch: 231 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 232 | Batch: 000 | Loss: 0.10422 | Correct: 098/100\n",
      "\n",
      "Epoch: 232 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 233 | Batch: 000 | Loss: 0.10391 | Correct: 098/100\n",
      "\n",
      "Epoch: 233 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 234 | Batch: 000 | Loss: 0.10360 | Correct: 098/100\n",
      "\n",
      "Epoch: 234 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 235 | Batch: 000 | Loss: 0.10329 | Correct: 098/100\n",
      "\n",
      "Epoch: 235 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 236 | Batch: 000 | Loss: 0.10300 | Correct: 098/100\n",
      "\n",
      "Epoch: 236 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 237 | Batch: 000 | Loss: 0.10270 | Correct: 098/100\n",
      "\n",
      "Epoch: 237 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 238 | Batch: 000 | Loss: 0.10241 | Correct: 098/100\n",
      "\n",
      "Epoch: 238 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 239 | Batch: 000 | Loss: 0.10212 | Correct: 098/100\n",
      "\n",
      "Epoch: 239 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 240 | Batch: 000 | Loss: 0.10184 | Correct: 098/100\n",
      "\n",
      "Epoch: 240 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 241 | Batch: 000 | Loss: 0.10156 | Correct: 098/100\n",
      "\n",
      "Epoch: 241 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 242 | Batch: 000 | Loss: 0.10129 | Correct: 098/100\n",
      "\n",
      "Epoch: 242 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 243 | Batch: 000 | Loss: 0.10102 | Correct: 098/100\n",
      "\n",
      "Epoch: 243 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 244 | Batch: 000 | Loss: 0.10075 | Correct: 098/100\n",
      "\n",
      "Epoch: 244 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 245 | Batch: 000 | Loss: 0.10049 | Correct: 098/100\n",
      "\n",
      "Epoch: 245 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 246 | Batch: 000 | Loss: 0.10023 | Correct: 098/100\n",
      "\n",
      "Epoch: 246 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 247 | Batch: 000 | Loss: 0.09998 | Correct: 098/100\n",
      "\n",
      "Epoch: 247 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 248 | Batch: 000 | Loss: 0.09973 | Correct: 098/100\n",
      "\n",
      "Epoch: 248 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 249 | Batch: 000 | Loss: 0.09948 | Correct: 098/100\n",
      "\n",
      "Epoch: 249 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 250 | Batch: 000 | Loss: 0.09923 | Correct: 098/100\n",
      "\n",
      "Epoch: 250 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 251 | Batch: 000 | Loss: 0.09899 | Correct: 098/100\n",
      "\n",
      "Epoch: 251 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 252 | Batch: 000 | Loss: 0.09875 | Correct: 098/100\n",
      "\n",
      "Epoch: 252 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 253 | Batch: 000 | Loss: 0.09852 | Correct: 098/100\n",
      "\n",
      "Epoch: 253 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 254 | Batch: 000 | Loss: 0.09828 | Correct: 098/100\n",
      "\n",
      "Epoch: 254 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 255 | Batch: 000 | Loss: 0.09805 | Correct: 098/100\n",
      "\n",
      "Epoch: 255 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 256 | Batch: 000 | Loss: 0.09783 | Correct: 098/100\n",
      "\n",
      "Epoch: 256 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 257 | Batch: 000 | Loss: 0.09760 | Correct: 098/100\n",
      "\n",
      "Epoch: 257 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 258 | Batch: 000 | Loss: 0.09738 | Correct: 098/100\n",
      "\n",
      "Epoch: 258 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 259 | Batch: 000 | Loss: 0.09717 | Correct: 098/100\n",
      "\n",
      "Epoch: 259 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 260 | Batch: 000 | Loss: 0.09695 | Correct: 098/100\n",
      "\n",
      "Epoch: 260 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 261 | Batch: 000 | Loss: 0.09674 | Correct: 098/100\n",
      "\n",
      "Epoch: 261 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 262 | Batch: 000 | Loss: 0.09653 | Correct: 098/100\n",
      "\n",
      "Epoch: 262 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 263 | Batch: 000 | Loss: 0.09632 | Correct: 098/100\n",
      "\n",
      "Epoch: 263 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 264 | Batch: 000 | Loss: 0.09611 | Correct: 098/100\n",
      "\n",
      "Epoch: 264 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 265 | Batch: 000 | Loss: 0.09591 | Correct: 098/100\n",
      "\n",
      "Epoch: 265 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 266 | Batch: 000 | Loss: 0.09571 | Correct: 098/100\n",
      "\n",
      "Epoch: 266 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 267 | Batch: 000 | Loss: 0.09551 | Correct: 098/100\n",
      "\n",
      "Epoch: 267 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 268 | Batch: 000 | Loss: 0.09532 | Correct: 098/100\n",
      "\n",
      "Epoch: 268 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 269 | Batch: 000 | Loss: 0.09512 | Correct: 098/100\n",
      "\n",
      "Epoch: 269 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 270 | Batch: 000 | Loss: 0.09493 | Correct: 098/100\n",
      "\n",
      "Epoch: 270 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 271 | Batch: 000 | Loss: 0.09474 | Correct: 098/100\n",
      "\n",
      "Epoch: 271 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 272 | Batch: 000 | Loss: 0.09455 | Correct: 098/100\n",
      "\n",
      "Epoch: 272 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 273 | Batch: 000 | Loss: 0.09437 | Correct: 098/100\n",
      "\n",
      "Epoch: 273 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 274 | Batch: 000 | Loss: 0.09419 | Correct: 098/100\n",
      "\n",
      "Epoch: 274 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 275 | Batch: 000 | Loss: 0.09400 | Correct: 098/100\n",
      "\n",
      "Epoch: 275 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 276 | Batch: 000 | Loss: 0.09383 | Correct: 098/100\n",
      "\n",
      "Epoch: 276 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 277 | Batch: 000 | Loss: 0.09365 | Correct: 098/100\n",
      "\n",
      "Epoch: 277 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 278 | Batch: 000 | Loss: 0.09348 | Correct: 098/100\n",
      "\n",
      "Epoch: 278 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 279 | Batch: 000 | Loss: 0.09331 | Correct: 099/100\n",
      "\n",
      "Epoch: 279 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 280 | Batch: 000 | Loss: 0.09314 | Correct: 098/100\n",
      "\n",
      "Epoch: 280 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 281 | Batch: 000 | Loss: 0.09297 | Correct: 099/100\n",
      "\n",
      "Epoch: 281 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 282 | Batch: 000 | Loss: 0.09279 | Correct: 098/100\n",
      "\n",
      "Epoch: 282 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 283 | Batch: 000 | Loss: 0.09262 | Correct: 098/100\n",
      "\n",
      "Epoch: 283 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 284 | Batch: 000 | Loss: 0.09246 | Correct: 099/100\n",
      "\n",
      "Epoch: 284 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 285 | Batch: 000 | Loss: 0.09230 | Correct: 098/100\n",
      "\n",
      "Epoch: 285 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 286 | Batch: 000 | Loss: 0.09213 | Correct: 099/100\n",
      "\n",
      "Epoch: 286 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 287 | Batch: 000 | Loss: 0.09197 | Correct: 099/100\n",
      "\n",
      "Epoch: 287 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 288 | Batch: 000 | Loss: 0.09182 | Correct: 098/100\n",
      "\n",
      "Epoch: 288 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 289 | Batch: 000 | Loss: 0.09166 | Correct: 099/100\n",
      "\n",
      "Epoch: 289 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 290 | Batch: 000 | Loss: 0.09150 | Correct: 098/100\n",
      "\n",
      "Epoch: 290 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 291 | Batch: 000 | Loss: 0.09135 | Correct: 098/100\n",
      "\n",
      "Epoch: 291 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 292 | Batch: 000 | Loss: 0.09120 | Correct: 099/100\n",
      "\n",
      "Epoch: 292 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 293 | Batch: 000 | Loss: 0.09105 | Correct: 098/100\n",
      "\n",
      "Epoch: 293 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 294 | Batch: 000 | Loss: 0.09090 | Correct: 099/100\n",
      "\n",
      "Epoch: 294 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 295 | Batch: 000 | Loss: 0.09075 | Correct: 099/100\n",
      "\n",
      "Epoch: 295 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 296 | Batch: 000 | Loss: 0.09061 | Correct: 099/100\n",
      "\n",
      "Epoch: 296 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 297 | Batch: 000 | Loss: 0.09046 | Correct: 099/100\n",
      "\n",
      "Epoch: 297 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 298 | Batch: 000 | Loss: 0.09031 | Correct: 099/100\n",
      "\n",
      "Epoch: 298 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 299 | Batch: 000 | Loss: 0.09017 | Correct: 099/100\n",
      "\n",
      "Epoch: 299 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 300 | Batch: 000 | Loss: 0.09003 | Correct: 099/100\n",
      "\n",
      "Epoch: 300 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 301 | Batch: 000 | Loss: 0.08989 | Correct: 099/100\n",
      "\n",
      "Epoch: 301 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 302 | Batch: 000 | Loss: 0.08975 | Correct: 099/100\n",
      "\n",
      "Epoch: 302 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 303 | Batch: 000 | Loss: 0.08961 | Correct: 099/100\n",
      "\n",
      "Epoch: 303 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 304 | Batch: 000 | Loss: 0.08948 | Correct: 099/100\n",
      "\n",
      "Epoch: 304 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 305 | Batch: 000 | Loss: 0.08934 | Correct: 099/100\n",
      "\n",
      "Epoch: 305 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 306 | Batch: 000 | Loss: 0.08921 | Correct: 099/100\n",
      "\n",
      "Epoch: 306 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 307 | Batch: 000 | Loss: 0.08908 | Correct: 099/100\n",
      "\n",
      "Epoch: 307 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 308 | Batch: 000 | Loss: 0.08894 | Correct: 099/100\n",
      "\n",
      "Epoch: 308 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 309 | Batch: 000 | Loss: 0.08881 | Correct: 099/100\n",
      "\n",
      "Epoch: 309 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 310 | Batch: 000 | Loss: 0.08868 | Correct: 099/100\n",
      "\n",
      "Epoch: 310 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 311 | Batch: 000 | Loss: 0.08856 | Correct: 099/100\n",
      "\n",
      "Epoch: 311 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 312 | Batch: 000 | Loss: 0.08843 | Correct: 099/100\n",
      "\n",
      "Epoch: 312 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 313 | Batch: 000 | Loss: 0.08830 | Correct: 099/100\n",
      "\n",
      "Epoch: 313 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 314 | Batch: 000 | Loss: 0.08818 | Correct: 099/100\n",
      "\n",
      "Epoch: 314 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 315 | Batch: 000 | Loss: 0.08805 | Correct: 099/100\n",
      "\n",
      "Epoch: 315 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 316 | Batch: 000 | Loss: 0.08793 | Correct: 099/100\n",
      "\n",
      "Epoch: 316 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 317 | Batch: 000 | Loss: 0.08781 | Correct: 099/100\n",
      "\n",
      "Epoch: 317 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 318 | Batch: 000 | Loss: 0.08769 | Correct: 099/100\n",
      "\n",
      "Epoch: 318 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 319 | Batch: 000 | Loss: 0.08757 | Correct: 099/100\n",
      "\n",
      "Epoch: 319 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 320 | Batch: 000 | Loss: 0.08745 | Correct: 099/100\n",
      "\n",
      "Epoch: 320 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 321 | Batch: 000 | Loss: 0.08733 | Correct: 099/100\n",
      "\n",
      "Epoch: 321 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 322 | Batch: 000 | Loss: 0.08721 | Correct: 099/100\n",
      "\n",
      "Epoch: 322 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 323 | Batch: 000 | Loss: 0.08709 | Correct: 099/100\n",
      "\n",
      "Epoch: 323 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 324 | Batch: 000 | Loss: 0.08698 | Correct: 099/100\n",
      "\n",
      "Epoch: 324 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 325 | Batch: 000 | Loss: 0.08686 | Correct: 099/100\n",
      "\n",
      "Epoch: 325 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 326 | Batch: 000 | Loss: 0.08675 | Correct: 099/100\n",
      "\n",
      "Epoch: 326 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 327 | Batch: 000 | Loss: 0.08664 | Correct: 099/100\n",
      "\n",
      "Epoch: 327 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 328 | Batch: 000 | Loss: 0.08653 | Correct: 099/100\n",
      "\n",
      "Epoch: 328 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 329 | Batch: 000 | Loss: 0.08641 | Correct: 099/100\n",
      "\n",
      "Epoch: 329 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 330 | Batch: 000 | Loss: 0.08630 | Correct: 099/100\n",
      "\n",
      "Epoch: 330 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 331 | Batch: 000 | Loss: 0.08619 | Correct: 099/100\n",
      "\n",
      "Epoch: 331 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 332 | Batch: 000 | Loss: 0.08608 | Correct: 099/100\n",
      "\n",
      "Epoch: 332 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 333 | Batch: 000 | Loss: 0.08598 | Correct: 099/100\n",
      "\n",
      "Epoch: 333 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 334 | Batch: 000 | Loss: 0.08587 | Correct: 099/100\n",
      "\n",
      "Epoch: 334 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 335 | Batch: 000 | Loss: 0.08576 | Correct: 099/100\n",
      "\n",
      "Epoch: 335 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 336 | Batch: 000 | Loss: 0.08566 | Correct: 099/100\n",
      "\n",
      "Epoch: 336 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 337 | Batch: 000 | Loss: 0.08555 | Correct: 099/100\n",
      "\n",
      "Epoch: 337 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 338 | Batch: 000 | Loss: 0.08545 | Correct: 099/100\n",
      "\n",
      "Epoch: 338 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 339 | Batch: 000 | Loss: 0.08534 | Correct: 099/100\n",
      "\n",
      "Epoch: 339 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 340 | Batch: 000 | Loss: 0.08524 | Correct: 099/100\n",
      "\n",
      "Epoch: 340 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 341 | Batch: 000 | Loss: 0.08514 | Correct: 099/100\n",
      "\n",
      "Epoch: 341 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 342 | Batch: 000 | Loss: 0.08504 | Correct: 099/100\n",
      "\n",
      "Epoch: 342 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 343 | Batch: 000 | Loss: 0.08494 | Correct: 099/100\n",
      "\n",
      "Epoch: 343 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 344 | Batch: 000 | Loss: 0.08484 | Correct: 099/100\n",
      "\n",
      "Epoch: 344 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 345 | Batch: 000 | Loss: 0.08474 | Correct: 099/100\n",
      "\n",
      "Epoch: 345 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 346 | Batch: 000 | Loss: 0.08464 | Correct: 099/100\n",
      "\n",
      "Epoch: 346 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 347 | Batch: 000 | Loss: 0.08454 | Correct: 099/100\n",
      "\n",
      "Epoch: 347 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 348 | Batch: 000 | Loss: 0.08444 | Correct: 099/100\n",
      "\n",
      "Epoch: 348 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 349 | Batch: 000 | Loss: 0.08434 | Correct: 099/100\n",
      "\n",
      "Epoch: 349 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 350 | Batch: 000 | Loss: 0.08425 | Correct: 099/100\n",
      "\n",
      "Epoch: 350 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 351 | Batch: 000 | Loss: 0.08415 | Correct: 099/100\n",
      "\n",
      "Epoch: 351 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 352 | Batch: 000 | Loss: 0.08406 | Correct: 099/100\n",
      "\n",
      "Epoch: 352 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 353 | Batch: 000 | Loss: 0.08396 | Correct: 099/100\n",
      "\n",
      "Epoch: 353 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 354 | Batch: 000 | Loss: 0.08387 | Correct: 099/100\n",
      "\n",
      "Epoch: 354 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 355 | Batch: 000 | Loss: 0.08378 | Correct: 099/100\n",
      "\n",
      "Epoch: 355 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 356 | Batch: 000 | Loss: 0.08368 | Correct: 099/100\n",
      "\n",
      "Epoch: 356 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 357 | Batch: 000 | Loss: 0.08359 | Correct: 099/100\n",
      "\n",
      "Epoch: 357 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 358 | Batch: 000 | Loss: 0.08350 | Correct: 099/100\n",
      "\n",
      "Epoch: 358 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 359 | Batch: 000 | Loss: 0.08341 | Correct: 099/100\n",
      "\n",
      "Epoch: 359 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 360 | Batch: 000 | Loss: 0.08332 | Correct: 099/100\n",
      "\n",
      "Epoch: 360 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 361 | Batch: 000 | Loss: 0.08323 | Correct: 099/100\n",
      "\n",
      "Epoch: 361 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 362 | Batch: 000 | Loss: 0.08314 | Correct: 099/100\n",
      "\n",
      "Epoch: 362 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 363 | Batch: 000 | Loss: 0.08305 | Correct: 099/100\n",
      "\n",
      "Epoch: 363 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 364 | Batch: 000 | Loss: 0.08296 | Correct: 099/100\n",
      "\n",
      "Epoch: 364 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 365 | Batch: 000 | Loss: 0.08287 | Correct: 099/100\n",
      "\n",
      "Epoch: 365 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 366 | Batch: 000 | Loss: 0.08279 | Correct: 099/100\n",
      "\n",
      "Epoch: 366 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 367 | Batch: 000 | Loss: 0.08270 | Correct: 099/100\n",
      "\n",
      "Epoch: 367 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 368 | Batch: 000 | Loss: 0.08261 | Correct: 099/100\n",
      "\n",
      "Epoch: 368 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 369 | Batch: 000 | Loss: 0.08253 | Correct: 099/100\n",
      "\n",
      "Epoch: 369 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 370 | Batch: 000 | Loss: 0.08244 | Correct: 099/100\n",
      "\n",
      "Epoch: 370 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 371 | Batch: 000 | Loss: 0.08236 | Correct: 099/100\n",
      "\n",
      "Epoch: 371 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 372 | Batch: 000 | Loss: 0.08227 | Correct: 099/100\n",
      "\n",
      "Epoch: 372 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 373 | Batch: 000 | Loss: 0.08219 | Correct: 099/100\n",
      "\n",
      "Epoch: 373 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 374 | Batch: 000 | Loss: 0.08211 | Correct: 099/100\n",
      "\n",
      "Epoch: 374 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 375 | Batch: 000 | Loss: 0.08203 | Correct: 099/100\n",
      "\n",
      "Epoch: 375 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 376 | Batch: 000 | Loss: 0.08194 | Correct: 099/100\n",
      "\n",
      "Epoch: 376 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 377 | Batch: 000 | Loss: 0.08186 | Correct: 099/100\n",
      "\n",
      "Epoch: 377 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 378 | Batch: 000 | Loss: 0.08178 | Correct: 099/100\n",
      "\n",
      "Epoch: 378 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 379 | Batch: 000 | Loss: 0.08170 | Correct: 099/100\n",
      "\n",
      "Epoch: 379 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 380 | Batch: 000 | Loss: 0.08162 | Correct: 099/100\n",
      "\n",
      "Epoch: 380 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 381 | Batch: 000 | Loss: 0.08154 | Correct: 099/100\n",
      "\n",
      "Epoch: 381 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 382 | Batch: 000 | Loss: 0.08146 | Correct: 099/100\n",
      "\n",
      "Epoch: 382 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 383 | Batch: 000 | Loss: 0.08138 | Correct: 099/100\n",
      "\n",
      "Epoch: 383 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 384 | Batch: 000 | Loss: 0.08130 | Correct: 099/100\n",
      "\n",
      "Epoch: 384 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 385 | Batch: 000 | Loss: 0.08122 | Correct: 099/100\n",
      "\n",
      "Epoch: 385 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 386 | Batch: 000 | Loss: 0.08114 | Correct: 099/100\n",
      "\n",
      "Epoch: 386 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 387 | Batch: 000 | Loss: 0.08107 | Correct: 099/100\n",
      "\n",
      "Epoch: 387 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 388 | Batch: 000 | Loss: 0.08099 | Correct: 099/100\n",
      "\n",
      "Epoch: 388 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 389 | Batch: 000 | Loss: 0.08091 | Correct: 099/100\n",
      "\n",
      "Epoch: 389 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 390 | Batch: 000 | Loss: 0.08084 | Correct: 099/100\n",
      "\n",
      "Epoch: 390 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 391 | Batch: 000 | Loss: 0.08076 | Correct: 099/100\n",
      "\n",
      "Epoch: 391 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 392 | Batch: 000 | Loss: 0.08069 | Correct: 099/100\n",
      "\n",
      "Epoch: 392 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 393 | Batch: 000 | Loss: 0.08061 | Correct: 099/100\n",
      "\n",
      "Epoch: 393 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 394 | Batch: 000 | Loss: 0.08054 | Correct: 099/100\n",
      "\n",
      "Epoch: 394 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 395 | Batch: 000 | Loss: 0.08046 | Correct: 099/100\n",
      "\n",
      "Epoch: 395 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 396 | Batch: 000 | Loss: 0.08039 | Correct: 099/100\n",
      "\n",
      "Epoch: 396 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 397 | Batch: 000 | Loss: 0.08031 | Correct: 099/100\n",
      "\n",
      "Epoch: 397 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 398 | Batch: 000 | Loss: 0.08024 | Correct: 099/100\n",
      "\n",
      "Epoch: 398 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 399 | Batch: 000 | Loss: 0.08017 | Correct: 099/100\n",
      "\n",
      "Epoch: 399 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 400 | Batch: 000 | Loss: 0.08010 | Correct: 099/100\n",
      "\n",
      "Epoch: 400 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 401 | Batch: 000 | Loss: 0.08002 | Correct: 099/100\n",
      "\n",
      "Epoch: 401 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 402 | Batch: 000 | Loss: 0.07995 | Correct: 099/100\n",
      "\n",
      "Epoch: 402 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 403 | Batch: 000 | Loss: 0.07988 | Correct: 099/100\n",
      "\n",
      "Epoch: 403 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 404 | Batch: 000 | Loss: 0.07981 | Correct: 099/100\n",
      "\n",
      "Epoch: 404 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 405 | Batch: 000 | Loss: 0.07974 | Correct: 099/100\n",
      "\n",
      "Epoch: 405 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 406 | Batch: 000 | Loss: 0.07967 | Correct: 099/100\n",
      "\n",
      "Epoch: 406 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 407 | Batch: 000 | Loss: 0.07960 | Correct: 099/100\n",
      "\n",
      "Epoch: 407 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 408 | Batch: 000 | Loss: 0.07953 | Correct: 099/100\n",
      "\n",
      "Epoch: 408 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 409 | Batch: 000 | Loss: 0.07946 | Correct: 099/100\n",
      "\n",
      "Epoch: 409 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 410 | Batch: 000 | Loss: 0.07939 | Correct: 099/100\n",
      "\n",
      "Epoch: 410 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 411 | Batch: 000 | Loss: 0.07933 | Correct: 099/100\n",
      "\n",
      "Epoch: 411 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 412 | Batch: 000 | Loss: 0.07926 | Correct: 099/100\n",
      "\n",
      "Epoch: 412 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 413 | Batch: 000 | Loss: 0.07919 | Correct: 099/100\n",
      "\n",
      "Epoch: 413 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 414 | Batch: 000 | Loss: 0.07913 | Correct: 099/100\n",
      "\n",
      "Epoch: 414 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 415 | Batch: 000 | Loss: 0.07907 | Correct: 099/100\n",
      "\n",
      "Epoch: 415 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 416 | Batch: 000 | Loss: 0.07901 | Correct: 099/100\n",
      "\n",
      "Epoch: 416 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 417 | Batch: 000 | Loss: 0.07896 | Correct: 099/100\n",
      "\n",
      "Epoch: 417 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 418 | Batch: 000 | Loss: 0.07888 | Correct: 099/100\n",
      "\n",
      "Epoch: 418 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 419 | Batch: 000 | Loss: 0.07879 | Correct: 099/100\n",
      "\n",
      "Epoch: 419 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 420 | Batch: 000 | Loss: 0.07873 | Correct: 099/100\n",
      "\n",
      "Epoch: 420 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 421 | Batch: 000 | Loss: 0.07868 | Correct: 099/100\n",
      "\n",
      "Epoch: 421 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 422 | Batch: 000 | Loss: 0.07861 | Correct: 099/100\n",
      "\n",
      "Epoch: 422 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 423 | Batch: 000 | Loss: 0.07853 | Correct: 099/100\n",
      "\n",
      "Epoch: 423 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 424 | Batch: 000 | Loss: 0.07848 | Correct: 099/100\n",
      "\n",
      "Epoch: 424 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 425 | Batch: 000 | Loss: 0.07842 | Correct: 099/100\n",
      "\n",
      "Epoch: 425 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 426 | Batch: 000 | Loss: 0.07834 | Correct: 099/100\n",
      "\n",
      "Epoch: 426 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 427 | Batch: 000 | Loss: 0.07828 | Correct: 099/100\n",
      "\n",
      "Epoch: 427 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 428 | Batch: 000 | Loss: 0.07823 | Correct: 099/100\n",
      "\n",
      "Epoch: 428 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 429 | Batch: 000 | Loss: 0.07816 | Correct: 099/100\n",
      "\n",
      "Epoch: 429 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 430 | Batch: 000 | Loss: 0.07810 | Correct: 099/100\n",
      "\n",
      "Epoch: 430 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 431 | Batch: 000 | Loss: 0.07804 | Correct: 099/100\n",
      "\n",
      "Epoch: 431 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 432 | Batch: 000 | Loss: 0.07797 | Correct: 099/100\n",
      "\n",
      "Epoch: 432 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 433 | Batch: 000 | Loss: 0.07791 | Correct: 099/100\n",
      "\n",
      "Epoch: 433 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 434 | Batch: 000 | Loss: 0.07786 | Correct: 099/100\n",
      "\n",
      "Epoch: 434 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 435 | Batch: 000 | Loss: 0.07779 | Correct: 099/100\n",
      "\n",
      "Epoch: 435 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 436 | Batch: 000 | Loss: 0.07773 | Correct: 099/100\n",
      "\n",
      "Epoch: 436 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 437 | Batch: 000 | Loss: 0.07767 | Correct: 099/100\n",
      "\n",
      "Epoch: 437 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 438 | Batch: 000 | Loss: 0.07761 | Correct: 099/100\n",
      "\n",
      "Epoch: 438 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 439 | Batch: 000 | Loss: 0.07755 | Correct: 099/100\n",
      "\n",
      "Epoch: 439 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 440 | Batch: 000 | Loss: 0.07750 | Correct: 099/100\n",
      "\n",
      "Epoch: 440 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 441 | Batch: 000 | Loss: 0.07744 | Correct: 099/100\n",
      "\n",
      "Epoch: 441 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 442 | Batch: 000 | Loss: 0.07738 | Correct: 099/100\n",
      "\n",
      "Epoch: 442 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 443 | Batch: 000 | Loss: 0.07732 | Correct: 099/100\n",
      "\n",
      "Epoch: 443 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 444 | Batch: 000 | Loss: 0.07726 | Correct: 099/100\n",
      "\n",
      "Epoch: 444 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 445 | Batch: 000 | Loss: 0.07720 | Correct: 099/100\n",
      "\n",
      "Epoch: 445 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 446 | Batch: 000 | Loss: 0.07715 | Correct: 099/100\n",
      "\n",
      "Epoch: 446 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 447 | Batch: 000 | Loss: 0.07709 | Correct: 099/100\n",
      "\n",
      "Epoch: 447 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 448 | Batch: 000 | Loss: 0.07703 | Correct: 099/100\n",
      "\n",
      "Epoch: 448 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 449 | Batch: 000 | Loss: 0.07698 | Correct: 099/100\n",
      "\n",
      "Epoch: 449 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 450 | Batch: 000 | Loss: 0.07692 | Correct: 099/100\n",
      "\n",
      "Epoch: 450 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 451 | Batch: 000 | Loss: 0.07686 | Correct: 099/100\n",
      "\n",
      "Epoch: 451 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 452 | Batch: 000 | Loss: 0.07681 | Correct: 099/100\n",
      "\n",
      "Epoch: 452 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 453 | Batch: 000 | Loss: 0.07675 | Correct: 099/100\n",
      "\n",
      "Epoch: 453 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 454 | Batch: 000 | Loss: 0.07670 | Correct: 099/100\n",
      "\n",
      "Epoch: 454 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 455 | Batch: 000 | Loss: 0.07664 | Correct: 099/100\n",
      "\n",
      "Epoch: 455 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 456 | Batch: 000 | Loss: 0.07659 | Correct: 099/100\n",
      "\n",
      "Epoch: 456 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 457 | Batch: 000 | Loss: 0.07653 | Correct: 099/100\n",
      "\n",
      "Epoch: 457 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 458 | Batch: 000 | Loss: 0.07648 | Correct: 099/100\n",
      "\n",
      "Epoch: 458 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 459 | Batch: 000 | Loss: 0.07642 | Correct: 099/100\n",
      "\n",
      "Epoch: 459 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 460 | Batch: 000 | Loss: 0.07637 | Correct: 099/100\n",
      "\n",
      "Epoch: 460 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 461 | Batch: 000 | Loss: 0.07632 | Correct: 099/100\n",
      "\n",
      "Epoch: 461 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 462 | Batch: 000 | Loss: 0.07626 | Correct: 099/100\n",
      "\n",
      "Epoch: 462 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 463 | Batch: 000 | Loss: 0.07621 | Correct: 099/100\n",
      "\n",
      "Epoch: 463 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 464 | Batch: 000 | Loss: 0.07616 | Correct: 099/100\n",
      "\n",
      "Epoch: 464 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 465 | Batch: 000 | Loss: 0.07611 | Correct: 099/100\n",
      "\n",
      "Epoch: 465 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 466 | Batch: 000 | Loss: 0.07605 | Correct: 099/100\n",
      "\n",
      "Epoch: 466 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 467 | Batch: 000 | Loss: 0.07600 | Correct: 099/100\n",
      "\n",
      "Epoch: 467 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 468 | Batch: 000 | Loss: 0.07595 | Correct: 099/100\n",
      "\n",
      "Epoch: 468 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 469 | Batch: 000 | Loss: 0.07590 | Correct: 099/100\n",
      "\n",
      "Epoch: 469 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 470 | Batch: 000 | Loss: 0.07585 | Correct: 099/100\n",
      "\n",
      "Epoch: 470 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 471 | Batch: 000 | Loss: 0.07579 | Correct: 099/100\n",
      "\n",
      "Epoch: 471 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 472 | Batch: 000 | Loss: 0.07574 | Correct: 099/100\n",
      "\n",
      "Epoch: 472 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 473 | Batch: 000 | Loss: 0.07569 | Correct: 099/100\n",
      "\n",
      "Epoch: 473 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 474 | Batch: 000 | Loss: 0.07564 | Correct: 099/100\n",
      "\n",
      "Epoch: 474 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 475 | Batch: 000 | Loss: 0.07559 | Correct: 099/100\n",
      "\n",
      "Epoch: 475 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 476 | Batch: 000 | Loss: 0.07554 | Correct: 099/100\n",
      "\n",
      "Epoch: 476 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 477 | Batch: 000 | Loss: 0.07549 | Correct: 099/100\n",
      "\n",
      "Epoch: 477 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 478 | Batch: 000 | Loss: 0.07544 | Correct: 099/100\n",
      "\n",
      "Epoch: 478 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 479 | Batch: 000 | Loss: 0.07539 | Correct: 099/100\n",
      "\n",
      "Epoch: 479 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 480 | Batch: 000 | Loss: 0.07534 | Correct: 099/100\n",
      "\n",
      "Epoch: 480 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 481 | Batch: 000 | Loss: 0.07529 | Correct: 099/100\n",
      "\n",
      "Epoch: 481 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 482 | Batch: 000 | Loss: 0.07524 | Correct: 099/100\n",
      "\n",
      "Epoch: 482 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 483 | Batch: 000 | Loss: 0.07520 | Correct: 099/100\n",
      "\n",
      "Epoch: 483 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 484 | Batch: 000 | Loss: 0.07515 | Correct: 099/100\n",
      "\n",
      "Epoch: 484 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 485 | Batch: 000 | Loss: 0.07510 | Correct: 099/100\n",
      "\n",
      "Epoch: 485 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 486 | Batch: 000 | Loss: 0.07505 | Correct: 099/100\n",
      "\n",
      "Epoch: 486 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 487 | Batch: 000 | Loss: 0.07500 | Correct: 099/100\n",
      "\n",
      "Epoch: 487 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 488 | Batch: 000 | Loss: 0.07496 | Correct: 099/100\n",
      "\n",
      "Epoch: 488 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 489 | Batch: 000 | Loss: 0.07491 | Correct: 099/100\n",
      "\n",
      "Epoch: 489 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 490 | Batch: 000 | Loss: 0.07486 | Correct: 099/100\n",
      "\n",
      "Epoch: 490 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 491 | Batch: 000 | Loss: 0.07481 | Correct: 099/100\n",
      "\n",
      "Epoch: 491 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 492 | Batch: 000 | Loss: 0.07477 | Correct: 099/100\n",
      "\n",
      "Epoch: 492 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 493 | Batch: 000 | Loss: 0.07472 | Correct: 099/100\n",
      "\n",
      "Epoch: 493 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 494 | Batch: 000 | Loss: 0.07467 | Correct: 099/100\n",
      "\n",
      "Epoch: 494 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 495 | Batch: 000 | Loss: 0.07463 | Correct: 099/100\n",
      "\n",
      "Epoch: 495 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 496 | Batch: 000 | Loss: 0.07458 | Correct: 099/100\n",
      "\n",
      "Epoch: 496 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 497 | Batch: 000 | Loss: 0.07454 | Correct: 099/100\n",
      "\n",
      "Epoch: 497 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 498 | Batch: 000 | Loss: 0.07449 | Correct: 099/100\n",
      "\n",
      "Epoch: 498 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 499 | Batch: 000 | Loss: 0.07444 | Correct: 099/100\n",
      "\n",
      "Epoch: 499 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 500 | Batch: 000 | Loss: 0.07440 | Correct: 099/100\n",
      "\n",
      "Epoch: 500 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 501 | Batch: 000 | Loss: 0.07435 | Correct: 099/100\n",
      "\n",
      "Epoch: 501 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 502 | Batch: 000 | Loss: 0.07431 | Correct: 099/100\n",
      "\n",
      "Epoch: 502 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 503 | Batch: 000 | Loss: 0.07426 | Correct: 099/100\n",
      "\n",
      "Epoch: 503 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 504 | Batch: 000 | Loss: 0.07422 | Correct: 099/100\n",
      "\n",
      "Epoch: 504 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 505 | Batch: 000 | Loss: 0.07418 | Correct: 099/100\n",
      "\n",
      "Epoch: 505 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 506 | Batch: 000 | Loss: 0.07413 | Correct: 099/100\n",
      "\n",
      "Epoch: 506 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 507 | Batch: 000 | Loss: 0.07409 | Correct: 099/100\n",
      "\n",
      "Epoch: 507 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 508 | Batch: 000 | Loss: 0.07404 | Correct: 099/100\n",
      "\n",
      "Epoch: 508 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 509 | Batch: 000 | Loss: 0.07400 | Correct: 099/100\n",
      "\n",
      "Epoch: 509 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 510 | Batch: 000 | Loss: 0.07396 | Correct: 099/100\n",
      "\n",
      "Epoch: 510 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 511 | Batch: 000 | Loss: 0.07391 | Correct: 099/100\n",
      "\n",
      "Epoch: 511 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 512 | Batch: 000 | Loss: 0.07387 | Correct: 099/100\n",
      "\n",
      "Epoch: 512 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 513 | Batch: 000 | Loss: 0.07383 | Correct: 099/100\n",
      "\n",
      "Epoch: 513 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 514 | Batch: 000 | Loss: 0.07378 | Correct: 099/100\n",
      "\n",
      "Epoch: 514 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 515 | Batch: 000 | Loss: 0.07374 | Correct: 099/100\n",
      "\n",
      "Epoch: 515 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 516 | Batch: 000 | Loss: 0.07370 | Correct: 099/100\n",
      "\n",
      "Epoch: 516 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 517 | Batch: 000 | Loss: 0.07366 | Correct: 099/100\n",
      "\n",
      "Epoch: 517 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 518 | Batch: 000 | Loss: 0.07362 | Correct: 099/100\n",
      "\n",
      "Epoch: 518 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 519 | Batch: 000 | Loss: 0.07357 | Correct: 099/100\n",
      "\n",
      "Epoch: 519 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 520 | Batch: 000 | Loss: 0.07353 | Correct: 099/100\n",
      "\n",
      "Epoch: 520 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 521 | Batch: 000 | Loss: 0.07349 | Correct: 099/100\n",
      "\n",
      "Epoch: 521 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 522 | Batch: 000 | Loss: 0.07345 | Correct: 099/100\n",
      "\n",
      "Epoch: 522 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 523 | Batch: 000 | Loss: 0.07341 | Correct: 099/100\n",
      "\n",
      "Epoch: 523 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 524 | Batch: 000 | Loss: 0.07337 | Correct: 099/100\n",
      "\n",
      "Epoch: 524 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 525 | Batch: 000 | Loss: 0.07333 | Correct: 099/100\n",
      "\n",
      "Epoch: 525 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 526 | Batch: 000 | Loss: 0.07328 | Correct: 099/100\n",
      "\n",
      "Epoch: 526 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 527 | Batch: 000 | Loss: 0.07324 | Correct: 099/100\n",
      "\n",
      "Epoch: 527 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 528 | Batch: 000 | Loss: 0.07320 | Correct: 099/100\n",
      "\n",
      "Epoch: 528 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 529 | Batch: 000 | Loss: 0.07316 | Correct: 099/100\n",
      "\n",
      "Epoch: 529 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 530 | Batch: 000 | Loss: 0.07312 | Correct: 099/100\n",
      "\n",
      "Epoch: 530 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 531 | Batch: 000 | Loss: 0.07308 | Correct: 099/100\n",
      "\n",
      "Epoch: 531 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 532 | Batch: 000 | Loss: 0.07304 | Correct: 099/100\n",
      "\n",
      "Epoch: 532 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 533 | Batch: 000 | Loss: 0.07300 | Correct: 099/100\n",
      "\n",
      "Epoch: 533 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 534 | Batch: 000 | Loss: 0.07297 | Correct: 099/100\n",
      "\n",
      "Epoch: 534 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 535 | Batch: 000 | Loss: 0.07293 | Correct: 099/100\n",
      "\n",
      "Epoch: 535 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 536 | Batch: 000 | Loss: 0.07289 | Correct: 099/100\n",
      "\n",
      "Epoch: 536 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 537 | Batch: 000 | Loss: 0.07285 | Correct: 099/100\n",
      "\n",
      "Epoch: 537 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 538 | Batch: 000 | Loss: 0.07281 | Correct: 099/100\n",
      "\n",
      "Epoch: 538 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 539 | Batch: 000 | Loss: 0.07277 | Correct: 099/100\n",
      "\n",
      "Epoch: 539 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 540 | Batch: 000 | Loss: 0.07273 | Correct: 099/100\n",
      "\n",
      "Epoch: 540 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 541 | Batch: 000 | Loss: 0.07269 | Correct: 099/100\n",
      "\n",
      "Epoch: 541 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 542 | Batch: 000 | Loss: 0.07266 | Correct: 099/100\n",
      "\n",
      "Epoch: 542 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 543 | Batch: 000 | Loss: 0.07262 | Correct: 099/100\n",
      "\n",
      "Epoch: 543 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 544 | Batch: 000 | Loss: 0.07258 | Correct: 099/100\n",
      "\n",
      "Epoch: 544 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 545 | Batch: 000 | Loss: 0.07254 | Correct: 099/100\n",
      "\n",
      "Epoch: 545 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 546 | Batch: 000 | Loss: 0.07250 | Correct: 099/100\n",
      "\n",
      "Epoch: 546 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 547 | Batch: 000 | Loss: 0.07247 | Correct: 099/100\n",
      "\n",
      "Epoch: 547 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 548 | Batch: 000 | Loss: 0.07243 | Correct: 099/100\n",
      "\n",
      "Epoch: 548 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 549 | Batch: 000 | Loss: 0.07239 | Correct: 099/100\n",
      "\n",
      "Epoch: 549 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 550 | Batch: 000 | Loss: 0.07236 | Correct: 099/100\n",
      "\n",
      "Epoch: 550 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 551 | Batch: 000 | Loss: 0.07232 | Correct: 099/100\n",
      "\n",
      "Epoch: 551 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 552 | Batch: 000 | Loss: 0.07228 | Correct: 099/100\n",
      "\n",
      "Epoch: 552 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 553 | Batch: 000 | Loss: 0.07225 | Correct: 099/100\n",
      "\n",
      "Epoch: 553 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 554 | Batch: 000 | Loss: 0.07221 | Correct: 099/100\n",
      "\n",
      "Epoch: 554 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 555 | Batch: 000 | Loss: 0.07217 | Correct: 099/100\n",
      "\n",
      "Epoch: 555 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 556 | Batch: 000 | Loss: 0.07214 | Correct: 099/100\n",
      "\n",
      "Epoch: 556 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 557 | Batch: 000 | Loss: 0.07210 | Correct: 099/100\n",
      "\n",
      "Epoch: 557 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 558 | Batch: 000 | Loss: 0.07207 | Correct: 099/100\n",
      "\n",
      "Epoch: 558 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 559 | Batch: 000 | Loss: 0.07203 | Correct: 099/100\n",
      "\n",
      "Epoch: 559 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 560 | Batch: 000 | Loss: 0.07199 | Correct: 099/100\n",
      "\n",
      "Epoch: 560 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 561 | Batch: 000 | Loss: 0.07196 | Correct: 099/100\n",
      "\n",
      "Epoch: 561 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 562 | Batch: 000 | Loss: 0.07192 | Correct: 099/100\n",
      "\n",
      "Epoch: 562 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 563 | Batch: 000 | Loss: 0.07189 | Correct: 099/100\n",
      "\n",
      "Epoch: 563 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 564 | Batch: 000 | Loss: 0.07185 | Correct: 099/100\n",
      "\n",
      "Epoch: 564 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 565 | Batch: 000 | Loss: 0.07182 | Correct: 099/100\n",
      "\n",
      "Epoch: 565 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 566 | Batch: 000 | Loss: 0.07178 | Correct: 099/100\n",
      "\n",
      "Epoch: 566 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 567 | Batch: 000 | Loss: 0.07175 | Correct: 099/100\n",
      "\n",
      "Epoch: 567 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 568 | Batch: 000 | Loss: 0.07172 | Correct: 099/100\n",
      "\n",
      "Epoch: 568 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 569 | Batch: 000 | Loss: 0.07168 | Correct: 099/100\n",
      "\n",
      "Epoch: 569 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 570 | Batch: 000 | Loss: 0.07165 | Correct: 099/100\n",
      "\n",
      "Epoch: 570 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 571 | Batch: 000 | Loss: 0.07161 | Correct: 099/100\n",
      "\n",
      "Epoch: 571 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 572 | Batch: 000 | Loss: 0.07158 | Correct: 099/100\n",
      "\n",
      "Epoch: 572 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 573 | Batch: 000 | Loss: 0.07155 | Correct: 099/100\n",
      "\n",
      "Epoch: 573 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 574 | Batch: 000 | Loss: 0.07151 | Correct: 099/100\n",
      "\n",
      "Epoch: 574 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 575 | Batch: 000 | Loss: 0.07148 | Correct: 099/100\n",
      "\n",
      "Epoch: 575 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 576 | Batch: 000 | Loss: 0.07145 | Correct: 099/100\n",
      "\n",
      "Epoch: 576 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 577 | Batch: 000 | Loss: 0.07141 | Correct: 099/100\n",
      "\n",
      "Epoch: 577 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 578 | Batch: 000 | Loss: 0.07138 | Correct: 099/100\n",
      "\n",
      "Epoch: 578 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 579 | Batch: 000 | Loss: 0.07135 | Correct: 099/100\n",
      "\n",
      "Epoch: 579 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 580 | Batch: 000 | Loss: 0.07133 | Correct: 099/100\n",
      "\n",
      "Epoch: 580 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 581 | Batch: 000 | Loss: 0.07131 | Correct: 099/100\n",
      "\n",
      "Epoch: 581 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 582 | Batch: 000 | Loss: 0.07129 | Correct: 099/100\n",
      "\n",
      "Epoch: 582 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 583 | Batch: 000 | Loss: 0.07127 | Correct: 099/100\n",
      "\n",
      "Epoch: 583 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 584 | Batch: 000 | Loss: 0.07121 | Correct: 099/100\n",
      "\n",
      "Epoch: 584 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 585 | Batch: 000 | Loss: 0.07116 | Correct: 099/100\n",
      "\n",
      "Epoch: 585 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 586 | Batch: 000 | Loss: 0.07113 | Correct: 099/100\n",
      "\n",
      "Epoch: 586 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 587 | Batch: 000 | Loss: 0.07111 | Correct: 099/100\n",
      "\n",
      "Epoch: 587 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 588 | Batch: 000 | Loss: 0.07108 | Correct: 099/100\n",
      "\n",
      "Epoch: 588 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 589 | Batch: 000 | Loss: 0.07103 | Correct: 099/100\n",
      "\n",
      "Epoch: 589 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 590 | Batch: 000 | Loss: 0.07100 | Correct: 099/100\n",
      "\n",
      "Epoch: 590 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 591 | Batch: 000 | Loss: 0.07098 | Correct: 099/100\n",
      "\n",
      "Epoch: 591 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 592 | Batch: 000 | Loss: 0.07095 | Correct: 099/100\n",
      "\n",
      "Epoch: 592 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 593 | Batch: 000 | Loss: 0.07091 | Correct: 099/100\n",
      "\n",
      "Epoch: 593 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 594 | Batch: 000 | Loss: 0.07087 | Correct: 099/100\n",
      "\n",
      "Epoch: 594 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 595 | Batch: 000 | Loss: 0.07085 | Correct: 099/100\n",
      "\n",
      "Epoch: 595 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 596 | Batch: 000 | Loss: 0.07082 | Correct: 099/100\n",
      "\n",
      "Epoch: 596 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 597 | Batch: 000 | Loss: 0.07078 | Correct: 099/100\n",
      "\n",
      "Epoch: 597 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 598 | Batch: 000 | Loss: 0.07075 | Correct: 099/100\n",
      "\n",
      "Epoch: 598 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 599 | Batch: 000 | Loss: 0.07073 | Correct: 099/100\n",
      "\n",
      "Epoch: 599 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 600 | Batch: 000 | Loss: 0.07069 | Correct: 099/100\n",
      "\n",
      "Epoch: 600 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 601 | Batch: 000 | Loss: 0.07066 | Correct: 099/100\n",
      "\n",
      "Epoch: 601 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 602 | Batch: 000 | Loss: 0.07063 | Correct: 099/100\n",
      "\n",
      "Epoch: 602 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 603 | Batch: 000 | Loss: 0.07060 | Correct: 099/100\n",
      "\n",
      "Epoch: 603 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 604 | Batch: 000 | Loss: 0.07057 | Correct: 099/100\n",
      "\n",
      "Epoch: 604 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 605 | Batch: 000 | Loss: 0.07054 | Correct: 099/100\n",
      "\n",
      "Epoch: 605 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 606 | Batch: 000 | Loss: 0.07051 | Correct: 099/100\n",
      "\n",
      "Epoch: 606 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 607 | Batch: 000 | Loss: 0.07049 | Correct: 099/100\n",
      "\n",
      "Epoch: 607 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 608 | Batch: 000 | Loss: 0.07045 | Correct: 099/100\n",
      "\n",
      "Epoch: 608 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 609 | Batch: 000 | Loss: 0.07042 | Correct: 099/100\n",
      "\n",
      "Epoch: 609 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 610 | Batch: 000 | Loss: 0.07040 | Correct: 099/100\n",
      "\n",
      "Epoch: 610 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 611 | Batch: 000 | Loss: 0.07037 | Correct: 099/100\n",
      "\n",
      "Epoch: 611 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 612 | Batch: 000 | Loss: 0.07034 | Correct: 099/100\n",
      "\n",
      "Epoch: 612 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 613 | Batch: 000 | Loss: 0.07031 | Correct: 099/100\n",
      "\n",
      "Epoch: 613 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 614 | Batch: 000 | Loss: 0.07028 | Correct: 099/100\n",
      "\n",
      "Epoch: 614 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 615 | Batch: 000 | Loss: 0.07025 | Correct: 099/100\n",
      "\n",
      "Epoch: 615 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 616 | Batch: 000 | Loss: 0.07022 | Correct: 099/100\n",
      "\n",
      "Epoch: 616 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 617 | Batch: 000 | Loss: 0.07020 | Correct: 099/100\n",
      "\n",
      "Epoch: 617 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 618 | Batch: 000 | Loss: 0.07017 | Correct: 099/100\n",
      "\n",
      "Epoch: 618 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 619 | Batch: 000 | Loss: 0.07014 | Correct: 099/100\n",
      "\n",
      "Epoch: 619 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 620 | Batch: 000 | Loss: 0.07011 | Correct: 099/100\n",
      "\n",
      "Epoch: 620 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 621 | Batch: 000 | Loss: 0.07009 | Correct: 099/100\n",
      "\n",
      "Epoch: 621 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 622 | Batch: 000 | Loss: 0.07006 | Correct: 099/100\n",
      "\n",
      "Epoch: 622 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 623 | Batch: 000 | Loss: 0.07003 | Correct: 099/100\n",
      "\n",
      "Epoch: 623 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 624 | Batch: 000 | Loss: 0.07000 | Correct: 099/100\n",
      "\n",
      "Epoch: 624 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 625 | Batch: 000 | Loss: 0.06998 | Correct: 099/100\n",
      "\n",
      "Epoch: 625 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 626 | Batch: 000 | Loss: 0.06995 | Correct: 099/100\n",
      "\n",
      "Epoch: 626 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 627 | Batch: 000 | Loss: 0.06992 | Correct: 099/100\n",
      "\n",
      "Epoch: 627 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 628 | Batch: 000 | Loss: 0.06990 | Correct: 099/100\n",
      "\n",
      "Epoch: 628 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 629 | Batch: 000 | Loss: 0.06987 | Correct: 099/100\n",
      "\n",
      "Epoch: 629 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 630 | Batch: 000 | Loss: 0.06984 | Correct: 099/100\n",
      "\n",
      "Epoch: 630 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 631 | Batch: 000 | Loss: 0.06982 | Correct: 099/100\n",
      "\n",
      "Epoch: 631 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 632 | Batch: 000 | Loss: 0.06979 | Correct: 099/100\n",
      "\n",
      "Epoch: 632 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 633 | Batch: 000 | Loss: 0.06976 | Correct: 099/100\n",
      "\n",
      "Epoch: 633 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 634 | Batch: 000 | Loss: 0.06974 | Correct: 099/100\n",
      "\n",
      "Epoch: 634 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 635 | Batch: 000 | Loss: 0.06971 | Correct: 099/100\n",
      "\n",
      "Epoch: 635 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 636 | Batch: 000 | Loss: 0.06969 | Correct: 099/100\n",
      "\n",
      "Epoch: 636 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 637 | Batch: 000 | Loss: 0.06966 | Correct: 099/100\n",
      "\n",
      "Epoch: 637 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 638 | Batch: 000 | Loss: 0.06963 | Correct: 099/100\n",
      "\n",
      "Epoch: 638 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 639 | Batch: 000 | Loss: 0.06961 | Correct: 099/100\n",
      "\n",
      "Epoch: 639 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 640 | Batch: 000 | Loss: 0.06958 | Correct: 099/100\n",
      "\n",
      "Epoch: 640 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 641 | Batch: 000 | Loss: 0.06956 | Correct: 099/100\n",
      "\n",
      "Epoch: 641 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 642 | Batch: 000 | Loss: 0.06953 | Correct: 099/100\n",
      "\n",
      "Epoch: 642 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 643 | Batch: 000 | Loss: 0.06951 | Correct: 099/100\n",
      "\n",
      "Epoch: 643 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 644 | Batch: 000 | Loss: 0.06948 | Correct: 099/100\n",
      "\n",
      "Epoch: 644 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 645 | Batch: 000 | Loss: 0.06946 | Correct: 099/100\n",
      "\n",
      "Epoch: 645 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 646 | Batch: 000 | Loss: 0.06943 | Correct: 099/100\n",
      "\n",
      "Epoch: 646 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 647 | Batch: 000 | Loss: 0.06941 | Correct: 099/100\n",
      "\n",
      "Epoch: 647 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 648 | Batch: 000 | Loss: 0.06938 | Correct: 099/100\n",
      "\n",
      "Epoch: 648 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 649 | Batch: 000 | Loss: 0.06936 | Correct: 099/100\n",
      "\n",
      "Epoch: 649 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 650 | Batch: 000 | Loss: 0.06933 | Correct: 099/100\n",
      "\n",
      "Epoch: 650 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 651 | Batch: 000 | Loss: 0.06931 | Correct: 099/100\n",
      "\n",
      "Epoch: 651 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 652 | Batch: 000 | Loss: 0.06928 | Correct: 099/100\n",
      "\n",
      "Epoch: 652 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 653 | Batch: 000 | Loss: 0.06926 | Correct: 099/100\n",
      "\n",
      "Epoch: 653 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 654 | Batch: 000 | Loss: 0.06923 | Correct: 099/100\n",
      "\n",
      "Epoch: 654 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 655 | Batch: 000 | Loss: 0.06921 | Correct: 099/100\n",
      "\n",
      "Epoch: 655 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 656 | Batch: 000 | Loss: 0.06919 | Correct: 099/100\n",
      "\n",
      "Epoch: 656 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 657 | Batch: 000 | Loss: 0.06916 | Correct: 099/100\n",
      "\n",
      "Epoch: 657 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 658 | Batch: 000 | Loss: 0.06914 | Correct: 099/100\n",
      "\n",
      "Epoch: 658 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 659 | Batch: 000 | Loss: 0.06912 | Correct: 099/100\n",
      "\n",
      "Epoch: 659 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 660 | Batch: 000 | Loss: 0.06909 | Correct: 099/100\n",
      "\n",
      "Epoch: 660 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 661 | Batch: 000 | Loss: 0.06907 | Correct: 099/100\n",
      "\n",
      "Epoch: 661 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 662 | Batch: 000 | Loss: 0.06904 | Correct: 099/100\n",
      "\n",
      "Epoch: 662 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 663 | Batch: 000 | Loss: 0.06902 | Correct: 099/100\n",
      "\n",
      "Epoch: 663 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 664 | Batch: 000 | Loss: 0.06900 | Correct: 099/100\n",
      "\n",
      "Epoch: 664 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 665 | Batch: 000 | Loss: 0.06897 | Correct: 099/100\n",
      "\n",
      "Epoch: 665 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 666 | Batch: 000 | Loss: 0.06895 | Correct: 099/100\n",
      "\n",
      "Epoch: 666 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 667 | Batch: 000 | Loss: 0.06893 | Correct: 099/100\n",
      "\n",
      "Epoch: 667 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 668 | Batch: 000 | Loss: 0.06891 | Correct: 099/100\n",
      "\n",
      "Epoch: 668 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 669 | Batch: 000 | Loss: 0.06888 | Correct: 099/100\n",
      "\n",
      "Epoch: 669 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 670 | Batch: 000 | Loss: 0.06886 | Correct: 099/100\n",
      "\n",
      "Epoch: 670 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 671 | Batch: 000 | Loss: 0.06884 | Correct: 099/100\n",
      "\n",
      "Epoch: 671 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 672 | Batch: 000 | Loss: 0.06882 | Correct: 099/100\n",
      "\n",
      "Epoch: 672 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 673 | Batch: 000 | Loss: 0.06879 | Correct: 099/100\n",
      "\n",
      "Epoch: 673 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 674 | Batch: 000 | Loss: 0.06877 | Correct: 099/100\n",
      "\n",
      "Epoch: 674 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 675 | Batch: 000 | Loss: 0.06875 | Correct: 099/100\n",
      "\n",
      "Epoch: 675 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 676 | Batch: 000 | Loss: 0.06873 | Correct: 099/100\n",
      "\n",
      "Epoch: 676 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 677 | Batch: 000 | Loss: 0.06870 | Correct: 099/100\n",
      "\n",
      "Epoch: 677 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 678 | Batch: 000 | Loss: 0.06868 | Correct: 099/100\n",
      "\n",
      "Epoch: 678 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 679 | Batch: 000 | Loss: 0.06866 | Correct: 099/100\n",
      "\n",
      "Epoch: 679 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 680 | Batch: 000 | Loss: 0.06864 | Correct: 099/100\n",
      "\n",
      "Epoch: 680 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 681 | Batch: 000 | Loss: 0.06862 | Correct: 099/100\n",
      "\n",
      "Epoch: 681 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 682 | Batch: 000 | Loss: 0.06859 | Correct: 099/100\n",
      "\n",
      "Epoch: 682 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 683 | Batch: 000 | Loss: 0.06857 | Correct: 099/100\n",
      "\n",
      "Epoch: 683 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 684 | Batch: 000 | Loss: 0.06855 | Correct: 099/100\n",
      "\n",
      "Epoch: 684 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 685 | Batch: 000 | Loss: 0.06853 | Correct: 099/100\n",
      "\n",
      "Epoch: 685 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 686 | Batch: 000 | Loss: 0.06851 | Correct: 099/100\n",
      "\n",
      "Epoch: 686 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 687 | Batch: 000 | Loss: 0.06849 | Correct: 099/100\n",
      "\n",
      "Epoch: 687 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 688 | Batch: 000 | Loss: 0.06847 | Correct: 099/100\n",
      "\n",
      "Epoch: 688 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 689 | Batch: 000 | Loss: 0.06845 | Correct: 099/100\n",
      "\n",
      "Epoch: 689 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 690 | Batch: 000 | Loss: 0.06842 | Correct: 099/100\n",
      "\n",
      "Epoch: 690 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 691 | Batch: 000 | Loss: 0.06840 | Correct: 099/100\n",
      "\n",
      "Epoch: 691 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 692 | Batch: 000 | Loss: 0.06838 | Correct: 099/100\n",
      "\n",
      "Epoch: 692 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 693 | Batch: 000 | Loss: 0.06836 | Correct: 099/100\n",
      "\n",
      "Epoch: 693 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 694 | Batch: 000 | Loss: 0.06834 | Correct: 099/100\n",
      "\n",
      "Epoch: 694 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 695 | Batch: 000 | Loss: 0.06832 | Correct: 099/100\n",
      "\n",
      "Epoch: 695 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 696 | Batch: 000 | Loss: 0.06830 | Correct: 099/100\n",
      "\n",
      "Epoch: 696 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 697 | Batch: 000 | Loss: 0.06828 | Correct: 099/100\n",
      "\n",
      "Epoch: 697 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 698 | Batch: 000 | Loss: 0.06826 | Correct: 099/100\n",
      "\n",
      "Epoch: 698 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 699 | Batch: 000 | Loss: 0.06824 | Correct: 099/100\n",
      "\n",
      "Epoch: 699 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 700 | Batch: 000 | Loss: 0.06822 | Correct: 099/100\n",
      "\n",
      "Epoch: 700 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 701 | Batch: 000 | Loss: 0.06820 | Correct: 099/100\n",
      "\n",
      "Epoch: 701 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 702 | Batch: 000 | Loss: 0.06818 | Correct: 099/100\n",
      "\n",
      "Epoch: 702 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 703 | Batch: 000 | Loss: 0.06816 | Correct: 099/100\n",
      "\n",
      "Epoch: 703 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 704 | Batch: 000 | Loss: 0.06814 | Correct: 099/100\n",
      "\n",
      "Epoch: 704 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 705 | Batch: 000 | Loss: 0.06812 | Correct: 099/100\n",
      "\n",
      "Epoch: 705 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 706 | Batch: 000 | Loss: 0.06810 | Correct: 099/100\n",
      "\n",
      "Epoch: 706 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 707 | Batch: 000 | Loss: 0.06808 | Correct: 099/100\n",
      "\n",
      "Epoch: 707 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 708 | Batch: 000 | Loss: 0.06806 | Correct: 099/100\n",
      "\n",
      "Epoch: 708 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 709 | Batch: 000 | Loss: 0.06804 | Correct: 099/100\n",
      "\n",
      "Epoch: 709 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 710 | Batch: 000 | Loss: 0.06802 | Correct: 099/100\n",
      "\n",
      "Epoch: 710 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 711 | Batch: 000 | Loss: 0.06800 | Correct: 099/100\n",
      "\n",
      "Epoch: 711 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 712 | Batch: 000 | Loss: 0.06798 | Correct: 099/100\n",
      "\n",
      "Epoch: 712 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 713 | Batch: 000 | Loss: 0.06796 | Correct: 099/100\n",
      "\n",
      "Epoch: 713 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 714 | Batch: 000 | Loss: 0.06794 | Correct: 099/100\n",
      "\n",
      "Epoch: 714 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 715 | Batch: 000 | Loss: 0.06792 | Correct: 099/100\n",
      "\n",
      "Epoch: 715 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 716 | Batch: 000 | Loss: 0.06791 | Correct: 099/100\n",
      "\n",
      "Epoch: 716 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 717 | Batch: 000 | Loss: 0.06789 | Correct: 099/100\n",
      "\n",
      "Epoch: 717 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 718 | Batch: 000 | Loss: 0.06787 | Correct: 099/100\n",
      "\n",
      "Epoch: 718 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 719 | Batch: 000 | Loss: 0.06785 | Correct: 099/100\n",
      "\n",
      "Epoch: 719 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 720 | Batch: 000 | Loss: 0.06783 | Correct: 099/100\n",
      "\n",
      "Epoch: 720 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 721 | Batch: 000 | Loss: 0.06781 | Correct: 099/100\n",
      "\n",
      "Epoch: 721 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 722 | Batch: 000 | Loss: 0.06779 | Correct: 099/100\n",
      "\n",
      "Epoch: 722 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 723 | Batch: 000 | Loss: 0.06777 | Correct: 099/100\n",
      "\n",
      "Epoch: 723 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 724 | Batch: 000 | Loss: 0.06776 | Correct: 099/100\n",
      "\n",
      "Epoch: 724 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 725 | Batch: 000 | Loss: 0.06774 | Correct: 099/100\n",
      "\n",
      "Epoch: 725 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 726 | Batch: 000 | Loss: 0.06772 | Correct: 099/100\n",
      "\n",
      "Epoch: 726 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 727 | Batch: 000 | Loss: 0.06770 | Correct: 099/100\n",
      "\n",
      "Epoch: 727 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 728 | Batch: 000 | Loss: 0.06768 | Correct: 099/100\n",
      "\n",
      "Epoch: 728 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 729 | Batch: 000 | Loss: 0.06767 | Correct: 099/100\n",
      "\n",
      "Epoch: 729 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 730 | Batch: 000 | Loss: 0.06766 | Correct: 099/100\n",
      "\n",
      "Epoch: 730 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 731 | Batch: 000 | Loss: 0.06765 | Correct: 099/100\n",
      "\n",
      "Epoch: 731 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 732 | Batch: 000 | Loss: 0.06764 | Correct: 099/100\n",
      "\n",
      "Epoch: 732 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 733 | Batch: 000 | Loss: 0.06764 | Correct: 099/100\n",
      "\n",
      "Epoch: 733 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 734 | Batch: 000 | Loss: 0.06763 | Correct: 099/100\n",
      "\n",
      "Epoch: 734 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 735 | Batch: 000 | Loss: 0.06761 | Correct: 099/100\n",
      "\n",
      "Epoch: 735 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 736 | Batch: 000 | Loss: 0.06756 | Correct: 099/100\n",
      "\n",
      "Epoch: 736 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 737 | Batch: 000 | Loss: 0.06752 | Correct: 099/100\n",
      "\n",
      "Epoch: 737 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 738 | Batch: 000 | Loss: 0.06751 | Correct: 099/100\n",
      "\n",
      "Epoch: 738 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 739 | Batch: 000 | Loss: 0.06751 | Correct: 099/100\n",
      "\n",
      "Epoch: 739 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 740 | Batch: 000 | Loss: 0.06750 | Correct: 099/100\n",
      "\n",
      "Epoch: 740 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 741 | Batch: 000 | Loss: 0.06747 | Correct: 099/100\n",
      "\n",
      "Epoch: 741 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 742 | Batch: 000 | Loss: 0.06744 | Correct: 099/100\n",
      "\n",
      "Epoch: 742 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 743 | Batch: 000 | Loss: 0.06743 | Correct: 099/100\n",
      "\n",
      "Epoch: 743 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 744 | Batch: 000 | Loss: 0.06742 | Correct: 099/100\n",
      "\n",
      "Epoch: 744 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 745 | Batch: 000 | Loss: 0.06740 | Correct: 099/100\n",
      "\n",
      "Epoch: 745 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 746 | Batch: 000 | Loss: 0.06737 | Correct: 099/100\n",
      "\n",
      "Epoch: 746 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 747 | Batch: 000 | Loss: 0.06735 | Correct: 099/100\n",
      "\n",
      "Epoch: 747 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 748 | Batch: 000 | Loss: 0.06734 | Correct: 099/100\n",
      "\n",
      "Epoch: 748 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 749 | Batch: 000 | Loss: 0.06733 | Correct: 099/100\n",
      "\n",
      "Epoch: 749 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 750 | Batch: 000 | Loss: 0.06731 | Correct: 099/100\n",
      "\n",
      "Epoch: 750 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 751 | Batch: 000 | Loss: 0.06729 | Correct: 099/100\n",
      "\n",
      "Epoch: 751 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 752 | Batch: 000 | Loss: 0.06727 | Correct: 099/100\n",
      "\n",
      "Epoch: 752 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 753 | Batch: 000 | Loss: 0.06726 | Correct: 099/100\n",
      "\n",
      "Epoch: 753 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 754 | Batch: 000 | Loss: 0.06724 | Correct: 099/100\n",
      "\n",
      "Epoch: 754 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 755 | Batch: 000 | Loss: 0.06722 | Correct: 099/100\n",
      "\n",
      "Epoch: 755 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 756 | Batch: 000 | Loss: 0.06720 | Correct: 099/100\n",
      "\n",
      "Epoch: 756 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 757 | Batch: 000 | Loss: 0.06719 | Correct: 099/100\n",
      "\n",
      "Epoch: 757 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 758 | Batch: 000 | Loss: 0.06718 | Correct: 099/100\n",
      "\n",
      "Epoch: 758 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 759 | Batch: 000 | Loss: 0.06716 | Correct: 099/100\n",
      "\n",
      "Epoch: 759 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 760 | Batch: 000 | Loss: 0.06714 | Correct: 099/100\n",
      "\n",
      "Epoch: 760 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 761 | Batch: 000 | Loss: 0.06713 | Correct: 099/100\n",
      "\n",
      "Epoch: 761 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 762 | Batch: 000 | Loss: 0.06711 | Correct: 099/100\n",
      "\n",
      "Epoch: 762 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 763 | Batch: 000 | Loss: 0.06709 | Correct: 099/100\n",
      "\n",
      "Epoch: 763 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 764 | Batch: 000 | Loss: 0.06708 | Correct: 099/100\n",
      "\n",
      "Epoch: 764 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 765 | Batch: 000 | Loss: 0.06706 | Correct: 099/100\n",
      "\n",
      "Epoch: 765 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 766 | Batch: 000 | Loss: 0.06705 | Correct: 099/100\n",
      "\n",
      "Epoch: 766 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 767 | Batch: 000 | Loss: 0.06703 | Correct: 099/100\n",
      "\n",
      "Epoch: 767 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 768 | Batch: 000 | Loss: 0.06701 | Correct: 099/100\n",
      "\n",
      "Epoch: 768 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 769 | Batch: 000 | Loss: 0.06700 | Correct: 099/100\n",
      "\n",
      "Epoch: 769 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 770 | Batch: 000 | Loss: 0.06698 | Correct: 099/100\n",
      "\n",
      "Epoch: 770 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 771 | Batch: 000 | Loss: 0.06697 | Correct: 099/100\n",
      "\n",
      "Epoch: 771 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 772 | Batch: 000 | Loss: 0.06695 | Correct: 099/100\n",
      "\n",
      "Epoch: 772 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 773 | Batch: 000 | Loss: 0.06694 | Correct: 099/100\n",
      "\n",
      "Epoch: 773 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 774 | Batch: 000 | Loss: 0.06692 | Correct: 099/100\n",
      "\n",
      "Epoch: 774 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 775 | Batch: 000 | Loss: 0.06691 | Correct: 099/100\n",
      "\n",
      "Epoch: 775 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 776 | Batch: 000 | Loss: 0.06689 | Correct: 099/100\n",
      "\n",
      "Epoch: 776 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 777 | Batch: 000 | Loss: 0.06688 | Correct: 099/100\n",
      "\n",
      "Epoch: 777 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 778 | Batch: 000 | Loss: 0.06686 | Correct: 099/100\n",
      "\n",
      "Epoch: 778 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 779 | Batch: 000 | Loss: 0.06685 | Correct: 099/100\n",
      "\n",
      "Epoch: 779 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 780 | Batch: 000 | Loss: 0.06683 | Correct: 099/100\n",
      "\n",
      "Epoch: 780 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 781 | Batch: 000 | Loss: 0.06682 | Correct: 099/100\n",
      "\n",
      "Epoch: 781 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 782 | Batch: 000 | Loss: 0.06680 | Correct: 099/100\n",
      "\n",
      "Epoch: 782 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 783 | Batch: 000 | Loss: 0.06679 | Correct: 099/100\n",
      "\n",
      "Epoch: 783 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 784 | Batch: 000 | Loss: 0.06678 | Correct: 099/100\n",
      "\n",
      "Epoch: 784 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 785 | Batch: 000 | Loss: 0.06676 | Correct: 099/100\n",
      "\n",
      "Epoch: 785 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 786 | Batch: 000 | Loss: 0.06675 | Correct: 099/100\n",
      "\n",
      "Epoch: 786 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 787 | Batch: 000 | Loss: 0.06673 | Correct: 099/100\n",
      "\n",
      "Epoch: 787 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 788 | Batch: 000 | Loss: 0.06672 | Correct: 099/100\n",
      "\n",
      "Epoch: 788 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 789 | Batch: 000 | Loss: 0.06670 | Correct: 099/100\n",
      "\n",
      "Epoch: 789 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 790 | Batch: 000 | Loss: 0.06669 | Correct: 099/100\n",
      "\n",
      "Epoch: 790 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 791 | Batch: 000 | Loss: 0.06668 | Correct: 099/100\n",
      "\n",
      "Epoch: 791 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 792 | Batch: 000 | Loss: 0.06666 | Correct: 099/100\n",
      "\n",
      "Epoch: 792 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 793 | Batch: 000 | Loss: 0.06665 | Correct: 099/100\n",
      "\n",
      "Epoch: 793 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 794 | Batch: 000 | Loss: 0.06663 | Correct: 099/100\n",
      "\n",
      "Epoch: 794 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 795 | Batch: 000 | Loss: 0.06662 | Correct: 099/100\n",
      "\n",
      "Epoch: 795 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 796 | Batch: 000 | Loss: 0.06661 | Correct: 099/100\n",
      "\n",
      "Epoch: 796 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 797 | Batch: 000 | Loss: 0.06659 | Correct: 099/100\n",
      "\n",
      "Epoch: 797 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 798 | Batch: 000 | Loss: 0.06658 | Correct: 099/100\n",
      "\n",
      "Epoch: 798 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 799 | Batch: 000 | Loss: 0.06656 | Correct: 099/100\n",
      "\n",
      "Epoch: 799 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 800 | Batch: 000 | Loss: 0.06655 | Correct: 099/100\n",
      "\n",
      "Epoch: 800 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 801 | Batch: 000 | Loss: 0.06654 | Correct: 099/100\n",
      "\n",
      "Epoch: 801 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 802 | Batch: 000 | Loss: 0.06652 | Correct: 099/100\n",
      "\n",
      "Epoch: 802 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 803 | Batch: 000 | Loss: 0.06651 | Correct: 099/100\n",
      "\n",
      "Epoch: 803 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 804 | Batch: 000 | Loss: 0.06650 | Correct: 099/100\n",
      "\n",
      "Epoch: 804 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 805 | Batch: 000 | Loss: 0.06648 | Correct: 099/100\n",
      "\n",
      "Epoch: 805 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 806 | Batch: 000 | Loss: 0.06647 | Correct: 099/100\n",
      "\n",
      "Epoch: 806 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 807 | Batch: 000 | Loss: 0.06646 | Correct: 099/100\n",
      "\n",
      "Epoch: 807 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 808 | Batch: 000 | Loss: 0.06644 | Correct: 099/100\n",
      "\n",
      "Epoch: 808 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 809 | Batch: 000 | Loss: 0.06643 | Correct: 099/100\n",
      "\n",
      "Epoch: 809 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 810 | Batch: 000 | Loss: 0.06642 | Correct: 099/100\n",
      "\n",
      "Epoch: 810 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 811 | Batch: 000 | Loss: 0.06640 | Correct: 099/100\n",
      "\n",
      "Epoch: 811 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 812 | Batch: 000 | Loss: 0.06639 | Correct: 099/100\n",
      "\n",
      "Epoch: 812 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 813 | Batch: 000 | Loss: 0.06638 | Correct: 099/100\n",
      "\n",
      "Epoch: 813 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 814 | Batch: 000 | Loss: 0.06636 | Correct: 099/100\n",
      "\n",
      "Epoch: 814 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 815 | Batch: 000 | Loss: 0.06635 | Correct: 099/100\n",
      "\n",
      "Epoch: 815 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 816 | Batch: 000 | Loss: 0.06634 | Correct: 099/100\n",
      "\n",
      "Epoch: 816 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 817 | Batch: 000 | Loss: 0.06633 | Correct: 099/100\n",
      "\n",
      "Epoch: 817 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 818 | Batch: 000 | Loss: 0.06631 | Correct: 099/100\n",
      "\n",
      "Epoch: 818 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 819 | Batch: 000 | Loss: 0.06630 | Correct: 099/100\n",
      "\n",
      "Epoch: 819 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 820 | Batch: 000 | Loss: 0.06629 | Correct: 099/100\n",
      "\n",
      "Epoch: 820 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 821 | Batch: 000 | Loss: 0.06628 | Correct: 099/100\n",
      "\n",
      "Epoch: 821 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 822 | Batch: 000 | Loss: 0.06626 | Correct: 099/100\n",
      "\n",
      "Epoch: 822 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 823 | Batch: 000 | Loss: 0.06625 | Correct: 099/100\n",
      "\n",
      "Epoch: 823 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 824 | Batch: 000 | Loss: 0.06624 | Correct: 099/100\n",
      "\n",
      "Epoch: 824 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 825 | Batch: 000 | Loss: 0.06623 | Correct: 099/100\n",
      "\n",
      "Epoch: 825 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 826 | Batch: 000 | Loss: 0.06621 | Correct: 099/100\n",
      "\n",
      "Epoch: 826 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 827 | Batch: 000 | Loss: 0.06620 | Correct: 099/100\n",
      "\n",
      "Epoch: 827 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 828 | Batch: 000 | Loss: 0.06619 | Correct: 099/100\n",
      "\n",
      "Epoch: 828 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 829 | Batch: 000 | Loss: 0.06618 | Correct: 099/100\n",
      "\n",
      "Epoch: 829 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 830 | Batch: 000 | Loss: 0.06616 | Correct: 099/100\n",
      "\n",
      "Epoch: 830 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 831 | Batch: 000 | Loss: 0.06615 | Correct: 099/100\n",
      "\n",
      "Epoch: 831 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 832 | Batch: 000 | Loss: 0.06614 | Correct: 099/100\n",
      "\n",
      "Epoch: 832 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 833 | Batch: 000 | Loss: 0.06613 | Correct: 099/100\n",
      "\n",
      "Epoch: 833 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 834 | Batch: 000 | Loss: 0.06612 | Correct: 099/100\n",
      "\n",
      "Epoch: 834 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 835 | Batch: 000 | Loss: 0.06610 | Correct: 099/100\n",
      "\n",
      "Epoch: 835 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 836 | Batch: 000 | Loss: 0.06609 | Correct: 099/100\n",
      "\n",
      "Epoch: 836 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 837 | Batch: 000 | Loss: 0.06608 | Correct: 099/100\n",
      "\n",
      "Epoch: 837 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 838 | Batch: 000 | Loss: 0.06607 | Correct: 099/100\n",
      "\n",
      "Epoch: 838 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 839 | Batch: 000 | Loss: 0.06606 | Correct: 099/100\n",
      "\n",
      "Epoch: 839 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 840 | Batch: 000 | Loss: 0.06604 | Correct: 099/100\n",
      "\n",
      "Epoch: 840 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 841 | Batch: 000 | Loss: 0.06603 | Correct: 099/100\n",
      "\n",
      "Epoch: 841 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 842 | Batch: 000 | Loss: 0.06602 | Correct: 099/100\n",
      "\n",
      "Epoch: 842 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 843 | Batch: 000 | Loss: 0.06601 | Correct: 099/100\n",
      "\n",
      "Epoch: 843 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 844 | Batch: 000 | Loss: 0.06600 | Correct: 099/100\n",
      "\n",
      "Epoch: 844 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 845 | Batch: 000 | Loss: 0.06599 | Correct: 099/100\n",
      "\n",
      "Epoch: 845 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 846 | Batch: 000 | Loss: 0.06597 | Correct: 099/100\n",
      "\n",
      "Epoch: 846 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 847 | Batch: 000 | Loss: 0.06596 | Correct: 099/100\n",
      "\n",
      "Epoch: 847 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 848 | Batch: 000 | Loss: 0.06595 | Correct: 099/100\n",
      "\n",
      "Epoch: 848 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 849 | Batch: 000 | Loss: 0.06594 | Correct: 099/100\n",
      "\n",
      "Epoch: 849 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 850 | Batch: 000 | Loss: 0.06593 | Correct: 099/100\n",
      "\n",
      "Epoch: 850 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 851 | Batch: 000 | Loss: 0.06592 | Correct: 099/100\n",
      "\n",
      "Epoch: 851 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 852 | Batch: 000 | Loss: 0.06591 | Correct: 099/100\n",
      "\n",
      "Epoch: 852 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 853 | Batch: 000 | Loss: 0.06590 | Correct: 099/100\n",
      "\n",
      "Epoch: 853 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 854 | Batch: 000 | Loss: 0.06588 | Correct: 099/100\n",
      "\n",
      "Epoch: 854 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 855 | Batch: 000 | Loss: 0.06587 | Correct: 099/100\n",
      "\n",
      "Epoch: 855 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 856 | Batch: 000 | Loss: 0.06586 | Correct: 099/100\n",
      "\n",
      "Epoch: 856 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 857 | Batch: 000 | Loss: 0.06585 | Correct: 099/100\n",
      "\n",
      "Epoch: 857 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 858 | Batch: 000 | Loss: 0.06584 | Correct: 099/100\n",
      "\n",
      "Epoch: 858 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 859 | Batch: 000 | Loss: 0.06583 | Correct: 099/100\n",
      "\n",
      "Epoch: 859 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 860 | Batch: 000 | Loss: 0.06582 | Correct: 099/100\n",
      "\n",
      "Epoch: 860 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 861 | Batch: 000 | Loss: 0.06581 | Correct: 099/100\n",
      "\n",
      "Epoch: 861 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 862 | Batch: 000 | Loss: 0.06580 | Correct: 099/100\n",
      "\n",
      "Epoch: 862 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 863 | Batch: 000 | Loss: 0.06579 | Correct: 099/100\n",
      "\n",
      "Epoch: 863 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 864 | Batch: 000 | Loss: 0.06577 | Correct: 099/100\n",
      "\n",
      "Epoch: 864 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 865 | Batch: 000 | Loss: 0.06576 | Correct: 099/100\n",
      "\n",
      "Epoch: 865 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 866 | Batch: 000 | Loss: 0.06575 | Correct: 099/100\n",
      "\n",
      "Epoch: 866 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 867 | Batch: 000 | Loss: 0.06574 | Correct: 099/100\n",
      "\n",
      "Epoch: 867 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 868 | Batch: 000 | Loss: 0.06573 | Correct: 099/100\n",
      "\n",
      "Epoch: 868 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 869 | Batch: 000 | Loss: 0.06572 | Correct: 099/100\n",
      "\n",
      "Epoch: 869 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 870 | Batch: 000 | Loss: 0.06571 | Correct: 099/100\n",
      "\n",
      "Epoch: 870 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 871 | Batch: 000 | Loss: 0.06570 | Correct: 099/100\n",
      "\n",
      "Epoch: 871 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 872 | Batch: 000 | Loss: 0.06569 | Correct: 099/100\n",
      "\n",
      "Epoch: 872 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 873 | Batch: 000 | Loss: 0.06568 | Correct: 099/100\n",
      "\n",
      "Epoch: 873 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 874 | Batch: 000 | Loss: 0.06567 | Correct: 099/100\n",
      "\n",
      "Epoch: 874 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 875 | Batch: 000 | Loss: 0.06566 | Correct: 099/100\n",
      "\n",
      "Epoch: 875 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 876 | Batch: 000 | Loss: 0.06565 | Correct: 099/100\n",
      "\n",
      "Epoch: 876 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 877 | Batch: 000 | Loss: 0.06565 | Correct: 099/100\n",
      "\n",
      "Epoch: 877 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 878 | Batch: 000 | Loss: 0.06564 | Correct: 099/100\n",
      "\n",
      "Epoch: 878 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 879 | Batch: 000 | Loss: 0.06564 | Correct: 099/100\n",
      "\n",
      "Epoch: 879 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 880 | Batch: 000 | Loss: 0.06564 | Correct: 099/100\n",
      "\n",
      "Epoch: 880 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 881 | Batch: 000 | Loss: 0.06565 | Correct: 099/100\n",
      "\n",
      "Epoch: 881 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 882 | Batch: 000 | Loss: 0.06565 | Correct: 099/100\n",
      "\n",
      "Epoch: 882 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 883 | Batch: 000 | Loss: 0.06564 | Correct: 099/100\n",
      "\n",
      "Epoch: 883 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 884 | Batch: 000 | Loss: 0.06560 | Correct: 099/100\n",
      "\n",
      "Epoch: 884 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 885 | Batch: 000 | Loss: 0.06556 | Correct: 099/100\n",
      "\n",
      "Epoch: 885 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 886 | Batch: 000 | Loss: 0.06555 | Correct: 099/100\n",
      "\n",
      "Epoch: 886 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 887 | Batch: 000 | Loss: 0.06556 | Correct: 099/100\n",
      "\n",
      "Epoch: 887 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 888 | Batch: 000 | Loss: 0.06556 | Correct: 099/100\n",
      "\n",
      "Epoch: 888 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 889 | Batch: 000 | Loss: 0.06554 | Correct: 099/100\n",
      "\n",
      "Epoch: 889 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 890 | Batch: 000 | Loss: 0.06551 | Correct: 099/100\n",
      "\n",
      "Epoch: 890 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 891 | Batch: 000 | Loss: 0.06550 | Correct: 099/100\n",
      "\n",
      "Epoch: 891 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 892 | Batch: 000 | Loss: 0.06550 | Correct: 099/100\n",
      "\n",
      "Epoch: 892 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 893 | Batch: 000 | Loss: 0.06550 | Correct: 099/100\n",
      "\n",
      "Epoch: 893 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 894 | Batch: 000 | Loss: 0.06548 | Correct: 099/100\n",
      "\n",
      "Epoch: 894 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 895 | Batch: 000 | Loss: 0.06546 | Correct: 099/100\n",
      "\n",
      "Epoch: 895 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 896 | Batch: 000 | Loss: 0.06545 | Correct: 099/100\n",
      "\n",
      "Epoch: 896 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 897 | Batch: 000 | Loss: 0.06545 | Correct: 099/100\n",
      "\n",
      "Epoch: 897 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 898 | Batch: 000 | Loss: 0.06544 | Correct: 099/100\n",
      "\n",
      "Epoch: 898 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 899 | Batch: 000 | Loss: 0.06543 | Correct: 099/100\n",
      "\n",
      "Epoch: 899 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 900 | Batch: 000 | Loss: 0.06541 | Correct: 099/100\n",
      "\n",
      "Epoch: 900 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 901 | Batch: 000 | Loss: 0.06541 | Correct: 099/100\n",
      "\n",
      "Epoch: 901 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 902 | Batch: 000 | Loss: 0.06540 | Correct: 099/100\n",
      "\n",
      "Epoch: 902 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 903 | Batch: 000 | Loss: 0.06539 | Correct: 099/100\n",
      "\n",
      "Epoch: 903 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 904 | Batch: 000 | Loss: 0.06538 | Correct: 099/100\n",
      "\n",
      "Epoch: 904 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 905 | Batch: 000 | Loss: 0.06537 | Correct: 099/100\n",
      "\n",
      "Epoch: 905 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 906 | Batch: 000 | Loss: 0.06536 | Correct: 099/100\n",
      "\n",
      "Epoch: 906 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 907 | Batch: 000 | Loss: 0.06535 | Correct: 099/100\n",
      "\n",
      "Epoch: 907 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 908 | Batch: 000 | Loss: 0.06534 | Correct: 099/100\n",
      "\n",
      "Epoch: 908 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 909 | Batch: 000 | Loss: 0.06533 | Correct: 099/100\n",
      "\n",
      "Epoch: 909 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 910 | Batch: 000 | Loss: 0.06532 | Correct: 099/100\n",
      "\n",
      "Epoch: 910 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 911 | Batch: 000 | Loss: 0.06531 | Correct: 099/100\n",
      "\n",
      "Epoch: 911 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 912 | Batch: 000 | Loss: 0.06530 | Correct: 099/100\n",
      "\n",
      "Epoch: 912 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 913 | Batch: 000 | Loss: 0.06530 | Correct: 099/100\n",
      "\n",
      "Epoch: 913 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 914 | Batch: 000 | Loss: 0.06528 | Correct: 099/100\n",
      "\n",
      "Epoch: 914 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 915 | Batch: 000 | Loss: 0.06528 | Correct: 099/100\n",
      "\n",
      "Epoch: 915 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 916 | Batch: 000 | Loss: 0.06527 | Correct: 099/100\n",
      "\n",
      "Epoch: 916 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 917 | Batch: 000 | Loss: 0.06526 | Correct: 099/100\n",
      "\n",
      "Epoch: 917 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 918 | Batch: 000 | Loss: 0.06525 | Correct: 099/100\n",
      "\n",
      "Epoch: 918 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 919 | Batch: 000 | Loss: 0.06524 | Correct: 099/100\n",
      "\n",
      "Epoch: 919 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 920 | Batch: 000 | Loss: 0.06523 | Correct: 099/100\n",
      "\n",
      "Epoch: 920 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 921 | Batch: 000 | Loss: 0.06522 | Correct: 099/100\n",
      "\n",
      "Epoch: 921 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 922 | Batch: 000 | Loss: 0.06521 | Correct: 099/100\n",
      "\n",
      "Epoch: 922 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 923 | Batch: 000 | Loss: 0.06521 | Correct: 099/100\n",
      "\n",
      "Epoch: 923 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 924 | Batch: 000 | Loss: 0.06520 | Correct: 099/100\n",
      "\n",
      "Epoch: 924 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 925 | Batch: 000 | Loss: 0.06519 | Correct: 099/100\n",
      "\n",
      "Epoch: 925 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 926 | Batch: 000 | Loss: 0.06518 | Correct: 099/100\n",
      "\n",
      "Epoch: 926 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 927 | Batch: 000 | Loss: 0.06517 | Correct: 099/100\n",
      "\n",
      "Epoch: 927 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 928 | Batch: 000 | Loss: 0.06516 | Correct: 099/100\n",
      "\n",
      "Epoch: 928 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 929 | Batch: 000 | Loss: 0.06515 | Correct: 099/100\n",
      "\n",
      "Epoch: 929 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 930 | Batch: 000 | Loss: 0.06515 | Correct: 099/100\n",
      "\n",
      "Epoch: 930 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 931 | Batch: 000 | Loss: 0.06514 | Correct: 099/100\n",
      "\n",
      "Epoch: 931 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 932 | Batch: 000 | Loss: 0.06513 | Correct: 099/100\n",
      "\n",
      "Epoch: 932 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 933 | Batch: 000 | Loss: 0.06512 | Correct: 099/100\n",
      "\n",
      "Epoch: 933 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 934 | Batch: 000 | Loss: 0.06511 | Correct: 099/100\n",
      "\n",
      "Epoch: 934 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 935 | Batch: 000 | Loss: 0.06510 | Correct: 099/100\n",
      "\n",
      "Epoch: 935 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 936 | Batch: 000 | Loss: 0.06510 | Correct: 099/100\n",
      "\n",
      "Epoch: 936 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 937 | Batch: 000 | Loss: 0.06509 | Correct: 099/100\n",
      "\n",
      "Epoch: 937 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 938 | Batch: 000 | Loss: 0.06508 | Correct: 099/100\n",
      "\n",
      "Epoch: 938 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 939 | Batch: 000 | Loss: 0.06507 | Correct: 099/100\n",
      "\n",
      "Epoch: 939 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 940 | Batch: 000 | Loss: 0.06506 | Correct: 099/100\n",
      "\n",
      "Epoch: 940 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 941 | Batch: 000 | Loss: 0.06505 | Correct: 099/100\n",
      "\n",
      "Epoch: 941 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 942 | Batch: 000 | Loss: 0.06505 | Correct: 099/100\n",
      "\n",
      "Epoch: 942 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 943 | Batch: 000 | Loss: 0.06504 | Correct: 099/100\n",
      "\n",
      "Epoch: 943 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 944 | Batch: 000 | Loss: 0.06503 | Correct: 099/100\n",
      "\n",
      "Epoch: 944 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 945 | Batch: 000 | Loss: 0.06502 | Correct: 099/100\n",
      "\n",
      "Epoch: 945 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 946 | Batch: 000 | Loss: 0.06501 | Correct: 099/100\n",
      "\n",
      "Epoch: 946 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 947 | Batch: 000 | Loss: 0.06501 | Correct: 099/100\n",
      "\n",
      "Epoch: 947 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 948 | Batch: 000 | Loss: 0.06500 | Correct: 099/100\n",
      "\n",
      "Epoch: 948 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 949 | Batch: 000 | Loss: 0.06499 | Correct: 099/100\n",
      "\n",
      "Epoch: 949 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 950 | Batch: 000 | Loss: 0.06498 | Correct: 099/100\n",
      "\n",
      "Epoch: 950 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 951 | Batch: 000 | Loss: 0.06498 | Correct: 099/100\n",
      "\n",
      "Epoch: 951 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 952 | Batch: 000 | Loss: 0.06497 | Correct: 099/100\n",
      "\n",
      "Epoch: 952 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 953 | Batch: 000 | Loss: 0.06496 | Correct: 099/100\n",
      "\n",
      "Epoch: 953 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 954 | Batch: 000 | Loss: 0.06495 | Correct: 099/100\n",
      "\n",
      "Epoch: 954 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 955 | Batch: 000 | Loss: 0.06494 | Correct: 099/100\n",
      "\n",
      "Epoch: 955 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 956 | Batch: 000 | Loss: 0.06494 | Correct: 099/100\n",
      "\n",
      "Epoch: 956 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 957 | Batch: 000 | Loss: 0.06493 | Correct: 099/100\n",
      "\n",
      "Epoch: 957 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 958 | Batch: 000 | Loss: 0.06492 | Correct: 099/100\n",
      "\n",
      "Epoch: 958 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 959 | Batch: 000 | Loss: 0.06491 | Correct: 099/100\n",
      "\n",
      "Epoch: 959 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 960 | Batch: 000 | Loss: 0.06491 | Correct: 099/100\n",
      "\n",
      "Epoch: 960 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 961 | Batch: 000 | Loss: 0.06490 | Correct: 099/100\n",
      "\n",
      "Epoch: 961 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 962 | Batch: 000 | Loss: 0.06489 | Correct: 099/100\n",
      "\n",
      "Epoch: 962 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 963 | Batch: 000 | Loss: 0.06488 | Correct: 099/100\n",
      "\n",
      "Epoch: 963 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 964 | Batch: 000 | Loss: 0.06488 | Correct: 099/100\n",
      "\n",
      "Epoch: 964 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 965 | Batch: 000 | Loss: 0.06487 | Correct: 099/100\n",
      "\n",
      "Epoch: 965 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 966 | Batch: 000 | Loss: 0.06486 | Correct: 099/100\n",
      "\n",
      "Epoch: 966 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 967 | Batch: 000 | Loss: 0.06485 | Correct: 099/100\n",
      "\n",
      "Epoch: 967 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 968 | Batch: 000 | Loss: 0.06485 | Correct: 099/100\n",
      "\n",
      "Epoch: 968 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 969 | Batch: 000 | Loss: 0.06484 | Correct: 099/100\n",
      "\n",
      "Epoch: 969 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 970 | Batch: 000 | Loss: 0.06483 | Correct: 099/100\n",
      "\n",
      "Epoch: 970 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 971 | Batch: 000 | Loss: 0.06482 | Correct: 099/100\n",
      "\n",
      "Epoch: 971 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 972 | Batch: 000 | Loss: 0.06482 | Correct: 099/100\n",
      "\n",
      "Epoch: 972 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 973 | Batch: 000 | Loss: 0.06481 | Correct: 099/100\n",
      "\n",
      "Epoch: 973 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 974 | Batch: 000 | Loss: 0.06480 | Correct: 099/100\n",
      "\n",
      "Epoch: 974 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 975 | Batch: 000 | Loss: 0.06480 | Correct: 099/100\n",
      "\n",
      "Epoch: 975 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 976 | Batch: 000 | Loss: 0.06479 | Correct: 099/100\n",
      "\n",
      "Epoch: 976 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 977 | Batch: 000 | Loss: 0.06478 | Correct: 099/100\n",
      "\n",
      "Epoch: 977 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 978 | Batch: 000 | Loss: 0.06477 | Correct: 099/100\n",
      "\n",
      "Epoch: 978 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 979 | Batch: 000 | Loss: 0.06477 | Correct: 099/100\n",
      "\n",
      "Epoch: 979 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 980 | Batch: 000 | Loss: 0.06476 | Correct: 099/100\n",
      "\n",
      "Epoch: 980 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 981 | Batch: 000 | Loss: 0.06475 | Correct: 099/100\n",
      "\n",
      "Epoch: 981 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 982 | Batch: 000 | Loss: 0.06475 | Correct: 099/100\n",
      "\n",
      "Epoch: 982 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 983 | Batch: 000 | Loss: 0.06474 | Correct: 099/100\n",
      "\n",
      "Epoch: 983 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 984 | Batch: 000 | Loss: 0.06473 | Correct: 099/100\n",
      "\n",
      "Epoch: 984 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 985 | Batch: 000 | Loss: 0.06472 | Correct: 099/100\n",
      "\n",
      "Epoch: 985 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 986 | Batch: 000 | Loss: 0.06472 | Correct: 099/100\n",
      "\n",
      "Epoch: 986 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 987 | Batch: 000 | Loss: 0.06471 | Correct: 099/100\n",
      "\n",
      "Epoch: 987 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 988 | Batch: 000 | Loss: 0.06470 | Correct: 099/100\n",
      "\n",
      "Epoch: 988 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 989 | Batch: 000 | Loss: 0.06470 | Correct: 099/100\n",
      "\n",
      "Epoch: 989 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 990 | Batch: 000 | Loss: 0.06469 | Correct: 099/100\n",
      "\n",
      "Epoch: 990 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 991 | Batch: 000 | Loss: 0.06468 | Correct: 099/100\n",
      "\n",
      "Epoch: 991 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 992 | Batch: 000 | Loss: 0.06468 | Correct: 099/100\n",
      "\n",
      "Epoch: 992 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 993 | Batch: 000 | Loss: 0.06467 | Correct: 099/100\n",
      "\n",
      "Epoch: 993 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 994 | Batch: 000 | Loss: 0.06466 | Correct: 099/100\n",
      "\n",
      "Epoch: 994 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 995 | Batch: 000 | Loss: 0.06466 | Correct: 099/100\n",
      "\n",
      "Epoch: 995 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 996 | Batch: 000 | Loss: 0.06465 | Correct: 099/100\n",
      "\n",
      "Epoch: 996 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 997 | Batch: 000 | Loss: 0.06464 | Correct: 099/100\n",
      "\n",
      "Epoch: 997 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 998 | Batch: 000 | Loss: 0.06464 | Correct: 099/100\n",
      "\n",
      "Epoch: 998 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n",
      "Epoch: 999 | Batch: 000 | Loss: 0.06463 | Correct: 099/100\n",
      "\n",
      "Epoch: 999 | Testing Accuracy: 49.0/50 (98.000%) | Historical Best: 100.000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "\n",
    "def onehot_coding(target, device, output_dim):\n",
    "    \"\"\"Convert the class labels into one-hot encoded vectors.\"\"\"\n",
    "    target_onehot = torch.FloatTensor(target.size()[0], output_dim).to(device)\n",
    "    target_onehot.data.zero_()\n",
    "    #print(target_onehot)\n",
    "    target_onehot.scatter_(1, target.view(-1, 1), 1.0)\n",
    "    return target_onehot\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parameters\n",
    "    input_dim = X_train.shape[1]#28 * 28    # the number of input dimensions\n",
    "    output_dim = 3#10        # the number of outputs (i.e., # classes on MNIST)\n",
    "    depth = 5              # tree depth\n",
    "    lamda = 1e-3           # coefficient of the regularization term\n",
    "    lr = 1e-2              # learning rate\n",
    "    weight_decaly = 5e-4   # weight decay\n",
    "    batch_size = 128       # batch size\n",
    "    epochs = 1000          # the number of training epochs\n",
    "    log_interval = 100     # the number of batches to wait before printing logs\n",
    "    use_cuda = False       # whether to use GPU\n",
    "\n",
    "    # Model and Optimizer\n",
    "    tree = SDT(input_dim, output_dim, depth, lamda, use_cuda)\n",
    "\n",
    "    optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decaly)\n",
    "\n",
    "    # Load data\n",
    "    data_dir = \"../Dataset/mnist\"\n",
    "\n",
    "    transformer = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    \n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.LongTensor(y_train)#torch.LongTensor(y.reshape(-1,1))\n",
    "    \n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_test = torch.LongTensor(y_test)    \n",
    "    #print('X', X.dtype)\n",
    "    #print('y', y.dtype)    \n",
    "    #print('X', X)\n",
    "    #print('y', y)\n",
    "    \n",
    "    \n",
    "    # Utils\n",
    "    best_testing_acc = 0.0\n",
    "    testing_acc_list = []\n",
    "    training_loss_list = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Training\n",
    "        tree.train()\n",
    "        \n",
    "        batch_size = X_train.shape[0]#data.size()[0]\n",
    "        data, target = X_train, y_train#data.to(device), target.to(device)\n",
    "        target_onehot = onehot_coding(target, device, output_dim)\n",
    "\n",
    "        #print(batch_size)\n",
    "        #print(data.shape)\n",
    "        #print(target.shape)\n",
    "        #print(target_onehot.shape)\n",
    "\n",
    "        \n",
    "        output, penalty = tree.forward(data, is_training_data=True)\n",
    "\n",
    "        loss = criterion(output, target.view(-1))\n",
    "        loss += penalty\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct = pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "            msg = (\n",
    "                \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                \" Correct: {:03d}/{:03d}\"\n",
    "            )\n",
    "            print(msg.format(epoch, batch_idx, loss, correct, batch_size))\n",
    "            training_loss_list.append(loss.cpu().data.numpy())\n",
    "\n",
    "        # Evaluating\n",
    "        tree.eval()\n",
    "        correct = 0.\n",
    "\n",
    "\n",
    "        batch_size = X_test.shape[0]#data.size()[0]\n",
    "        data, target = X_test, y_test#data.to(device), target.to(device)\n",
    "\n",
    "        output = F.softmax(tree.forward(data), dim=1)\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "        accuracy = 100.0 * float(correct) / target.shape[0]\n",
    "\n",
    "        if accuracy > best_testing_acc:\n",
    "            best_testing_acc = accuracy\n",
    "\n",
    "        msg = (\n",
    "            \"\\nEpoch: {:02d} | Testing Accuracy: {}/{} ({:.3f}%) |\"\n",
    "            \" Historical Best: {:.3f}%\\n\"\n",
    "        )\n",
    "        print(\n",
    "            msg.format(\n",
    "                epoch, correct,\n",
    "                target.shape[0],\n",
    "                accuracy,\n",
    "                best_testing_acc\n",
    "            )\n",
    "        )\n",
    "        testing_acc_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0230,  0.1037,  0.0170,  0.0782, -0.0251,  0.1144,  0.0525, -0.1145,\n",
       "         -0.0837,  0.0280, -0.0622,  0.0481, -0.0381, -0.1141, -0.0541, -0.1151,\n",
       "         -0.0957,  0.0267,  0.0018,  0.0714,  0.0183, -0.0907, -0.0742,  0.0593,\n",
       "         -0.1175, -0.0281,  0.0955, -0.1078, -0.0944,  0.1281,  0.0142, -0.0354,\n",
       "         -0.0673,  0.1071,  0.0290,  0.0405,  0.0146,  0.0628, -0.1075,  0.0575,\n",
       "         -0.0404,  0.1007,  0.0715,  0.0778, -0.0295, -0.0013,  0.0014, -0.0843,\n",
       "          0.0857,  0.0264,  0.1017, -0.0677, -0.0819, -0.0488,  0.0175,  0.0238,\n",
       "          0.0960, -0.1120,  0.0750,  0.1001, -0.0276, -0.0807,  0.0375, -0.0681],\n",
       "        [ 0.1084,  0.0509,  0.0367,  0.1000,  0.0191,  0.0120, -0.1097, -0.1095,\n",
       "          0.0797, -0.0630, -0.0239,  0.0278,  0.0503, -0.1013,  0.1144, -0.0482,\n",
       "          0.1286, -0.1089, -0.0005, -0.0057,  0.0923,  0.0121, -0.0005,  0.0693,\n",
       "          0.0428,  0.0831,  0.1023, -0.0549, -0.0004,  0.0824,  0.0744,  0.1081,\n",
       "          0.0593,  0.0789, -0.0071,  0.0159, -0.1131, -0.1052, -0.1047, -0.0808,\n",
       "          0.0861, -0.0569, -0.0815,  0.0202,  0.0149,  0.0050,  0.0004, -0.0697,\n",
       "          0.1011,  0.0302, -0.0332,  0.0873,  0.1106,  0.0982, -0.1338,  0.0042,\n",
       "          0.0612,  0.0204, -0.0648, -0.0200,  0.0451, -0.0210, -0.1084,  0.0868],\n",
       "        [-0.1045,  0.1244,  0.0800,  0.0701,  0.0230, -0.0762, -0.1071, -0.1129,\n",
       "          0.0682,  0.0682, -0.1214,  0.0637,  0.0736, -0.0425,  0.0948, -0.0003,\n",
       "         -0.0242,  0.0101, -0.1082,  0.0343,  0.1047,  0.0275,  0.0648, -0.0144,\n",
       "          0.0442, -0.0980,  0.1218,  0.0020,  0.0292, -0.0996, -0.0071, -0.0647,\n",
       "          0.0648,  0.0033, -0.0794,  0.0902,  0.1143, -0.1136, -0.0954,  0.0779,\n",
       "          0.0905,  0.1023, -0.0097,  0.0387,  0.0703,  0.0681, -0.0638,  0.0826,\n",
       "          0.0954,  0.0385,  0.0393, -0.0889, -0.0687, -0.0964, -0.0990,  0.1073,\n",
       "          0.0667,  0.0688, -0.1249,  0.1067, -0.1347, -0.0544, -0.0414, -0.0427]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.leaf_nodes.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.leaf_nodes.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sigmoid()"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 5])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:90]\n",
    "y = y[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([X, X])\n",
    "y = np.hstack([y, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "normal_dt = DecisionTreeClassifier(max_depth=4)\n",
    "normal_dt.fit(X_train, y_train)\n",
    "preds = normal_dt.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import fuzzytree\n",
    "from fuzzytree import FuzzyDecisionTreeClassifier\n",
    "\n",
    "fuzzy_dt = FuzzyDecisionTreeClassifier(max_depth=4)\n",
    "fuzzy_dt.fit(X_train, y_train)\n",
    "preds = fuzzy_dt.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_X_y, check_is_fitted, _check_sample_weight\n",
    "def _get_sample_weight(X, sample_weight=None):\n",
    "    sample_weight = _check_sample_weight(sample_weight, X, np.float64)\n",
    "    indices = np.flatnonzero(sample_weight)\n",
    "    sample_weight = sample_weight[indices] / sample_weight[indices].mean()\n",
    "\n",
    "    return indices, sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 4)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360,)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_sample_weight(X)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "        85, 86, 87, 88, 89]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_sample_weight(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76800, 4)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76800,)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FuzzyDecisionTreeClassifier(max_depth=4)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fuzzytree._fuzzy_tree.FuzzyTree at 0x7fea6eb90790>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_sorted_cols(X):\n",
    "    _sorted_cols = {}\n",
    "\n",
    "    for feature_idx in range(X.shape[1]):\n",
    "        _sorted_cols[feature_idx] = np.sort(X[:, feature_idx])\n",
    "    return _sorted_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dt.tree_.true_branch.rule.split_val = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzytree._fuzzy_decision_rule import FuzzyDecisionRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fuzzytree._fuzzy_tree.FuzzyTree at 0x7fea6eb4c910>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fuzzy_dt.tree_.true_branch.true_branch.true_branch.rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dt.tree_.true_branch.rule = FuzzyDecisionRule(sorted_feature= _init_sorted_cols(X)[3], \n",
    "                                                    split_val=0.5, \n",
    "                                                    fuzziness=0.8, \n",
    "                                                    feature_idx=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "       0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "       0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "       0.3, 0.3, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.5, 0.6, 1. , 1. ,\n",
       "       1. , 1. , 1. , 1. , 1. , 1.1, 1.1, 1.1, 1.2, 1.2, 1.2, 1.2, 1.2,\n",
       "       1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3, 1.3,\n",
       "       1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
       "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.6, 1.6, 1.6, 1.6, 1.7, 1.7,\n",
       "       1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.9,\n",
       "       1.9, 1.9, 1.9, 1.9, 2. , 2. , 2. , 2. , 2. , 2. , 2.1, 2.1, 2.1,\n",
       "       2.1, 2.1, 2.1, 2.2, 2.2, 2.2, 2.3, 2.3, 2.3, 2.3, 2.3, 2.3, 2.3,\n",
       "       2.3, 2.4, 2.4, 2.4, 2.5, 2.5, 2.5], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_init_sorted_cols(X)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 4)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.16934445e-04, 2.20780176e-03,\n",
       "       5.41041589e-03, 1.00247768e-02, 1.60508846e-02, 2.34887392e-02,\n",
       "       3.23383407e-02, 4.25996889e-02, 5.42727840e-02, 6.73576259e-02,\n",
       "       8.18542146e-02, 9.77625502e-02, 1.15082633e-01, 1.33814462e-01,\n",
       "       1.53958038e-01, 1.75513361e-01, 1.98480430e-01, 2.22859247e-01,\n",
       "       2.48649810e-01, 2.75852120e-01, 3.04466177e-01, 3.34491981e-01,\n",
       "       3.65929532e-01, 3.98778829e-01, 4.33039873e-01, 4.68712664e-01,\n",
       "       5.05763788e-01, 5.42413943e-01, 5.77652350e-01, 6.11479011e-01,\n",
       "       6.43893924e-01, 6.74897091e-01, 7.04488512e-01, 7.32668185e-01,\n",
       "       7.59436111e-01, 7.84792291e-01, 8.08736724e-01, 8.31269410e-01,\n",
       "       8.52390349e-01, 8.72099542e-01, 8.90396987e-01, 9.07282686e-01,\n",
       "       9.22756638e-01, 9.36818843e-01, 9.49469301e-01, 9.60708013e-01,\n",
       "       9.70534977e-01, 9.78950195e-01, 9.85953666e-01, 9.91545390e-01,\n",
       "       9.95725367e-01, 9.98493598e-01, 9.99850082e-01, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.true_branch.rule.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.true_branch.rule.membership.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.true_branch.rule.split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x[3] >= 0.7?"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.true_branch.rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def membership_ratio(y, membership):\n",
    "    \"\"\"Calculate the membership ratio of each class in y.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like of shape (n_samples,)\n",
    "        The array of labels.\n",
    "    membership : array-like of shape (n_samples,)\n",
    "        The membership of each sample that corresponding label is\n",
    "        coming from.\n",
    "    Returns\n",
    "    -------\n",
    "    membership_by_class : array-like of float of shape (len(np.unique(y)),)\n",
    "        The membership ratio for each class in np.unique(y).\n",
    "    \"\"\"\n",
    "\n",
    "    membership_by_class = np.bincount(y, weights=membership)\n",
    "    if membership_by_class.any():\n",
    "        membership_by_class /= membership_by_class.sum()\n",
    "\n",
    "    return membership_by_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The weights and list don't have the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-279-dabb30d728ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmembership_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuzzy_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-218-68db4afa5f8a>\u001b[0m in \u001b[0;36mmembership_ratio\u001b[0;34m(y, membership)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmembership_by_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmembership\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmembership_by_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmembership_by_class\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmembership_by_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbincount\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The weights and list don't have the same length."
     ]
    }
   ],
   "source": [
    "membership_ratio(y, fuzzy_dt.tree_.class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7036657])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.true_branch.rule.evaluate(np.array([[1.7, 1.7, 1.7, 0.64]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x[3] >= 1.65?"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.true_branch.true_branch.rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dt.tree_.true_branch.true_branch.true_branch.rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weights': array([0.        , 0.10485109, 0.89514891]),\n",
       " 'level': 2,\n",
       " 'rule': x[3] >= 1.65?,\n",
       " 'true_branch': <fuzzytree._fuzzy_tree.FuzzyTree at 0x7fea6eb90460>,\n",
       " 'false_branch': <fuzzytree._fuzzy_tree.FuzzyTree at 0x7fea6eb90a30>}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_dt.tree_.true_branch.true_branch.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_dt.tree_.__weakref__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FuzzyTree in module fuzzytree._fuzzy_tree:\n",
      "\n",
      "class FuzzyTree(builtins.object)\n",
      " |  FuzzyTree(y, membership, level=0, rule=None, true_branch=None, false_branch=None)\n",
      " |  \n",
      " |  Fuzzy decision tree representation.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  y : array-like of shape (n_samples,)\n",
      " |      The array of labels.\n",
      " |  membership : array-like of shape (n_samples,)\n",
      " |      The membership of samples that respective labels are\n",
      " |      coming from.\n",
      " |  level : int\n",
      " |      Depth of the node.\n",
      " |  rule : FuzzyDecisionRule, default=None\n",
      " |      The rule that was used to split this node. If None,\n",
      " |      then the node is a leaf.\n",
      " |  true_branch : FuzzyTree, default=None\n",
      " |      The child node containing labels of samples which\n",
      " |      memberships of were evaluated as non-zero by the\n",
      " |      fuzzy decision rule. If None, then the node is a leaf.\n",
      " |  false_branch : FuzzyTree, default=None\n",
      " |      The child node containing labels of samples which\n",
      " |      memberships of were evaluated as non-zero by the\n",
      " |      inverse of the fuzzy decision rule. If None, then\n",
      " |      the node is a leaf.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_weights : ndarray of shape (n_classes,)\n",
      " |      The membership ratio for each labels of n_classes.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, y, membership, level=0, rule=None, true_branch=None, false_branch=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X, membership)\n",
      " |      Predict labels of each sample in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples,n_features)\n",
      " |          The array of input samples.\n",
      " |      membership : array-like of shape (n_samples,)\n",
      " |          The array-like of membership of each sample.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,n_classes)\n",
      " |          The probability of predicted classes, for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  is_leaf\n",
      " |      Return whether the node is a leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool : True if node is a leaf.\n",
      " |  \n",
      " |  max_depth\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int : The maximum depth of the tree.\n",
      " |  \n",
      " |  n_leaves\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int : The number of leaves.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fuzzytree._fuzzy_tree.FuzzyTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'fully_grown': True,       \n",
    "        'balance': 0.5\n",
    "        'balancing_tolerance': 0.05               \n",
    "    }\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'make_classification', #'random'\n",
    "        'objective': 'classification' # 'multiclass_classification', 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "        \n",
    "        'lambda_dataset_size': 5000, #number of samples per polynomial\n",
    "        'number_of_generated_datasets': 10000,\n",
    "    }, \n",
    "    'computation':{\n",
    "        'n_jobs': 5,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 0,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:58.879427Z",
     "start_time": "2020-09-16T12:26:58.874894Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T21:12:40.476681Z",
     "start_time": "2021-01-13T21:12:38.298249Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product       # forms cartesian products\n",
    "from more_itertools import random_product \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import random \n",
    "from random import sample \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sympy import Symbol, sympify\n",
    "\n",
    "        \n",
    "import seaborn as sns\n",
    "        \n",
    "import random \n",
    "\n",
    "import warnings\n",
    "\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'KeyError'>\n",
      "<class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='data_creation'))\n",
    "generate_directory_structure()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly_1000_train_5000_var_15_d_3_negd_0_prob_0_spars_15_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1_diffX\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_polynomial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:28:46.853042Z",
     "start_time": "2020-09-16T12:28:46.848346Z"
    }
   },
   "source": [
    "# Function Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_decision_tree():\n",
    "    \n",
    "    \n",
    "    \n",
    "number_of_variables,\n",
    "maximum_depth,\n",
    "num_classes,\n",
    "fully_grown\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decision_tree_data(n_samples, noise, noise_dist, seed):\n",
    "    \n",
    "    decision_tree = generate_random_decision_tree()\n",
    "    \n",
    "    return decision_tree, X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend MultiprocessingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=5)]: Done 278 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=5)]: Done 502 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=5)]: Done 1000 out of 1000 | elapsed:  3.9min finished\n"
     ]
    }
   ],
   "source": [
    "if same_training_all_lambda_nets:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(generate_decision_tree_data)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))  \n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(gen_regression_symbolic)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED+i, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))\n",
    "\n",
    "X_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[1], columns=list(variables[:n]))] for result in result_list]\n",
    "y_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[2], columns=['result'])] for result in result_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000000000000   0.000\n",
       "210000000000000   0.000\n",
       "201000000000000   0.000\n",
       "200100000000000   0.000\n",
       "200010000000000   0.000\n",
       "200001000000000   0.000\n",
       "200000100000000   0.000\n",
       "200000010000000   0.000\n",
       "200000001000000   0.000\n",
       "200000000100000   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.265</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.671</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.159</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.319</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.576</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.953</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.591</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a     b     c     d     e     f     g     h     i     j     k     l  \\\n",
       "0 0.549 0.715 0.603 0.545 0.424 0.646 0.438 0.892 0.964 0.383 0.792 0.529   \n",
       "1 0.087 0.020 0.833 0.778 0.870 0.979 0.799 0.461 0.781 0.118 0.640 0.143   \n",
       "2 0.265 0.774 0.456 0.568 0.019 0.618 0.612 0.617 0.944 0.682 0.360 0.437   \n",
       "3 0.671 0.210 0.129 0.315 0.364 0.570 0.439 0.988 0.102 0.209 0.161 0.653   \n",
       "4 0.159 0.110 0.656 0.138 0.197 0.369 0.821 0.097 0.838 0.096 0.976 0.469   \n",
       "5 0.039 0.283 0.120 0.296 0.119 0.318 0.414 0.064 0.692 0.567 0.265 0.523   \n",
       "6 0.319 0.667 0.132 0.716 0.289 0.183 0.587 0.020 0.829 0.005 0.678 0.270   \n",
       "7 0.576 0.592 0.572 0.223 0.953 0.447 0.846 0.699 0.297 0.814 0.397 0.881   \n",
       "8 0.725 0.501 0.956 0.644 0.424 0.606 0.019 0.302 0.660 0.290 0.618 0.429   \n",
       "9 0.591 0.574 0.653 0.652 0.431 0.897 0.368 0.436 0.892 0.806 0.704 0.100   \n",
       "\n",
       "      m     n     o  \n",
       "0 0.568 0.926 0.071  \n",
       "1 0.945 0.522 0.415  \n",
       "2 0.698 0.060 0.667  \n",
       "3 0.253 0.466 0.244  \n",
       "4 0.977 0.605 0.739  \n",
       "5 0.094 0.576 0.929  \n",
       "6 0.735 0.962 0.249  \n",
       "7 0.581 0.882 0.693  \n",
       "8 0.135 0.298 0.570  \n",
       "9 0.919 0.714 0.999  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000000000000   0.000\n",
       "210000000000000   0.000\n",
       "201000000000000   0.000\n",
       "200100000000000   0.000\n",
       "200010000000000   0.000\n",
       "200001000000000   0.000\n",
       "200000100000000   0.000\n",
       "200000010000000   0.000\n",
       "200000001000000   0.000\n",
       "200000000100000   0.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result\n",
       "0   0.149\n",
       "1   0.250\n",
       "2   0.050\n",
       "3  -0.125\n",
       "4  -0.085\n",
       "5   0.198\n",
       "6   0.083\n",
       "7   0.247\n",
       "8   0.587\n",
       "9  -0.159"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T12:59:22.156778Z",
     "start_time": "2021-01-14T12:57:34.187753Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample_' + path_identifier_polynomial_data + '.csv'\n",
    "polynomials_list_df.to_csv(path_polynomials, index=False)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_X_data, 'wb') as f:\n",
    "    pickle.dump(X_data_list, f)#, protocol=2)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_y_data, 'wb') as f:\n",
    "    pickle.dump(y_data_list, f)#, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
