{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Generation for the Training of Î»-Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:49.061308Z",
     "start_time": "2020-09-16T12:26:49.055692Z"
    }
   },
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utilities.DecisionTree_BASIC import SDT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=X_train.shape[1], \n",
    "          output_dim=int(max(y_train))+1, \n",
    "          depth=2,\n",
    "          use_cuda=False,\n",
    "          verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3823,  0.4150, -0.1171,  0.4593],\n",
       "        [-0.1096,  0.1009, -0.2434,  0.2936],\n",
       "        [ 0.4408, -0.3668,  0.4346,  0.0936]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.3694, 0.0677, 0.2411], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0706,  0.3854,  0.0739, -0.2334],\n",
       "        [ 0.1274, -0.2304, -0.0586, -0.2031],\n",
       "        [ 0.3317, -0.3947, -0.2305, -0.1412]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.leaf_nodes.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525d4c0dc17c403d99f25aa61b3bb959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.6984, 0.5194, 0.6359],\n",
      "        [0.7621, 0.5057, 0.7277],\n",
      "        [0.7066, 0.5243, 0.6364],\n",
      "        [0.6737, 0.5255, 0.5357],\n",
      "        [0.7538, 0.4988, 0.7367],\n",
      "        [0.6814, 0.5084, 0.6413],\n",
      "        [0.6722, 0.5237, 0.5426],\n",
      "        [0.6760, 0.5304, 0.5318],\n",
      "        [0.6911, 0.5296, 0.5354],\n",
      "        [0.6379, 0.5133, 0.6332],\n",
      "        [0.7213, 0.5208, 0.6754],\n",
      "        [0.6606, 0.5216, 0.5396],\n",
      "        [0.6511, 0.5190, 0.5553],\n",
      "        [0.6774, 0.5295, 0.5517],\n",
      "        [0.6888, 0.5181, 0.6401],\n",
      "        [0.6694, 0.5234, 0.5414],\n",
      "        [0.6980, 0.5174, 0.6314],\n",
      "        [0.7909, 0.5154, 0.7077],\n",
      "        [0.6502, 0.5262, 0.5319],\n",
      "        [0.7091, 0.5146, 0.6516],\n",
      "        [0.7086, 0.5284, 0.6693],\n",
      "        [0.7086, 0.5298, 0.5213],\n",
      "        [0.7257, 0.5273, 0.6553],\n",
      "        [0.7136, 0.5256, 0.6660],\n",
      "        [0.6676, 0.5027, 0.6626]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.6984],\n",
      "         [0.5194],\n",
      "         [0.6359]],\n",
      "\n",
      "        [[0.7621],\n",
      "         [0.5057],\n",
      "         [0.7277]],\n",
      "\n",
      "        [[0.7066],\n",
      "         [0.5243],\n",
      "         [0.6364]],\n",
      "\n",
      "        [[0.6737],\n",
      "         [0.5255],\n",
      "         [0.5357]],\n",
      "\n",
      "        [[0.7538],\n",
      "         [0.4988],\n",
      "         [0.7367]],\n",
      "\n",
      "        [[0.6814],\n",
      "         [0.5084],\n",
      "         [0.6413]],\n",
      "\n",
      "        [[0.6722],\n",
      "         [0.5237],\n",
      "         [0.5426]],\n",
      "\n",
      "        [[0.6760],\n",
      "         [0.5304],\n",
      "         [0.5318]],\n",
      "\n",
      "        [[0.6911],\n",
      "         [0.5296],\n",
      "         [0.5354]],\n",
      "\n",
      "        [[0.6379],\n",
      "         [0.5133],\n",
      "         [0.6332]],\n",
      "\n",
      "        [[0.7213],\n",
      "         [0.5208],\n",
      "         [0.6754]],\n",
      "\n",
      "        [[0.6606],\n",
      "         [0.5216],\n",
      "         [0.5396]],\n",
      "\n",
      "        [[0.6511],\n",
      "         [0.5190],\n",
      "         [0.5553]],\n",
      "\n",
      "        [[0.6774],\n",
      "         [0.5295],\n",
      "         [0.5517]],\n",
      "\n",
      "        [[0.6888],\n",
      "         [0.5181],\n",
      "         [0.6401]],\n",
      "\n",
      "        [[0.6694],\n",
      "         [0.5234],\n",
      "         [0.5414]],\n",
      "\n",
      "        [[0.6980],\n",
      "         [0.5174],\n",
      "         [0.6314]],\n",
      "\n",
      "        [[0.7909],\n",
      "         [0.5154],\n",
      "         [0.7077]],\n",
      "\n",
      "        [[0.6502],\n",
      "         [0.5262],\n",
      "         [0.5319]],\n",
      "\n",
      "        [[0.7091],\n",
      "         [0.5146],\n",
      "         [0.6516]],\n",
      "\n",
      "        [[0.7086],\n",
      "         [0.5284],\n",
      "         [0.6693]],\n",
      "\n",
      "        [[0.7086],\n",
      "         [0.5298],\n",
      "         [0.5213]],\n",
      "\n",
      "        [[0.7257],\n",
      "         [0.5273],\n",
      "         [0.6553]],\n",
      "\n",
      "        [[0.7136],\n",
      "         [0.5256],\n",
      "         [0.6660]],\n",
      "\n",
      "        [[0.6676],\n",
      "         [0.5027],\n",
      "         [0.6626]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.6984, 0.3016],\n",
      "         [0.5194, 0.4806],\n",
      "         [0.6359, 0.3641]],\n",
      "\n",
      "        [[0.7621, 0.2379],\n",
      "         [0.5057, 0.4943],\n",
      "         [0.7277, 0.2723]],\n",
      "\n",
      "        [[0.7066, 0.2934],\n",
      "         [0.5243, 0.4757],\n",
      "         [0.6364, 0.3636]],\n",
      "\n",
      "        [[0.6737, 0.3263],\n",
      "         [0.5255, 0.4745],\n",
      "         [0.5357, 0.4643]],\n",
      "\n",
      "        [[0.7538, 0.2462],\n",
      "         [0.4988, 0.5012],\n",
      "         [0.7367, 0.2633]],\n",
      "\n",
      "        [[0.6814, 0.3186],\n",
      "         [0.5084, 0.4916],\n",
      "         [0.6413, 0.3587]],\n",
      "\n",
      "        [[0.6722, 0.3278],\n",
      "         [0.5237, 0.4763],\n",
      "         [0.5426, 0.4574]],\n",
      "\n",
      "        [[0.6760, 0.3240],\n",
      "         [0.5304, 0.4696],\n",
      "         [0.5318, 0.4682]],\n",
      "\n",
      "        [[0.6911, 0.3089],\n",
      "         [0.5296, 0.4704],\n",
      "         [0.5354, 0.4646]],\n",
      "\n",
      "        [[0.6379, 0.3621],\n",
      "         [0.5133, 0.4867],\n",
      "         [0.6332, 0.3668]],\n",
      "\n",
      "        [[0.7213, 0.2787],\n",
      "         [0.5208, 0.4792],\n",
      "         [0.6754, 0.3246]],\n",
      "\n",
      "        [[0.6606, 0.3394],\n",
      "         [0.5216, 0.4784],\n",
      "         [0.5396, 0.4604]],\n",
      "\n",
      "        [[0.6511, 0.3489],\n",
      "         [0.5190, 0.4810],\n",
      "         [0.5553, 0.4447]],\n",
      "\n",
      "        [[0.6774, 0.3226],\n",
      "         [0.5295, 0.4705],\n",
      "         [0.5517, 0.4483]],\n",
      "\n",
      "        [[0.6888, 0.3112],\n",
      "         [0.5181, 0.4819],\n",
      "         [0.6401, 0.3599]],\n",
      "\n",
      "        [[0.6694, 0.3306],\n",
      "         [0.5234, 0.4766],\n",
      "         [0.5414, 0.4586]],\n",
      "\n",
      "        [[0.6980, 0.3020],\n",
      "         [0.5174, 0.4826],\n",
      "         [0.6314, 0.3686]],\n",
      "\n",
      "        [[0.7909, 0.2091],\n",
      "         [0.5154, 0.4846],\n",
      "         [0.7077, 0.2923]],\n",
      "\n",
      "        [[0.6502, 0.3498],\n",
      "         [0.5262, 0.4738],\n",
      "         [0.5319, 0.4681]],\n",
      "\n",
      "        [[0.7091, 0.2909],\n",
      "         [0.5146, 0.4854],\n",
      "         [0.6516, 0.3484]],\n",
      "\n",
      "        [[0.7086, 0.2914],\n",
      "         [0.5284, 0.4716],\n",
      "         [0.6693, 0.3307]],\n",
      "\n",
      "        [[0.7086, 0.2914],\n",
      "         [0.5298, 0.4702],\n",
      "         [0.5213, 0.4787]],\n",
      "\n",
      "        [[0.7257, 0.2743],\n",
      "         [0.5273, 0.4727],\n",
      "         [0.6553, 0.3447]],\n",
      "\n",
      "        [[0.7136, 0.2864],\n",
      "         [0.5256, 0.4744],\n",
      "         [0.6660, 0.3340]],\n",
      "\n",
      "        [[0.6676, 0.3324],\n",
      "         [0.5027, 0.4973],\n",
      "         [0.6626, 0.3374]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3628, 0.3356, 0.1918, 0.1098],\n",
      "        [0.3854, 0.3767, 0.1731, 0.0648],\n",
      "        [0.3704, 0.3362, 0.1867, 0.1067],\n",
      "        [0.3540, 0.3197, 0.1748, 0.1515],\n",
      "        [0.3760, 0.3778, 0.1814, 0.0648],\n",
      "        [0.3465, 0.3350, 0.2043, 0.1143],\n",
      "        [0.3521, 0.3202, 0.1778, 0.1499],\n",
      "        [0.3585, 0.3175, 0.1723, 0.1517],\n",
      "        [0.3660, 0.3251, 0.1654, 0.1435],\n",
      "        [0.3275, 0.3104, 0.2293, 0.1328],\n",
      "        [0.3757, 0.3456, 0.1882, 0.0905],\n",
      "        [0.3446, 0.3160, 0.1831, 0.1563],\n",
      "        [0.3379, 0.3132, 0.1937, 0.1552],\n",
      "        [0.3587, 0.3187, 0.1780, 0.1446],\n",
      "        [0.3569, 0.3320, 0.1992, 0.1120],\n",
      "        [0.3504, 0.3190, 0.1790, 0.1516],\n",
      "        [0.3612, 0.3369, 0.1907, 0.1113],\n",
      "        [0.4076, 0.3833, 0.1480, 0.0611],\n",
      "        [0.3421, 0.3081, 0.1861, 0.1637],\n",
      "        [0.3649, 0.3442, 0.1896, 0.1014],\n",
      "        [0.3744, 0.3342, 0.1950, 0.0964],\n",
      "        [0.3754, 0.3332, 0.1519, 0.1395],\n",
      "        [0.3826, 0.3430, 0.1798, 0.0946],\n",
      "        [0.3751, 0.3385, 0.1907, 0.0957],\n",
      "        [0.3356, 0.3320, 0.2203, 0.1122]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3628, 0.3356, 0.1918, 0.1098],\n",
      "        [0.3854, 0.3767, 0.1731, 0.0648],\n",
      "        [0.3704, 0.3362, 0.1867, 0.1067],\n",
      "        [0.3540, 0.3197, 0.1748, 0.1515],\n",
      "        [0.3760, 0.3778, 0.1814, 0.0648],\n",
      "        [0.3465, 0.3350, 0.2043, 0.1143],\n",
      "        [0.3521, 0.3202, 0.1778, 0.1499],\n",
      "        [0.3585, 0.3175, 0.1723, 0.1517],\n",
      "        [0.3660, 0.3251, 0.1654, 0.1435],\n",
      "        [0.3275, 0.3104, 0.2293, 0.1328],\n",
      "        [0.3757, 0.3456, 0.1882, 0.0905],\n",
      "        [0.3446, 0.3160, 0.1831, 0.1563],\n",
      "        [0.3379, 0.3132, 0.1937, 0.1552],\n",
      "        [0.3587, 0.3187, 0.1780, 0.1446],\n",
      "        [0.3569, 0.3320, 0.1992, 0.1120],\n",
      "        [0.3504, 0.3190, 0.1790, 0.1516],\n",
      "        [0.3612, 0.3369, 0.1907, 0.1113],\n",
      "        [0.4076, 0.3833, 0.1480, 0.0611],\n",
      "        [0.3421, 0.3081, 0.1861, 0.1637],\n",
      "        [0.3649, 0.3442, 0.1896, 0.1014],\n",
      "        [0.3744, 0.3342, 0.1950, 0.0964],\n",
      "        [0.3754, 0.3332, 0.1519, 0.1395],\n",
      "        [0.3826, 0.3430, 0.1798, 0.0946],\n",
      "        [0.3751, 0.3385, 0.1907, 0.0957],\n",
      "        [0.3356, 0.3320, 0.2203, 0.1122]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0288,  0.0520,  0.1352],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 00 | Loss: 1.09837 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7043, 0.5264, 0.6311],\n",
      "        [0.7695, 0.5160, 0.7211],\n",
      "        [0.7127, 0.5316, 0.6315],\n",
      "        [0.6780, 0.5304, 0.5340],\n",
      "        [0.7613, 0.5089, 0.7301],\n",
      "        [0.6871, 0.5149, 0.6367],\n",
      "        [0.6765, 0.5286, 0.5406],\n",
      "        [0.6803, 0.5352, 0.5301],\n",
      "        [0.6959, 0.5352, 0.5336],\n",
      "        [0.6425, 0.5183, 0.6286],\n",
      "        [0.7277, 0.5289, 0.6696],\n",
      "        [0.6649, 0.5264, 0.5377],\n",
      "        [0.6551, 0.5234, 0.5530],\n",
      "        [0.6819, 0.5346, 0.5493],\n",
      "        [0.6946, 0.5248, 0.6352],\n",
      "        [0.6737, 0.5283, 0.5395],\n",
      "        [0.7039, 0.5244, 0.6268],\n",
      "        [0.7983, 0.5267, 0.7013],\n",
      "        [0.6540, 0.5304, 0.5302],\n",
      "        [0.7152, 0.5220, 0.6466],\n",
      "        [0.7149, 0.5360, 0.6634],\n",
      "        [0.7134, 0.5357, 0.5200],\n",
      "        [0.7321, 0.5354, 0.6499],\n",
      "        [0.7200, 0.5335, 0.6602],\n",
      "        [0.6730, 0.5088, 0.6575]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7043],\n",
      "         [0.5264],\n",
      "         [0.6311]],\n",
      "\n",
      "        [[0.7695],\n",
      "         [0.5160],\n",
      "         [0.7211]],\n",
      "\n",
      "        [[0.7127],\n",
      "         [0.5316],\n",
      "         [0.6315]],\n",
      "\n",
      "        [[0.6780],\n",
      "         [0.5304],\n",
      "         [0.5340]],\n",
      "\n",
      "        [[0.7613],\n",
      "         [0.5089],\n",
      "         [0.7301]],\n",
      "\n",
      "        [[0.6871],\n",
      "         [0.5149],\n",
      "         [0.6367]],\n",
      "\n",
      "        [[0.6765],\n",
      "         [0.5286],\n",
      "         [0.5406]],\n",
      "\n",
      "        [[0.6803],\n",
      "         [0.5352],\n",
      "         [0.5301]],\n",
      "\n",
      "        [[0.6959],\n",
      "         [0.5352],\n",
      "         [0.5336]],\n",
      "\n",
      "        [[0.6425],\n",
      "         [0.5183],\n",
      "         [0.6286]],\n",
      "\n",
      "        [[0.7277],\n",
      "         [0.5289],\n",
      "         [0.6696]],\n",
      "\n",
      "        [[0.6649],\n",
      "         [0.5264],\n",
      "         [0.5377]],\n",
      "\n",
      "        [[0.6551],\n",
      "         [0.5234],\n",
      "         [0.5530]],\n",
      "\n",
      "        [[0.6819],\n",
      "         [0.5346],\n",
      "         [0.5493]],\n",
      "\n",
      "        [[0.6946],\n",
      "         [0.5248],\n",
      "         [0.6352]],\n",
      "\n",
      "        [[0.6737],\n",
      "         [0.5283],\n",
      "         [0.5395]],\n",
      "\n",
      "        [[0.7039],\n",
      "         [0.5244],\n",
      "         [0.6268]],\n",
      "\n",
      "        [[0.7983],\n",
      "         [0.5267],\n",
      "         [0.7013]],\n",
      "\n",
      "        [[0.6540],\n",
      "         [0.5304],\n",
      "         [0.5302]],\n",
      "\n",
      "        [[0.7152],\n",
      "         [0.5220],\n",
      "         [0.6466]],\n",
      "\n",
      "        [[0.7149],\n",
      "         [0.5360],\n",
      "         [0.6634]],\n",
      "\n",
      "        [[0.7134],\n",
      "         [0.5357],\n",
      "         [0.5200]],\n",
      "\n",
      "        [[0.7321],\n",
      "         [0.5354],\n",
      "         [0.6499]],\n",
      "\n",
      "        [[0.7200],\n",
      "         [0.5335],\n",
      "         [0.6602]],\n",
      "\n",
      "        [[0.6730],\n",
      "         [0.5088],\n",
      "         [0.6575]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7043, 0.2957],\n",
      "         [0.5264, 0.4736],\n",
      "         [0.6311, 0.3689]],\n",
      "\n",
      "        [[0.7695, 0.2305],\n",
      "         [0.5160, 0.4840],\n",
      "         [0.7211, 0.2789]],\n",
      "\n",
      "        [[0.7127, 0.2873],\n",
      "         [0.5316, 0.4684],\n",
      "         [0.6315, 0.3685]],\n",
      "\n",
      "        [[0.6780, 0.3220],\n",
      "         [0.5304, 0.4696],\n",
      "         [0.5340, 0.4660]],\n",
      "\n",
      "        [[0.7613, 0.2387],\n",
      "         [0.5089, 0.4911],\n",
      "         [0.7301, 0.2699]],\n",
      "\n",
      "        [[0.6871, 0.3129],\n",
      "         [0.5149, 0.4851],\n",
      "         [0.6367, 0.3633]],\n",
      "\n",
      "        [[0.6765, 0.3235],\n",
      "         [0.5286, 0.4714],\n",
      "         [0.5406, 0.4594]],\n",
      "\n",
      "        [[0.6803, 0.3197],\n",
      "         [0.5352, 0.4648],\n",
      "         [0.5301, 0.4699]],\n",
      "\n",
      "        [[0.6959, 0.3041],\n",
      "         [0.5352, 0.4648],\n",
      "         [0.5336, 0.4664]],\n",
      "\n",
      "        [[0.6425, 0.3575],\n",
      "         [0.5183, 0.4817],\n",
      "         [0.6286, 0.3714]],\n",
      "\n",
      "        [[0.7277, 0.2723],\n",
      "         [0.5289, 0.4711],\n",
      "         [0.6696, 0.3304]],\n",
      "\n",
      "        [[0.6649, 0.3351],\n",
      "         [0.5264, 0.4736],\n",
      "         [0.5377, 0.4623]],\n",
      "\n",
      "        [[0.6551, 0.3449],\n",
      "         [0.5234, 0.4766],\n",
      "         [0.5530, 0.4470]],\n",
      "\n",
      "        [[0.6819, 0.3181],\n",
      "         [0.5346, 0.4654],\n",
      "         [0.5493, 0.4507]],\n",
      "\n",
      "        [[0.6946, 0.3054],\n",
      "         [0.5248, 0.4752],\n",
      "         [0.6352, 0.3648]],\n",
      "\n",
      "        [[0.6737, 0.3263],\n",
      "         [0.5283, 0.4717],\n",
      "         [0.5395, 0.4605]],\n",
      "\n",
      "        [[0.7039, 0.2961],\n",
      "         [0.5244, 0.4756],\n",
      "         [0.6268, 0.3732]],\n",
      "\n",
      "        [[0.7983, 0.2017],\n",
      "         [0.5267, 0.4733],\n",
      "         [0.7013, 0.2987]],\n",
      "\n",
      "        [[0.6540, 0.3460],\n",
      "         [0.5304, 0.4696],\n",
      "         [0.5302, 0.4698]],\n",
      "\n",
      "        [[0.7152, 0.2848],\n",
      "         [0.5220, 0.4780],\n",
      "         [0.6466, 0.3534]],\n",
      "\n",
      "        [[0.7149, 0.2851],\n",
      "         [0.5360, 0.4640],\n",
      "         [0.6634, 0.3366]],\n",
      "\n",
      "        [[0.7134, 0.2866],\n",
      "         [0.5357, 0.4643],\n",
      "         [0.5200, 0.4800]],\n",
      "\n",
      "        [[0.7321, 0.2679],\n",
      "         [0.5354, 0.4646],\n",
      "         [0.6499, 0.3501]],\n",
      "\n",
      "        [[0.7200, 0.2800],\n",
      "         [0.5335, 0.4665],\n",
      "         [0.6602, 0.3398]],\n",
      "\n",
      "        [[0.6730, 0.3270],\n",
      "         [0.5088, 0.4912],\n",
      "         [0.6575, 0.3425]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3708, 0.3335, 0.1866, 0.1091],\n",
      "        [0.3971, 0.3724, 0.1662, 0.0643],\n",
      "        [0.3789, 0.3338, 0.1815, 0.1059],\n",
      "        [0.3596, 0.3184, 0.1719, 0.1501],\n",
      "        [0.3874, 0.3739, 0.1743, 0.0644],\n",
      "        [0.3538, 0.3333, 0.1992, 0.1137],\n",
      "        [0.3576, 0.3189, 0.1749, 0.1486],\n",
      "        [0.3641, 0.3162, 0.1695, 0.1503],\n",
      "        [0.3724, 0.3234, 0.1623, 0.1419],\n",
      "        [0.3330, 0.3095, 0.2247, 0.1328],\n",
      "        [0.3848, 0.3428, 0.1823, 0.0900],\n",
      "        [0.3500, 0.3149, 0.1802, 0.1549],\n",
      "        [0.3428, 0.3122, 0.1907, 0.1542],\n",
      "        [0.3645, 0.3173, 0.1748, 0.1434],\n",
      "        [0.3645, 0.3301, 0.1940, 0.1114],\n",
      "        [0.3559, 0.3178, 0.1760, 0.1503],\n",
      "        [0.3692, 0.3348, 0.1856, 0.1105],\n",
      "        [0.4205, 0.3778, 0.1415, 0.0602],\n",
      "        [0.3469, 0.3072, 0.1834, 0.1625],\n",
      "        [0.3733, 0.3418, 0.1842, 0.1007],\n",
      "        [0.3832, 0.3317, 0.1891, 0.0960],\n",
      "        [0.3822, 0.3312, 0.1490, 0.1376],\n",
      "        [0.3919, 0.3402, 0.1741, 0.0938],\n",
      "        [0.3841, 0.3359, 0.1849, 0.0951],\n",
      "        [0.3424, 0.3306, 0.2150, 0.1120]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3708, 0.3335, 0.1866, 0.1091],\n",
      "        [0.3971, 0.3724, 0.1662, 0.0643],\n",
      "        [0.3789, 0.3338, 0.1815, 0.1059],\n",
      "        [0.3596, 0.3184, 0.1719, 0.1501],\n",
      "        [0.3874, 0.3739, 0.1743, 0.0644],\n",
      "        [0.3538, 0.3333, 0.1992, 0.1137],\n",
      "        [0.3576, 0.3189, 0.1749, 0.1486],\n",
      "        [0.3641, 0.3162, 0.1695, 0.1503],\n",
      "        [0.3724, 0.3234, 0.1623, 0.1419],\n",
      "        [0.3330, 0.3095, 0.2247, 0.1328],\n",
      "        [0.3848, 0.3428, 0.1823, 0.0900],\n",
      "        [0.3500, 0.3149, 0.1802, 0.1549],\n",
      "        [0.3428, 0.3122, 0.1907, 0.1542],\n",
      "        [0.3645, 0.3173, 0.1748, 0.1434],\n",
      "        [0.3645, 0.3301, 0.1940, 0.1114],\n",
      "        [0.3559, 0.3178, 0.1760, 0.1503],\n",
      "        [0.3692, 0.3348, 0.1856, 0.1105],\n",
      "        [0.4205, 0.3778, 0.1415, 0.0602],\n",
      "        [0.3469, 0.3072, 0.1834, 0.1625],\n",
      "        [0.3733, 0.3418, 0.1842, 0.1007],\n",
      "        [0.3832, 0.3317, 0.1891, 0.0960],\n",
      "        [0.3822, 0.3312, 0.1490, 0.1376],\n",
      "        [0.3919, 0.3402, 0.1741, 0.0938],\n",
      "        [0.3841, 0.3359, 0.1849, 0.0951],\n",
      "        [0.3424, 0.3306, 0.2150, 0.1120]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0339,  0.0494,  0.1437],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 01 | Loss: 1.09807 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7101, 0.5334, 0.6263],\n",
      "        [0.7767, 0.5263, 0.7145],\n",
      "        [0.7187, 0.5390, 0.6265],\n",
      "        [0.6822, 0.5353, 0.5322],\n",
      "        [0.7685, 0.5190, 0.7235],\n",
      "        [0.6927, 0.5215, 0.6320],\n",
      "        [0.6807, 0.5334, 0.5387],\n",
      "        [0.6845, 0.5401, 0.5283],\n",
      "        [0.7006, 0.5408, 0.5317],\n",
      "        [0.6471, 0.5233, 0.6239],\n",
      "        [0.7340, 0.5369, 0.6637],\n",
      "        [0.6691, 0.5312, 0.5358],\n",
      "        [0.6590, 0.5277, 0.5507],\n",
      "        [0.6863, 0.5397, 0.5469],\n",
      "        [0.7003, 0.5315, 0.6303],\n",
      "        [0.6779, 0.5331, 0.5376],\n",
      "        [0.7097, 0.5314, 0.6222],\n",
      "        [0.8055, 0.5380, 0.6949],\n",
      "        [0.6579, 0.5346, 0.5285],\n",
      "        [0.7211, 0.5294, 0.6415],\n",
      "        [0.7211, 0.5436, 0.6574],\n",
      "        [0.7182, 0.5416, 0.5186],\n",
      "        [0.7384, 0.5434, 0.6444],\n",
      "        [0.7263, 0.5413, 0.6544],\n",
      "        [0.6783, 0.5149, 0.6523]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7101],\n",
      "         [0.5334],\n",
      "         [0.6263]],\n",
      "\n",
      "        [[0.7767],\n",
      "         [0.5263],\n",
      "         [0.7145]],\n",
      "\n",
      "        [[0.7187],\n",
      "         [0.5390],\n",
      "         [0.6265]],\n",
      "\n",
      "        [[0.6822],\n",
      "         [0.5353],\n",
      "         [0.5322]],\n",
      "\n",
      "        [[0.7685],\n",
      "         [0.5190],\n",
      "         [0.7235]],\n",
      "\n",
      "        [[0.6927],\n",
      "         [0.5215],\n",
      "         [0.6320]],\n",
      "\n",
      "        [[0.6807],\n",
      "         [0.5334],\n",
      "         [0.5387]],\n",
      "\n",
      "        [[0.6845],\n",
      "         [0.5401],\n",
      "         [0.5283]],\n",
      "\n",
      "        [[0.7006],\n",
      "         [0.5408],\n",
      "         [0.5317]],\n",
      "\n",
      "        [[0.6471],\n",
      "         [0.5233],\n",
      "         [0.6239]],\n",
      "\n",
      "        [[0.7340],\n",
      "         [0.5369],\n",
      "         [0.6637]],\n",
      "\n",
      "        [[0.6691],\n",
      "         [0.5312],\n",
      "         [0.5358]],\n",
      "\n",
      "        [[0.6590],\n",
      "         [0.5277],\n",
      "         [0.5507]],\n",
      "\n",
      "        [[0.6863],\n",
      "         [0.5397],\n",
      "         [0.5469]],\n",
      "\n",
      "        [[0.7003],\n",
      "         [0.5315],\n",
      "         [0.6303]],\n",
      "\n",
      "        [[0.6779],\n",
      "         [0.5331],\n",
      "         [0.5376]],\n",
      "\n",
      "        [[0.7097],\n",
      "         [0.5314],\n",
      "         [0.6222]],\n",
      "\n",
      "        [[0.8055],\n",
      "         [0.5380],\n",
      "         [0.6949]],\n",
      "\n",
      "        [[0.6579],\n",
      "         [0.5346],\n",
      "         [0.5285]],\n",
      "\n",
      "        [[0.7211],\n",
      "         [0.5294],\n",
      "         [0.6415]],\n",
      "\n",
      "        [[0.7211],\n",
      "         [0.5436],\n",
      "         [0.6574]],\n",
      "\n",
      "        [[0.7182],\n",
      "         [0.5416],\n",
      "         [0.5186]],\n",
      "\n",
      "        [[0.7384],\n",
      "         [0.5434],\n",
      "         [0.6444]],\n",
      "\n",
      "        [[0.7263],\n",
      "         [0.5413],\n",
      "         [0.6544]],\n",
      "\n",
      "        [[0.6783],\n",
      "         [0.5149],\n",
      "         [0.6523]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7101, 0.2899],\n",
      "         [0.5334, 0.4666],\n",
      "         [0.6263, 0.3737]],\n",
      "\n",
      "        [[0.7767, 0.2233],\n",
      "         [0.5263, 0.4737],\n",
      "         [0.7145, 0.2855]],\n",
      "\n",
      "        [[0.7187, 0.2813],\n",
      "         [0.5390, 0.4610],\n",
      "         [0.6265, 0.3735]],\n",
      "\n",
      "        [[0.6822, 0.3178],\n",
      "         [0.5353, 0.4647],\n",
      "         [0.5322, 0.4678]],\n",
      "\n",
      "        [[0.7685, 0.2315],\n",
      "         [0.5190, 0.4810],\n",
      "         [0.7235, 0.2765]],\n",
      "\n",
      "        [[0.6927, 0.3073],\n",
      "         [0.5215, 0.4785],\n",
      "         [0.6320, 0.3680]],\n",
      "\n",
      "        [[0.6807, 0.3193],\n",
      "         [0.5334, 0.4666],\n",
      "         [0.5387, 0.4613]],\n",
      "\n",
      "        [[0.6845, 0.3155],\n",
      "         [0.5401, 0.4599],\n",
      "         [0.5283, 0.4717]],\n",
      "\n",
      "        [[0.7006, 0.2994],\n",
      "         [0.5408, 0.4592],\n",
      "         [0.5317, 0.4683]],\n",
      "\n",
      "        [[0.6471, 0.3529],\n",
      "         [0.5233, 0.4767],\n",
      "         [0.6239, 0.3761]],\n",
      "\n",
      "        [[0.7340, 0.2660],\n",
      "         [0.5369, 0.4631],\n",
      "         [0.6637, 0.3363]],\n",
      "\n",
      "        [[0.6691, 0.3309],\n",
      "         [0.5312, 0.4688],\n",
      "         [0.5358, 0.4642]],\n",
      "\n",
      "        [[0.6590, 0.3410],\n",
      "         [0.5277, 0.4723],\n",
      "         [0.5507, 0.4493]],\n",
      "\n",
      "        [[0.6863, 0.3137],\n",
      "         [0.5397, 0.4603],\n",
      "         [0.5469, 0.4531]],\n",
      "\n",
      "        [[0.7003, 0.2997],\n",
      "         [0.5315, 0.4685],\n",
      "         [0.6303, 0.3697]],\n",
      "\n",
      "        [[0.6779, 0.3221],\n",
      "         [0.5331, 0.4669],\n",
      "         [0.5376, 0.4624]],\n",
      "\n",
      "        [[0.7097, 0.2903],\n",
      "         [0.5314, 0.4686],\n",
      "         [0.6222, 0.3778]],\n",
      "\n",
      "        [[0.8055, 0.1945],\n",
      "         [0.5380, 0.4620],\n",
      "         [0.6949, 0.3051]],\n",
      "\n",
      "        [[0.6579, 0.3421],\n",
      "         [0.5346, 0.4654],\n",
      "         [0.5285, 0.4715]],\n",
      "\n",
      "        [[0.7211, 0.2789],\n",
      "         [0.5294, 0.4706],\n",
      "         [0.6415, 0.3585]],\n",
      "\n",
      "        [[0.7211, 0.2789],\n",
      "         [0.5436, 0.4564],\n",
      "         [0.6574, 0.3426]],\n",
      "\n",
      "        [[0.7182, 0.2818],\n",
      "         [0.5416, 0.4584],\n",
      "         [0.5186, 0.4814]],\n",
      "\n",
      "        [[0.7384, 0.2616],\n",
      "         [0.5434, 0.4566],\n",
      "         [0.6444, 0.3556]],\n",
      "\n",
      "        [[0.7263, 0.2737],\n",
      "         [0.5413, 0.4587],\n",
      "         [0.6544, 0.3456]],\n",
      "\n",
      "        [[0.6783, 0.3217],\n",
      "         [0.5149, 0.4851],\n",
      "         [0.6523, 0.3477]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3788, 0.3313, 0.1816, 0.1083],\n",
      "        [0.4088, 0.3679, 0.1595, 0.0638],\n",
      "        [0.3873, 0.3313, 0.1763, 0.1051],\n",
      "        [0.3652, 0.3171, 0.1691, 0.1486],\n",
      "        [0.3988, 0.3697, 0.1675, 0.0640],\n",
      "        [0.3612, 0.3315, 0.1942, 0.1131],\n",
      "        [0.3631, 0.3176, 0.1720, 0.1473],\n",
      "        [0.3697, 0.3148, 0.1667, 0.1488],\n",
      "        [0.3789, 0.3217, 0.1592, 0.1402],\n",
      "        [0.3386, 0.3085, 0.2202, 0.1327],\n",
      "        [0.3941, 0.3399, 0.1765, 0.0894],\n",
      "        [0.3554, 0.3137, 0.1773, 0.1536],\n",
      "        [0.3478, 0.3112, 0.1878, 0.1532],\n",
      "        [0.3704, 0.3159, 0.1716, 0.1421],\n",
      "        [0.3722, 0.3280, 0.1889, 0.1108],\n",
      "        [0.3614, 0.3165, 0.1731, 0.1489],\n",
      "        [0.3772, 0.3326, 0.1806, 0.1097],\n",
      "        [0.4334, 0.3721, 0.1352, 0.0593],\n",
      "        [0.3517, 0.3062, 0.1808, 0.1613],\n",
      "        [0.3818, 0.3394, 0.1789, 0.1000],\n",
      "        [0.3920, 0.3291, 0.1833, 0.0955],\n",
      "        [0.3890, 0.3293, 0.1461, 0.1356],\n",
      "        [0.4012, 0.3371, 0.1686, 0.0930],\n",
      "        [0.3932, 0.3331, 0.1791, 0.0946],\n",
      "        [0.3492, 0.3291, 0.2099, 0.1118]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3788, 0.3313, 0.1816, 0.1083],\n",
      "        [0.4088, 0.3679, 0.1595, 0.0638],\n",
      "        [0.3873, 0.3313, 0.1763, 0.1051],\n",
      "        [0.3652, 0.3171, 0.1691, 0.1486],\n",
      "        [0.3988, 0.3697, 0.1675, 0.0640],\n",
      "        [0.3612, 0.3315, 0.1942, 0.1131],\n",
      "        [0.3631, 0.3176, 0.1720, 0.1473],\n",
      "        [0.3697, 0.3148, 0.1667, 0.1488],\n",
      "        [0.3789, 0.3217, 0.1592, 0.1402],\n",
      "        [0.3386, 0.3085, 0.2202, 0.1327],\n",
      "        [0.3941, 0.3399, 0.1765, 0.0894],\n",
      "        [0.3554, 0.3137, 0.1773, 0.1536],\n",
      "        [0.3478, 0.3112, 0.1878, 0.1532],\n",
      "        [0.3704, 0.3159, 0.1716, 0.1421],\n",
      "        [0.3722, 0.3280, 0.1889, 0.1108],\n",
      "        [0.3614, 0.3165, 0.1731, 0.1489],\n",
      "        [0.3772, 0.3326, 0.1806, 0.1097],\n",
      "        [0.4334, 0.3721, 0.1352, 0.0593],\n",
      "        [0.3517, 0.3062, 0.1808, 0.1613],\n",
      "        [0.3818, 0.3394, 0.1789, 0.1000],\n",
      "        [0.3920, 0.3291, 0.1833, 0.0955],\n",
      "        [0.3890, 0.3293, 0.1461, 0.1356],\n",
      "        [0.4012, 0.3371, 0.1686, 0.0930],\n",
      "        [0.3932, 0.3331, 0.1791, 0.0946],\n",
      "        [0.3492, 0.3291, 0.2099, 0.1118]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0393,  0.0466,  0.1524],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 02 | Loss: 1.09775 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7159, 0.5404, 0.6215],\n",
      "        [0.7838, 0.5366, 0.7078],\n",
      "        [0.7246, 0.5463, 0.6216],\n",
      "        [0.6865, 0.5401, 0.5305],\n",
      "        [0.7756, 0.5291, 0.7167],\n",
      "        [0.6982, 0.5280, 0.6273],\n",
      "        [0.6849, 0.5382, 0.5368],\n",
      "        [0.6887, 0.5449, 0.5266],\n",
      "        [0.7053, 0.5464, 0.5298],\n",
      "        [0.6516, 0.5283, 0.6192],\n",
      "        [0.7403, 0.5449, 0.6578],\n",
      "        [0.6733, 0.5359, 0.5340],\n",
      "        [0.6630, 0.5321, 0.5485],\n",
      "        [0.6907, 0.5448, 0.5445],\n",
      "        [0.7059, 0.5383, 0.6254],\n",
      "        [0.6821, 0.5379, 0.5357],\n",
      "        [0.7155, 0.5384, 0.6176],\n",
      "        [0.8125, 0.5493, 0.6885],\n",
      "        [0.6617, 0.5388, 0.5267],\n",
      "        [0.7271, 0.5368, 0.6364],\n",
      "        [0.7273, 0.5512, 0.6515],\n",
      "        [0.7230, 0.5474, 0.5173],\n",
      "        [0.7446, 0.5515, 0.6389],\n",
      "        [0.7325, 0.5492, 0.6486],\n",
      "        [0.6836, 0.5210, 0.6472]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7159],\n",
      "         [0.5404],\n",
      "         [0.6215]],\n",
      "\n",
      "        [[0.7838],\n",
      "         [0.5366],\n",
      "         [0.7078]],\n",
      "\n",
      "        [[0.7246],\n",
      "         [0.5463],\n",
      "         [0.6216]],\n",
      "\n",
      "        [[0.6865],\n",
      "         [0.5401],\n",
      "         [0.5305]],\n",
      "\n",
      "        [[0.7756],\n",
      "         [0.5291],\n",
      "         [0.7167]],\n",
      "\n",
      "        [[0.6982],\n",
      "         [0.5280],\n",
      "         [0.6273]],\n",
      "\n",
      "        [[0.6849],\n",
      "         [0.5382],\n",
      "         [0.5368]],\n",
      "\n",
      "        [[0.6887],\n",
      "         [0.5449],\n",
      "         [0.5266]],\n",
      "\n",
      "        [[0.7053],\n",
      "         [0.5464],\n",
      "         [0.5298]],\n",
      "\n",
      "        [[0.6516],\n",
      "         [0.5283],\n",
      "         [0.6192]],\n",
      "\n",
      "        [[0.7403],\n",
      "         [0.5449],\n",
      "         [0.6578]],\n",
      "\n",
      "        [[0.6733],\n",
      "         [0.5359],\n",
      "         [0.5340]],\n",
      "\n",
      "        [[0.6630],\n",
      "         [0.5321],\n",
      "         [0.5485]],\n",
      "\n",
      "        [[0.6907],\n",
      "         [0.5448],\n",
      "         [0.5445]],\n",
      "\n",
      "        [[0.7059],\n",
      "         [0.5383],\n",
      "         [0.6254]],\n",
      "\n",
      "        [[0.6821],\n",
      "         [0.5379],\n",
      "         [0.5357]],\n",
      "\n",
      "        [[0.7155],\n",
      "         [0.5384],\n",
      "         [0.6176]],\n",
      "\n",
      "        [[0.8125],\n",
      "         [0.5493],\n",
      "         [0.6885]],\n",
      "\n",
      "        [[0.6617],\n",
      "         [0.5388],\n",
      "         [0.5267]],\n",
      "\n",
      "        [[0.7271],\n",
      "         [0.5368],\n",
      "         [0.6364]],\n",
      "\n",
      "        [[0.7273],\n",
      "         [0.5512],\n",
      "         [0.6515]],\n",
      "\n",
      "        [[0.7230],\n",
      "         [0.5474],\n",
      "         [0.5173]],\n",
      "\n",
      "        [[0.7446],\n",
      "         [0.5515],\n",
      "         [0.6389]],\n",
      "\n",
      "        [[0.7325],\n",
      "         [0.5492],\n",
      "         [0.6486]],\n",
      "\n",
      "        [[0.6836],\n",
      "         [0.5210],\n",
      "         [0.6472]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7159, 0.2841],\n",
      "         [0.5404, 0.4596],\n",
      "         [0.6215, 0.3785]],\n",
      "\n",
      "        [[0.7838, 0.2162],\n",
      "         [0.5366, 0.4634],\n",
      "         [0.7078, 0.2922]],\n",
      "\n",
      "        [[0.7246, 0.2754],\n",
      "         [0.5463, 0.4537],\n",
      "         [0.6216, 0.3784]],\n",
      "\n",
      "        [[0.6865, 0.3135],\n",
      "         [0.5401, 0.4599],\n",
      "         [0.5305, 0.4695]],\n",
      "\n",
      "        [[0.7756, 0.2244],\n",
      "         [0.5291, 0.4709],\n",
      "         [0.7167, 0.2833]],\n",
      "\n",
      "        [[0.6982, 0.3018],\n",
      "         [0.5280, 0.4720],\n",
      "         [0.6273, 0.3727]],\n",
      "\n",
      "        [[0.6849, 0.3151],\n",
      "         [0.5382, 0.4618],\n",
      "         [0.5368, 0.4632]],\n",
      "\n",
      "        [[0.6887, 0.3113],\n",
      "         [0.5449, 0.4551],\n",
      "         [0.5266, 0.4734]],\n",
      "\n",
      "        [[0.7053, 0.2947],\n",
      "         [0.5464, 0.4536],\n",
      "         [0.5298, 0.4702]],\n",
      "\n",
      "        [[0.6516, 0.3484],\n",
      "         [0.5283, 0.4717],\n",
      "         [0.6192, 0.3808]],\n",
      "\n",
      "        [[0.7403, 0.2597],\n",
      "         [0.5449, 0.4551],\n",
      "         [0.6578, 0.3422]],\n",
      "\n",
      "        [[0.6733, 0.3267],\n",
      "         [0.5359, 0.4641],\n",
      "         [0.5340, 0.4660]],\n",
      "\n",
      "        [[0.6630, 0.3370],\n",
      "         [0.5321, 0.4679],\n",
      "         [0.5485, 0.4515]],\n",
      "\n",
      "        [[0.6907, 0.3093],\n",
      "         [0.5448, 0.4552],\n",
      "         [0.5445, 0.4555]],\n",
      "\n",
      "        [[0.7059, 0.2941],\n",
      "         [0.5383, 0.4617],\n",
      "         [0.6254, 0.3746]],\n",
      "\n",
      "        [[0.6821, 0.3179],\n",
      "         [0.5379, 0.4621],\n",
      "         [0.5357, 0.4643]],\n",
      "\n",
      "        [[0.7155, 0.2845],\n",
      "         [0.5384, 0.4616],\n",
      "         [0.6176, 0.3824]],\n",
      "\n",
      "        [[0.8125, 0.1875],\n",
      "         [0.5493, 0.4507],\n",
      "         [0.6885, 0.3115]],\n",
      "\n",
      "        [[0.6617, 0.3383],\n",
      "         [0.5388, 0.4612],\n",
      "         [0.5267, 0.4733]],\n",
      "\n",
      "        [[0.7271, 0.2729],\n",
      "         [0.5368, 0.4632],\n",
      "         [0.6364, 0.3636]],\n",
      "\n",
      "        [[0.7273, 0.2727],\n",
      "         [0.5512, 0.4488],\n",
      "         [0.6515, 0.3485]],\n",
      "\n",
      "        [[0.7230, 0.2770],\n",
      "         [0.5474, 0.4526],\n",
      "         [0.5173, 0.4827]],\n",
      "\n",
      "        [[0.7446, 0.2554],\n",
      "         [0.5515, 0.4485],\n",
      "         [0.6389, 0.3611]],\n",
      "\n",
      "        [[0.7325, 0.2675],\n",
      "         [0.5492, 0.4508],\n",
      "         [0.6486, 0.3514]],\n",
      "\n",
      "        [[0.6836, 0.3164],\n",
      "         [0.5210, 0.4790],\n",
      "         [0.6472, 0.3528]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3869, 0.3290, 0.1766, 0.1075],\n",
      "        [0.4206, 0.3632, 0.1530, 0.0632],\n",
      "        [0.3959, 0.3287, 0.1712, 0.1042],\n",
      "        [0.3708, 0.3157, 0.1663, 0.1472],\n",
      "        [0.4104, 0.3653, 0.1608, 0.0636],\n",
      "        [0.3686, 0.3296, 0.1893, 0.1125],\n",
      "        [0.3687, 0.3163, 0.1691, 0.1459],\n",
      "        [0.3753, 0.3134, 0.1639, 0.1474],\n",
      "        [0.3854, 0.3199, 0.1561, 0.1386],\n",
      "        [0.3442, 0.3074, 0.2157, 0.1326],\n",
      "        [0.4034, 0.3369, 0.1709, 0.0889],\n",
      "        [0.3609, 0.3125, 0.1744, 0.1522],\n",
      "        [0.3528, 0.3102, 0.1849, 0.1522],\n",
      "        [0.3763, 0.3144, 0.1684, 0.1409],\n",
      "        [0.3800, 0.3259, 0.1839, 0.1102],\n",
      "        [0.3669, 0.3152, 0.1703, 0.1476],\n",
      "        [0.3852, 0.3303, 0.1757, 0.1088],\n",
      "        [0.4463, 0.3662, 0.1291, 0.0584],\n",
      "        [0.3565, 0.3051, 0.1782, 0.1601],\n",
      "        [0.3903, 0.3368, 0.1737, 0.0992],\n",
      "        [0.4009, 0.3264, 0.1777, 0.0951],\n",
      "        [0.3958, 0.3272, 0.1433, 0.1337],\n",
      "        [0.4106, 0.3340, 0.1632, 0.0922],\n",
      "        [0.4023, 0.3303, 0.1735, 0.0940],\n",
      "        [0.3562, 0.3275, 0.2048, 0.1116]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3869, 0.3290, 0.1766, 0.1075],\n",
      "        [0.4206, 0.3632, 0.1530, 0.0632],\n",
      "        [0.3959, 0.3287, 0.1712, 0.1042],\n",
      "        [0.3708, 0.3157, 0.1663, 0.1472],\n",
      "        [0.4104, 0.3653, 0.1608, 0.0636],\n",
      "        [0.3686, 0.3296, 0.1893, 0.1125],\n",
      "        [0.3687, 0.3163, 0.1691, 0.1459],\n",
      "        [0.3753, 0.3134, 0.1639, 0.1474],\n",
      "        [0.3854, 0.3199, 0.1561, 0.1386],\n",
      "        [0.3442, 0.3074, 0.2157, 0.1326],\n",
      "        [0.4034, 0.3369, 0.1709, 0.0889],\n",
      "        [0.3609, 0.3125, 0.1744, 0.1522],\n",
      "        [0.3528, 0.3102, 0.1849, 0.1522],\n",
      "        [0.3763, 0.3144, 0.1684, 0.1409],\n",
      "        [0.3800, 0.3259, 0.1839, 0.1102],\n",
      "        [0.3669, 0.3152, 0.1703, 0.1476],\n",
      "        [0.3852, 0.3303, 0.1757, 0.1088],\n",
      "        [0.4463, 0.3662, 0.1291, 0.0584],\n",
      "        [0.3565, 0.3051, 0.1782, 0.1601],\n",
      "        [0.3903, 0.3368, 0.1737, 0.0992],\n",
      "        [0.4009, 0.3264, 0.1777, 0.0951],\n",
      "        [0.3958, 0.3272, 0.1433, 0.1337],\n",
      "        [0.4106, 0.3340, 0.1632, 0.0922],\n",
      "        [0.4023, 0.3303, 0.1735, 0.0940],\n",
      "        [0.3562, 0.3275, 0.2048, 0.1116]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0449,  0.0435,  0.1614],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 03 | Loss: 1.09742 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7216, 0.5474, 0.6167],\n",
      "        [0.7907, 0.5469, 0.7010],\n",
      "        [0.7305, 0.5537, 0.6166],\n",
      "        [0.6907, 0.5450, 0.5287],\n",
      "        [0.7826, 0.5392, 0.7099],\n",
      "        [0.7037, 0.5345, 0.6226],\n",
      "        [0.6891, 0.5431, 0.5349],\n",
      "        [0.6929, 0.5498, 0.5248],\n",
      "        [0.7100, 0.5520, 0.5280],\n",
      "        [0.6562, 0.5333, 0.6146],\n",
      "        [0.7464, 0.5529, 0.6519],\n",
      "        [0.6776, 0.5407, 0.5321],\n",
      "        [0.6669, 0.5365, 0.5462],\n",
      "        [0.6951, 0.5499, 0.5422],\n",
      "        [0.7115, 0.5450, 0.6205],\n",
      "        [0.6863, 0.5427, 0.5338],\n",
      "        [0.7212, 0.5454, 0.6129],\n",
      "        [0.8194, 0.5606, 0.6820],\n",
      "        [0.6655, 0.5431, 0.5250],\n",
      "        [0.7329, 0.5442, 0.6313],\n",
      "        [0.7333, 0.5589, 0.6455],\n",
      "        [0.7277, 0.5533, 0.5160],\n",
      "        [0.7508, 0.5595, 0.6334],\n",
      "        [0.7387, 0.5570, 0.6428],\n",
      "        [0.6889, 0.5271, 0.6420]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7216],\n",
      "         [0.5474],\n",
      "         [0.6167]],\n",
      "\n",
      "        [[0.7907],\n",
      "         [0.5469],\n",
      "         [0.7010]],\n",
      "\n",
      "        [[0.7305],\n",
      "         [0.5537],\n",
      "         [0.6166]],\n",
      "\n",
      "        [[0.6907],\n",
      "         [0.5450],\n",
      "         [0.5287]],\n",
      "\n",
      "        [[0.7826],\n",
      "         [0.5392],\n",
      "         [0.7099]],\n",
      "\n",
      "        [[0.7037],\n",
      "         [0.5345],\n",
      "         [0.6226]],\n",
      "\n",
      "        [[0.6891],\n",
      "         [0.5431],\n",
      "         [0.5349]],\n",
      "\n",
      "        [[0.6929],\n",
      "         [0.5498],\n",
      "         [0.5248]],\n",
      "\n",
      "        [[0.7100],\n",
      "         [0.5520],\n",
      "         [0.5280]],\n",
      "\n",
      "        [[0.6562],\n",
      "         [0.5333],\n",
      "         [0.6146]],\n",
      "\n",
      "        [[0.7464],\n",
      "         [0.5529],\n",
      "         [0.6519]],\n",
      "\n",
      "        [[0.6776],\n",
      "         [0.5407],\n",
      "         [0.5321]],\n",
      "\n",
      "        [[0.6669],\n",
      "         [0.5365],\n",
      "         [0.5462]],\n",
      "\n",
      "        [[0.6951],\n",
      "         [0.5499],\n",
      "         [0.5422]],\n",
      "\n",
      "        [[0.7115],\n",
      "         [0.5450],\n",
      "         [0.6205]],\n",
      "\n",
      "        [[0.6863],\n",
      "         [0.5427],\n",
      "         [0.5338]],\n",
      "\n",
      "        [[0.7212],\n",
      "         [0.5454],\n",
      "         [0.6129]],\n",
      "\n",
      "        [[0.8194],\n",
      "         [0.5606],\n",
      "         [0.6820]],\n",
      "\n",
      "        [[0.6655],\n",
      "         [0.5431],\n",
      "         [0.5250]],\n",
      "\n",
      "        [[0.7329],\n",
      "         [0.5442],\n",
      "         [0.6313]],\n",
      "\n",
      "        [[0.7333],\n",
      "         [0.5589],\n",
      "         [0.6455]],\n",
      "\n",
      "        [[0.7277],\n",
      "         [0.5533],\n",
      "         [0.5160]],\n",
      "\n",
      "        [[0.7508],\n",
      "         [0.5595],\n",
      "         [0.6334]],\n",
      "\n",
      "        [[0.7387],\n",
      "         [0.5570],\n",
      "         [0.6428]],\n",
      "\n",
      "        [[0.6889],\n",
      "         [0.5271],\n",
      "         [0.6420]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7216, 0.2784],\n",
      "         [0.5474, 0.4526],\n",
      "         [0.6167, 0.3833]],\n",
      "\n",
      "        [[0.7907, 0.2093],\n",
      "         [0.5469, 0.4531],\n",
      "         [0.7010, 0.2990]],\n",
      "\n",
      "        [[0.7305, 0.2695],\n",
      "         [0.5537, 0.4463],\n",
      "         [0.6166, 0.3834]],\n",
      "\n",
      "        [[0.6907, 0.3093],\n",
      "         [0.5450, 0.4550],\n",
      "         [0.5287, 0.4713]],\n",
      "\n",
      "        [[0.7826, 0.2174],\n",
      "         [0.5392, 0.4608],\n",
      "         [0.7099, 0.2901]],\n",
      "\n",
      "        [[0.7037, 0.2963],\n",
      "         [0.5345, 0.4655],\n",
      "         [0.6226, 0.3774]],\n",
      "\n",
      "        [[0.6891, 0.3109],\n",
      "         [0.5431, 0.4569],\n",
      "         [0.5349, 0.4651]],\n",
      "\n",
      "        [[0.6929, 0.3071],\n",
      "         [0.5498, 0.4502],\n",
      "         [0.5248, 0.4752]],\n",
      "\n",
      "        [[0.7100, 0.2900],\n",
      "         [0.5520, 0.4480],\n",
      "         [0.5280, 0.4720]],\n",
      "\n",
      "        [[0.6562, 0.3438],\n",
      "         [0.5333, 0.4667],\n",
      "         [0.6146, 0.3854]],\n",
      "\n",
      "        [[0.7464, 0.2536],\n",
      "         [0.5529, 0.4471],\n",
      "         [0.6519, 0.3481]],\n",
      "\n",
      "        [[0.6776, 0.3224],\n",
      "         [0.5407, 0.4593],\n",
      "         [0.5321, 0.4679]],\n",
      "\n",
      "        [[0.6669, 0.3331],\n",
      "         [0.5365, 0.4635],\n",
      "         [0.5462, 0.4538]],\n",
      "\n",
      "        [[0.6951, 0.3049],\n",
      "         [0.5499, 0.4501],\n",
      "         [0.5422, 0.4578]],\n",
      "\n",
      "        [[0.7115, 0.2885],\n",
      "         [0.5450, 0.4550],\n",
      "         [0.6205, 0.3795]],\n",
      "\n",
      "        [[0.6863, 0.3137],\n",
      "         [0.5427, 0.4573],\n",
      "         [0.5338, 0.4662]],\n",
      "\n",
      "        [[0.7212, 0.2788],\n",
      "         [0.5454, 0.4546],\n",
      "         [0.6129, 0.3871]],\n",
      "\n",
      "        [[0.8194, 0.1806],\n",
      "         [0.5606, 0.4394],\n",
      "         [0.6820, 0.3180]],\n",
      "\n",
      "        [[0.6655, 0.3345],\n",
      "         [0.5431, 0.4569],\n",
      "         [0.5250, 0.4750]],\n",
      "\n",
      "        [[0.7329, 0.2671],\n",
      "         [0.5442, 0.4558],\n",
      "         [0.6313, 0.3687]],\n",
      "\n",
      "        [[0.7333, 0.2667],\n",
      "         [0.5589, 0.4411],\n",
      "         [0.6455, 0.3545]],\n",
      "\n",
      "        [[0.7277, 0.2723],\n",
      "         [0.5533, 0.4467],\n",
      "         [0.5160, 0.4840]],\n",
      "\n",
      "        [[0.7508, 0.2492],\n",
      "         [0.5595, 0.4405],\n",
      "         [0.6334, 0.3666]],\n",
      "\n",
      "        [[0.7387, 0.2613],\n",
      "         [0.5570, 0.4430],\n",
      "         [0.6428, 0.3572]],\n",
      "\n",
      "        [[0.6889, 0.3111],\n",
      "         [0.5271, 0.4729],\n",
      "         [0.6420, 0.3580]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.3950, 0.3266, 0.1717, 0.1067],\n",
      "        [0.4324, 0.3583, 0.1467, 0.0626],\n",
      "        [0.4045, 0.3260, 0.1662, 0.1033],\n",
      "        [0.3764, 0.3142, 0.1635, 0.1458],\n",
      "        [0.4220, 0.3607, 0.1543, 0.0631],\n",
      "        [0.3761, 0.3276, 0.1845, 0.1118],\n",
      "        [0.3743, 0.3149, 0.1663, 0.1446],\n",
      "        [0.3810, 0.3119, 0.1612, 0.1459],\n",
      "        [0.3919, 0.3181, 0.1531, 0.1369],\n",
      "        [0.3499, 0.3062, 0.2113, 0.1325],\n",
      "        [0.4127, 0.3337, 0.1653, 0.0883],\n",
      "        [0.3664, 0.3112, 0.1716, 0.1509],\n",
      "        [0.3578, 0.3091, 0.1819, 0.1512],\n",
      "        [0.3822, 0.3129, 0.1653, 0.1396],\n",
      "        [0.3878, 0.3237, 0.1790, 0.1095],\n",
      "        [0.3725, 0.3138, 0.1674, 0.1462],\n",
      "        [0.3934, 0.3278, 0.1709, 0.1079],\n",
      "        [0.4593, 0.3601, 0.1232, 0.0574],\n",
      "        [0.3614, 0.3041, 0.1756, 0.1589],\n",
      "        [0.3989, 0.3341, 0.1686, 0.0985],\n",
      "        [0.4098, 0.3235, 0.1721, 0.0945],\n",
      "        [0.4026, 0.3251, 0.1405, 0.1318],\n",
      "        [0.4201, 0.3307, 0.1579, 0.0914],\n",
      "        [0.4115, 0.3272, 0.1680, 0.0933],\n",
      "        [0.3631, 0.3258, 0.1997, 0.1114]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.3950, 0.3266, 0.1717, 0.1067],\n",
      "        [0.4324, 0.3583, 0.1467, 0.0626],\n",
      "        [0.4045, 0.3260, 0.1662, 0.1033],\n",
      "        [0.3764, 0.3142, 0.1635, 0.1458],\n",
      "        [0.4220, 0.3607, 0.1543, 0.0631],\n",
      "        [0.3761, 0.3276, 0.1845, 0.1118],\n",
      "        [0.3743, 0.3149, 0.1663, 0.1446],\n",
      "        [0.3810, 0.3119, 0.1612, 0.1459],\n",
      "        [0.3919, 0.3181, 0.1531, 0.1369],\n",
      "        [0.3499, 0.3062, 0.2113, 0.1325],\n",
      "        [0.4127, 0.3337, 0.1653, 0.0883],\n",
      "        [0.3664, 0.3112, 0.1716, 0.1509],\n",
      "        [0.3578, 0.3091, 0.1819, 0.1512],\n",
      "        [0.3822, 0.3129, 0.1653, 0.1396],\n",
      "        [0.3878, 0.3237, 0.1790, 0.1095],\n",
      "        [0.3725, 0.3138, 0.1674, 0.1462],\n",
      "        [0.3934, 0.3278, 0.1709, 0.1079],\n",
      "        [0.4593, 0.3601, 0.1232, 0.0574],\n",
      "        [0.3614, 0.3041, 0.1756, 0.1589],\n",
      "        [0.3989, 0.3341, 0.1686, 0.0985],\n",
      "        [0.4098, 0.3235, 0.1721, 0.0945],\n",
      "        [0.4026, 0.3251, 0.1405, 0.1318],\n",
      "        [0.4201, 0.3307, 0.1579, 0.0914],\n",
      "        [0.4115, 0.3272, 0.1680, 0.0933],\n",
      "        [0.3631, 0.3258, 0.1997, 0.1114]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0508,  0.0401,  0.1708],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 04 | Loss: 1.09709 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7272, 0.5544, 0.6119],\n",
      "        [0.7975, 0.5571, 0.6941],\n",
      "        [0.7363, 0.5610, 0.6117],\n",
      "        [0.6949, 0.5499, 0.5270],\n",
      "        [0.7895, 0.5493, 0.7030],\n",
      "        [0.7092, 0.5411, 0.6179],\n",
      "        [0.6933, 0.5480, 0.5330],\n",
      "        [0.6971, 0.5547, 0.5231],\n",
      "        [0.7146, 0.5576, 0.5261],\n",
      "        [0.6607, 0.5383, 0.6099],\n",
      "        [0.7525, 0.5609, 0.6460],\n",
      "        [0.6818, 0.5455, 0.5303],\n",
      "        [0.6708, 0.5409, 0.5439],\n",
      "        [0.6994, 0.5550, 0.5398],\n",
      "        [0.7170, 0.5517, 0.6156],\n",
      "        [0.6905, 0.5475, 0.5319],\n",
      "        [0.7269, 0.5524, 0.6083],\n",
      "        [0.8261, 0.5718, 0.6755],\n",
      "        [0.6693, 0.5473, 0.5233],\n",
      "        [0.7388, 0.5516, 0.6262],\n",
      "        [0.7393, 0.5665, 0.6395],\n",
      "        [0.7324, 0.5592, 0.5147],\n",
      "        [0.7568, 0.5676, 0.6278],\n",
      "        [0.7448, 0.5648, 0.6370],\n",
      "        [0.6941, 0.5332, 0.6368]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7272],\n",
      "         [0.5544],\n",
      "         [0.6119]],\n",
      "\n",
      "        [[0.7975],\n",
      "         [0.5571],\n",
      "         [0.6941]],\n",
      "\n",
      "        [[0.7363],\n",
      "         [0.5610],\n",
      "         [0.6117]],\n",
      "\n",
      "        [[0.6949],\n",
      "         [0.5499],\n",
      "         [0.5270]],\n",
      "\n",
      "        [[0.7895],\n",
      "         [0.5493],\n",
      "         [0.7030]],\n",
      "\n",
      "        [[0.7092],\n",
      "         [0.5411],\n",
      "         [0.6179]],\n",
      "\n",
      "        [[0.6933],\n",
      "         [0.5480],\n",
      "         [0.5330]],\n",
      "\n",
      "        [[0.6971],\n",
      "         [0.5547],\n",
      "         [0.5231]],\n",
      "\n",
      "        [[0.7146],\n",
      "         [0.5576],\n",
      "         [0.5261]],\n",
      "\n",
      "        [[0.6607],\n",
      "         [0.5383],\n",
      "         [0.6099]],\n",
      "\n",
      "        [[0.7525],\n",
      "         [0.5609],\n",
      "         [0.6460]],\n",
      "\n",
      "        [[0.6818],\n",
      "         [0.5455],\n",
      "         [0.5303]],\n",
      "\n",
      "        [[0.6708],\n",
      "         [0.5409],\n",
      "         [0.5439]],\n",
      "\n",
      "        [[0.6994],\n",
      "         [0.5550],\n",
      "         [0.5398]],\n",
      "\n",
      "        [[0.7170],\n",
      "         [0.5517],\n",
      "         [0.6156]],\n",
      "\n",
      "        [[0.6905],\n",
      "         [0.5475],\n",
      "         [0.5319]],\n",
      "\n",
      "        [[0.7269],\n",
      "         [0.5524],\n",
      "         [0.6083]],\n",
      "\n",
      "        [[0.8261],\n",
      "         [0.5718],\n",
      "         [0.6755]],\n",
      "\n",
      "        [[0.6693],\n",
      "         [0.5473],\n",
      "         [0.5233]],\n",
      "\n",
      "        [[0.7388],\n",
      "         [0.5516],\n",
      "         [0.6262]],\n",
      "\n",
      "        [[0.7393],\n",
      "         [0.5665],\n",
      "         [0.6395]],\n",
      "\n",
      "        [[0.7324],\n",
      "         [0.5592],\n",
      "         [0.5147]],\n",
      "\n",
      "        [[0.7568],\n",
      "         [0.5676],\n",
      "         [0.6278]],\n",
      "\n",
      "        [[0.7448],\n",
      "         [0.5648],\n",
      "         [0.6370]],\n",
      "\n",
      "        [[0.6941],\n",
      "         [0.5332],\n",
      "         [0.6368]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7272, 0.2728],\n",
      "         [0.5544, 0.4456],\n",
      "         [0.6119, 0.3881]],\n",
      "\n",
      "        [[0.7975, 0.2025],\n",
      "         [0.5571, 0.4429],\n",
      "         [0.6941, 0.3059]],\n",
      "\n",
      "        [[0.7363, 0.2637],\n",
      "         [0.5610, 0.4390],\n",
      "         [0.6117, 0.3883]],\n",
      "\n",
      "        [[0.6949, 0.3051],\n",
      "         [0.5499, 0.4501],\n",
      "         [0.5270, 0.4730]],\n",
      "\n",
      "        [[0.7895, 0.2105],\n",
      "         [0.5493, 0.4507],\n",
      "         [0.7030, 0.2970]],\n",
      "\n",
      "        [[0.7092, 0.2908],\n",
      "         [0.5411, 0.4589],\n",
      "         [0.6179, 0.3821]],\n",
      "\n",
      "        [[0.6933, 0.3067],\n",
      "         [0.5480, 0.4520],\n",
      "         [0.5330, 0.4670]],\n",
      "\n",
      "        [[0.6971, 0.3029],\n",
      "         [0.5547, 0.4453],\n",
      "         [0.5231, 0.4769]],\n",
      "\n",
      "        [[0.7146, 0.2854],\n",
      "         [0.5576, 0.4424],\n",
      "         [0.5261, 0.4739]],\n",
      "\n",
      "        [[0.6607, 0.3393],\n",
      "         [0.5383, 0.4617],\n",
      "         [0.6099, 0.3901]],\n",
      "\n",
      "        [[0.7525, 0.2475],\n",
      "         [0.5609, 0.4391],\n",
      "         [0.6460, 0.3540]],\n",
      "\n",
      "        [[0.6818, 0.3182],\n",
      "         [0.5455, 0.4545],\n",
      "         [0.5303, 0.4697]],\n",
      "\n",
      "        [[0.6708, 0.3292],\n",
      "         [0.5409, 0.4591],\n",
      "         [0.5439, 0.4561]],\n",
      "\n",
      "        [[0.6994, 0.3006],\n",
      "         [0.5550, 0.4450],\n",
      "         [0.5398, 0.4602]],\n",
      "\n",
      "        [[0.7170, 0.2830],\n",
      "         [0.5517, 0.4483],\n",
      "         [0.6156, 0.3844]],\n",
      "\n",
      "        [[0.6905, 0.3095],\n",
      "         [0.5475, 0.4525],\n",
      "         [0.5319, 0.4681]],\n",
      "\n",
      "        [[0.7269, 0.2731],\n",
      "         [0.5524, 0.4476],\n",
      "         [0.6083, 0.3917]],\n",
      "\n",
      "        [[0.8261, 0.1739],\n",
      "         [0.5718, 0.4282],\n",
      "         [0.6755, 0.3245]],\n",
      "\n",
      "        [[0.6693, 0.3307],\n",
      "         [0.5473, 0.4527],\n",
      "         [0.5233, 0.4767]],\n",
      "\n",
      "        [[0.7388, 0.2612],\n",
      "         [0.5516, 0.4484],\n",
      "         [0.6262, 0.3738]],\n",
      "\n",
      "        [[0.7393, 0.2607],\n",
      "         [0.5665, 0.4335],\n",
      "         [0.6395, 0.3605]],\n",
      "\n",
      "        [[0.7324, 0.2676],\n",
      "         [0.5592, 0.4408],\n",
      "         [0.5147, 0.4853]],\n",
      "\n",
      "        [[0.7568, 0.2432],\n",
      "         [0.5676, 0.4324],\n",
      "         [0.6278, 0.3722]],\n",
      "\n",
      "        [[0.7448, 0.2552],\n",
      "         [0.5648, 0.4352],\n",
      "         [0.6370, 0.3630]],\n",
      "\n",
      "        [[0.6941, 0.3059],\n",
      "         [0.5332, 0.4668],\n",
      "         [0.6368, 0.3632]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.4032, 0.3240, 0.1669, 0.1059],\n",
      "        [0.4443, 0.3532, 0.1406, 0.0619],\n",
      "        [0.4131, 0.3232, 0.1613, 0.1024],\n",
      "        [0.3821, 0.3128, 0.1608, 0.1443],\n",
      "        [0.4336, 0.3558, 0.1480, 0.0625],\n",
      "        [0.3837, 0.3255, 0.1797, 0.1111],\n",
      "        [0.3799, 0.3134, 0.1635, 0.1432],\n",
      "        [0.3867, 0.3104, 0.1585, 0.1445],\n",
      "        [0.3985, 0.3161, 0.1501, 0.1352],\n",
      "        [0.3556, 0.3050, 0.2070, 0.1324],\n",
      "        [0.4221, 0.3304, 0.1599, 0.0876],\n",
      "        [0.3719, 0.3098, 0.1687, 0.1495],\n",
      "        [0.3628, 0.3080, 0.1791, 0.1501],\n",
      "        [0.3882, 0.3112, 0.1622, 0.1383],\n",
      "        [0.3956, 0.3214, 0.1742, 0.1088],\n",
      "        [0.3781, 0.3124, 0.1646, 0.1449],\n",
      "        [0.4015, 0.3253, 0.1662, 0.1070],\n",
      "        [0.4724, 0.3537, 0.1175, 0.0564],\n",
      "        [0.3663, 0.3030, 0.1731, 0.1577],\n",
      "        [0.4075, 0.3313, 0.1636, 0.0976],\n",
      "        [0.4188, 0.3205, 0.1667, 0.0940],\n",
      "        [0.4095, 0.3229, 0.1378, 0.1299],\n",
      "        [0.4295, 0.3273, 0.1527, 0.0905],\n",
      "        [0.4207, 0.3241, 0.1626, 0.0927],\n",
      "        [0.3701, 0.3240, 0.1948, 0.1111]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.4032, 0.3240, 0.1669, 0.1059],\n",
      "        [0.4443, 0.3532, 0.1406, 0.0619],\n",
      "        [0.4131, 0.3232, 0.1613, 0.1024],\n",
      "        [0.3821, 0.3128, 0.1608, 0.1443],\n",
      "        [0.4336, 0.3558, 0.1480, 0.0625],\n",
      "        [0.3837, 0.3255, 0.1797, 0.1111],\n",
      "        [0.3799, 0.3134, 0.1635, 0.1432],\n",
      "        [0.3867, 0.3104, 0.1585, 0.1445],\n",
      "        [0.3985, 0.3161, 0.1501, 0.1352],\n",
      "        [0.3556, 0.3050, 0.2070, 0.1324],\n",
      "        [0.4221, 0.3304, 0.1599, 0.0876],\n",
      "        [0.3719, 0.3098, 0.1687, 0.1495],\n",
      "        [0.3628, 0.3080, 0.1791, 0.1501],\n",
      "        [0.3882, 0.3112, 0.1622, 0.1383],\n",
      "        [0.3956, 0.3214, 0.1742, 0.1088],\n",
      "        [0.3781, 0.3124, 0.1646, 0.1449],\n",
      "        [0.4015, 0.3253, 0.1662, 0.1070],\n",
      "        [0.4724, 0.3537, 0.1175, 0.0564],\n",
      "        [0.3663, 0.3030, 0.1731, 0.1577],\n",
      "        [0.4075, 0.3313, 0.1636, 0.0976],\n",
      "        [0.4188, 0.3205, 0.1667, 0.0940],\n",
      "        [0.4095, 0.3229, 0.1378, 0.1299],\n",
      "        [0.4295, 0.3273, 0.1527, 0.0905],\n",
      "        [0.4207, 0.3241, 0.1626, 0.0927],\n",
      "        [0.3701, 0.3240, 0.1948, 0.1111]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0570,  0.0365,  0.1803],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 05 | Loss: 1.09674 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7328, 0.5614, 0.6071],\n",
      "        [0.8042, 0.5674, 0.6873],\n",
      "        [0.7421, 0.5684, 0.6067],\n",
      "        [0.6991, 0.5548, 0.5253],\n",
      "        [0.7962, 0.5593, 0.6960],\n",
      "        [0.7146, 0.5476, 0.6131],\n",
      "        [0.6975, 0.5528, 0.5311],\n",
      "        [0.7012, 0.5596, 0.5214],\n",
      "        [0.7193, 0.5632, 0.5243],\n",
      "        [0.6652, 0.5433, 0.6052],\n",
      "        [0.7585, 0.5689, 0.6401],\n",
      "        [0.6860, 0.5503, 0.5284],\n",
      "        [0.6747, 0.5453, 0.5417],\n",
      "        [0.7038, 0.5601, 0.5374],\n",
      "        [0.7225, 0.5585, 0.6107],\n",
      "        [0.6947, 0.5524, 0.5300],\n",
      "        [0.7325, 0.5594, 0.6037],\n",
      "        [0.8326, 0.5830, 0.6689],\n",
      "        [0.6730, 0.5515, 0.5216],\n",
      "        [0.7445, 0.5590, 0.6211],\n",
      "        [0.7453, 0.5741, 0.6335],\n",
      "        [0.7371, 0.5651, 0.5135],\n",
      "        [0.7628, 0.5756, 0.6223],\n",
      "        [0.7508, 0.5727, 0.6311],\n",
      "        [0.6994, 0.5394, 0.6317]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7328],\n",
      "         [0.5614],\n",
      "         [0.6071]],\n",
      "\n",
      "        [[0.8042],\n",
      "         [0.5674],\n",
      "         [0.6873]],\n",
      "\n",
      "        [[0.7421],\n",
      "         [0.5684],\n",
      "         [0.6067]],\n",
      "\n",
      "        [[0.6991],\n",
      "         [0.5548],\n",
      "         [0.5253]],\n",
      "\n",
      "        [[0.7962],\n",
      "         [0.5593],\n",
      "         [0.6960]],\n",
      "\n",
      "        [[0.7146],\n",
      "         [0.5476],\n",
      "         [0.6131]],\n",
      "\n",
      "        [[0.6975],\n",
      "         [0.5528],\n",
      "         [0.5311]],\n",
      "\n",
      "        [[0.7012],\n",
      "         [0.5596],\n",
      "         [0.5214]],\n",
      "\n",
      "        [[0.7193],\n",
      "         [0.5632],\n",
      "         [0.5243]],\n",
      "\n",
      "        [[0.6652],\n",
      "         [0.5433],\n",
      "         [0.6052]],\n",
      "\n",
      "        [[0.7585],\n",
      "         [0.5689],\n",
      "         [0.6401]],\n",
      "\n",
      "        [[0.6860],\n",
      "         [0.5503],\n",
      "         [0.5284]],\n",
      "\n",
      "        [[0.6747],\n",
      "         [0.5453],\n",
      "         [0.5417]],\n",
      "\n",
      "        [[0.7038],\n",
      "         [0.5601],\n",
      "         [0.5374]],\n",
      "\n",
      "        [[0.7225],\n",
      "         [0.5585],\n",
      "         [0.6107]],\n",
      "\n",
      "        [[0.6947],\n",
      "         [0.5524],\n",
      "         [0.5300]],\n",
      "\n",
      "        [[0.7325],\n",
      "         [0.5594],\n",
      "         [0.6037]],\n",
      "\n",
      "        [[0.8326],\n",
      "         [0.5830],\n",
      "         [0.6689]],\n",
      "\n",
      "        [[0.6730],\n",
      "         [0.5515],\n",
      "         [0.5216]],\n",
      "\n",
      "        [[0.7445],\n",
      "         [0.5590],\n",
      "         [0.6211]],\n",
      "\n",
      "        [[0.7453],\n",
      "         [0.5741],\n",
      "         [0.6335]],\n",
      "\n",
      "        [[0.7371],\n",
      "         [0.5651],\n",
      "         [0.5135]],\n",
      "\n",
      "        [[0.7628],\n",
      "         [0.5756],\n",
      "         [0.6223]],\n",
      "\n",
      "        [[0.7508],\n",
      "         [0.5727],\n",
      "         [0.6311]],\n",
      "\n",
      "        [[0.6994],\n",
      "         [0.5394],\n",
      "         [0.6317]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7328, 0.2672],\n",
      "         [0.5614, 0.4386],\n",
      "         [0.6071, 0.3929]],\n",
      "\n",
      "        [[0.8042, 0.1958],\n",
      "         [0.5674, 0.4326],\n",
      "         [0.6873, 0.3127]],\n",
      "\n",
      "        [[0.7421, 0.2579],\n",
      "         [0.5684, 0.4316],\n",
      "         [0.6067, 0.3933]],\n",
      "\n",
      "        [[0.6991, 0.3009],\n",
      "         [0.5548, 0.4452],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.7962, 0.2038],\n",
      "         [0.5593, 0.4407],\n",
      "         [0.6960, 0.3040]],\n",
      "\n",
      "        [[0.7146, 0.2854],\n",
      "         [0.5476, 0.4524],\n",
      "         [0.6131, 0.3869]],\n",
      "\n",
      "        [[0.6975, 0.3025],\n",
      "         [0.5528, 0.4472],\n",
      "         [0.5311, 0.4689]],\n",
      "\n",
      "        [[0.7012, 0.2988],\n",
      "         [0.5596, 0.4404],\n",
      "         [0.5214, 0.4786]],\n",
      "\n",
      "        [[0.7193, 0.2807],\n",
      "         [0.5632, 0.4368],\n",
      "         [0.5243, 0.4757]],\n",
      "\n",
      "        [[0.6652, 0.3348],\n",
      "         [0.5433, 0.4567],\n",
      "         [0.6052, 0.3948]],\n",
      "\n",
      "        [[0.7585, 0.2415],\n",
      "         [0.5689, 0.4311],\n",
      "         [0.6401, 0.3599]],\n",
      "\n",
      "        [[0.6860, 0.3140],\n",
      "         [0.5503, 0.4497],\n",
      "         [0.5284, 0.4716]],\n",
      "\n",
      "        [[0.6747, 0.3253],\n",
      "         [0.5453, 0.4547],\n",
      "         [0.5417, 0.4583]],\n",
      "\n",
      "        [[0.7038, 0.2962],\n",
      "         [0.5601, 0.4399],\n",
      "         [0.5374, 0.4626]],\n",
      "\n",
      "        [[0.7225, 0.2775],\n",
      "         [0.5585, 0.4415],\n",
      "         [0.6107, 0.3893]],\n",
      "\n",
      "        [[0.6947, 0.3053],\n",
      "         [0.5524, 0.4476],\n",
      "         [0.5300, 0.4700]],\n",
      "\n",
      "        [[0.7325, 0.2675],\n",
      "         [0.5594, 0.4406],\n",
      "         [0.6037, 0.3963]],\n",
      "\n",
      "        [[0.8326, 0.1674],\n",
      "         [0.5830, 0.4170],\n",
      "         [0.6689, 0.3311]],\n",
      "\n",
      "        [[0.6730, 0.3270],\n",
      "         [0.5515, 0.4485],\n",
      "         [0.5216, 0.4784]],\n",
      "\n",
      "        [[0.7445, 0.2555],\n",
      "         [0.5590, 0.4410],\n",
      "         [0.6211, 0.3789]],\n",
      "\n",
      "        [[0.7453, 0.2547],\n",
      "         [0.5741, 0.4259],\n",
      "         [0.6335, 0.3665]],\n",
      "\n",
      "        [[0.7371, 0.2629],\n",
      "         [0.5651, 0.4349],\n",
      "         [0.5135, 0.4865]],\n",
      "\n",
      "        [[0.7628, 0.2372],\n",
      "         [0.5756, 0.4244],\n",
      "         [0.6223, 0.3777]],\n",
      "\n",
      "        [[0.7508, 0.2492],\n",
      "         [0.5727, 0.4273],\n",
      "         [0.6311, 0.3689]],\n",
      "\n",
      "        [[0.6994, 0.3006],\n",
      "         [0.5394, 0.4606],\n",
      "         [0.6317, 0.3683]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.4114, 0.3214, 0.1622, 0.1050],\n",
      "        [0.4563, 0.3479, 0.1346, 0.0612],\n",
      "        [0.4218, 0.3203, 0.1565, 0.1014],\n",
      "        [0.3879, 0.3112, 0.1581, 0.1429],\n",
      "        [0.4453, 0.3508, 0.1419, 0.0620],\n",
      "        [0.3913, 0.3233, 0.1750, 0.1104],\n",
      "        [0.3856, 0.3119, 0.1607, 0.1419],\n",
      "        [0.3924, 0.3089, 0.1558, 0.1430],\n",
      "        [0.4051, 0.3142, 0.1472, 0.1335],\n",
      "        [0.3614, 0.3038, 0.2026, 0.1322],\n",
      "        [0.4315, 0.3270, 0.1546, 0.0869],\n",
      "        [0.3775, 0.3085, 0.1659, 0.1481],\n",
      "        [0.3679, 0.3068, 0.1762, 0.1491],\n",
      "        [0.3942, 0.3096, 0.1592, 0.1370],\n",
      "        [0.4035, 0.3190, 0.1694, 0.1080],\n",
      "        [0.3837, 0.3110, 0.1618, 0.1435],\n",
      "        [0.4098, 0.3227, 0.1615, 0.1060],\n",
      "        [0.4854, 0.3472, 0.1120, 0.0554],\n",
      "        [0.3712, 0.3018, 0.1705, 0.1564],\n",
      "        [0.4162, 0.3283, 0.1587, 0.0968],\n",
      "        [0.4278, 0.3174, 0.1614, 0.0934],\n",
      "        [0.4165, 0.3206, 0.1350, 0.1279],\n",
      "        [0.4391, 0.3237, 0.1476, 0.0896],\n",
      "        [0.4300, 0.3209, 0.1573, 0.0919],\n",
      "        [0.3772, 0.3221, 0.1899, 0.1107]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.4114, 0.3214, 0.1622, 0.1050],\n",
      "        [0.4563, 0.3479, 0.1346, 0.0612],\n",
      "        [0.4218, 0.3203, 0.1565, 0.1014],\n",
      "        [0.3879, 0.3112, 0.1581, 0.1429],\n",
      "        [0.4453, 0.3508, 0.1419, 0.0620],\n",
      "        [0.3913, 0.3233, 0.1750, 0.1104],\n",
      "        [0.3856, 0.3119, 0.1607, 0.1419],\n",
      "        [0.3924, 0.3089, 0.1558, 0.1430],\n",
      "        [0.4051, 0.3142, 0.1472, 0.1335],\n",
      "        [0.3614, 0.3038, 0.2026, 0.1322],\n",
      "        [0.4315, 0.3270, 0.1546, 0.0869],\n",
      "        [0.3775, 0.3085, 0.1659, 0.1481],\n",
      "        [0.3679, 0.3068, 0.1762, 0.1491],\n",
      "        [0.3942, 0.3096, 0.1592, 0.1370],\n",
      "        [0.4035, 0.3190, 0.1694, 0.1080],\n",
      "        [0.3837, 0.3110, 0.1618, 0.1435],\n",
      "        [0.4098, 0.3227, 0.1615, 0.1060],\n",
      "        [0.4854, 0.3472, 0.1120, 0.0554],\n",
      "        [0.3712, 0.3018, 0.1705, 0.1564],\n",
      "        [0.4162, 0.3283, 0.1587, 0.0968],\n",
      "        [0.4278, 0.3174, 0.1614, 0.0934],\n",
      "        [0.4165, 0.3206, 0.1350, 0.1279],\n",
      "        [0.4391, 0.3237, 0.1476, 0.0896],\n",
      "        [0.4300, 0.3209, 0.1573, 0.0919],\n",
      "        [0.3772, 0.3221, 0.1899, 0.1107]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0635,  0.0327,  0.1902],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 06 | Loss: 1.09638 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7384, 0.5684, 0.6023],\n",
      "        [0.8107, 0.5776, 0.6804],\n",
      "        [0.7478, 0.5757, 0.6018],\n",
      "        [0.7033, 0.5597, 0.5235],\n",
      "        [0.8028, 0.5694, 0.6890],\n",
      "        [0.7200, 0.5542, 0.6085],\n",
      "        [0.7017, 0.5577, 0.5292],\n",
      "        [0.7054, 0.5644, 0.5197],\n",
      "        [0.7239, 0.5688, 0.5225],\n",
      "        [0.6697, 0.5483, 0.6006],\n",
      "        [0.7645, 0.5769, 0.6342],\n",
      "        [0.6902, 0.5551, 0.5266],\n",
      "        [0.6786, 0.5497, 0.5395],\n",
      "        [0.7081, 0.5652, 0.5351],\n",
      "        [0.7280, 0.5652, 0.6058],\n",
      "        [0.6988, 0.5572, 0.5282],\n",
      "        [0.7380, 0.5665, 0.5991],\n",
      "        [0.8389, 0.5942, 0.6624],\n",
      "        [0.6768, 0.5558, 0.5200],\n",
      "        [0.7502, 0.5664, 0.6161],\n",
      "        [0.7512, 0.5817, 0.6275],\n",
      "        [0.7417, 0.5709, 0.5122],\n",
      "        [0.7687, 0.5836, 0.6168],\n",
      "        [0.7568, 0.5805, 0.6253],\n",
      "        [0.7045, 0.5455, 0.6265]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7384],\n",
      "         [0.5684],\n",
      "         [0.6023]],\n",
      "\n",
      "        [[0.8107],\n",
      "         [0.5776],\n",
      "         [0.6804]],\n",
      "\n",
      "        [[0.7478],\n",
      "         [0.5757],\n",
      "         [0.6018]],\n",
      "\n",
      "        [[0.7033],\n",
      "         [0.5597],\n",
      "         [0.5235]],\n",
      "\n",
      "        [[0.8028],\n",
      "         [0.5694],\n",
      "         [0.6890]],\n",
      "\n",
      "        [[0.7200],\n",
      "         [0.5542],\n",
      "         [0.6085]],\n",
      "\n",
      "        [[0.7017],\n",
      "         [0.5577],\n",
      "         [0.5292]],\n",
      "\n",
      "        [[0.7054],\n",
      "         [0.5644],\n",
      "         [0.5197]],\n",
      "\n",
      "        [[0.7239],\n",
      "         [0.5688],\n",
      "         [0.5225]],\n",
      "\n",
      "        [[0.6697],\n",
      "         [0.5483],\n",
      "         [0.6006]],\n",
      "\n",
      "        [[0.7645],\n",
      "         [0.5769],\n",
      "         [0.6342]],\n",
      "\n",
      "        [[0.6902],\n",
      "         [0.5551],\n",
      "         [0.5266]],\n",
      "\n",
      "        [[0.6786],\n",
      "         [0.5497],\n",
      "         [0.5395]],\n",
      "\n",
      "        [[0.7081],\n",
      "         [0.5652],\n",
      "         [0.5351]],\n",
      "\n",
      "        [[0.7280],\n",
      "         [0.5652],\n",
      "         [0.6058]],\n",
      "\n",
      "        [[0.6988],\n",
      "         [0.5572],\n",
      "         [0.5282]],\n",
      "\n",
      "        [[0.7380],\n",
      "         [0.5665],\n",
      "         [0.5991]],\n",
      "\n",
      "        [[0.8389],\n",
      "         [0.5942],\n",
      "         [0.6624]],\n",
      "\n",
      "        [[0.6768],\n",
      "         [0.5558],\n",
      "         [0.5200]],\n",
      "\n",
      "        [[0.7502],\n",
      "         [0.5664],\n",
      "         [0.6161]],\n",
      "\n",
      "        [[0.7512],\n",
      "         [0.5817],\n",
      "         [0.6275]],\n",
      "\n",
      "        [[0.7417],\n",
      "         [0.5709],\n",
      "         [0.5122]],\n",
      "\n",
      "        [[0.7687],\n",
      "         [0.5836],\n",
      "         [0.6168]],\n",
      "\n",
      "        [[0.7568],\n",
      "         [0.5805],\n",
      "         [0.6253]],\n",
      "\n",
      "        [[0.7045],\n",
      "         [0.5455],\n",
      "         [0.6265]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7384, 0.2616],\n",
      "         [0.5684, 0.4316],\n",
      "         [0.6023, 0.3977]],\n",
      "\n",
      "        [[0.8107, 0.1893],\n",
      "         [0.5776, 0.4224],\n",
      "         [0.6804, 0.3196]],\n",
      "\n",
      "        [[0.7478, 0.2522],\n",
      "         [0.5757, 0.4243],\n",
      "         [0.6018, 0.3982]],\n",
      "\n",
      "        [[0.7033, 0.2967],\n",
      "         [0.5597, 0.4403],\n",
      "         [0.5235, 0.4765]],\n",
      "\n",
      "        [[0.8028, 0.1972],\n",
      "         [0.5694, 0.4306],\n",
      "         [0.6890, 0.3110]],\n",
      "\n",
      "        [[0.7200, 0.2800],\n",
      "         [0.5542, 0.4458],\n",
      "         [0.6085, 0.3915]],\n",
      "\n",
      "        [[0.7017, 0.2983],\n",
      "         [0.5577, 0.4423],\n",
      "         [0.5292, 0.4708]],\n",
      "\n",
      "        [[0.7054, 0.2946],\n",
      "         [0.5644, 0.4356],\n",
      "         [0.5197, 0.4803]],\n",
      "\n",
      "        [[0.7239, 0.2761],\n",
      "         [0.5688, 0.4312],\n",
      "         [0.5225, 0.4775]],\n",
      "\n",
      "        [[0.6697, 0.3303],\n",
      "         [0.5483, 0.4517],\n",
      "         [0.6006, 0.3994]],\n",
      "\n",
      "        [[0.7645, 0.2355],\n",
      "         [0.5769, 0.4231],\n",
      "         [0.6342, 0.3658]],\n",
      "\n",
      "        [[0.6902, 0.3098],\n",
      "         [0.5551, 0.4449],\n",
      "         [0.5266, 0.4734]],\n",
      "\n",
      "        [[0.6786, 0.3214],\n",
      "         [0.5497, 0.4503],\n",
      "         [0.5395, 0.4605]],\n",
      "\n",
      "        [[0.7081, 0.2919],\n",
      "         [0.5652, 0.4348],\n",
      "         [0.5351, 0.4649]],\n",
      "\n",
      "        [[0.7280, 0.2720],\n",
      "         [0.5652, 0.4348],\n",
      "         [0.6058, 0.3942]],\n",
      "\n",
      "        [[0.6988, 0.3012],\n",
      "         [0.5572, 0.4428],\n",
      "         [0.5282, 0.4718]],\n",
      "\n",
      "        [[0.7380, 0.2620],\n",
      "         [0.5665, 0.4335],\n",
      "         [0.5991, 0.4009]],\n",
      "\n",
      "        [[0.8389, 0.1611],\n",
      "         [0.5942, 0.4058],\n",
      "         [0.6624, 0.3376]],\n",
      "\n",
      "        [[0.6768, 0.3232],\n",
      "         [0.5558, 0.4442],\n",
      "         [0.5200, 0.4800]],\n",
      "\n",
      "        [[0.7502, 0.2498],\n",
      "         [0.5664, 0.4336],\n",
      "         [0.6161, 0.3839]],\n",
      "\n",
      "        [[0.7512, 0.2488],\n",
      "         [0.5817, 0.4183],\n",
      "         [0.6275, 0.3725]],\n",
      "\n",
      "        [[0.7417, 0.2583],\n",
      "         [0.5709, 0.4291],\n",
      "         [0.5122, 0.4878]],\n",
      "\n",
      "        [[0.7687, 0.2313],\n",
      "         [0.5836, 0.4164],\n",
      "         [0.6168, 0.3832]],\n",
      "\n",
      "        [[0.7568, 0.2432],\n",
      "         [0.5805, 0.4195],\n",
      "         [0.6253, 0.3747]],\n",
      "\n",
      "        [[0.7045, 0.2955],\n",
      "         [0.5455, 0.4545],\n",
      "         [0.6265, 0.3735]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.4197, 0.3187, 0.1576, 0.1040],\n",
      "        [0.4682, 0.3424, 0.1288, 0.0605],\n",
      "        [0.4305, 0.3173, 0.1518, 0.1004],\n",
      "        [0.3936, 0.3096, 0.1554, 0.1414],\n",
      "        [0.4571, 0.3457, 0.1359, 0.0613],\n",
      "        [0.3990, 0.3210, 0.1704, 0.1096],\n",
      "        [0.3913, 0.3103, 0.1579, 0.1405],\n",
      "        [0.3982, 0.3072, 0.1531, 0.1415],\n",
      "        [0.4118, 0.3121, 0.1443, 0.1318],\n",
      "        [0.3672, 0.3025, 0.1984, 0.1319],\n",
      "        [0.4410, 0.3235, 0.1494, 0.0862],\n",
      "        [0.3831, 0.3070, 0.1632, 0.1467],\n",
      "        [0.3731, 0.3056, 0.1734, 0.1480],\n",
      "        [0.4003, 0.3079, 0.1562, 0.1357],\n",
      "        [0.4115, 0.3165, 0.1648, 0.1072],\n",
      "        [0.3894, 0.3094, 0.1591, 0.1421],\n",
      "        [0.4181, 0.3200, 0.1569, 0.1050],\n",
      "        [0.4984, 0.3405, 0.1067, 0.0544],\n",
      "        [0.3762, 0.3007, 0.1680, 0.1551],\n",
      "        [0.4249, 0.3253, 0.1539, 0.0959],\n",
      "        [0.4369, 0.3142, 0.1561, 0.0927],\n",
      "        [0.4235, 0.3182, 0.1323, 0.1260],\n",
      "        [0.4486, 0.3201, 0.1426, 0.0886],\n",
      "        [0.4393, 0.3175, 0.1521, 0.0911],\n",
      "        [0.3843, 0.3202, 0.1851, 0.1104]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.4197, 0.3187, 0.1576, 0.1040],\n",
      "        [0.4682, 0.3424, 0.1288, 0.0605],\n",
      "        [0.4305, 0.3173, 0.1518, 0.1004],\n",
      "        [0.3936, 0.3096, 0.1554, 0.1414],\n",
      "        [0.4571, 0.3457, 0.1359, 0.0613],\n",
      "        [0.3990, 0.3210, 0.1704, 0.1096],\n",
      "        [0.3913, 0.3103, 0.1579, 0.1405],\n",
      "        [0.3982, 0.3072, 0.1531, 0.1415],\n",
      "        [0.4118, 0.3121, 0.1443, 0.1318],\n",
      "        [0.3672, 0.3025, 0.1984, 0.1319],\n",
      "        [0.4410, 0.3235, 0.1494, 0.0862],\n",
      "        [0.3831, 0.3070, 0.1632, 0.1467],\n",
      "        [0.3731, 0.3056, 0.1734, 0.1480],\n",
      "        [0.4003, 0.3079, 0.1562, 0.1357],\n",
      "        [0.4115, 0.3165, 0.1648, 0.1072],\n",
      "        [0.3894, 0.3094, 0.1591, 0.1421],\n",
      "        [0.4181, 0.3200, 0.1569, 0.1050],\n",
      "        [0.4984, 0.3405, 0.1067, 0.0544],\n",
      "        [0.3762, 0.3007, 0.1680, 0.1551],\n",
      "        [0.4249, 0.3253, 0.1539, 0.0959],\n",
      "        [0.4369, 0.3142, 0.1561, 0.0927],\n",
      "        [0.4235, 0.3182, 0.1323, 0.1260],\n",
      "        [0.4486, 0.3201, 0.1426, 0.0886],\n",
      "        [0.4393, 0.3175, 0.1521, 0.0911],\n",
      "        [0.3843, 0.3202, 0.1851, 0.1104]], grad_fn=<ViewBackward>) tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0702,  0.0285,  0.2003],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 07 | Loss: 1.09600 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7439, 0.5754, 0.5976],\n",
      "        [0.8170, 0.5878, 0.6734],\n",
      "        [0.7535, 0.5831, 0.5969],\n",
      "        [0.7074, 0.5646, 0.5219],\n",
      "        [0.8092, 0.5795, 0.6820],\n",
      "        [0.7253, 0.5607, 0.6038],\n",
      "        [0.7058, 0.5626, 0.5273],\n",
      "        [0.7095, 0.5693, 0.5180],\n",
      "        [0.7285, 0.5745, 0.5207],\n",
      "        [0.6742, 0.5533, 0.5960],\n",
      "        [0.7704, 0.5849, 0.6283],\n",
      "        [0.6944, 0.5600, 0.5248],\n",
      "        [0.6825, 0.5541, 0.5373],\n",
      "        [0.7124, 0.5704, 0.5328],\n",
      "        [0.7334, 0.5719, 0.6009],\n",
      "        [0.7030, 0.5621, 0.5263],\n",
      "        [0.7435, 0.5735, 0.5945],\n",
      "        [0.8451, 0.6053, 0.6558],\n",
      "        [0.6806, 0.5600, 0.5183],\n",
      "        [0.7558, 0.5738, 0.6110],\n",
      "        [0.7570, 0.5893, 0.6215],\n",
      "        [0.7463, 0.5768, 0.5110],\n",
      "        [0.7746, 0.5916, 0.6113],\n",
      "        [0.7626, 0.5883, 0.6195],\n",
      "        [0.7097, 0.5517, 0.6213]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7439],\n",
      "         [0.5754],\n",
      "         [0.5976]],\n",
      "\n",
      "        [[0.8170],\n",
      "         [0.5878],\n",
      "         [0.6734]],\n",
      "\n",
      "        [[0.7535],\n",
      "         [0.5831],\n",
      "         [0.5969]],\n",
      "\n",
      "        [[0.7074],\n",
      "         [0.5646],\n",
      "         [0.5219]],\n",
      "\n",
      "        [[0.8092],\n",
      "         [0.5795],\n",
      "         [0.6820]],\n",
      "\n",
      "        [[0.7253],\n",
      "         [0.5607],\n",
      "         [0.6038]],\n",
      "\n",
      "        [[0.7058],\n",
      "         [0.5626],\n",
      "         [0.5273]],\n",
      "\n",
      "        [[0.7095],\n",
      "         [0.5693],\n",
      "         [0.5180]],\n",
      "\n",
      "        [[0.7285],\n",
      "         [0.5745],\n",
      "         [0.5207]],\n",
      "\n",
      "        [[0.6742],\n",
      "         [0.5533],\n",
      "         [0.5960]],\n",
      "\n",
      "        [[0.7704],\n",
      "         [0.5849],\n",
      "         [0.6283]],\n",
      "\n",
      "        [[0.6944],\n",
      "         [0.5600],\n",
      "         [0.5248]],\n",
      "\n",
      "        [[0.6825],\n",
      "         [0.5541],\n",
      "         [0.5373]],\n",
      "\n",
      "        [[0.7124],\n",
      "         [0.5704],\n",
      "         [0.5328]],\n",
      "\n",
      "        [[0.7334],\n",
      "         [0.5719],\n",
      "         [0.6009]],\n",
      "\n",
      "        [[0.7030],\n",
      "         [0.5621],\n",
      "         [0.5263]],\n",
      "\n",
      "        [[0.7435],\n",
      "         [0.5735],\n",
      "         [0.5945]],\n",
      "\n",
      "        [[0.8451],\n",
      "         [0.6053],\n",
      "         [0.6558]],\n",
      "\n",
      "        [[0.6806],\n",
      "         [0.5600],\n",
      "         [0.5183]],\n",
      "\n",
      "        [[0.7558],\n",
      "         [0.5738],\n",
      "         [0.6110]],\n",
      "\n",
      "        [[0.7570],\n",
      "         [0.5893],\n",
      "         [0.6215]],\n",
      "\n",
      "        [[0.7463],\n",
      "         [0.5768],\n",
      "         [0.5110]],\n",
      "\n",
      "        [[0.7746],\n",
      "         [0.5916],\n",
      "         [0.6113]],\n",
      "\n",
      "        [[0.7626],\n",
      "         [0.5883],\n",
      "         [0.6195]],\n",
      "\n",
      "        [[0.7097],\n",
      "         [0.5517],\n",
      "         [0.6213]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7439, 0.2561],\n",
      "         [0.5754, 0.4246],\n",
      "         [0.5976, 0.4024]],\n",
      "\n",
      "        [[0.8170, 0.1830],\n",
      "         [0.5878, 0.4122],\n",
      "         [0.6734, 0.3266]],\n",
      "\n",
      "        [[0.7535, 0.2465],\n",
      "         [0.5831, 0.4169],\n",
      "         [0.5969, 0.4031]],\n",
      "\n",
      "        [[0.7074, 0.2926],\n",
      "         [0.5646, 0.4354],\n",
      "         [0.5219, 0.4781]],\n",
      "\n",
      "        [[0.8092, 0.1908],\n",
      "         [0.5795, 0.4205],\n",
      "         [0.6820, 0.3180]],\n",
      "\n",
      "        [[0.7253, 0.2747],\n",
      "         [0.5607, 0.4393],\n",
      "         [0.6038, 0.3962]],\n",
      "\n",
      "        [[0.7058, 0.2942],\n",
      "         [0.5626, 0.4374],\n",
      "         [0.5273, 0.4727]],\n",
      "\n",
      "        [[0.7095, 0.2905],\n",
      "         [0.5693, 0.4307],\n",
      "         [0.5180, 0.4820]],\n",
      "\n",
      "        [[0.7285, 0.2715],\n",
      "         [0.5745, 0.4255],\n",
      "         [0.5207, 0.4793]],\n",
      "\n",
      "        [[0.6742, 0.3258],\n",
      "         [0.5533, 0.4467],\n",
      "         [0.5960, 0.4040]],\n",
      "\n",
      "        [[0.7704, 0.2296],\n",
      "         [0.5849, 0.4151],\n",
      "         [0.6283, 0.3717]],\n",
      "\n",
      "        [[0.6944, 0.3056],\n",
      "         [0.5600, 0.4400],\n",
      "         [0.5248, 0.4752]],\n",
      "\n",
      "        [[0.6825, 0.3175],\n",
      "         [0.5541, 0.4459],\n",
      "         [0.5373, 0.4627]],\n",
      "\n",
      "        [[0.7124, 0.2876],\n",
      "         [0.5704, 0.4296],\n",
      "         [0.5328, 0.4672]],\n",
      "\n",
      "        [[0.7334, 0.2666],\n",
      "         [0.5719, 0.4281],\n",
      "         [0.6009, 0.3991]],\n",
      "\n",
      "        [[0.7030, 0.2970],\n",
      "         [0.5621, 0.4379],\n",
      "         [0.5263, 0.4737]],\n",
      "\n",
      "        [[0.7435, 0.2565],\n",
      "         [0.5735, 0.4265],\n",
      "         [0.5945, 0.4055]],\n",
      "\n",
      "        [[0.8451, 0.1549],\n",
      "         [0.6053, 0.3947],\n",
      "         [0.6558, 0.3442]],\n",
      "\n",
      "        [[0.6806, 0.3194],\n",
      "         [0.5600, 0.4400],\n",
      "         [0.5183, 0.4817]],\n",
      "\n",
      "        [[0.7558, 0.2442],\n",
      "         [0.5738, 0.4262],\n",
      "         [0.6110, 0.3890]],\n",
      "\n",
      "        [[0.7570, 0.2430],\n",
      "         [0.5893, 0.4107],\n",
      "         [0.6215, 0.3785]],\n",
      "\n",
      "        [[0.7463, 0.2537],\n",
      "         [0.5768, 0.4232],\n",
      "         [0.5110, 0.4890]],\n",
      "\n",
      "        [[0.7746, 0.2254],\n",
      "         [0.5916, 0.4084],\n",
      "         [0.6113, 0.3887]],\n",
      "\n",
      "        [[0.7626, 0.2374],\n",
      "         [0.5883, 0.4117],\n",
      "         [0.6195, 0.3805]],\n",
      "\n",
      "        [[0.7097, 0.2903],\n",
      "         [0.5517, 0.4483],\n",
      "         [0.6213, 0.3787]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.4281, 0.3158, 0.1530, 0.1031],\n",
      "        [0.4803, 0.3368, 0.1232, 0.0597],\n",
      "        [0.4393, 0.3142, 0.1472, 0.0994],\n",
      "        [0.3994, 0.3080, 0.1527, 0.1399],\n",
      "        [0.4689, 0.3403, 0.1301, 0.0607],\n",
      "        [0.4067, 0.3186, 0.1659, 0.1088],\n",
      "        [0.3971, 0.3087, 0.1551, 0.1391],\n",
      "        [0.4040, 0.3056, 0.1505, 0.1400],\n",
      "        [0.4185, 0.3100, 0.1414, 0.1301],\n",
      "        [0.3730, 0.3011, 0.1942, 0.1316],\n",
      "        [0.4506, 0.3198, 0.1443, 0.0854],\n",
      "        [0.3888, 0.3055, 0.1604, 0.1452],\n",
      "        [0.3782, 0.3043, 0.1706, 0.1469],\n",
      "        [0.4063, 0.3061, 0.1532, 0.1343],\n",
      "        [0.4195, 0.3140, 0.1602, 0.1064],\n",
      "        [0.3951, 0.3079, 0.1563, 0.1407],\n",
      "        [0.4264, 0.3171, 0.1525, 0.1040],\n",
      "        [0.5115, 0.3336, 0.1016, 0.0533],\n",
      "        [0.3812, 0.2994, 0.1655, 0.1538],\n",
      "        [0.4337, 0.3221, 0.1492, 0.0950],\n",
      "        [0.4461, 0.3109, 0.1510, 0.0920],\n",
      "        [0.4305, 0.3158, 0.1296, 0.1241],\n",
      "        [0.4583, 0.3163, 0.1378, 0.0876],\n",
      "        [0.4487, 0.3140, 0.1470, 0.0903],\n",
      "        [0.3915, 0.3182, 0.1804, 0.1099]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.4281, 0.3158, 0.1530, 0.1031],\n",
      "        [0.4803, 0.3368, 0.1232, 0.0597],\n",
      "        [0.4393, 0.3142, 0.1472, 0.0994],\n",
      "        [0.3994, 0.3080, 0.1527, 0.1399],\n",
      "        [0.4689, 0.3403, 0.1301, 0.0607],\n",
      "        [0.4067, 0.3186, 0.1659, 0.1088],\n",
      "        [0.3971, 0.3087, 0.1551, 0.1391],\n",
      "        [0.4040, 0.3056, 0.1505, 0.1400],\n",
      "        [0.4185, 0.3100, 0.1414, 0.1301],\n",
      "        [0.3730, 0.3011, 0.1942, 0.1316],\n",
      "        [0.4506, 0.3198, 0.1443, 0.0854],\n",
      "        [0.3888, 0.3055, 0.1604, 0.1452],\n",
      "        [0.3782, 0.3043, 0.1706, 0.1469],\n",
      "        [0.4063, 0.3061, 0.1532, 0.1343],\n",
      "        [0.4195, 0.3140, 0.1602, 0.1064],\n",
      "        [0.3951, 0.3079, 0.1563, 0.1407],\n",
      "        [0.4264, 0.3171, 0.1525, 0.1040],\n",
      "        [0.5115, 0.3336, 0.1016, 0.0533],\n",
      "        [0.3812, 0.2994, 0.1655, 0.1538],\n",
      "        [0.4337, 0.3221, 0.1492, 0.0950],\n",
      "        [0.4461, 0.3109, 0.1510, 0.0920],\n",
      "        [0.4305, 0.3158, 0.1296, 0.1241],\n",
      "        [0.4583, 0.3163, 0.1378, 0.0876],\n",
      "        [0.4487, 0.3140, 0.1470, 0.0903],\n",
      "        [0.3915, 0.3182, 0.1804, 0.1099]], grad_fn=<ViewBackward>) tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0772,  0.0241,  0.2108],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 08 | Loss: 1.09562 | Correct: 011/025\n",
      "X tensor([[0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750]])\n",
      "path_prob tensor([[0.7493, 0.5824, 0.5929],\n",
      "        [0.8233, 0.5980, 0.6665],\n",
      "        [0.7591, 0.5904, 0.5921],\n",
      "        [0.7116, 0.5695, 0.5202],\n",
      "        [0.8155, 0.5895, 0.6750],\n",
      "        [0.7306, 0.5673, 0.5992],\n",
      "        [0.7099, 0.5675, 0.5255],\n",
      "        [0.7137, 0.5742, 0.5164],\n",
      "        [0.7330, 0.5801, 0.5190],\n",
      "        [0.6787, 0.5584, 0.5914],\n",
      "        [0.7761, 0.5928, 0.6225],\n",
      "        [0.6985, 0.5648, 0.5231],\n",
      "        [0.6865, 0.5586, 0.5351],\n",
      "        [0.7167, 0.5755, 0.5305],\n",
      "        [0.7388, 0.5787, 0.5961],\n",
      "        [0.7071, 0.5670, 0.5245],\n",
      "        [0.7490, 0.5805, 0.5900],\n",
      "        [0.8511, 0.6163, 0.6493],\n",
      "        [0.6844, 0.5643, 0.5167],\n",
      "        [0.7614, 0.5812, 0.6060],\n",
      "        [0.7627, 0.5968, 0.6156],\n",
      "        [0.7509, 0.5827, 0.5097],\n",
      "        [0.7803, 0.5996, 0.6059],\n",
      "        [0.7685, 0.5961, 0.6138],\n",
      "        [0.7148, 0.5578, 0.6162]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7493],\n",
      "         [0.5824],\n",
      "         [0.5929]],\n",
      "\n",
      "        [[0.8233],\n",
      "         [0.5980],\n",
      "         [0.6665]],\n",
      "\n",
      "        [[0.7591],\n",
      "         [0.5904],\n",
      "         [0.5921]],\n",
      "\n",
      "        [[0.7116],\n",
      "         [0.5695],\n",
      "         [0.5202]],\n",
      "\n",
      "        [[0.8155],\n",
      "         [0.5895],\n",
      "         [0.6750]],\n",
      "\n",
      "        [[0.7306],\n",
      "         [0.5673],\n",
      "         [0.5992]],\n",
      "\n",
      "        [[0.7099],\n",
      "         [0.5675],\n",
      "         [0.5255]],\n",
      "\n",
      "        [[0.7137],\n",
      "         [0.5742],\n",
      "         [0.5164]],\n",
      "\n",
      "        [[0.7330],\n",
      "         [0.5801],\n",
      "         [0.5190]],\n",
      "\n",
      "        [[0.6787],\n",
      "         [0.5584],\n",
      "         [0.5914]],\n",
      "\n",
      "        [[0.7761],\n",
      "         [0.5928],\n",
      "         [0.6225]],\n",
      "\n",
      "        [[0.6985],\n",
      "         [0.5648],\n",
      "         [0.5231]],\n",
      "\n",
      "        [[0.6865],\n",
      "         [0.5586],\n",
      "         [0.5351]],\n",
      "\n",
      "        [[0.7167],\n",
      "         [0.5755],\n",
      "         [0.5305]],\n",
      "\n",
      "        [[0.7388],\n",
      "         [0.5787],\n",
      "         [0.5961]],\n",
      "\n",
      "        [[0.7071],\n",
      "         [0.5670],\n",
      "         [0.5245]],\n",
      "\n",
      "        [[0.7490],\n",
      "         [0.5805],\n",
      "         [0.5900]],\n",
      "\n",
      "        [[0.8511],\n",
      "         [0.6163],\n",
      "         [0.6493]],\n",
      "\n",
      "        [[0.6844],\n",
      "         [0.5643],\n",
      "         [0.5167]],\n",
      "\n",
      "        [[0.7614],\n",
      "         [0.5812],\n",
      "         [0.6060]],\n",
      "\n",
      "        [[0.7627],\n",
      "         [0.5968],\n",
      "         [0.6156]],\n",
      "\n",
      "        [[0.7509],\n",
      "         [0.5827],\n",
      "         [0.5097]],\n",
      "\n",
      "        [[0.7803],\n",
      "         [0.5996],\n",
      "         [0.6059]],\n",
      "\n",
      "        [[0.7685],\n",
      "         [0.5961],\n",
      "         [0.6138]],\n",
      "\n",
      "        [[0.7148],\n",
      "         [0.5578],\n",
      "         [0.6162]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7493, 0.2507],\n",
      "         [0.5824, 0.4176],\n",
      "         [0.5929, 0.4071]],\n",
      "\n",
      "        [[0.8233, 0.1767],\n",
      "         [0.5980, 0.4020],\n",
      "         [0.6665, 0.3335]],\n",
      "\n",
      "        [[0.7591, 0.2409],\n",
      "         [0.5904, 0.4096],\n",
      "         [0.5921, 0.4079]],\n",
      "\n",
      "        [[0.7116, 0.2884],\n",
      "         [0.5695, 0.4305],\n",
      "         [0.5202, 0.4798]],\n",
      "\n",
      "        [[0.8155, 0.1845],\n",
      "         [0.5895, 0.4105],\n",
      "         [0.6750, 0.3250]],\n",
      "\n",
      "        [[0.7306, 0.2694],\n",
      "         [0.5673, 0.4327],\n",
      "         [0.5992, 0.4008]],\n",
      "\n",
      "        [[0.7099, 0.2901],\n",
      "         [0.5675, 0.4325],\n",
      "         [0.5255, 0.4745]],\n",
      "\n",
      "        [[0.7137, 0.2863],\n",
      "         [0.5742, 0.4258],\n",
      "         [0.5164, 0.4836]],\n",
      "\n",
      "        [[0.7330, 0.2670],\n",
      "         [0.5801, 0.4199],\n",
      "         [0.5190, 0.4810]],\n",
      "\n",
      "        [[0.6787, 0.3213],\n",
      "         [0.5584, 0.4416],\n",
      "         [0.5914, 0.4086]],\n",
      "\n",
      "        [[0.7761, 0.2239],\n",
      "         [0.5928, 0.4072],\n",
      "         [0.6225, 0.3775]],\n",
      "\n",
      "        [[0.6985, 0.3015],\n",
      "         [0.5648, 0.4352],\n",
      "         [0.5231, 0.4769]],\n",
      "\n",
      "        [[0.6865, 0.3135],\n",
      "         [0.5586, 0.4414],\n",
      "         [0.5351, 0.4649]],\n",
      "\n",
      "        [[0.7167, 0.2833],\n",
      "         [0.5755, 0.4245],\n",
      "         [0.5305, 0.4695]],\n",
      "\n",
      "        [[0.7388, 0.2612],\n",
      "         [0.5787, 0.4213],\n",
      "         [0.5961, 0.4039]],\n",
      "\n",
      "        [[0.7071, 0.2929],\n",
      "         [0.5670, 0.4330],\n",
      "         [0.5245, 0.4755]],\n",
      "\n",
      "        [[0.7490, 0.2510],\n",
      "         [0.5805, 0.4195],\n",
      "         [0.5900, 0.4100]],\n",
      "\n",
      "        [[0.8511, 0.1489],\n",
      "         [0.6163, 0.3837],\n",
      "         [0.6493, 0.3507]],\n",
      "\n",
      "        [[0.6844, 0.3156],\n",
      "         [0.5643, 0.4357],\n",
      "         [0.5167, 0.4833]],\n",
      "\n",
      "        [[0.7614, 0.2386],\n",
      "         [0.5812, 0.4188],\n",
      "         [0.6060, 0.3940]],\n",
      "\n",
      "        [[0.7627, 0.2373],\n",
      "         [0.5968, 0.4032],\n",
      "         [0.6156, 0.3844]],\n",
      "\n",
      "        [[0.7509, 0.2491],\n",
      "         [0.5827, 0.4173],\n",
      "         [0.5097, 0.4903]],\n",
      "\n",
      "        [[0.7803, 0.2197],\n",
      "         [0.5996, 0.4004],\n",
      "         [0.6059, 0.3941]],\n",
      "\n",
      "        [[0.7685, 0.2315],\n",
      "         [0.5961, 0.4039],\n",
      "         [0.6138, 0.3862]],\n",
      "\n",
      "        [[0.7148, 0.2852],\n",
      "         [0.5578, 0.4422],\n",
      "         [0.6162, 0.3838]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.4365, 0.3129, 0.1486, 0.1020],\n",
      "        [0.4923, 0.3310, 0.1178, 0.0589],\n",
      "        [0.4482, 0.3109, 0.1427, 0.0983],\n",
      "        [0.4053, 0.3063, 0.1500, 0.1384],\n",
      "        [0.4808, 0.3348, 0.1245, 0.0599],\n",
      "        [0.4145, 0.3161, 0.1614, 0.1080],\n",
      "        [0.4029, 0.3071, 0.1524, 0.1376],\n",
      "        [0.4098, 0.3039, 0.1478, 0.1385],\n",
      "        [0.4252, 0.3078, 0.1386, 0.1284],\n",
      "        [0.3789, 0.2997, 0.1900, 0.1313],\n",
      "        [0.4601, 0.3160, 0.1393, 0.0845],\n",
      "        [0.3945, 0.3040, 0.1577, 0.1438],\n",
      "        [0.3834, 0.3030, 0.1678, 0.1458],\n",
      "        [0.4125, 0.3043, 0.1503, 0.1330],\n",
      "        [0.4275, 0.3113, 0.1557, 0.1055],\n",
      "        [0.4009, 0.3062, 0.1536, 0.1392],\n",
      "        [0.4348, 0.3142, 0.1481, 0.1029],\n",
      "        [0.5245, 0.3265, 0.0967, 0.0522],\n",
      "        [0.3862, 0.2982, 0.1631, 0.1525],\n",
      "        [0.4425, 0.3189, 0.1446, 0.0940],\n",
      "        [0.4552, 0.3075, 0.1461, 0.0912],\n",
      "        [0.4375, 0.3133, 0.1270, 0.1221],\n",
      "        [0.4679, 0.3124, 0.1331, 0.0866],\n",
      "        [0.4581, 0.3104, 0.1421, 0.0894],\n",
      "        [0.3988, 0.3161, 0.1757, 0.1094]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.4365, 0.3129, 0.1486, 0.1020],\n",
      "        [0.4923, 0.3310, 0.1178, 0.0589],\n",
      "        [0.4482, 0.3109, 0.1427, 0.0983],\n",
      "        [0.4053, 0.3063, 0.1500, 0.1384],\n",
      "        [0.4808, 0.3348, 0.1245, 0.0599],\n",
      "        [0.4145, 0.3161, 0.1614, 0.1080],\n",
      "        [0.4029, 0.3071, 0.1524, 0.1376],\n",
      "        [0.4098, 0.3039, 0.1478, 0.1385],\n",
      "        [0.4252, 0.3078, 0.1386, 0.1284],\n",
      "        [0.3789, 0.2997, 0.1900, 0.1313],\n",
      "        [0.4601, 0.3160, 0.1393, 0.0845],\n",
      "        [0.3945, 0.3040, 0.1577, 0.1438],\n",
      "        [0.3834, 0.3030, 0.1678, 0.1458],\n",
      "        [0.4125, 0.3043, 0.1503, 0.1330],\n",
      "        [0.4275, 0.3113, 0.1557, 0.1055],\n",
      "        [0.4009, 0.3062, 0.1536, 0.1392],\n",
      "        [0.4348, 0.3142, 0.1481, 0.1029],\n",
      "        [0.5245, 0.3265, 0.0967, 0.0522],\n",
      "        [0.3862, 0.2982, 0.1631, 0.1525],\n",
      "        [0.4425, 0.3189, 0.1446, 0.0940],\n",
      "        [0.4552, 0.3075, 0.1461, 0.0912],\n",
      "        [0.4375, 0.3133, 0.1270, 0.1221],\n",
      "        [0.4679, 0.3124, 0.1331, 0.0866],\n",
      "        [0.4581, 0.3104, 0.1421, 0.0894],\n",
      "        [0.3988, 0.3161, 0.1757, 0.1094]], grad_fn=<ViewBackward>) tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0845,  0.0194,  0.2214],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<MmBackward>)\n",
      "Epoch: 09 | Loss: 1.09523 | Correct: 011/025\n"
     ]
    }
   ],
   "source": [
    "tree.fit(X_train[:25], y_train[:25], batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[0.5000, 0.3333, 0.6271, 0.4583],\n",
      "        [0.3889, 0.7500, 0.1186, 0.0833],\n",
      "        [0.9444, 0.2500, 1.0000, 0.9167],\n",
      "        [0.4722, 0.3750, 0.5932, 0.5833],\n",
      "        [0.6944, 0.3333, 0.6441, 0.5417],\n",
      "        [0.3056, 0.5833, 0.0847, 0.1250],\n",
      "        [0.3611, 0.3750, 0.4407, 0.5000],\n",
      "        [0.7222, 0.4583, 0.6949, 0.9167],\n",
      "        [0.5278, 0.0833, 0.5932, 0.5833],\n",
      "        [0.4167, 0.2917, 0.4915, 0.4583],\n",
      "        [0.6111, 0.5000, 0.6949, 0.7917],\n",
      "        [0.1389, 0.4167, 0.0678, 0.0000],\n",
      "        [0.3333, 0.6250, 0.0508, 0.0417],\n",
      "        [0.1667, 0.4583, 0.0847, 0.0000],\n",
      "        [0.2222, 0.7500, 0.0847, 0.0833],\n",
      "        [0.5556, 0.5417, 0.6271, 0.6250],\n",
      "        [0.6111, 0.4167, 0.8136, 0.8750],\n",
      "        [0.3611, 0.2083, 0.4915, 0.4167],\n",
      "        [0.3889, 0.3333, 0.5932, 0.5000],\n",
      "        [0.5833, 0.3333, 0.7797, 0.8750],\n",
      "        [0.1111, 0.5000, 0.1017, 0.0417],\n",
      "        [0.5000, 0.4167, 0.6610, 0.7083],\n",
      "        [0.1944, 0.5833, 0.1017, 0.1250],\n",
      "        [0.5833, 0.3333, 0.7797, 0.8333],\n",
      "        [1.0000, 0.7500, 0.9153, 0.7917],\n",
      "        [0.6667, 0.4167, 0.7119, 0.9167],\n",
      "        [0.6667, 0.2083, 0.8136, 0.7083],\n",
      "        [0.6944, 0.5000, 0.8305, 0.9167],\n",
      "        [0.1389, 0.4167, 0.0678, 0.0833],\n",
      "        [0.1389, 0.4583, 0.1017, 0.0417],\n",
      "        [0.0833, 0.6667, 0.0000, 0.0417],\n",
      "        [0.3889, 1.0000, 0.0847, 0.1250],\n",
      "        [0.6667, 0.4583, 0.5763, 0.5417],\n",
      "        [0.1389, 0.5833, 0.1017, 0.0417],\n",
      "        [0.0278, 0.5000, 0.0508, 0.0417],\n",
      "        [0.5556, 0.2083, 0.6780, 0.7500],\n",
      "        [0.5833, 0.5000, 0.5932, 0.5833],\n",
      "        [0.2500, 0.6250, 0.0847, 0.0417],\n",
      "        [0.1944, 0.6667, 0.0678, 0.0417],\n",
      "        [0.2500, 0.8750, 0.0847, 0.0000],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.4722, 0.5833, 0.5932, 0.6250],\n",
      "        [0.6667, 0.4583, 0.6271, 0.5833],\n",
      "        [0.3056, 0.7917, 0.0508, 0.1250],\n",
      "        [0.3056, 0.7083, 0.0847, 0.0417],\n",
      "        [0.3333, 0.1667, 0.4576, 0.3750],\n",
      "        [0.5556, 0.3333, 0.6949, 0.5833],\n",
      "        [0.5833, 0.4583, 0.7627, 0.7083],\n",
      "        [0.6389, 0.4167, 0.5763, 0.5417],\n",
      "        [0.8056, 0.6667, 0.8644, 1.0000]])\n",
      "path_prob tensor([[0.7561, 0.5803, 0.6069],\n",
      "        [0.7486, 0.5825, 0.5286],\n",
      "        [0.8255, 0.6062, 0.6754],\n",
      "        [0.7704, 0.5955, 0.5992],\n",
      "        [0.7811, 0.5873, 0.6239],\n",
      "        [0.7288, 0.5797, 0.5298],\n",
      "        [0.7526, 0.5930, 0.5776],\n",
      "        [0.8263, 0.6268, 0.6222],\n",
      "        [0.7477, 0.5811, 0.6223],\n",
      "        [0.7450, 0.5831, 0.5918],\n",
      "        [0.8111, 0.6174, 0.6106],\n",
      "        [0.6796, 0.5603, 0.5254],\n",
      "        [0.7266, 0.5748, 0.5266],\n",
      "        [0.6871, 0.5617, 0.5264],\n",
      "        [0.7332, 0.5841, 0.5116],\n",
      "        [0.7957, 0.6062, 0.5980],\n",
      "        [0.8114, 0.6172, 0.6252],\n",
      "        [0.7268, 0.5751, 0.5926],\n",
      "        [0.7506, 0.5857, 0.5950],\n",
      "        [0.8027, 0.6144, 0.6256],\n",
      "        [0.6909, 0.5673, 0.5203],\n",
      "        [0.7882, 0.6070, 0.6042],\n",
      "        [0.7180, 0.5793, 0.5218],\n",
      "        [0.7989, 0.6105, 0.6256],\n",
      "        [0.8546, 0.6212, 0.6433],\n",
      "        [0.8192, 0.6244, 0.6217],\n",
      "        [0.7833, 0.5913, 0.6426],\n",
      "        [0.8271, 0.6243, 0.6280],\n",
      "        [0.6897, 0.5684, 0.5253],\n",
      "        [0.6892, 0.5652, 0.5254],\n",
      "        [0.7066, 0.5792, 0.4982],\n",
      "        [0.7763, 0.5999, 0.5091],\n",
      "        [0.7899, 0.5958, 0.6084],\n",
      "        [0.7029, 0.5714, 0.5171],\n",
      "        [0.6824, 0.5693, 0.5089],\n",
      "        [0.7784, 0.6001, 0.6232],\n",
      "        [0.7906, 0.6013, 0.6003],\n",
      "        [0.7184, 0.5738, 0.5223],\n",
      "        [0.7174, 0.5766, 0.5134],\n",
      "        [0.7393, 0.5821, 0.5056],\n",
      "        [0.7742, 0.6039, 0.6081],\n",
      "        [0.7927, 0.6095, 0.5857],\n",
      "        [0.7936, 0.5980, 0.6124],\n",
      "        [0.7496, 0.5911, 0.5131],\n",
      "        [0.7323, 0.5778, 0.5215],\n",
      "        [0.7152, 0.5702, 0.5903],\n",
      "        [0.7734, 0.5898, 0.6168],\n",
      "        [0.7981, 0.6054, 0.6164],\n",
      "        [0.7840, 0.5938, 0.6088],\n",
      "        [0.8521, 0.6385, 0.6291]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.7561],\n",
      "         [0.5803],\n",
      "         [0.6069]],\n",
      "\n",
      "        [[0.7486],\n",
      "         [0.5825],\n",
      "         [0.5286]],\n",
      "\n",
      "        [[0.8255],\n",
      "         [0.6062],\n",
      "         [0.6754]],\n",
      "\n",
      "        [[0.7704],\n",
      "         [0.5955],\n",
      "         [0.5992]],\n",
      "\n",
      "        [[0.7811],\n",
      "         [0.5873],\n",
      "         [0.6239]],\n",
      "\n",
      "        [[0.7288],\n",
      "         [0.5797],\n",
      "         [0.5298]],\n",
      "\n",
      "        [[0.7526],\n",
      "         [0.5930],\n",
      "         [0.5776]],\n",
      "\n",
      "        [[0.8263],\n",
      "         [0.6268],\n",
      "         [0.6222]],\n",
      "\n",
      "        [[0.7477],\n",
      "         [0.5811],\n",
      "         [0.6223]],\n",
      "\n",
      "        [[0.7450],\n",
      "         [0.5831],\n",
      "         [0.5918]],\n",
      "\n",
      "        [[0.8111],\n",
      "         [0.6174],\n",
      "         [0.6106]],\n",
      "\n",
      "        [[0.6796],\n",
      "         [0.5603],\n",
      "         [0.5254]],\n",
      "\n",
      "        [[0.7266],\n",
      "         [0.5748],\n",
      "         [0.5266]],\n",
      "\n",
      "        [[0.6871],\n",
      "         [0.5617],\n",
      "         [0.5264]],\n",
      "\n",
      "        [[0.7332],\n",
      "         [0.5841],\n",
      "         [0.5116]],\n",
      "\n",
      "        [[0.7957],\n",
      "         [0.6062],\n",
      "         [0.5980]],\n",
      "\n",
      "        [[0.8114],\n",
      "         [0.6172],\n",
      "         [0.6252]],\n",
      "\n",
      "        [[0.7268],\n",
      "         [0.5751],\n",
      "         [0.5926]],\n",
      "\n",
      "        [[0.7506],\n",
      "         [0.5857],\n",
      "         [0.5950]],\n",
      "\n",
      "        [[0.8027],\n",
      "         [0.6144],\n",
      "         [0.6256]],\n",
      "\n",
      "        [[0.6909],\n",
      "         [0.5673],\n",
      "         [0.5203]],\n",
      "\n",
      "        [[0.7882],\n",
      "         [0.6070],\n",
      "         [0.6042]],\n",
      "\n",
      "        [[0.7180],\n",
      "         [0.5793],\n",
      "         [0.5218]],\n",
      "\n",
      "        [[0.7989],\n",
      "         [0.6105],\n",
      "         [0.6256]],\n",
      "\n",
      "        [[0.8546],\n",
      "         [0.6212],\n",
      "         [0.6433]],\n",
      "\n",
      "        [[0.8192],\n",
      "         [0.6244],\n",
      "         [0.6217]],\n",
      "\n",
      "        [[0.7833],\n",
      "         [0.5913],\n",
      "         [0.6426]],\n",
      "\n",
      "        [[0.8271],\n",
      "         [0.6243],\n",
      "         [0.6280]],\n",
      "\n",
      "        [[0.6897],\n",
      "         [0.5684],\n",
      "         [0.5253]],\n",
      "\n",
      "        [[0.6892],\n",
      "         [0.5652],\n",
      "         [0.5254]],\n",
      "\n",
      "        [[0.7066],\n",
      "         [0.5792],\n",
      "         [0.4982]],\n",
      "\n",
      "        [[0.7763],\n",
      "         [0.5999],\n",
      "         [0.5091]],\n",
      "\n",
      "        [[0.7899],\n",
      "         [0.5958],\n",
      "         [0.6084]],\n",
      "\n",
      "        [[0.7029],\n",
      "         [0.5714],\n",
      "         [0.5171]],\n",
      "\n",
      "        [[0.6824],\n",
      "         [0.5693],\n",
      "         [0.5089]],\n",
      "\n",
      "        [[0.7784],\n",
      "         [0.6001],\n",
      "         [0.6232]],\n",
      "\n",
      "        [[0.7906],\n",
      "         [0.6013],\n",
      "         [0.6003]],\n",
      "\n",
      "        [[0.7184],\n",
      "         [0.5738],\n",
      "         [0.5223]],\n",
      "\n",
      "        [[0.7174],\n",
      "         [0.5766],\n",
      "         [0.5134]],\n",
      "\n",
      "        [[0.7393],\n",
      "         [0.5821],\n",
      "         [0.5056]],\n",
      "\n",
      "        [[0.7742],\n",
      "         [0.6039],\n",
      "         [0.6081]],\n",
      "\n",
      "        [[0.7927],\n",
      "         [0.6095],\n",
      "         [0.5857]],\n",
      "\n",
      "        [[0.7936],\n",
      "         [0.5980],\n",
      "         [0.6124]],\n",
      "\n",
      "        [[0.7496],\n",
      "         [0.5911],\n",
      "         [0.5131]],\n",
      "\n",
      "        [[0.7323],\n",
      "         [0.5778],\n",
      "         [0.5215]],\n",
      "\n",
      "        [[0.7152],\n",
      "         [0.5702],\n",
      "         [0.5903]],\n",
      "\n",
      "        [[0.7734],\n",
      "         [0.5898],\n",
      "         [0.6168]],\n",
      "\n",
      "        [[0.7981],\n",
      "         [0.6054],\n",
      "         [0.6164]],\n",
      "\n",
      "        [[0.7840],\n",
      "         [0.5938],\n",
      "         [0.6088]],\n",
      "\n",
      "        [[0.8521],\n",
      "         [0.6385],\n",
      "         [0.6291]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.7561, 0.2439],\n",
      "         [0.5803, 0.4197],\n",
      "         [0.6069, 0.3931]],\n",
      "\n",
      "        [[0.7486, 0.2514],\n",
      "         [0.5825, 0.4175],\n",
      "         [0.5286, 0.4714]],\n",
      "\n",
      "        [[0.8255, 0.1745],\n",
      "         [0.6062, 0.3938],\n",
      "         [0.6754, 0.3246]],\n",
      "\n",
      "        [[0.7704, 0.2296],\n",
      "         [0.5955, 0.4045],\n",
      "         [0.5992, 0.4008]],\n",
      "\n",
      "        [[0.7811, 0.2189],\n",
      "         [0.5873, 0.4127],\n",
      "         [0.6239, 0.3761]],\n",
      "\n",
      "        [[0.7288, 0.2712],\n",
      "         [0.5797, 0.4203],\n",
      "         [0.5298, 0.4702]],\n",
      "\n",
      "        [[0.7526, 0.2474],\n",
      "         [0.5930, 0.4070],\n",
      "         [0.5776, 0.4224]],\n",
      "\n",
      "        [[0.8263, 0.1737],\n",
      "         [0.6268, 0.3732],\n",
      "         [0.6222, 0.3778]],\n",
      "\n",
      "        [[0.7477, 0.2523],\n",
      "         [0.5811, 0.4189],\n",
      "         [0.6223, 0.3777]],\n",
      "\n",
      "        [[0.7450, 0.2550],\n",
      "         [0.5831, 0.4169],\n",
      "         [0.5918, 0.4082]],\n",
      "\n",
      "        [[0.8111, 0.1889],\n",
      "         [0.6174, 0.3826],\n",
      "         [0.6106, 0.3894]],\n",
      "\n",
      "        [[0.6796, 0.3204],\n",
      "         [0.5603, 0.4397],\n",
      "         [0.5254, 0.4746]],\n",
      "\n",
      "        [[0.7266, 0.2734],\n",
      "         [0.5748, 0.4252],\n",
      "         [0.5266, 0.4734]],\n",
      "\n",
      "        [[0.6871, 0.3129],\n",
      "         [0.5617, 0.4383],\n",
      "         [0.5264, 0.4736]],\n",
      "\n",
      "        [[0.7332, 0.2668],\n",
      "         [0.5841, 0.4159],\n",
      "         [0.5116, 0.4884]],\n",
      "\n",
      "        [[0.7957, 0.2043],\n",
      "         [0.6062, 0.3938],\n",
      "         [0.5980, 0.4020]],\n",
      "\n",
      "        [[0.8114, 0.1886],\n",
      "         [0.6172, 0.3828],\n",
      "         [0.6252, 0.3748]],\n",
      "\n",
      "        [[0.7268, 0.2732],\n",
      "         [0.5751, 0.4249],\n",
      "         [0.5926, 0.4074]],\n",
      "\n",
      "        [[0.7506, 0.2494],\n",
      "         [0.5857, 0.4143],\n",
      "         [0.5950, 0.4050]],\n",
      "\n",
      "        [[0.8027, 0.1973],\n",
      "         [0.6144, 0.3856],\n",
      "         [0.6256, 0.3744]],\n",
      "\n",
      "        [[0.6909, 0.3091],\n",
      "         [0.5673, 0.4327],\n",
      "         [0.5203, 0.4797]],\n",
      "\n",
      "        [[0.7882, 0.2118],\n",
      "         [0.6070, 0.3930],\n",
      "         [0.6042, 0.3958]],\n",
      "\n",
      "        [[0.7180, 0.2820],\n",
      "         [0.5793, 0.4207],\n",
      "         [0.5218, 0.4782]],\n",
      "\n",
      "        [[0.7989, 0.2011],\n",
      "         [0.6105, 0.3895],\n",
      "         [0.6256, 0.3744]],\n",
      "\n",
      "        [[0.8546, 0.1454],\n",
      "         [0.6212, 0.3788],\n",
      "         [0.6433, 0.3567]],\n",
      "\n",
      "        [[0.8192, 0.1808],\n",
      "         [0.6244, 0.3756],\n",
      "         [0.6217, 0.3783]],\n",
      "\n",
      "        [[0.7833, 0.2167],\n",
      "         [0.5913, 0.4087],\n",
      "         [0.6426, 0.3574]],\n",
      "\n",
      "        [[0.8271, 0.1729],\n",
      "         [0.6243, 0.3757],\n",
      "         [0.6280, 0.3720]],\n",
      "\n",
      "        [[0.6897, 0.3103],\n",
      "         [0.5684, 0.4316],\n",
      "         [0.5253, 0.4747]],\n",
      "\n",
      "        [[0.6892, 0.3108],\n",
      "         [0.5652, 0.4348],\n",
      "         [0.5254, 0.4746]],\n",
      "\n",
      "        [[0.7066, 0.2934],\n",
      "         [0.5792, 0.4208],\n",
      "         [0.4982, 0.5018]],\n",
      "\n",
      "        [[0.7763, 0.2237],\n",
      "         [0.5999, 0.4001],\n",
      "         [0.5091, 0.4909]],\n",
      "\n",
      "        [[0.7899, 0.2101],\n",
      "         [0.5958, 0.4042],\n",
      "         [0.6084, 0.3916]],\n",
      "\n",
      "        [[0.7029, 0.2971],\n",
      "         [0.5714, 0.4286],\n",
      "         [0.5171, 0.4829]],\n",
      "\n",
      "        [[0.6824, 0.3176],\n",
      "         [0.5693, 0.4307],\n",
      "         [0.5089, 0.4911]],\n",
      "\n",
      "        [[0.7784, 0.2216],\n",
      "         [0.6001, 0.3999],\n",
      "         [0.6232, 0.3768]],\n",
      "\n",
      "        [[0.7906, 0.2094],\n",
      "         [0.6013, 0.3987],\n",
      "         [0.6003, 0.3997]],\n",
      "\n",
      "        [[0.7184, 0.2816],\n",
      "         [0.5738, 0.4262],\n",
      "         [0.5223, 0.4777]],\n",
      "\n",
      "        [[0.7174, 0.2826],\n",
      "         [0.5766, 0.4234],\n",
      "         [0.5134, 0.4866]],\n",
      "\n",
      "        [[0.7393, 0.2607],\n",
      "         [0.5821, 0.4179],\n",
      "         [0.5056, 0.4944]],\n",
      "\n",
      "        [[0.7742, 0.2258],\n",
      "         [0.6039, 0.3961],\n",
      "         [0.6081, 0.3919]],\n",
      "\n",
      "        [[0.7927, 0.2073],\n",
      "         [0.6095, 0.3905],\n",
      "         [0.5857, 0.4143]],\n",
      "\n",
      "        [[0.7936, 0.2064],\n",
      "         [0.5980, 0.4020],\n",
      "         [0.6124, 0.3876]],\n",
      "\n",
      "        [[0.7496, 0.2504],\n",
      "         [0.5911, 0.4089],\n",
      "         [0.5131, 0.4869]],\n",
      "\n",
      "        [[0.7323, 0.2677],\n",
      "         [0.5778, 0.4222],\n",
      "         [0.5215, 0.4785]],\n",
      "\n",
      "        [[0.7152, 0.2848],\n",
      "         [0.5702, 0.4298],\n",
      "         [0.5903, 0.4097]],\n",
      "\n",
      "        [[0.7734, 0.2266],\n",
      "         [0.5898, 0.4102],\n",
      "         [0.6168, 0.3832]],\n",
      "\n",
      "        [[0.7981, 0.2019],\n",
      "         [0.6054, 0.3946],\n",
      "         [0.6164, 0.3836]],\n",
      "\n",
      "        [[0.7840, 0.2160],\n",
      "         [0.5938, 0.4062],\n",
      "         [0.6088, 0.3912]],\n",
      "\n",
      "        [[0.8521, 0.1479],\n",
      "         [0.6385, 0.3615],\n",
      "         [0.6291, 0.3709]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.4387, 0.3174, 0.1480, 0.0959],\n",
      "        [0.4361, 0.3125, 0.1329, 0.1185],\n",
      "        [0.5004, 0.3251, 0.1178, 0.0566],\n",
      "        [0.4587, 0.3116, 0.1376, 0.0920],\n",
      "        [0.4587, 0.3224, 0.1366, 0.0823],\n",
      "        [0.4225, 0.3063, 0.1437, 0.1275],\n",
      "        [0.4463, 0.3063, 0.1429, 0.1045],\n",
      "        [0.5179, 0.3084, 0.1081, 0.0656],\n",
      "        [0.4345, 0.3132, 0.1570, 0.0953],\n",
      "        [0.4344, 0.3106, 0.1509, 0.1041],\n",
      "        [0.5008, 0.3103, 0.1153, 0.0736],\n",
      "        [0.3808, 0.2988, 0.1683, 0.1521],\n",
      "        [0.4177, 0.3089, 0.1440, 0.1294],\n",
      "        [0.3860, 0.3012, 0.1647, 0.1482],\n",
      "        [0.4282, 0.3050, 0.1365, 0.1303],\n",
      "        [0.4823, 0.3134, 0.1222, 0.0821],\n",
      "        [0.5008, 0.3106, 0.1179, 0.0707],\n",
      "        [0.4180, 0.3089, 0.1619, 0.1113],\n",
      "        [0.4396, 0.3110, 0.1484, 0.1010],\n",
      "        [0.4932, 0.3095, 0.1234, 0.0739],\n",
      "        [0.3920, 0.2989, 0.1608, 0.1483],\n",
      "        [0.4784, 0.3098, 0.1280, 0.0838],\n",
      "        [0.4160, 0.3020, 0.1471, 0.1349],\n",
      "        [0.4878, 0.3112, 0.1258, 0.0753],\n",
      "        [0.5308, 0.3237, 0.0936, 0.0519],\n",
      "        [0.5115, 0.3077, 0.1124, 0.0684],\n",
      "        [0.4631, 0.3201, 0.1393, 0.0775],\n",
      "        [0.5164, 0.3107, 0.1086, 0.0643],\n",
      "        [0.3920, 0.2977, 0.1630, 0.1473],\n",
      "        [0.3895, 0.2997, 0.1633, 0.1475],\n",
      "        [0.4092, 0.2973, 0.1462, 0.1472],\n",
      "        [0.4657, 0.3106, 0.1139, 0.1098],\n",
      "        [0.4706, 0.3193, 0.1278, 0.0823],\n",
      "        [0.4016, 0.3012, 0.1536, 0.1435],\n",
      "        [0.3885, 0.2939, 0.1616, 0.1560],\n",
      "        [0.4672, 0.3113, 0.1381, 0.0835],\n",
      "        [0.4754, 0.3152, 0.1257, 0.0837],\n",
      "        [0.4122, 0.3062, 0.1471, 0.1345],\n",
      "        [0.4136, 0.3037, 0.1451, 0.1375],\n",
      "        [0.4304, 0.3089, 0.1318, 0.1289],\n",
      "        [0.4675, 0.3067, 0.1373, 0.0885],\n",
      "        [0.4832, 0.3095, 0.1214, 0.0859],\n",
      "        [0.4746, 0.3190, 0.1264, 0.0800],\n",
      "        [0.4431, 0.3065, 0.1284, 0.1219],\n",
      "        [0.4232, 0.3092, 0.1396, 0.1281],\n",
      "        [0.4078, 0.3074, 0.1681, 0.1167],\n",
      "        [0.4561, 0.3172, 0.1398, 0.0869],\n",
      "        [0.4831, 0.3150, 0.1245, 0.0774],\n",
      "        [0.4656, 0.3185, 0.1315, 0.0845],\n",
      "        [0.5441, 0.3081, 0.0930, 0.0549]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.4387, 0.3174, 0.1480, 0.0959],\n",
      "        [0.4361, 0.3125, 0.1329, 0.1185],\n",
      "        [0.5004, 0.3251, 0.1178, 0.0566],\n",
      "        [0.4587, 0.3116, 0.1376, 0.0920],\n",
      "        [0.4587, 0.3224, 0.1366, 0.0823],\n",
      "        [0.4225, 0.3063, 0.1437, 0.1275],\n",
      "        [0.4463, 0.3063, 0.1429, 0.1045],\n",
      "        [0.5179, 0.3084, 0.1081, 0.0656],\n",
      "        [0.4345, 0.3132, 0.1570, 0.0953],\n",
      "        [0.4344, 0.3106, 0.1509, 0.1041],\n",
      "        [0.5008, 0.3103, 0.1153, 0.0736],\n",
      "        [0.3808, 0.2988, 0.1683, 0.1521],\n",
      "        [0.4177, 0.3089, 0.1440, 0.1294],\n",
      "        [0.3860, 0.3012, 0.1647, 0.1482],\n",
      "        [0.4282, 0.3050, 0.1365, 0.1303],\n",
      "        [0.4823, 0.3134, 0.1222, 0.0821],\n",
      "        [0.5008, 0.3106, 0.1179, 0.0707],\n",
      "        [0.4180, 0.3089, 0.1619, 0.1113],\n",
      "        [0.4396, 0.3110, 0.1484, 0.1010],\n",
      "        [0.4932, 0.3095, 0.1234, 0.0739],\n",
      "        [0.3920, 0.2989, 0.1608, 0.1483],\n",
      "        [0.4784, 0.3098, 0.1280, 0.0838],\n",
      "        [0.4160, 0.3020, 0.1471, 0.1349],\n",
      "        [0.4878, 0.3112, 0.1258, 0.0753],\n",
      "        [0.5308, 0.3237, 0.0936, 0.0519],\n",
      "        [0.5115, 0.3077, 0.1124, 0.0684],\n",
      "        [0.4631, 0.3201, 0.1393, 0.0775],\n",
      "        [0.5164, 0.3107, 0.1086, 0.0643],\n",
      "        [0.3920, 0.2977, 0.1630, 0.1473],\n",
      "        [0.3895, 0.2997, 0.1633, 0.1475],\n",
      "        [0.4092, 0.2973, 0.1462, 0.1472],\n",
      "        [0.4657, 0.3106, 0.1139, 0.1098],\n",
      "        [0.4706, 0.3193, 0.1278, 0.0823],\n",
      "        [0.4016, 0.3012, 0.1536, 0.1435],\n",
      "        [0.3885, 0.2939, 0.1616, 0.1560],\n",
      "        [0.4672, 0.3113, 0.1381, 0.0835],\n",
      "        [0.4754, 0.3152, 0.1257, 0.0837],\n",
      "        [0.4122, 0.3062, 0.1471, 0.1345],\n",
      "        [0.4136, 0.3037, 0.1451, 0.1375],\n",
      "        [0.4304, 0.3089, 0.1318, 0.1289],\n",
      "        [0.4675, 0.3067, 0.1373, 0.0885],\n",
      "        [0.4832, 0.3095, 0.1214, 0.0859],\n",
      "        [0.4746, 0.3190, 0.1264, 0.0800],\n",
      "        [0.4431, 0.3065, 0.1284, 0.1219],\n",
      "        [0.4232, 0.3092, 0.1396, 0.1281],\n",
      "        [0.4078, 0.3074, 0.1681, 0.1167],\n",
      "        [0.4561, 0.3172, 0.1398, 0.0869],\n",
      "        [0.4831, 0.3150, 0.1245, 0.0774],\n",
      "        [0.4656, 0.3185, 0.1315, 0.0845],\n",
      "        [0.5441, 0.3081, 0.0930, 0.0549]], grad_fn=<ViewBackward>) tensor(0.0031, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0932,  0.0146,  0.2352]], grad_fn=<MmBackward>)\n",
      "\n",
      "Testing Accuracy: 20.0/50 (40.000%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "tree.plot_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[ 0.,  0.,  0., -2.]])\n",
      "path_prob tensor([[0.3428, 0.3496, 0.5365]], grad_fn=<SigmoidBackward>)\n",
      "path_prob tensor([[[0.3428],\n",
      "         [0.3496],\n",
      "         [0.5365]]], grad_fn=<UnsqueezeBackward0>)\n",
      "path_prob tensor([[[0.3428, 0.6572],\n",
      "         [0.3496, 0.6504],\n",
      "         [0.5365, 0.4635]]], grad_fn=<CatBackward>)\n",
      "_mu tensor([[[1.]]])\n",
      "_penalty tensor(0.)\n",
      "mu tensor([[0.1198, 0.2230, 0.3525, 0.3046]], grad_fn=<ViewBackward>)\n",
      "_mu, _penalty tensor([[0.1198, 0.2230, 0.3525, 0.3046]], grad_fn=<ViewBackward>) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "y_pred tensor([[-0.0043,  0.0069, -0.0467]], grad_fn=<MmBackward>)\n",
      "output tensor([[0.3367, 0.3405, 0.3228]], grad_fn=<SoftmaxBackward>)\n",
      "pred tensor([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.predict([[0,0,0,-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "value=2\n",
    "feature_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.6006495060573165>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy())*tf.math.sigmoid(value*tree.inner_nodes[0].weight[1][feature_index].detach().numpy()+tree.inner_nodes[0].bias[1].detach().numpy())\n",
    "prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.2302840874230937>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob2 = tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy())*(1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[1][feature_index].detach().numpy()+tree.inner_nodes[0].bias[1].detach().numpy()))\n",
    "prob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.09042762086644066>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob3 = (1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy()))*tf.math.sigmoid(value*tree.inner_nodes[0].weight[2][feature_index].detach().numpy()+tree.inner_nodes[0].bias[2].detach().numpy())\n",
    "prob3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.07863878565314916>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob4 = (1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[0][feature_index].detach().numpy()+tree.inner_nodes[0].bias[0].detach().numpy()))*(1-tf.math.sigmoid(value*tree.inner_nodes[0].weight[2][feature_index].detach().numpy()+tree.inner_nodes[0].bias[2].detach().numpy()))\n",
    "prob4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([-0.10283867,  0.01608272,  0.25969243])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1 = prob1*tree.leaf_nodes.weight.T[0].detach().numpy()\n",
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 0.06597296, -0.03048799, -0.06809449])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2 = prob2*tree.leaf_nodes.weight.T[1].detach().numpy()\n",
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([-0.00111432,  0.00175781, -0.01198421])>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3 = prob3*tree.leaf_nodes.weight.T[2].detach().numpy()\n",
    "o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([-0.01064835, -0.008296  , -0.00356906])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o4 = prob4*tree.leaf_nodes.weight.T[3].detach().numpy()\n",
    "o4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.30487944, 0.31343792, 0.38168264])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(tf.reduce_sum([o1, o2, o3, o4], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([0.2806432 , 0.31608321, 0.40327359])>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(o1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4836,  0.5165, -0.0162,  0.5607],\n",
       "        [-0.0084,  0.2021, -0.1423,  0.3949],\n",
       "        [ 0.3418, -0.2677,  0.3356, -0.0016]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.inner_nodes[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4708, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1712,  0.0268,  0.4324], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.sigmoid(2*1.115+0.014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.sigmoid(2*1.01-0.142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "parameters = tree.to_array()\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'fully_grown': True,       \n",
    "        'balance': 0.5\n",
    "        'balancing_tolerance': 0.05               \n",
    "    }\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'make_classification', #'random'\n",
    "        'objective': 'classification' # 'multiclass_classification', 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "        \n",
    "        'lambda_dataset_size': 5000, #number of samples per polynomial\n",
    "        'number_of_generated_datasets': 10000,\n",
    "    }, \n",
    "    'computation':{\n",
    "        'n_jobs': 5,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 0,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:26:58.879427Z",
     "start_time": "2020-09-16T12:26:58.874894Z"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T21:12:40.476681Z",
     "start_time": "2021-01-13T21:12:38.298249Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product       # forms cartesian products\n",
    "from more_itertools import random_product \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import random \n",
    "from random import sample \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sympy import Symbol, sympify\n",
    "\n",
    "        \n",
    "import seaborn as sns\n",
    "        \n",
    "import random \n",
    "\n",
    "import warnings\n",
    "\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='data_creation'))\n",
    "generate_directory_structure()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_identifier_polynomial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T12:28:46.853042Z",
     "start_time": "2020-09-16T12:28:46.848346Z"
    }
   },
   "source": [
    "# Function Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_decision_tree():\n",
    "    \n",
    "    \n",
    "    \n",
    "number_of_variables,\n",
    "maximum_depth,\n",
    "num_classes,\n",
    "fully_grown\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decision_tree_data(n_samples, noise, noise_dist, seed):\n",
    "    \n",
    "    decision_tree = generate_random_decision_tree()\n",
    "    \n",
    "    return decision_tree, X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if same_training_all_lambda_nets:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(generate_decision_tree_data)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))  \n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    result_list = parallel(delayed(gen_regression_symbolic)(polynomial_array=list_of_polynomials[i], \n",
    "                                                               n_samples=lambda_dataset_size,\n",
    "                                                               noise=noise,\n",
    "                                                               noise_dist=noise_distrib, \n",
    "                                                               seed=RANDOM_SEED+i, \n",
    "                                                               sympy_calculation=False) for i in range(polynomial_data_size))\n",
    "\n",
    "X_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[1], columns=list(variables[:n]))] for result in result_list]\n",
    "y_data_list = [[pd.Series(result[0],  index=list_of_monomial_identifiers_string), pd.DataFrame(result[2], columns=['result'])] for result in result_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T12:59:22.156778Z",
     "start_time": "2021-01-14T12:57:34.187753Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample_' + path_identifier_polynomial_data + '.csv'\n",
    "polynomials_list_df.to_csv(path_polynomials, index=False)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_X_data, 'wb') as f:\n",
    "    pickle.dump(X_data_list, f)#, protocol=2)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample_' + path_identifier_polynomial_data + '.pkl'\n",
    "with open(path_y_data, 'wb') as f:\n",
    "    pickle.dump(y_data_list, f)#, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
