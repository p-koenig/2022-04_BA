{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': -10,\n",
    "        'fully_grown': True,                      \n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 3, \n",
    "        'num_classes': 2,\n",
    "        \n",
    "        'function_generation_type': 'random_decision_tree', # 'make_classification' 'random_decision_tree'\n",
    "        'objective': 'classification', # 'regression'\n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 1000, #number of samples per function\n",
    "        #'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', # '' 'normal' 'uniform' 'normal_range' 'uniform_range'\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-2,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [64],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', #categorical_crossentropy\n",
    "        \n",
    "        'number_of_lambda_weights': None,\n",
    "        \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    \n",
    "    'i_net': {\n",
    "        'dense_layers': [1056, 512],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'dropout': [0.2, 0.1],\n",
    "        \n",
    "        'optimizer': 'adam', #adam\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': ['binary_accuracy'],\n",
    "        \n",
    "        'epochs': 2000, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'test_size': 50, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'function_representation_type': 1, \n",
    "\n",
    "        'optimize_decision_function': True, #False\n",
    "        'function_value_loss': True, #False\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        \n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },    \n",
    "    \n",
    "    'evaluation': {   \n",
    "        #'inet_holdout_seed_evaluation': False,\n",
    "            \n",
    "        'random_evaluation_dataset_size': 5000,\n",
    "        'per_network_optimization_dataset_size': 5000,\n",
    "\n",
    "        'sklearn_dt_benchmark': False,\n",
    "        'sdt_benchmark': False,\n",
    "        \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'load_model': False,\n",
    "        \n",
    "        'n_jobs': -3,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "#from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['i_net']['data_reshape_version'] = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/lib/cuda-10.1'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "from utilities.DecisionTree_BASIC import *\n",
    "\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(lambda_network_layers, number_of_variables, num_classes)\n",
    "config['function_family']['function_representation_length'] = (2 ** maximum_depth - 1) * (number_of_variables + 1) + (2 ** maximum_depth) * num_classes\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='interpretation_net'))\n",
    "create_folders_inet(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lNetSize1000_numLNets10000_var3_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42/inet_dense1056-512_drop0.2-0.1e2000b256_adam\n",
      "lNetSize1000_numLNets10000_var3_class2_random_decision_tree_xMax1_xMin0_xDistuniform_depth3_fullyGrown/64_e1000ES0.01_b64_drop0_adam_binary_crossentropy_fixedInit1-seed42\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(config, no_noise=False, n_jobs=1):\n",
    "    \n",
    "    #def generate_lambda_net()\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    if no_noise==True:\n",
    "        config['noise_injected_level'] = 0\n",
    "    path_dict = generate_paths(config, path_type='interpretation_net')        \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_dict['path_identifier_lambda_net_data'] + '/'\n",
    "    path_network_parameters = directory + 'weights' + '.txt'\n",
    "    path_X_data = directory + 'X_test_lambda.txt'\n",
    "    path_y_data = directory + 'y_test_lambda.txt'        \n",
    "    \n",
    "    network_parameters = pd.read_csv(path_network_parameters, sep=\",\", header=None)\n",
    "    network_parameters = network_parameters.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        network_parameters = network_parameters.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    X_test_lambda = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    X_test_lambda = X_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        X_test_lambda = X_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "    \n",
    "    y_test_lambda = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    y_test_lambda = y_test_lambda.sort_values(by=0)\n",
    "    if no_noise == False:\n",
    "        y_test_lambda = y_test_lambda.sample(n=config['i_net']['interpretation_dataset_size'], random_state=config['computation']['RANDOM_SEED'])\n",
    "        \n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "\n",
    "    lambda_nets = parallel(delayed(LambdaNet)(network_parameters_row, \n",
    "                                              X_test_lambda_row, \n",
    "                                              y_test_lambda_row, \n",
    "                                              config) for network_parameters_row, X_test_lambda_row, y_test_lambda_row in zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values))          \n",
    "    del parallel\n",
    "    \n",
    "    base_model = generate_base_model(config)  \n",
    "    \n",
    "    def initialize_network_wrapper(config, lambda_net, base_model):\n",
    "        lambda_net.initialize_network(config, base_model)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_network_wrapper)(config, lambda_net, base_model) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "    \n",
    "    def initialize_target_function_wrapper(config, lambda_net):\n",
    "        lambda_net.initialize_target_function(config)\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='sequential')\n",
    "    _ = parallel(delayed(initialize_target_function_wrapper)(config, lambda_net) for lambda_net in lambda_nets)   \n",
    "    del parallel\n",
    "        \n",
    "    \n",
    "    #lambda_nets = [None] * network_parameters.shape[0]\n",
    "    #for i, (network_parameters_row, X_test_lambda_row, y_test_lambda_row) in tqdm(enumerate(zip(network_parameters.values, X_test_lambda.values, y_test_lambda.values)), total=network_parameters.values.shape[0]):        \n",
    "    #    lambda_net = LambdaNet(network_parameters_row, X_test_lambda_row, y_test_lambda_row, config)\n",
    "    #    lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-3)]: Done 781 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-3)]: Done 7254 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   13.6s finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:  4.6min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-3)]: Done 10000 out of 10000 | elapsed:   11.6s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if noise_injected_level > 0:\n",
    "    lambda_net_dataset_training = load_lambda_nets(config, no_noise=True, n_jobs=n_jobs)\n",
    "    lambda_net_dataset_evaluation = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_training, test_split=0.1)\n",
    "    _, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset_evaluation, test_split=test_size)\n",
    "    \n",
    "else:\n",
    "    lambda_net_dataset = load_lambda_nets(config, n_jobs=n_jobs)\n",
    "\n",
    "    lambda_net_dataset_train_with_valid, lambda_net_dataset_test = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "    lambda_net_dataset_train, lambda_net_dataset_valid = split_LambdaNetDataset(lambda_net_dataset_train_with_valid, test_split=0.1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8955, 367)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995, 367)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 367)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>6671.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.570</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.517</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>1.039</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>1.006</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.066</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>1.017</td>\n",
       "      <td>-1.176</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.060</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-1.304</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.927</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-1.485</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.501</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-1.239</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-1.215</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>3274.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-1.536</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-1.514</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>-1.547</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.488</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-1.574</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-1.531</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-1.588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.621</td>\n",
       "      <td>-0.666</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.698</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>-2.168</td>\n",
       "      <td>0.787</td>\n",
       "      <td>1.503</td>\n",
       "      <td>-2.301</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-2.088</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>1.540</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>1.679</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.836</td>\n",
       "      <td>-2.122</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.917</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-2.016</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-2.146</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.220</td>\n",
       "      <td>1.970</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.129</td>\n",
       "      <td>2.356</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.141</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-1.947</td>\n",
       "      <td>1.558</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-2.080</td>\n",
       "      <td>-0.979</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0.816</td>\n",
       "      <td>-1.077</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-2.329</td>\n",
       "      <td>1.484</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>3095.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.826</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.782</td>\n",
       "      <td>1.226</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.051</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0.806</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.821</td>\n",
       "      <td>-0.726</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-0.576</td>\n",
       "      <td>0.797</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.191</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.295</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.807</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.741</td>\n",
       "      <td>1.241</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.262</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.718</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>1.047</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>8379.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.541</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.790</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.794</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.778</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.139</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.117</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.572</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.527</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-1.089</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.488</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.599</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.009</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.733</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.014</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.899</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.909</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.564</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>0.685</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.563</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>0.885</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>0.840</td>\n",
       "      <td>-0.206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "6671 6671.000    42  0.502 -0.378 -0.264 -0.008 -0.265  0.279  0.570 -0.147   \n",
       "3274 3274.000    42 -0.496  0.144  0.130 -0.219  0.271 -0.167  0.361  0.125   \n",
       "3095 3095.000    42 -0.098  0.327  0.458 -0.250 -0.521 -0.547  0.483 -0.043   \n",
       "8379 8379.000    42 -0.127 -0.364  0.387  0.083  0.305 -0.370 -0.362 -0.083   \n",
       "3043 3043.000    42 -0.214 -0.220 -0.451  0.410 -0.059  0.014 -0.464  0.360   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "6671  0.004  0.189  0.151 -0.510  0.033  0.414  0.258  0.517 -0.245  0.558   \n",
       "3274  0.172 -0.518  0.038 -0.159  0.378 -0.553 -0.339 -0.011 -0.498  0.567   \n",
       "3095  0.471  0.272 -0.133 -0.243 -0.397 -0.419  0.074  0.237 -0.522 -0.468   \n",
       "8379 -0.307 -0.541  0.348 -0.278 -0.199 -0.373 -0.200 -0.465  0.322 -0.287   \n",
       "3043 -0.289  0.027 -0.050 -0.094  0.461  0.044 -0.484  0.009 -0.571 -0.127   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "6671 -0.145 -0.534 -0.484  0.306 -0.361 -0.302  0.274  0.006  0.265 -0.190   \n",
       "3274  0.479  0.102  0.261 -0.037  0.176 -0.537  0.264 -0.489  0.187 -0.373   \n",
       "3095 -0.138  0.441  0.547 -0.051  0.316  0.389  0.004  0.349  0.178 -0.163   \n",
       "8379 -0.245  0.379  0.326 -0.079 -0.555  0.098 -0.366  0.109 -0.155  0.283   \n",
       "3043  0.167 -0.120 -0.495 -0.534  0.413  0.376 -0.156 -0.111 -0.042 -0.508   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "6671  0.121 -0.250  0.341  0.299 -0.007 -0.313 -0.256 -0.102  0.158  0.160   \n",
       "3274  0.281 -0.235  0.325 -0.223 -0.180 -0.052 -0.046  0.115 -0.129  0.207   \n",
       "3095  0.252  0.270 -0.272  0.185 -0.064  0.323 -0.081 -0.236 -0.132  0.280   \n",
       "8379  0.324  0.010 -0.076 -0.095  0.111 -0.309 -0.283  0.266 -0.184 -0.079   \n",
       "3043  0.179 -0.209 -0.030 -0.027  0.135  0.046 -0.240  0.129  0.306 -0.301   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3  \\\n",
       "6671 -0.292 -0.145  0.144 -0.186 -0.260 -0.022 -0.127  1.039 -0.650 -0.292   \n",
       "3274  0.248  0.186  0.115 -0.285  0.021  0.102 -1.536 -0.793 -0.250 -1.586   \n",
       "3095 -0.109 -0.316  0.200  0.192  0.339 -0.186  0.486  0.647 -0.453 -0.134   \n",
       "8379 -0.000 -0.104  0.281  0.054 -0.061 -0.095  0.282  0.337 -0.079  0.215   \n",
       "3043  0.182 -0.223  0.031  0.276  0.012  0.312 -0.260  0.510  0.425 -0.287   \n",
       "\n",
       "       wb_4   wb_5  wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "6671 -0.252  0.002 0.089  0.194 -0.255 -0.268 -0.072 -0.169  1.006 -0.152   \n",
       "3274 -0.252 -0.665 0.098 -0.015 -1.514 -0.268 -0.106 -0.172 -0.021 -0.152   \n",
       "3095 -0.252 -0.642 0.530  0.201  0.439 -0.268 -0.001 -0.172 -0.021 -0.152   \n",
       "8379 -0.252 -0.223 0.092  0.076  0.150 -0.268  0.072 -0.173 -0.019 -0.152   \n",
       "3043 -0.252  0.496 0.523 -0.327 -0.337 -0.268  0.482 -0.170 -0.020 -0.152   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "6671 -0.526  0.098  0.012 -0.192  1.054  1.066 -0.301 -0.098  1.017 -1.176   \n",
       "3274 -0.782  0.115  0.015 -0.238 -0.798 -1.547 -0.303 -0.142 -0.777 -0.893   \n",
       "3095  0.017  0.565 -0.342 -0.235 -0.916  0.339 -0.300 -0.141 -0.841  0.641   \n",
       "8379  0.030  0.094  0.012 -0.238 -0.298  0.177  0.213 -0.141 -0.297  0.435   \n",
       "3043  0.030  0.593  0.488 -0.261  0.513 -0.284 -0.386 -0.413  0.482 -0.271   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "6671  0.097 -1.100  0.702  0.097 -0.224  0.026  0.820  0.127  0.175  0.060   \n",
       "3274 -0.248 -0.703  0.995  0.109 -0.224  0.024 -1.488  0.123  0.162  0.050   \n",
       "3095 -0.070 -0.842  0.377  0.309 -0.224  0.629  0.379  0.464  0.541  0.258   \n",
       "8379 -0.026 -0.223  0.221  0.091 -0.224  0.025  0.225  0.122  0.162  0.064   \n",
       "3043  0.478  0.468 -0.254  0.560 -0.224  0.024 -0.275  0.480  0.576  0.042   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "6671  1.202  0.032  0.013  0.145  0.055 -0.289 -1.062  0.049  0.113  0.100   \n",
       "3274 -1.574 -0.641 -0.872  0.225  0.067 -0.289 -0.664  0.045  0.101  0.090   \n",
       "3095  0.531 -0.473 -0.412 -0.003 -0.106 -0.289  0.484  0.376  0.390  0.325   \n",
       "8379  0.341 -0.198 -0.256  0.065  0.055 -0.289  0.170  0.049  0.112  0.097   \n",
       "3043 -0.206  0.505  0.493  0.582  0.539 -0.289 -0.188  0.037  0.106  0.095   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "6671  0.143  0.113  0.135 -0.294  0.091 -0.210 -0.343 -0.184 -0.254  0.870   \n",
       "3274  0.539  0.117  0.117 -0.294  0.099 -1.531 -0.204 -0.184 -0.254 -1.588   \n",
       "3095 -0.078  0.542  0.314 -0.294  0.589  0.159 -0.127 -0.184 -0.254  0.437   \n",
       "8379 -0.026  0.113  0.128 -0.294  0.096  0.048 -0.018 -0.184 -0.254  0.252   \n",
       "3043  0.575  0.538  0.416 -0.294  0.136 -0.332  0.453 -0.184 -0.254 -0.228   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "6671  ...  -0.032   0.316  -0.077  -0.089  -0.095   0.377  -0.069  -0.027   \n",
       "3274  ...  -0.034   0.064  -0.088  -0.104  -0.110   0.062   0.551   0.621   \n",
       "3095  ...   0.227  -0.159   0.111   0.193  -0.027  -0.116   0.358   0.480   \n",
       "8379  ...  -0.032   0.127  -0.079  -0.099  -0.085   0.220   0.241   0.325   \n",
       "3043  ...  -0.032   0.324  -0.112  -0.052  -0.094   0.414  -0.081  -0.042   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "6671  -0.149  -0.030   0.000   0.009  -0.064  -0.099  -0.095  -0.094  -0.068   \n",
       "3274  -0.666  -0.016   0.000   0.476  -0.072  -0.116  -0.111  -0.698  -0.067   \n",
       "3095   0.294   0.451   0.000  -0.336   0.037   0.010   0.004   0.389   0.220   \n",
       "8379   0.102  -0.031   0.000  -0.070  -0.061  -0.091  -0.090   0.202  -0.067   \n",
       "3043  -0.108  -0.015   0.000   0.181  -0.065  -0.094  -0.092  -0.051  -0.058   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "6671  -0.101   0.000  -0.063  -0.106  -0.048   0.000   0.000   0.458   0.366   \n",
       "3274  -0.457   0.000  -0.062   0.044  -0.454   0.000   0.000   0.056   0.112   \n",
       "3095   0.011   0.000   0.225  -0.064   0.140   0.000   0.000  -0.106  -0.145   \n",
       "8379  -0.099   0.000  -0.058   0.146   0.019   0.000   0.000   0.194   0.163   \n",
       "3043  -0.112   0.000  -0.037   0.268  -0.155   0.000   0.000   0.357   0.373   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "6671  -0.073   0.028   0.458   0.164  -0.074   0.000  -0.110  -0.006  -0.107   \n",
       "3274  -0.514   0.493   0.105  -0.103  -0.086   0.000  -0.133   0.064  -0.591   \n",
       "3095   0.033   0.057  -0.080  -0.047   0.113   0.000  -0.013  -0.006   0.033   \n",
       "8379  -0.042   0.077   0.195   0.117  -0.080   0.000  -0.101   0.173  -0.091   \n",
       "3043  -0.143  -0.152   0.334   0.360  -0.109   0.000  -0.108   0.366  -0.124   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "6671  -0.115  -0.781   0.692  -0.287  -0.256   0.020   0.108  -0.190  -0.195   \n",
       "3274  -2.168   0.787   1.503  -2.301  -0.256   0.998   0.120  -0.100  -2.088   \n",
       "3095  -0.826  -0.273   0.415  -0.936  -0.256   0.782   1.226  -0.491  -0.805   \n",
       "8379  -0.870  -0.105   0.510  -1.180  -0.256   0.894   0.110  -0.582  -0.880   \n",
       "3043  -0.794   0.641   0.599  -0.292  -0.256   0.730   1.009  -0.609  -0.797   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "6671  -0.272   0.282  -0.156  -0.971  -0.154   0.588   0.119   0.013  -0.162   \n",
       "3274  -0.272   1.540  -0.152  -0.005  -0.154   1.679   0.130   0.018  -0.160   \n",
       "3095  -0.272   0.242  -0.156  -0.008  -0.154   0.051   1.180   0.806  -0.159   \n",
       "8379  -0.272   0.541  -0.152  -0.006  -0.154   0.060   0.117   0.016  -0.152   \n",
       "3043  -0.272   0.733  -0.154  -0.006  -0.154   0.037   1.000   1.014  -0.149   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "6671  -0.841  -1.304  -0.258  -0.083  -0.927   1.026   0.135   0.993  -0.858   \n",
       "3274   0.836  -2.122  -0.260  -0.084   0.952   1.245   0.725   0.917  -1.000   \n",
       "3095   0.821  -0.726  -0.258  -0.082   0.783  -0.625   0.354   0.891  -0.576   \n",
       "8379   0.790  -0.831  -1.100  -0.079   0.794  -0.751   0.759   0.778  -0.646   \n",
       "3043   0.658  -0.748  -0.928  -0.852   0.687  -0.686   0.809   0.730  -0.608   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "6671   0.120  -0.227   0.032  -1.012   0.159   0.201   0.092  -1.485   0.051   \n",
       "3274   0.126  -0.227   0.034  -2.016   0.164   0.210   0.085  -2.146   1.068   \n",
       "3095   0.797  -0.227   1.191  -0.629   1.042   1.295   0.236  -0.807   0.738   \n",
       "8379   0.110  -0.227   0.032  -0.711   0.160   0.200   0.096  -0.884   0.930   \n",
       "3043   0.899  -0.227   0.033  -0.676   0.864   0.951   0.093  -0.808   0.740   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "6671  -0.017   0.197   0.066  -0.294   0.776   0.070   0.150   0.138   0.180   \n",
       "3274   1.220   1.970   0.071  -0.294   0.709   0.071   0.150   0.129   2.356   \n",
       "3095   0.672   0.539   0.717  -0.294  -0.322   0.576   0.550   0.427   0.741   \n",
       "8379   0.882   0.922   0.068  -0.294  -0.178   0.070   0.151   0.139   1.035   \n",
       "3043   0.717   0.853   0.909  -0.294  -0.222   0.072   0.150   0.134   0.838   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "6671   0.131   0.182  -0.298   0.114  -0.151   0.501  -0.187  -0.257  -1.239   \n",
       "3274   0.145   1.141  -0.298   0.121  -1.947   1.558  -0.187  -0.257  -2.080   \n",
       "3095   1.241   0.457  -0.298   1.262  -0.664   0.259  -0.187  -0.257  -0.781   \n",
       "8379   0.134   0.180  -0.298   0.117  -0.875   0.572  -0.187  -0.257  -0.876   \n",
       "3043   0.973   0.564  -0.298   0.210  -0.791   0.685  -0.187  -0.257  -0.792   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "6671  -0.999   0.502   0.890  -1.215  -0.165   0.157  -0.130   0.160  -0.211   \n",
       "3274  -0.979   1.443   0.816  -1.077  -0.156   0.161  -0.130   0.154  -2.329   \n",
       "3095  -0.629   0.073   0.565  -0.718  -0.173   1.047  -0.130   0.414  -0.211   \n",
       "8379  -0.680   0.194   0.527  -0.845  -0.167   0.156  -0.130   0.163  -1.089   \n",
       "3043  -0.644   0.315   0.563  -0.761  -0.778   0.885  -0.130   0.156  -0.896   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "6671   0.235  -0.125  \n",
       "3274   1.484   0.027  \n",
       "3095   0.650   0.083  \n",
       "8379   0.228  -0.092  \n",
       "3043   0.840  -0.206  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_train.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.530</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.489</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-1.018</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.861</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.680</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.708</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.737</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>0.857</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.011</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.841</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.956</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>0.896</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.793</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>689.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.021</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.801</td>\n",
       "      <td>1.091</td>\n",
       "      <td>0.942</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-1.036</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>1.151</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.878</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.131</td>\n",
       "      <td>0.871</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.199</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.063</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>1.048</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.920</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>4148.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>1.007</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>1.007</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.958</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.813</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.050</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.839</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>0.655</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.998</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.117</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1.077</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.856</td>\n",
       "      <td>1.122</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.052</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.335</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.930</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2815.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.846</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>0.498</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.797</td>\n",
       "      <td>-0.712</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.864</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.195</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-1.082</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-1.023</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.718</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.016</td>\n",
       "      <td>0.178</td>\n",
       "      <td>1.265</td>\n",
       "      <td>0.911</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.136</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>1.194</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.006</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>5185.000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.622</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.734</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.721</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0   f1v1   f1v2   f2v0   f2v1  \\\n",
       "3466 3466.000    42 -0.165 -0.392  0.210 -0.089 -0.369 -0.041  0.111 -0.381   \n",
       "689   689.000    42  0.127 -0.542  0.507 -0.476  0.274  0.481 -0.116  0.012   \n",
       "4148 4148.000    42  0.256 -0.097  0.066 -0.488 -0.416  0.465  0.050 -0.523   \n",
       "2815 2815.000    42 -0.338  0.312 -0.402 -0.424  0.081  0.288  0.403 -0.394   \n",
       "5185 5185.000    42  0.119 -0.530 -0.255  0.386  0.172 -0.343  0.325  0.208   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2   f4v0   f4v1   f4v2  f5v0   f5v1   f5v2  \\\n",
       "3466 -0.025 -0.503 -0.008  0.230 -0.051  0.011  0.385 0.400 -0.257 -0.418   \n",
       "689   0.492 -0.112 -0.399 -0.265  0.511 -0.066  0.488 0.055  0.503 -0.330   \n",
       "4148 -0.379  0.419 -0.266  0.116 -0.494  0.465 -0.543 0.279 -0.065 -0.457   \n",
       "2815  0.263  0.012  0.500  0.551  0.264 -0.512  0.123 0.192 -0.174  0.541   \n",
       "5185 -0.245 -0.012 -0.156 -0.200 -0.443  0.072  0.561 0.275  0.319  0.033   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "3466 -0.512  0.569  0.418 -0.160 -0.063  0.232 -0.521  0.398  0.365  0.266   \n",
       "689   0.084  0.421  0.292 -0.289  0.379 -0.035 -0.135 -0.165 -0.207  0.489   \n",
       "4148  0.553 -0.158 -0.048  0.511  0.344  0.147 -0.227  0.196  0.190  0.078   \n",
       "2815  0.017 -0.150  0.543 -0.322  0.514 -0.421  0.177  0.468  0.499 -0.239   \n",
       "5185 -0.214  0.432 -0.102  0.177  0.086  0.417 -0.348  0.385 -0.284  0.079   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "3466 -0.019  0.033 -0.070 -0.110  0.249  0.259  0.347  0.025 -0.257 -0.074   \n",
       "689   0.156 -0.154  0.093 -0.084  0.018  0.059 -0.128  0.020 -0.242  0.062   \n",
       "4148 -0.184 -0.247 -0.321 -0.127  0.071  0.223  0.223  0.074  0.211  0.081   \n",
       "2815  0.044  0.209 -0.009  0.107  0.118  0.169  0.240 -0.106 -0.151 -0.227   \n",
       "5185 -0.084 -0.171 -0.133 -0.345 -0.215 -0.077  0.292  0.083  0.109 -0.118   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0  wb_1   wb_2   wb_3  \\\n",
       "3466  0.096  0.019  0.327 -0.248 -0.233  0.209 -0.144 0.098  0.244 -0.206   \n",
       "689   0.268 -0.276 -0.216  0.030  0.011  0.024 -0.140 0.226  0.086 -0.292   \n",
       "4148  0.075  0.178 -0.245  0.001  0.226 -0.082 -0.146 1.007 -0.608 -0.287   \n",
       "2815  0.244  0.073 -0.189  0.323  0.312  0.329 -0.144 0.846 -0.324 -0.292   \n",
       "5185 -0.237  0.201  0.060  0.351  0.073 -0.143  0.059 0.622 -0.116 -0.297   \n",
       "\n",
       "       wb_4   wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "3466 -0.252  0.357  0.371 -0.224 -0.209 -0.268  0.372 -0.173 -0.020 -0.152   \n",
       "689  -0.252  0.003  0.181 -0.030 -0.005 -0.268  0.169  0.062  0.203 -0.152   \n",
       "4148 -0.252 -0.001  0.086 -0.180 -0.253 -0.268 -0.154 -0.174  0.982 -0.152   \n",
       "2815 -0.252  0.010 -0.535  0.498 -0.251 -0.268 -0.064  0.641  0.831 -0.152   \n",
       "5185 -0.252 -0.592  0.090  0.187  0.434 -0.268  0.047 -0.175 -0.020 -0.152   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "3466  0.266  0.416  0.300 -0.243  0.209 -0.170 -0.235 -0.149  0.142 -0.070   \n",
       "689   0.080  0.248  0.086  0.012  0.224 -0.214 -0.040  0.073  0.206  0.177   \n",
       "4148 -0.664  0.100  0.014 -0.224  1.007 -0.221 -0.302 -0.121  0.958 -0.880   \n",
       "2815 -0.515  0.087  0.014  0.538  0.848 -0.215 -0.299  0.638  0.797 -0.712   \n",
       "5185  0.027  0.094  0.011 -0.243  0.648  0.370 -0.016 -0.139  0.633  0.731   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "3466  0.356  0.292 -0.137  0.412 -0.224  0.530 -0.152  0.442  0.469  0.438   \n",
       "689   0.083  0.184 -0.156  0.237 -0.224  0.147  0.075  0.164  0.247  0.129   \n",
       "4148 -0.253 -0.896 -0.166  0.097 -0.224 -0.813 -0.150  0.116  0.158 -0.664   \n",
       "2815  0.089  0.864 -0.160  0.086 -0.224 -0.602 -0.149 -0.406  0.318 -0.504   \n",
       "5185  0.087 -0.516  0.403  0.093 -0.224  0.025  0.393  0.125  0.173  0.061   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "3466 -0.115  0.375  0.363  0.503  0.355 -0.289 -0.047  0.543  0.558  0.499   \n",
       "689  -0.118  0.027  0.000  0.114  0.158 -0.289  0.152  0.116  0.186  0.146   \n",
       "4148 -0.124  0.031  1.050  0.137  0.056 -0.289 -0.839 -0.746 -0.570 -0.584   \n",
       "2815 -0.121  0.033  0.001  0.144  0.051 -0.289 -0.681 -0.570 -0.439 -0.451   \n",
       "5185 -0.081 -0.428 -0.754  0.093  0.051 -0.289  0.372  0.047  0.110  0.097   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "3466  0.489  0.397  0.489 -0.294  0.463 -0.242  0.313 -0.184 -0.254 -0.128   \n",
       "689   0.140  0.206  0.152 -0.294  0.211 -0.369  0.108 -0.184 -0.254 -0.189   \n",
       "4148  0.139  0.107 -0.100 -0.294 -0.755 -0.362 -0.324 -0.184 -0.254 -0.194   \n",
       "2815  0.129 -0.495 -0.356 -0.294 -0.502 -0.357 -0.091 -0.184 -0.254 -0.184   \n",
       "5185  0.138  0.114  0.131 -0.294  0.091  0.202 -0.044 -0.184 -0.254  0.527   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "3466  ...   0.326  -0.046   0.235   0.322   0.109  -0.002   0.374   0.431   \n",
       "689   ...   0.419  -0.094   0.350   0.393   0.334  -0.014  -0.071  -0.025   \n",
       "4148  ...   0.631  -0.041  -0.098  -0.112   0.518  -0.016  -0.075  -0.126   \n",
       "2815  ...   0.518  -0.043   0.372   0.289   0.414  -0.014  -0.071  -0.025   \n",
       "5185  ...  -0.033  -0.050  -0.081  -0.091  -0.092  -0.037   0.308   0.531   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "3466   0.300   0.386   0.000  -0.132   0.233   0.200   0.164   0.343   0.339   \n",
       "689   -0.160   0.478   0.000  -0.167   0.370   0.341   0.317  -0.084   0.415   \n",
       "4148  -0.174  -0.030   0.000   0.547   0.567   0.487   0.476  -0.108  -0.077   \n",
       "2815  -0.156  -0.030   0.000   0.422   0.458   0.398   0.381  -0.103   0.472   \n",
       "5185  -0.065  -0.033   0.000  -0.154  -0.065  -0.100  -0.095  -0.102  -0.068   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "3466   0.171   0.000   0.299   0.024   0.176   0.000   0.000   0.009  -0.047   \n",
       "689    0.266   0.000   0.403  -0.112  -0.039   0.000   0.000  -0.043  -0.040   \n",
       "4148   0.092   0.000   0.620  -0.092   0.231   0.000   0.000  -0.046  -0.046   \n",
       "2815   0.296   0.000   0.507  -0.089   0.039   0.000   0.000  -0.048  -0.044   \n",
       "5185  -0.104   0.000  -0.064  -0.032  -0.020   0.000   0.000  -0.000  -0.027   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "3466   0.010  -0.044   0.035  -0.012   0.290   0.000   0.137  -0.016   0.168   \n",
       "689    0.051  -0.149  -0.061  -0.013   0.337   0.000   0.318  -0.006   0.246   \n",
       "4148   0.331   0.592  -0.076  -0.138  -0.094   0.000   0.438  -0.005  -0.138   \n",
       "2815   0.202   0.222  -0.073  -0.022   0.284   0.000   0.374  -0.006   0.021   \n",
       "5185  -0.020   0.135   0.019   0.018  -0.079   0.000  -0.110  -0.131  -0.112   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "3466  -0.797   0.688   0.363  -1.018  -0.256   0.712   0.861  -0.629  -0.777   \n",
       "689   -0.118  -0.564  -0.220  -0.286  -0.256   0.021   1.021  -0.752  -0.958   \n",
       "4148  -0.117  -0.812   0.655  -0.292  -0.256   0.016   0.106  -0.106  -0.195   \n",
       "2815  -0.116  -0.687   0.194  -0.285  -0.256   0.023   1.195  -0.599  -0.195   \n",
       "5185  -0.230  -0.354   0.161  -0.431  -0.256   0.682   0.108  -0.347  -0.734   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "3466  -0.272   0.614  -0.154  -0.006  -0.154   0.187   0.902   0.831  -0.160   \n",
       "689   -0.272   0.078  -0.942  -0.773  -0.154   0.801   1.091   0.942  -0.884   \n",
       "4148  -0.272   0.168  -0.151  -0.978  -0.154   0.942   0.121   0.016  -0.158   \n",
       "2815  -0.272   0.141  -1.082  -0.897  -0.154   0.868   0.112   0.015  -1.023   \n",
       "5185  -0.272   0.188  -0.154  -0.006  -0.154   0.057   0.115   0.014  -0.148   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "3466   0.680  -0.742  -0.884  -0.060   0.708  -0.608   0.799   0.737  -0.612   \n",
       "689   -0.590  -0.144  -1.036  -0.808  -0.631  -0.783   0.124  -0.628  -0.054   \n",
       "4148  -0.840  -0.146  -0.260  -0.080  -0.868   1.008   0.302   0.998  -0.054   \n",
       "2815  -0.719  -0.139  -0.261  -0.957  -0.753   0.923   0.123  -0.810  -0.054   \n",
       "5185  -0.409  -0.622  -0.554  -0.078  -0.465  -0.567   0.134   0.469  -0.501   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "3466   0.857  -0.227   1.011  -0.670   0.968   0.888   0.625  -0.835   0.717   \n",
       "689    1.151  -0.227   0.878  -0.898   1.013   1.131   0.871  -0.103   0.051   \n",
       "4148   0.115  -0.227   1.117  -0.090   0.153   0.197   1.077  -0.101   0.050   \n",
       "2815   0.109  -0.227   0.999  -0.088   1.224   1.270   0.974  -0.101   0.054   \n",
       "5185   0.113  -0.227   0.032  -0.524   0.159   0.205   0.090  -0.250   0.580   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "3466   0.696   0.808   0.841  -0.294  -0.385   0.981   0.962   0.839   0.802   \n",
       "689   -0.003   0.199   1.056  -0.294  -0.494   0.884   0.920   0.923   0.170   \n",
       "4148  -0.993   0.196   0.067  -0.294   0.856   1.122   1.065   1.052   0.182   \n",
       "2815   0.001   0.208   0.062  -0.294   0.718   1.016   1.051   1.016   0.178   \n",
       "5185   0.769   0.207   0.067  -0.294  -0.164   0.067   0.146   0.136   0.177   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "3466   0.867   0.919  -0.298   0.956  -0.795   0.550  -0.187  -0.257  -0.800   \n",
       "689    1.063   0.970  -0.298   0.974  -0.256  -0.075  -0.187  -0.257  -0.121   \n",
       "4148   0.133   0.500  -0.298   1.335  -0.231   0.299  -0.187  -0.257  -0.127   \n",
       "2815   1.265   0.911  -0.298   1.136  -0.230   0.084  -0.187  -0.257  -0.124   \n",
       "5185   0.133   0.182  -0.298   0.115  -0.530   0.179  -0.187  -0.257  -0.780   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "3466  -0.651   0.076  -0.044  -0.761  -0.677   0.896  -0.130   0.793  -0.866   \n",
       "689   -0.016   0.215  -0.528  -0.119  -0.794   1.048  -0.130   0.920  -0.210   \n",
       "4148  -0.010   0.619   0.930  -0.111  -0.084   0.157  -0.130   0.950  -0.211   \n",
       "2815  -0.011   0.549   0.540  -0.115  -0.925   1.194  -0.130   1.006  -0.211   \n",
       "5185  -0.578   0.091   0.181  -0.721  -0.174   0.157  -0.130   0.158  -0.911   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "3466   0.907   0.040  \n",
       "689    0.968   0.047  \n",
       "4148   0.211   0.172  \n",
       "2815   0.502   0.087  \n",
       "5185   0.234   0.001  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_valid.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seed</th>\n",
       "      <th>f0v0</th>\n",
       "      <th>f0v1</th>\n",
       "      <th>f0v2</th>\n",
       "      <th>f1v0</th>\n",
       "      <th>f1v1</th>\n",
       "      <th>f1v2</th>\n",
       "      <th>f2v0</th>\n",
       "      <th>f2v1</th>\n",
       "      <th>f2v2</th>\n",
       "      <th>f3v0</th>\n",
       "      <th>f3v1</th>\n",
       "      <th>f3v2</th>\n",
       "      <th>f4v0</th>\n",
       "      <th>f4v1</th>\n",
       "      <th>f4v2</th>\n",
       "      <th>f5v0</th>\n",
       "      <th>f5v1</th>\n",
       "      <th>f5v2</th>\n",
       "      <th>f6v0</th>\n",
       "      <th>f6v1</th>\n",
       "      <th>f6v2</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>lp0c0</th>\n",
       "      <th>lp0c1</th>\n",
       "      <th>lp1c0</th>\n",
       "      <th>lp1c1</th>\n",
       "      <th>lp2c0</th>\n",
       "      <th>lp2c1</th>\n",
       "      <th>lp3c0</th>\n",
       "      <th>lp3c1</th>\n",
       "      <th>lp4c0</th>\n",
       "      <th>lp4c1</th>\n",
       "      <th>lp5c0</th>\n",
       "      <th>lp5c1</th>\n",
       "      <th>lp6c0</th>\n",
       "      <th>lp6c1</th>\n",
       "      <th>lp7c0</th>\n",
       "      <th>lp7c1</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>...</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>7217.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.546</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>0.546</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.369</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.447</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.405</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.792</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.764</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.760</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.706</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>-0.594</td>\n",
       "      <td>0.764</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>8291.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.695</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.699</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.287</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-1.067</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-1.349</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.025</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>1.145</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.513</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.081</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.937</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.135</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.054</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>4607.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.348</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.870</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.816</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.866</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.809</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.576</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.707</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.831</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>5114.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.884</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.868</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.884</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>-0.585</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.441</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.015</td>\n",
       "      <td>1.242</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.904</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.029</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>1.287</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.969</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.747</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.048</td>\n",
       "      <td>0.182</td>\n",
       "      <td>1.218</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.200</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.834</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1859.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.505</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.724</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>-0.835</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.932</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.691</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.827</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.761</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>0.836</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.775</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>0.728</td>\n",
       "      <td>-0.057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  seed   f0v0   f0v1   f0v2   f1v0   f1v1  f1v2   f2v0   f2v1  \\\n",
       "7217 7217.000    42 -0.477  0.431  0.546 -0.394 -0.350 0.184 -0.532  0.546   \n",
       "8291 8291.000    42 -0.425  0.249 -0.116  0.085  0.347 0.573  0.251  0.111   \n",
       "4607 4607.000    42 -0.127  0.379  0.442  0.034 -0.430 0.069  0.507 -0.059   \n",
       "5114 5114.000    42 -0.364  0.381 -0.540  0.432  0.375 0.496 -0.362  0.034   \n",
       "1859 1859.000    42 -0.023 -0.259  0.265  0.202  0.408 0.285 -0.123 -0.033   \n",
       "\n",
       "       f2v2   f3v0   f3v1   f3v2   f4v0   f4v1   f4v2   f5v0   f5v1   f5v2  \\\n",
       "7217 -0.531  0.219 -0.297  0.460 -0.346  0.553  0.460  0.235 -0.508  0.427   \n",
       "8291 -0.386 -0.009  0.027 -0.354  0.356  0.108 -0.550 -0.138  0.232 -0.223   \n",
       "4607  0.189 -0.229 -0.546  0.089 -0.385 -0.237 -0.241 -0.074 -0.045 -0.282   \n",
       "5114 -0.290  0.510 -0.564 -0.127  0.332 -0.358 -0.400  0.318 -0.278 -0.560   \n",
       "1859  0.426  0.505 -0.036  0.325  0.054  0.177 -0.313  0.216 -0.560  0.130   \n",
       "\n",
       "       f6v0   f6v1   f6v2     b0     b1     b2     b3     b4     b5     b6  \\\n",
       "7217  0.128  0.115  0.172 -0.534 -0.226 -0.013 -0.549  0.547  0.469 -0.427   \n",
       "8291 -0.199 -0.179  0.450 -0.542  0.554  0.478  0.288  0.011  0.038 -0.168   \n",
       "4607  0.541  0.002 -0.257 -0.383 -0.389 -0.222 -0.345  0.226 -0.272 -0.533   \n",
       "5114 -0.409 -0.086 -0.306  0.326  0.568 -0.396 -0.274  0.012 -0.562 -0.024   \n",
       "1859 -0.574 -0.086 -0.333 -0.423  0.550 -0.320  0.280 -0.509 -0.215  0.248   \n",
       "\n",
       "      lp0c0  lp0c1  lp1c0  lp1c1  lp2c0  lp2c1  lp3c0  lp3c1  lp4c0  lp4c1  \\\n",
       "7217 -0.264  0.115  0.077 -0.167  0.154 -0.098 -0.346 -0.206 -0.182 -0.345   \n",
       "8291  0.255  0.022  0.273 -0.105  0.059 -0.351  0.349  0.095 -0.045  0.152   \n",
       "4607  0.111 -0.022 -0.042  0.348 -0.272  0.018 -0.210  0.049  0.309 -0.101   \n",
       "5114  0.268  0.116 -0.058  0.162 -0.095  0.286  0.240  0.216  0.002  0.045   \n",
       "1859  0.349  0.276  0.080 -0.012 -0.177 -0.319 -0.301 -0.223 -0.190 -0.226   \n",
       "\n",
       "      lp5c0  lp5c1  lp6c0  lp6c1  lp7c0  lp7c1   wb_0   wb_1   wb_2   wb_3  \\\n",
       "7217  0.056  0.090  0.264 -0.216  0.224  0.299 -0.221 -0.212  0.223 -0.292   \n",
       "8291  0.049 -0.280  0.342  0.008  0.287 -0.269 -0.143  0.695 -0.006 -0.291   \n",
       "4607 -0.142 -0.285 -0.117  0.320 -0.212  0.224 -0.145  0.870 -0.450 -0.296   \n",
       "5114 -0.201 -0.247 -0.326 -0.253  0.062 -0.350 -0.142  0.884 -0.512 -0.287   \n",
       "1859  0.120 -0.073 -0.156  0.051 -0.334  0.177 -0.141  0.164  0.018 -0.292   \n",
       "\n",
       "       wb_4  wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  \\\n",
       "7217 -0.252 0.399  0.429 -0.269 -0.262 -0.268  0.380 -0.287 -0.199 -0.152   \n",
       "8291 -0.252 0.012  0.084  0.288 -0.253 -0.268  0.074  0.462  0.679 -0.152   \n",
       "4607 -0.252 0.691  0.089 -0.276 -0.252 -0.268 -0.179 -0.170  0.816 -0.152   \n",
       "5114 -0.252 0.004 -0.579 -0.247 -0.252 -0.268 -0.166 -0.171  0.868 -0.152   \n",
       "1859 -0.252 0.003  0.270 -0.080 -0.070 -0.268  0.104  0.008  0.144 -0.152   \n",
       "\n",
       "      wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  \\\n",
       "7217  0.039  0.459  0.369 -0.325 -0.235 -0.228 -0.303 -0.260  0.401 -0.167   \n",
       "8291 -0.373  0.102  0.013  0.383  0.699 -0.217  0.473  0.482  0.647 -0.573   \n",
       "4607 -0.451  0.103  0.013 -0.197  0.866 -0.215 -0.301 -0.094  0.809 -0.629   \n",
       "5114 -0.527  0.088  0.011 -0.177  0.884 -0.218 -0.302  0.007  0.838 -0.702   \n",
       "1859  0.162  0.339  0.176 -0.038  0.160 -0.087 -0.101  0.018  0.142  0.094   \n",
       "\n",
       "      wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  \\\n",
       "7217  0.380  0.256 -0.198  0.447 -0.224  0.411 -0.210  0.424  0.510  0.112   \n",
       "8291  0.092  0.684 -0.157  0.094 -0.224 -0.493 -0.149 -0.283  0.157 -0.365   \n",
       "4607 -0.286 -0.598 -0.159  0.102 -0.224 -0.647 -0.148  0.105  0.160 -0.429   \n",
       "5114 -0.314 -0.749 -0.158  0.092 -0.224 -0.624 -0.147 -0.494  0.156 -0.514   \n",
       "1859  0.082  0.084 -0.082  0.318 -0.224  0.240  0.015  0.237  0.316  0.220   \n",
       "\n",
       "      wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  \\\n",
       "7217 -0.197  0.414  0.413  0.516  0.405 -0.289 -0.125  0.149  0.281  0.219   \n",
       "8291 -0.122  0.034  0.002  0.150  0.057 -0.289 -0.414 -0.435 -0.295 -0.302   \n",
       "4607 -0.122  0.024  0.790  0.112  0.059 -0.289 -0.588 -0.502 -0.360 -0.380   \n",
       "5114 -0.119  0.032  0.938  0.098  0.048 -0.289 -0.657 -0.585 -0.454 -0.465   \n",
       "1859 -0.123  0.026  0.001  0.123  0.256 -0.289  0.071  0.206  0.265  0.228   \n",
       "\n",
       "      wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  \\\n",
       "7217  0.514  0.453  0.364 -0.294  0.444 -0.280  0.311 -0.184 -0.254 -0.204   \n",
       "8291  0.138  0.107 -0.192 -0.294 -0.388 -0.361  0.039 -0.184 -0.254 -0.193   \n",
       "4607  0.137  0.109 -0.265 -0.294  0.089 -0.358 -0.336 -0.184 -0.254 -0.191   \n",
       "5114  0.134 -0.301 -0.408 -0.294 -0.548 -0.356 -0.290 -0.184 -0.254 -0.186   \n",
       "1859  0.138  0.286  0.224 -0.294  0.297 -0.746  0.045 -0.184 -0.254 -0.188   \n",
       "\n",
       "      ...  wb_221  wb_222  wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  \\\n",
       "7217  ...   0.333  -0.010   0.235   0.279  -0.080   0.019   0.324   0.401   \n",
       "8291  ...   0.417  -0.047   0.246  -0.107   0.287  -0.013  -0.065  -0.022   \n",
       "4607  ...   0.246  -0.045  -0.103  -0.103   0.130  -0.016  -0.079   0.219   \n",
       "5114  ...   0.550  -0.040   0.461  -0.111   0.456  -0.011  -0.076  -0.068   \n",
       "1859  ...   0.324   0.066   0.249   0.278   0.227  -0.014  -0.070  -0.025   \n",
       "\n",
       "      wb_229  wb_230  wb_231  wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  \\\n",
       "7217   0.236   0.341   0.000  -0.106  -0.034   0.009  -0.030   0.304   0.299   \n",
       "8291  -0.138  -0.028   0.000   0.168   0.339   0.281   0.252  -0.096  -0.074   \n",
       "4607  -0.191  -0.025   0.000   0.040   0.167   0.102   0.062  -0.103  -0.073   \n",
       "5114  -0.135  -0.023   0.000   0.465   0.495   0.455   0.441  -0.108   0.429   \n",
       "1859  -0.153   0.369   0.000  -0.055   0.266   0.239   0.208  -0.094   0.313   \n",
       "\n",
       "      wb_238  wb_239  wb_240  wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  \\\n",
       "7217   0.105   0.000   0.308   0.050   0.085   0.000   0.000   0.031  -0.018   \n",
       "8291   0.160   0.000   0.391  -0.101  -0.145   0.000   0.000  -0.044  -0.046   \n",
       "4607  -0.118   0.000  -0.070  -0.093  -0.035   0.000   0.000  -0.050  -0.047   \n",
       "5114   0.393   0.000   0.547  -0.082   0.212   0.000   0.000  -0.036  -0.043   \n",
       "1859   0.143   0.000   0.306  -0.074  -0.146   0.000   0.000  -0.046  -0.046   \n",
       "\n",
       "      wb_247  wb_248  wb_249  wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  \\\n",
       "7217  -0.021  -0.072   0.053   0.024   0.235   0.000  -0.047   0.019   0.158   \n",
       "8291   0.059  -0.004  -0.080   0.100  -0.088   0.000   0.246  -0.008  -0.127   \n",
       "4607  -0.183   0.062  -0.079  -0.120  -0.095   0.000   0.047  -0.008  -0.211   \n",
       "5114   0.309   0.504  -0.067  -0.063   0.016   0.000   0.427  -0.006   0.288   \n",
       "1859  -0.116  -0.033  -0.073   0.070   0.231   0.000   0.207   0.064   0.093   \n",
       "\n",
       "      wb_256  wb_257  wb_258  wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  \\\n",
       "7217  -0.661  -0.349   0.149  -0.904  -0.256   0.649   0.735  -0.514  -0.651   \n",
       "8291  -0.118  -0.657   0.040  -0.287  -0.256   0.022   0.103  -0.689  -0.188   \n",
       "4607  -0.115  -0.738   0.669  -0.285  -0.256  -0.589   0.114  -0.127  -0.195   \n",
       "5114  -0.121  -0.750   0.601  -0.292  -0.256   0.015   1.242  -0.122  -0.195   \n",
       "1859  -0.117  -0.425   0.012  -0.288  -0.256   0.022   0.810  -0.569  -0.743   \n",
       "\n",
       "      wb_265  wb_266  wb_267  wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  \\\n",
       "7217  -0.272   0.419  -0.792  -0.772  -0.154   0.047   0.767   0.688  -0.759   \n",
       "8291  -0.272   0.134  -1.067  -0.850  -0.154   0.902   0.125   0.013  -1.032   \n",
       "4607  -0.272   0.492  -0.153  -0.846  -0.154   0.734   0.123   0.015  -0.151   \n",
       "5114  -0.272   0.201  -0.155  -0.904  -0.154   0.858   0.111   0.016  -0.162   \n",
       "1859  -0.272   0.159  -0.763  -0.555  -0.154   0.588   0.875   0.724  -0.736   \n",
       "\n",
       "      wb_274  wb_275  wb_276  wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  \\\n",
       "7217  -0.419  -0.616  -0.764  -0.627   0.592  -0.451   0.649   0.318  -0.484   \n",
       "8291  -0.690  -0.142  -1.349  -0.937  -0.736   0.956   0.136  -0.841  -0.052   \n",
       "4607  -0.753  -0.139  -0.258  -0.077  -0.763   0.759   0.473   0.763  -0.053   \n",
       "5114  -0.770  -0.147  -0.258  -0.130  -0.793   0.907   0.442   0.904  -0.056   \n",
       "1859  -0.447  -0.854  -0.835  -0.631  -0.488  -0.580   0.132  -0.424  -0.536   \n",
       "\n",
       "      wb_283  wb_284  wb_285  wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  \\\n",
       "7217   0.735  -0.227   0.760  -0.529   0.774   0.796   0.111  -0.703   0.667   \n",
       "8291   0.116  -0.227   1.025  -0.082   1.145   0.197   1.020  -0.101   0.053   \n",
       "4607   0.120  -0.227   0.950  -0.087   0.147   0.206   0.838  -0.100   0.038   \n",
       "5114   0.113  -0.227   1.029  -0.090   1.287   0.192   0.969  -0.104   0.054   \n",
       "1859   0.932  -0.227   0.691  -0.609   0.826   0.928   0.723  -0.100   0.053   \n",
       "\n",
       "      wb_292  wb_293  wb_294  wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  \\\n",
       "7217   0.616   0.744   0.706  -0.294  -0.219   0.134   0.274   0.210   0.742   \n",
       "8291  -0.003   0.210   0.066  -0.294   0.513   1.046   1.081   1.053   0.181   \n",
       "4607  -0.777   0.118   0.068  -0.294   0.638   0.880   0.847   0.852   0.182   \n",
       "5114  -0.943   0.143   0.061  -0.294   0.747   1.006   1.074   1.048   0.182   \n",
       "1859   0.002   0.201   0.827  -0.294  -0.311   0.719   0.772   0.766   0.187   \n",
       "\n",
       "      wb_301  wb_302  wb_303  wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  \\\n",
       "7217   0.747   0.449  -0.298   0.783  -0.687   0.318  -0.187  -0.257  -0.681   \n",
       "8291   0.135   0.937  -0.298   1.135  -0.230   0.075  -0.187  -0.257  -0.126   \n",
       "4607   0.138   0.730  -0.298   0.114  -0.231   0.576  -0.187  -0.257  -0.122   \n",
       "5114   1.218   0.982  -0.298   1.200  -0.236   0.280  -0.187  -0.257  -0.130   \n",
       "1859   0.847   0.761  -0.298   0.783  -0.976   0.088  -0.187  -0.257  -0.124   \n",
       "\n",
       "      wb_310  wb_311  wb_312  wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  \\\n",
       "7217  -0.515   0.098  -0.063  -0.647  -0.594   0.764  -0.130   0.223  -0.736   \n",
       "8291  -0.007   0.515  -0.628  -0.108  -0.932   0.149  -0.130   1.054  -0.210   \n",
       "4607  -0.002   0.712   0.707  -0.104  -0.144   0.147  -0.130   0.831  -0.209   \n",
       "5114  -0.018   0.706   0.834  -0.125  -0.879   0.595  -0.130   1.038  -0.211   \n",
       "1859  -0.014   0.148  -0.310  -0.116  -0.659   0.836  -0.130   0.775  -0.852   \n",
       "\n",
       "      wb_319  wb_320  \n",
       "7217   0.717  -0.010  \n",
       "8291   0.225  -0.063  \n",
       "4607   0.472  -0.061  \n",
       "5114   0.596   0.159  \n",
       "1859   0.728  -0.057  \n",
       "\n",
       "[5 rows x 367 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset_test.as_pandas(config).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_accuracy_inet_decision_function_fv_metric\n",
      "\ttraining         \t (min:    0.756, max:    0.977, cur:    0.971)\n",
      "\tvalidation       \t (min:    0.917, max:    0.975, cur:    0.971)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.076, max:    0.478, cur:    0.092)\n",
      "\tvalidation       \t (min:    0.078, max:    0.239, cur:    0.087)\n",
      "Training Time: 2:11:01\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "((X_valid, y_valid), \n",
    " (X_test, y_test),\n",
    " history,\n",
    "\n",
    " model) = interpretation_net_training(lambda_net_dataset_train, \n",
    "                                      lambda_net_dataset_valid, \n",
    "                                      lambda_net_dataset_test,\n",
    "                                      config,\n",
    "                                      callback_names=['plot_losses'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 321)]             0         \n",
      "_________________________________________________________________\n",
      "hidden1_1056 (Dense)         (None, 1056)              340032    \n",
      "_________________________________________________________________\n",
      "activation1_relu (Activation (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "dropout1_0.2 (Dropout)       (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "hidden2_512 (Dense)          (None, 512)               541184    \n",
      "_________________________________________________________________\n",
      "activation2_relu (Activation (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout2_0.1 (Dropout)       (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output_44 (Dense)            (None, 44)                22572     \n",
      "=================================================================\n",
      "Total params: 903,788\n",
      "Trainable params: 903,788\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2f358e60b141af8b535bf625735461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Target Lambda 0.9616000000000001\n",
      "Binary Crossentropy Target Lambda 0.12624551379358342\n",
      "Accuracy Lambda Decision 0.9792000000000001\n",
      "Binary Crossentropy Lambda Decision 0.11795165820937198\n",
      "Accuracy Target Decision 0.95168\n",
      "Binary Crossentropy Target Decision 0.3386087520118171\n"
     ]
    }
   ],
   "source": [
    "acc_target_lambda_list = []\n",
    "bc_target_lambda_list = []\n",
    "\n",
    "acc_lambda_decision_list = []\n",
    "bc_lambda_decision_list = []\n",
    "\n",
    "acc_target_decision_list = []\n",
    "bc_target_decision_list = []\n",
    "\n",
    "decision_function_parameters_list = []\n",
    "decision_functio_list = []\n",
    "\n",
    "for lambda_net in tqdm(lambda_net_dataset_test.lambda_net_list):\n",
    "    \n",
    "    target_function_parameters = lambda_net.target_function_parameters\n",
    "    target_function = lambda_net.target_function\n",
    "    \n",
    "    X_test_lambda = lambda_net.X_test_lambda\n",
    "    y_test_lambda = lambda_net.y_test_lambda\n",
    "    \n",
    "    network = lambda_net.network\n",
    "    network_parameters = lambda_net.network_parameters\n",
    "    \n",
    "    if config['i_net']['convolution_layers'] != None or config['i_net']['lstm_layers'] != None or (config['i_net']['nas'] and config['nas_type']['convolution_layers'] != 'SEQUENTIAL'):\n",
    "        network_parameters, network_parameters_flat = restructure_data_cnn_lstm(np.array([network_parameters]), config, subsequences=None)    \n",
    "      \n",
    "    decision_function_parameters= model.predict(np.array([network_parameters]))[0]\n",
    "    decision_function = generate_decision_tree_from_array(decision_function_parameters, config)\n",
    "    \n",
    "    decision_function_parameters_list.append(decision_function_parameters)\n",
    "    decision_functio_list.append(decision_function)\n",
    "    \n",
    "    y_test_network = network.predict(X_test_lambda)\n",
    "    y_test_decision_function = decision_function.predict_proba(X_test_lambda)\n",
    "    y_test_target_function = target_function.predict_proba(X_test_lambda)  \n",
    "    \n",
    "    acc_target_lambda = accuracy_score(np.round(y_test_target_function), np.round(y_test_network))\n",
    "    bc_target_lambda = log_loss(np.round(y_test_target_function), y_test_network, labels=[0, 1])\n",
    "    \n",
    "    acc_lambda_decision = accuracy_score(np.round(y_test_network), np.round(y_test_decision_function))\n",
    "    bc_lambda_decision = log_loss(np.round(y_test_network), y_test_decision_function, labels=[0, 1])        \n",
    "    \n",
    "    acc_target_decision = accuracy_score(np.round(y_test_target_function), np.round(y_test_decision_function))\n",
    "    bc_target_decision = log_loss(np.round(y_test_target_function), y_test_decision_function, labels=[0, 1])   \n",
    "    \n",
    "    \n",
    "    acc_target_lambda_list.append(acc_target_lambda)\n",
    "    bc_target_lambda_list.append(bc_target_lambda)\n",
    "\n",
    "    acc_lambda_decision_list.append(acc_lambda_decision)\n",
    "    bc_lambda_decision_list.append(bc_lambda_decision)\n",
    "\n",
    "    acc_target_decision_list.append(acc_target_decision)\n",
    "    bc_target_decision_list.append(bc_target_decision)\n",
    "    \n",
    "\n",
    "acc_target_lambda_array = np.array(acc_target_lambda_list)\n",
    "bc_target_lambda_array = np.array(bc_target_lambda_list)\n",
    "\n",
    "acc_lambda_decision_array = np.array(acc_lambda_decision_list)\n",
    "bc_lambda_decision_array = np.array(bc_lambda_decision_list)\n",
    "\n",
    "acc_target_decision_array = np.array(acc_target_decision_list)\n",
    "bc_target_decision_array = np.array(bc_target_decision_list)\n",
    "    \n",
    "    \n",
    "acc_target_lambda = np.mean(acc_target_lambda_array)\n",
    "bc_target_lambda = np.mean(bc_target_lambda_array[~np.isnan(bc_target_lambda_array)])\n",
    "\n",
    "acc_lambda_decision = np.mean(acc_lambda_decision_array)\n",
    "bc_lambda_decision = np.mean(bc_lambda_decision_array[~np.isnan(bc_lambda_decision_array)])\n",
    "\n",
    "acc_target_decision = np.mean(acc_target_decision_array)\n",
    "bc_target_decision = np.mean(bc_target_decision_array[~np.isnan(bc_target_decision_array)])\n",
    "\n",
    "\n",
    "print('Accuracy Target Lambda', acc_target_lambda)\n",
    "print('Binary Crossentropy Target Lambda', bc_target_lambda)\n",
    "print('Accuracy Lambda Decision', acc_lambda_decision)\n",
    "print('Binary Crossentropy Lambda Decision', bc_lambda_decision)\n",
    "print('Accuracy Target Decision', acc_target_decision)\n",
    "print('Binary Crossentropy Target Decision', bc_target_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO BENCHMARK RANDOM GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################################################################\n",
    "#################################################################################################### END WORKING CODE ####################################################################################################\n",
    "##########################################################################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_dict_valid_list = []\n",
    "polynomial_dict_test_list = []  \n",
    "\n",
    "\n",
    "for lambda_net_valid_dataset, lambda_net_test_dataset in zip(lambda_net_valid_dataset_list, lambda_net_test_dataset_list):\n",
    "\n",
    "    #polynomial_dict_valid = {'lstsq_lambda_pred_polynomials': lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "    #                        'lstsq_target_polynomials': lambda_net_valid_dataset.lstsq_target_polynomial_list,\n",
    "    #                        'target_polynomials': lambda_net_valid_dataset.target_polynomial_list}    \n",
    "\n",
    "    polynomial_dict_test = {'lstsq_lambda_pred_polynomials': lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list,\n",
    "                            'lstsq_target_polynomials': lambda_net_test_dataset.lstsq_target_polynomial_list,\n",
    "                            'target_polynomials': lambda_net_test_dataset.target_polynomial_list}    \n",
    "\n",
    "    #polynomial_dict_valid_list.append(polynomial_dict_valid)  \n",
    "    polynomial_dict_test_list.append(polynomial_dict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------- PREDICT INET ------------------------------------------------------')\n",
    "\n",
    "start = time.time() \n",
    "\n",
    "for i, (X_test, model) in enumerate(zip(X_test_list, model_list)):\n",
    "    #y_test_pred = model.predict(X_test)    \n",
    "    #print(model.summary())\n",
    "    #print(X_test.shape)\n",
    "    y_test_pred = make_inet_prediction(model, X_test, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    #print(y_test_pred.shape)   \n",
    "    polynomial_dict_test_list[i]['inet_polynomials'] = y_test_pred\n",
    "\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Predict Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_poly_evaluation:\n",
    "    print('-------------------------------------------------- CALCULATE METAMODEL POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_poly'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Poly Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_evaluation:\n",
    "    print('---------------------------------------------------- CALCULATE METAMODEL --------------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=False, force_polynomial=False)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_metamodeling_function_evaluation:\n",
    "    print('----------------------------------------------- CALCULATE METAMODEL FUNCTION ----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        metamodel_functions_test = symbolic_metamodeling_function_generation(lambda_net_test_dataset, return_expression='approx', function_metamodeling=True)\n",
    "        polynomial_dict_test_list[i]['metamodel_functions_no_GD'] = metamodel_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Metamodel Function Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symbolic_regression_evaluation:\n",
    "    print('----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        symbolic_regression_functions_test = symbolic_regression_function_generation(lambda_net_test_dataset)\n",
    "        polynomial_dict_test_list[i]['symbolic_regression_functions'] = symbolic_regression_functions_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Symbolic Regression Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if per_network_evaluation:\n",
    "    print('------------------------------------------------ CALCULATE PER NETWORK POLY -----------------------------------------------')\n",
    "\n",
    "    start = time.time() \n",
    "\n",
    "    for i, lambda_net_test_dataset in enumerate(lambda_net_test_dataset_list): \n",
    "        per_network_poly_test = per_network_poly_generation(lambda_net_test_dataset, optimization_type='scipy')\n",
    "        polynomial_dict_test_list[i]['per_network_polynomials'] = per_network_poly_test       \n",
    "\n",
    "    end = time.time()     \n",
    "    inet_train_time = (end - start) \n",
    "    minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    print('Per Network Optimization Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "print('------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "function_values_test_list = []\n",
    "for lambda_net_test_dataset, polynomial_dict_test in zip(lambda_net_test_dataset_list, polynomial_dict_test_list):\n",
    "    function_values_test = calculate_all_function_values(lambda_net_test_dataset, polynomial_dict_test)\n",
    "    function_values_test_list.append(function_values_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('FV Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------')                \n",
    "\n",
    "start = time.time() \n",
    "\n",
    "scores_test_list = []\n",
    "distrib_dict_test_list = []\n",
    "\n",
    "for function_values_test, polynomial_dict_test in zip(function_values_test_list, polynomial_dict_test_list):\n",
    "    scores_test, distrib_test = evaluate_all_predictions(function_values_test, polynomial_dict_test)\n",
    "    scores_test_list.append(scores_test)\n",
    "    distrib_dict_test_list.append(distrib_test)\n",
    "\n",
    "end = time.time()     \n",
    "inet_train_time = (end - start) \n",
    "minutes, seconds = divmod(int(inet_train_time), 60)\n",
    "hours, minutes = divmod(minutes, 60)        \n",
    "print('Score Calculation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')     \n",
    "print('---------------------------------------------------------------------------------------------------------------------------')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------')         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_type = 'epochs' if samples_list == None else 'samples'\n",
    "save_results(scores_list=scores_test_list, by=identifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not optimize_decision_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.183\t0.234\t3.604\t0.143\t0.687\t2.559\t0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    try:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except:\n",
    "        pass\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "        print(polynomial_dict_test_list[-1][key][index])\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if not optimize_decision_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
