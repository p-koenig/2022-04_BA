{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:10.894916Z",
     "start_time": "2020-10-06T14:28:10.891392Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:10.903906Z",
     "start_time": "2020-10-06T14:28:10.897003Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 500 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 5  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:10.912776Z",
     "start_time": "2020-10-06T14:28:10.905548Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "same_training_all_polynomials = True\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "seed_method = True\n",
    "shuffle = True\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_polynomials:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleSeedMethod'\n",
    "elif not seed_method and shuffle:\n",
    "    seed_shuffle_string = '_shuffleNoSeedMethod'\n",
    "elif seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleSeedMethod'\n",
    "elif not seed_method and not shuffle:\n",
    "    seed_shuffle_string = '_noShuffleNoSeedMethod'\n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:15.398109Z",
     "start_time": "2020-10-06T14:28:10.914793Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:15.405198Z",
     "start_time": "2020-10-06T14:28:15.400568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:15.414565Z",
     "start_time": "2020-10-06T14:28:15.406924Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:15.422258Z",
     "start_time": "2020-10-06T14:28:15.416316Z"
    }
   },
   "outputs": [],
   "source": [
    "def calcualate_function_with_data(coefficient_list, variable_values):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [variable_value**int(coefficient_multiplier) for coefficient_multiplier, variable_value in zip(coefficient_multipliers, variable_values)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "\n",
    "    return result, variable_values\n",
    " \n",
    "def calculate_function_values_from_polynomial(true_value_test, evaluation_dataset):\n",
    "\n",
    "    if isinstance(true_value_test, pd.DataFrame):\n",
    "        true_value_test = true_value_test.values\n",
    "        \n",
    "    true_value_fv = []\n",
    "    true_value_coeff = []\n",
    "        \n",
    "    for evaluation in evaluation_dataset:\n",
    "        true_function_value, true_coeff = calcualate_function_with_data(true_value_test, evaluation)\n",
    "       \n",
    "        true_value_fv.append(true_function_value) \n",
    "        true_value_coeff.append(true_coeff)\n",
    "        \n",
    "    return np.array(true_value_coeff), np.array(true_value_fv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:15.447197Z",
     "start_time": "2020-10-06T14:28:15.423896Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:28:15.500639Z",
     "start_time": "2020-10-06T14:28:15.448713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502571d2308149789b3fa0e4dc79641d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f7d8c0eaf84d379842221c3a8a3f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:07.784864Z",
     "start_time": "2020-10-06T14:28:15.502777Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:07.830945Z",
     "start_time": "2020-10-06T14:29:07.790748Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        for i in range(epochs//each_epochs_save):    \n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:07.846196Z",
     "start_time": "2020-10-06T14:29:07.832400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.970</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.830  0.820  0.720 -0.790\n",
       "1 -0.220 -0.070 -0.630  0.700\n",
       "2 -0.830  0.070 -0.490 -0.480\n",
       "3  0.970 -0.070  0.480  0.990\n",
       "4 -0.940 -0.840  0.570  0.770"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:07.854876Z",
     "start_time": "2020-10-06T14:29:07.847688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.720</td>\n",
       "      <td>-0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.970</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.830  0.820  0.720 -0.790\n",
       "1 -0.220 -0.070 -0.630  0.700\n",
       "2 -0.830  0.070 -0.490 -0.480\n",
       "3  0.970 -0.070  0.480  0.990\n",
       "4 -0.940 -0.840  0.570  0.770"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:07.860517Z",
     "start_time": "2020-10-06T14:29:07.856447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:07.866228Z",
     "start_time": "2020-10-06T14:29:07.861918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add train to pred and save pred_eval_dataset_train containing train data and use for loss function in interpretation network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:08.062321Z",
     "start_time": "2020-10-06T14:29:07.868175Z"
    },
    "code_folding": [
     48
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn(X_data, y_data, polynomial, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    if int(tf.__version__[0]) >= 2:\n",
    "        tf.random.set_seed(RANDOM_SEED)\n",
    "    else:\n",
    "        tf.set_random_seed(RANDOM_SEED) \n",
    "        \n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_data = X_data.values\n",
    "    if isinstance(y_data, pd.DataFrame):\n",
    "        y_data = y_data.values\n",
    "        \n",
    "    #split train test valid\n",
    "        \n",
    "    X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "     \n",
    "    #create neural network structure       \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "            \n",
    "    model.add(Dense(1), activation='linear')\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae', #huber_loss(val_min, val_max), #'mape',#'mean_absolute_error',#root_mean_squared_error,\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "        \n",
    "        \n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        #training neurl net\n",
    "        model_history = model.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_pred_train = model.predict(X_train) \n",
    "        y_pred_valid = model.predict(X_valid)                \n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        term_list_all = []\n",
    "        y = 0\n",
    "        for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "            term_list = [int(value_mult) for value_mult in term]\n",
    "            term_list_all.append(term_list)\n",
    "\n",
    "        #print(trm_list_all)\n",
    "\n",
    "        #generate separate arrays for each variable combination\n",
    "        terms_matrix = []\n",
    "        for unknowns in X_train:\n",
    "            terms = []\n",
    "            for term_multipliers in term_list_all:\n",
    "                term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                terms.append(term_value)\n",
    "            terms_matrix.append(np.array(terms))\n",
    "\n",
    "        terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "        polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_train.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_pred_list.append(polynomial_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_test_polynomial = []\n",
    "        y_test_lstsq = []\n",
    "        for entry in X_test:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)   \n",
    "            y_test_polynomial.append(true_function_value_pred)\n",
    "            y_test_lstsq.append(true_function_value_lstsq)\n",
    "        y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "        y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "        y_valid_polynomial = []  \n",
    "        y_valid_lstsq = []\n",
    "        for entry in X_valid:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            y_valid_polynomial.append(true_function_value_pred)     \n",
    "            y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "        y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "        y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1)     \n",
    "        \n",
    "        y_train_polynomial = []  \n",
    "        y_train_lstsq = []\n",
    "        for entry in X_train:\n",
    "            true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "            true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "            y_train_polynomial.append(true_function_value_pred)     \n",
    "            y_train_lstsq.append(true_function_value_lstsq)     \n",
    "        y_train_polynomial = np.array(y_train_polynomial).reshape(len(y_train_polynomial), 1)     \n",
    "        y_train_lstsq = np.array(y_train_lstsq).reshape(len(y_train_lstsq), 1)    \n",
    "        \n",
    "        \n",
    "        pred_list = (y_train, y_pred_train, y_train_polynomial, X_train, y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test)\n",
    "\n",
    "        mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "        mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "        mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "        mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "        rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "        rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "        rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "        rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "        \n",
    "        mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "        mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "        mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "        mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "        r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "        r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "        r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "        r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "        raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "        raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "        rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "        rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "        rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "        \n",
    "        fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "        dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "        dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "        dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "        dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "        mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "        mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "        mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "        rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "        rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "        mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "        mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "        mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "        r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "        r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "        r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "        raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "        raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "        raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "        \n",
    "        rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "        rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "        rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "        rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "        \n",
    "        fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "        dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "        dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "        dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    #CHANGE TRAIN\n",
    "        mae_train_pred = np.round(mean_absolute_error(y_train, y_pred_train), 4)\n",
    "        mae_train_polynomial = np.round(mean_absolute_error(y_train, y_train_polynomial), 4)\n",
    "        mae_train_polynomial_pred = np.round(mean_absolute_error(y_train_polynomial, y_pred_train), 4)\n",
    "        mae_train_lstsq = np.round(mean_absolute_error(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        rmse_train_pred = np.round(root_mean_squared_error(y_train, y_pred_train), 4)\n",
    "        rmse_train_polynomial = np.round(root_mean_squared_error(y_train, y_train_polynomial), 4)\n",
    "        rmse_train_polynomial_pred = np.round(root_mean_squared_error(y_train_polynomial, y_pred_train), 4)\n",
    "        rmse_train_lstsq = np.round(root_mean_squared_error(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        mape_train_pred = np.round(mean_absolute_percentage_error_keras(y_train, y_pred_train), 4)\n",
    "        mape_train_polynomial = np.round(mean_absolute_percentage_error_keras(y_train, y_train_polynomial), 4)\n",
    "        mape_train_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_train_polynomial, y_pred_train), 4)\n",
    "        mape_train_lstsq = np.round(mean_absolute_percentage_error_keras(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        r2_train_pred = np.round(r2_score(y_train, y_pred_train), 4)\n",
    "        r2_train_polynomial = np.round(r2_score(y_train, y_train_polynomial), 4)\n",
    "        r2_train_polynomial_pred = np.round(r2_score(y_train_polynomial, y_pred_train), 4)\n",
    "        r2_train_lstsq = np.round(r2_score(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        raae_train_pred = np.round(relative_absolute_average_error(y_train, y_pred_train), 4)\n",
    "        raae_train_polynomial = np.round(relative_absolute_average_error(y_train, y_train_polynomial), 4)\n",
    "        raae_train_polynomial_pred = np.round(relative_absolute_average_error(y_train_polynomial, y_pred_train), 4)\n",
    "        raae_train_lstsq = np.round(relative_absolute_average_error(y_train, y_train_lstsq), 4)\n",
    "        \n",
    "        rmae_train_pred = np.round(relative_maximum_average_error(y_train, y_pred_train), 4) \n",
    "        rmae_train_polynomial = np.round(relative_maximum_average_error(y_train, y_train_polynomial), 4) \n",
    "        rmae_train_polynomial_pred = np.round(relative_maximum_average_error(y_train_polynomial, y_pred_train), 4) \n",
    "        rmae_train_lstsq = np.round(relative_maximum_average_error(y_train, y_train_lstsq), 4) \n",
    "        \n",
    "        fd_train_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_train_polynomial = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_train_polynomial_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "        fd_train_lstsq = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "        \n",
    "        dtw_train_pred, dtw_complete_train_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "        dtw_train_pred = np.round(dtw_train_pred, 4)    \n",
    "        dtw_train_polynomial, dtw_complete_train_polynomial = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])))\n",
    "        dtw_train_polynomial = np.round(dtw_train_polynomial, 4)       \n",
    "        dtw_train_polynomial_pred, dtw_complete_train_polynomial_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "        dtw_train_polynomial_pred = np.round(dtw_train_polynomial_pred, 4)   \n",
    "        dtw_train_lstsq, dtw_complete_train_lstsq = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size])))\n",
    "        dtw_train_lstsq = np.round(dtw_train_lstsq, 4)\n",
    "\n",
    "        \n",
    "        std_train_real = np.std(y_train)\n",
    "        std_valid_real = np.std(y_valid)\n",
    "        std_test_real = np.std(y_test)\n",
    "\n",
    "        std_train_pred = np.std(y_pred_train)\n",
    "        std_valid_pred = np.std(y_pred_valid)\n",
    "        std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "        std_train_polynomial = np.std(y_train_polynomial)\n",
    "        std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "        std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "        std_train_lstsq = np.std(y_train_lstsq)\n",
    "        std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "        std_test_lstsq = np.std(y_test_lstsq)\n",
    "        \n",
    "        \n",
    "        mean_train_real = np.mean(y_train)\n",
    "        mean_valid_real = np.mean(y_valid)\n",
    "        mean_test_real = np.mean(y_test)\n",
    "\n",
    "        mean_train_pred = np.mean(y_pred_train)\n",
    "        mean_valid_pred = np.mean(y_pred_valid)\n",
    "        mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "        mean_train_polynomial = np.mean(y_train_polynomial)\n",
    "        mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "        mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "        mean_train_lstsq = np.mean(y_train_lstsq)\n",
    "        mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "        mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "\n",
    "        result_dict_list =  [{\n",
    "                         'MAE FV TRAIN PRED': mae_valid_pred,\n",
    "                         'MAE FV TRAIN POLY': mae_valid_polynomial,\n",
    "                         'MAE FV TRAIN POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV TRAIN LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV TRAIN PRED': rmse_valid_pred,\n",
    "                         'RMSE FV TRAIN POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV TRAIN POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV TRAIN LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV TRAIN PRED': mape_valid_pred,\n",
    "                         'MAPE FV TRAIN POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV TRAIN POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV TRAIN LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV TRAIN PRED': r2_valid_pred,\n",
    "                         'R2 FV TRAIN POLY': r2_valid_polynomial,\n",
    "                         'R2 FV TRAIN POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV TRAIN LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED TRAIN': raae_valid_pred,\n",
    "                         'RAAE FV TRAIN POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV TRAIN POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV TRAIN LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV TRAIN PRED': rmae_valid_pred,\n",
    "                         'RMAE FV TRAIN POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV TRAIN POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV TRAIN LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV TRAIN PRED': fd_valid_pred,   \n",
    "                         'FD FV TRAIN POLY': fd_valid_polynomial,   \n",
    "                         'FD FV TRAIN POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV TRAIN LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV TRAIN PRED': dtw_valid_pred, \n",
    "                         'DTW FV TRAIN POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV TRAIN POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV TRAIN LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV TRAIN PRED': std_train_pred,\n",
    "                         'STD FV TRAIN POLY': std_train_polynomial, \n",
    "                         'STD FV TRAIN LSTSQ': std_train_lstsq, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV TRAIN PRED': mean_train_pred,\n",
    "                         'MEAN FV TRAIN POLY': mean_train_polynomial, \n",
    "                         'MEAN FV TRAIN LSTSQ': mean_train_lstsq, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]            \n",
    "                            \n",
    "    else:\n",
    "        result_dict_list = []\n",
    "        pred_list = []\n",
    "        for i in range(epochs//each_epochs_save):\n",
    "            if i == 0:\n",
    "                history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "                history = history.history\n",
    "            else:\n",
    "                model_history = model.fit(X_train, \n",
    "                          y_train, \n",
    "                          epochs=each_epochs_save, \n",
    "                          batch_size=batch_size, \n",
    "                          callbacks=callbacks,\n",
    "                          validation_data=(X_valid, y_valid),\n",
    "                          verbose=0,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=False\n",
    "                          )\n",
    "\n",
    "                for key_1 in history.keys():\n",
    "                    for key_2 in model_history.history.keys():\n",
    "                        if key_1 == key_2:\n",
    "                            history[key_1] += model_history.history[key_2]  \n",
    "            \n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_pred_train = model.predict(X_train)                \n",
    "            y_pred_valid = model.predict(X_valid)                \n",
    "            y_pred_test = model.predict(X_test)        \n",
    "\n",
    "            term_list_all = []\n",
    "            y = 0\n",
    "            for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "                term_list = [int(value_mult) for value_mult in term]\n",
    "                term_list_all.append(term_list)\n",
    "\n",
    "\n",
    "            terms_matrix = []\n",
    "            for unknowns in X_train:\n",
    "                terms = []\n",
    "                for term_multipliers in term_list_all:\n",
    "                    term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "                    terms.append(term_value)\n",
    "                terms_matrix.append(np.array(terms))\n",
    "\n",
    "            terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "            polynomial_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_pred_train.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_pred_list.append(polynomial_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            y_test_polynomial = []\n",
    "            if i == 0:\n",
    "                y_test_lstsq = []\n",
    "            for entry in X_test:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_test_polynomial.append(true_function_value_pred)\n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_test_lstsq.append(true_function_value_lstsq)\n",
    "\n",
    "            y_test_polynomial = np.array(y_test_polynomial).reshape(len(y_test_polynomial), 1)\n",
    "            if i == 0:\n",
    "                y_test_lstsq = np.array(y_test_lstsq).reshape(len(y_test_lstsq), 1)\n",
    "\n",
    "\n",
    "            y_valid_polynomial = []  \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = []\n",
    "            for entry in X_valid:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_valid_polynomial.append(true_function_value_pred)   \n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_valid_lstsq.append(true_function_value_lstsq)     \n",
    "            y_valid_polynomial = np.array(y_valid_polynomial).reshape(len(y_valid_polynomial), 1)     \n",
    "            if i == 0:\n",
    "                y_valid_lstsq = np.array(y_valid_lstsq).reshape(len(y_valid_lstsq), 1) \n",
    "                \n",
    "                \n",
    "            y_train_polynomial = []  \n",
    "            if i == 0:\n",
    "                y_train_lstsq = []\n",
    "            for entry in X_train:\n",
    "                true_function_value_pred, _ = calcualate_function_with_data(polynomial_pred, entry)\n",
    "                y_train_polynomial.append(true_function_value_pred)   \n",
    "                if i == 0:\n",
    "                    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "                    y_train_lstsq.append(true_function_value_lstsq)     \n",
    "            y_train_polynomial = np.array(y_train_polynomial).reshape(len(y_train_polynomial), 1)     \n",
    "            if i == 0:\n",
    "                y_train_lstsq = np.array(y_train_lstsq).reshape(len(y_train_lstsq), 1)    \n",
    "                \n",
    "                \n",
    "            pred_list.append((y_train, y_pred_train, y_train_polynomial, X_train, y_valid, y_pred_valid, y_valid_polynomial, X_valid, y_test, y_pred_test, y_test_polynomial, X_test))\n",
    "\n",
    "            \n",
    "            mae_test_pred = np.round(mean_absolute_error(y_test, y_pred_test), 4)\n",
    "            mae_test_polynomial = np.round(mean_absolute_error(y_test, y_test_polynomial), 4)\n",
    "            mae_test_polynomial_pred = np.round(mean_absolute_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                mae_test_lstsq = np.round(mean_absolute_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            rmse_test_pred = np.round(root_mean_squared_error(y_test, y_pred_test), 4)    \n",
    "            rmse_test_polynomial = np.round(root_mean_squared_error(y_test, y_test_polynomial), 4)    \n",
    "            rmse_test_polynomial_pred = np.round(root_mean_squared_error(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                rmse_test_lstsq = np.round(root_mean_squared_error(y_test, y_test_lstsq), 4)    \n",
    "\n",
    "            mape_test_pred = np.round(mean_absolute_percentage_error_keras(y_test, y_pred_test), 4)    \n",
    "            mape_test_polynomial = np.round(mean_absolute_percentage_error_keras(y_test, y_test_polynomial), 4)    \n",
    "            mape_test_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_test_polynomial, y_pred_test), 4)    \n",
    "            if i == 0:\n",
    "                mape_test_lstsq = np.round(mean_absolute_percentage_error_keras(y_test, y_test_lstsq), 4)            \n",
    "\n",
    "            r2_test_pred = np.round(r2_score(y_test, y_pred_test), 4)\n",
    "            r2_test_polynomial = np.round(r2_score(y_test, y_test_polynomial), 4)\n",
    "            r2_test_polynomial_pred = np.round(r2_score(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                r2_test_lstsq = np.round(r2_score(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            raae_test_pred = np.round(relative_absolute_average_error(y_test, y_pred_test), 4)\n",
    "            raae_test_polynomial = np.round(relative_absolute_average_error(y_test, y_test_polynomial), 4)\n",
    "            raae_test_polynomial_pred = np.round(relative_absolute_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                raae_test_lstsq = np.round(relative_absolute_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            rmae_test_pred = np.round(relative_maximum_average_error(y_test, y_pred_test), 4)\n",
    "            rmae_test_polynomial = np.round(relative_maximum_average_error(y_test, y_test_polynomial), 4)\n",
    "            rmae_test_polynomial_pred = np.round(relative_maximum_average_error(y_test_polynomial, y_pred_test), 4)\n",
    "            if i == 0:\n",
    "                rmae_test_lstsq = np.round(relative_maximum_average_error(y_test, y_test_lstsq), 4)\n",
    "\n",
    "            fd_test_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_test_polynomial_pred = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_test_lstsq = np.round(frechet_dist(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_test_pred, dtw_complete_test_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_pred = np.round(dtw_test_pred, 4)\n",
    "            dtw_test_polynomial, dtw_complete_test_polynomial = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial = np.round(dtw_test_polynomial, 4)\n",
    "            dtw_test_polynomial_pred, dtw_complete_test_polynomial_pred = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_pred_test[:advanced_metric_dataset_size])))\n",
    "            dtw_test_polynomial_pred = np.round(dtw_test_polynomial_pred, 4)    \n",
    "            if i == 0:\n",
    "                dtw_test_lstsq, dtw_complete_test_lstsq = dtw(np.column_stack((X_test[:advanced_metric_dataset_size], y_test[:advanced_metric_dataset_size])), np.column_stack((X_test[:advanced_metric_dataset_size], y_test_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_test_lstsq = np.round(dtw_test_lstsq, 4)        \n",
    "\n",
    "\n",
    "            mae_valid_pred = np.round(mean_absolute_error(y_valid, y_pred_valid), 4)\n",
    "            mae_valid_polynomial = np.round(mean_absolute_error(y_valid, y_valid_polynomial), 4)\n",
    "            mae_valid_polynomial_pred = np.round(mean_absolute_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mae_valid_lstsq = np.round(mean_absolute_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmse_valid_pred = np.round(root_mean_squared_error(y_valid, y_pred_valid), 4)\n",
    "            rmse_valid_polynomial = np.round(root_mean_squared_error(y_valid, y_valid_polynomial), 4)\n",
    "            rmse_valid_polynomial_pred = np.round(root_mean_squared_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                rmse_valid_lstsq = np.round(root_mean_squared_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            mape_valid_pred = np.round(mean_absolute_percentage_error_keras(y_valid, y_pred_valid), 4)\n",
    "            mape_valid_polynomial = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_polynomial), 4)\n",
    "            mape_valid_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                mape_valid_lstsq = np.round(mean_absolute_percentage_error_keras(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            r2_valid_pred = np.round(r2_score(y_valid, y_pred_valid), 4)\n",
    "            r2_valid_polynomial = np.round(r2_score(y_valid, y_valid_polynomial), 4)\n",
    "            r2_valid_polynomial_pred = np.round(r2_score(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                r2_valid_lstsq = np.round(r2_score(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            raae_valid_pred = np.round(relative_absolute_average_error(y_valid, y_pred_valid), 4)\n",
    "            raae_valid_polynomial = np.round(relative_absolute_average_error(y_valid, y_valid_polynomial), 4)\n",
    "            raae_valid_polynomial_pred = np.round(relative_absolute_average_error(y_valid_polynomial, y_pred_valid), 4)\n",
    "            if i == 0:\n",
    "                raae_valid_lstsq = np.round(relative_absolute_average_error(y_valid, y_valid_lstsq), 4)\n",
    "\n",
    "            rmae_valid_pred = np.round(relative_maximum_average_error(y_valid, y_pred_valid), 4) \n",
    "            rmae_valid_polynomial = np.round(relative_maximum_average_error(y_valid, y_valid_polynomial), 4) \n",
    "            rmae_valid_polynomial_pred = np.round(relative_maximum_average_error(y_valid_polynomial, y_pred_valid), 4) \n",
    "            if i == 0:\n",
    "                rmae_valid_lstsq = np.round(relative_maximum_average_error(y_valid, y_valid_lstsq), 4) \n",
    "\n",
    "            fd_valid_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_valid_polynomial_pred = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_valid_lstsq = np.round(frechet_dist(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_valid_pred, dtw_complete_valid_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_pred = np.round(dtw_valid_pred, 4)    \n",
    "            dtw_valid_polynomial, dtw_complete_valid_polynomial = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial = np.round(dtw_valid_polynomial, 4)       \n",
    "            dtw_valid_polynomial_pred, dtw_complete_valid_polynomial_pred = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_pred_valid[:advanced_metric_dataset_size])))\n",
    "            dtw_valid_polynomial_pred = np.round(dtw_valid_polynomial_pred, 4)   \n",
    "            if i == 0:\n",
    "                dtw_valid_lstsq, dtw_complete_valid_lstsq = dtw(np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid[:advanced_metric_dataset_size])), np.column_stack((X_valid[:advanced_metric_dataset_size], y_valid_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_valid_lstsq = np.round(dtw_valid_lstsq, 4)\n",
    "\n",
    "                \n",
    "            mae_train_pred = np.round(mean_absolute_error(y_train, y_pred_train), 4)\n",
    "            mae_train_polynomial = np.round(mean_absolute_error(y_train, y_train_polynomial), 4)\n",
    "            mae_train_polynomial_pred = np.round(mean_absolute_error(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                mae_train_lstsq = np.round(mean_absolute_error(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            rmse_train_pred = np.round(root_mean_squared_error(y_train, y_pred_train), 4)\n",
    "            rmse_train_polynomial = np.round(root_mean_squared_error(y_train, y_train_polynomial), 4)\n",
    "            rmse_train_polynomial_pred = np.round(root_mean_squared_error(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                rmse_train_lstsq = np.round(root_mean_squared_error(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            mape_train_pred = np.round(mean_absolute_percentage_error_keras(y_train, y_pred_train), 4)\n",
    "            mape_train_polynomial = np.round(mean_absolute_percentage_error_keras(y_train, y_train_polynomial), 4)\n",
    "            mape_train_polynomial_pred = np.round(mean_absolute_percentage_error_keras(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                mape_train_lstsq = np.round(mean_absolute_percentage_error_keras(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            r2_train_pred = np.round(r2_score(y_train, y_pred_train), 4)\n",
    "            r2_train_polynomial = np.round(r2_score(y_train, y_train_polynomial), 4)\n",
    "            r2_train_polynomial_pred = np.round(r2_score(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                r2_train_lstsq = np.round(r2_score(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            raae_train_pred = np.round(relative_absolute_average_error(y_train, y_pred_train), 4)\n",
    "            raae_train_polynomial = np.round(relative_absolute_average_error(y_train, y_train_polynomial), 4)\n",
    "            raae_train_polynomial_pred = np.round(relative_absolute_average_error(y_train_polynomial, y_pred_train), 4)\n",
    "            if i == 0:\n",
    "                raae_train_lstsq = np.round(relative_absolute_average_error(y_train, y_train_lstsq), 4)\n",
    "\n",
    "            rmae_train_pred = np.round(relative_maximum_average_error(y_train, y_pred_train), 4) \n",
    "            rmae_train_polynomial = np.round(relative_maximum_average_error(y_train, y_train_polynomial), 4) \n",
    "            rmae_train_polynomial_pred = np.round(relative_maximum_average_error(y_train_polynomial, y_pred_train), 4) \n",
    "            if i == 0:\n",
    "                rmae_train_lstsq = np.round(relative_maximum_average_error(y_train, y_train_lstsq), 4) \n",
    "\n",
    "            fd_train_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_train_polynomial = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size]))), 4)\n",
    "            fd_train_polynomial_pred = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size]))), 4)\n",
    "            if i == 0:\n",
    "                fd_train_lstsq = np.round(frechet_dist(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "            dtw_train_pred, dtw_complete_train_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "            dtw_train_pred = np.round(dtw_train_pred, 4)    \n",
    "            dtw_train_polynomial, dtw_complete_train_polynomial = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])))\n",
    "            dtw_train_polynomial = np.round(dtw_train_polynomial, 4)       \n",
    "            dtw_train_polynomial_pred, dtw_complete_train_polynomial_pred = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train_polynomial[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_pred_train[:advanced_metric_dataset_size])))\n",
    "            dtw_train_polynomial_pred = np.round(dtw_train_polynomial_pred, 4)   \n",
    "            if i == 0:\n",
    "                dtw_train_lstsq, dtw_complete_train_lstsq = dtw(np.column_stack((X_train[:advanced_metric_dataset_size], y_train[:advanced_metric_dataset_size])), np.column_stack((X_train[:advanced_metric_dataset_size], y_train_lstsq[:advanced_metric_dataset_size])))\n",
    "                dtw_train_lstsq = np.round(dtw_train_lstsq, 4)\n",
    "          \n",
    "            std_train_real = np.std(y_train)\n",
    "            std_valid_real = np.std(y_valid)\n",
    "            std_test_real = np.std(y_test)\n",
    "\n",
    "            std_train_pred = np.std(y_pred_train)\n",
    "            std_valid_pred = np.std(y_pred_valid)\n",
    "            std_test_pred = np.std(y_pred_test)\n",
    "\n",
    "            std_train_polynomial = np.std(y_train_polynomial)\n",
    "            std_valid_polynomial = np.std(y_valid_polynomial)\n",
    "            std_test_polynomial = np.std(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                std_train_lstsq = np.std(y_train_lstsq)\n",
    "                std_valid_lstsq = np.std(y_valid_lstsq)\n",
    "                std_test_lstsq = np.std(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            mean_train_real = np.mean(y_train)\n",
    "            mean_valid_real = np.mean(y_valid)\n",
    "            mean_test_real = np.mean(y_test)\n",
    "\n",
    "            mean_train_pred = np.mean(y_pred_train)\n",
    "            mean_valid_pred = np.mean(y_pred_valid)\n",
    "            mean_test_pred = np.mean(y_pred_test)\n",
    "\n",
    "            mean_train_polynomial = np.mean(y_train_polynomial)\n",
    "            mean_valid_polynomial = np.mean(y_valid_polynomial)\n",
    "            mean_test_polynomial = np.mean(y_test_polynomial)\n",
    "\n",
    "            if i == 0:\n",
    "                mean_train_lstsq = np.mean(y_train_lstsq)\n",
    "                mean_valid_lstsq = np.mean(y_valid_lstsq)\n",
    "                mean_test_lstsq = np.mean(y_test_lstsq)\n",
    "                \n",
    "                \n",
    "            result_dict_list_single_epoch =  [{\n",
    "                         'MAE FV TRAIN PRED': mae_train_pred,\n",
    "                         'MAE FV TRAIN POLY': mae_train_polynomial,\n",
    "                         'MAE FV TRAIN POLY PRED': mae_train_polynomial_pred,\n",
    "                         'MAE FV TRAIN LSTSQ': mae_train_lstsq,\n",
    "                         'RMSE FV TRAIN PRED': rmse_train_pred,\n",
    "                         'RMSE FV TRAIN POLY': rmse_train_polynomial,\n",
    "                         'RMSE FV TRAIN POLY PRED': rmse_train_polynomial_pred,\n",
    "                         'RMSE FV TRAIN LSTSQ': rmse_train_lstsq,\n",
    "                         'MAPE FV TRAIN PRED': mape_train_pred,\n",
    "                         'MAPE FV TRAIN POLY': mape_train_polynomial,\n",
    "                         'MAPE FV TRAIN POLY PRED': mape_train_polynomial_pred,\n",
    "                         'MAPE FV TRAIN LSTSQ': mape_train_lstsq,\n",
    "                         'R2 FV TRAIN PRED': r2_train_pred,\n",
    "                         'R2 FV TRAIN POLY': r2_train_polynomial,\n",
    "                         'R2 FV TRAIN POLY PRED': r2_train_polynomial_pred,\n",
    "                         'R2 FV TRAIN LSTSQ': r2_train_lstsq,\n",
    "                         'RAAE FV PRED TRAIN': raae_train_pred,\n",
    "                         'RAAE FV TRAIN POLY': raae_train_polynomial,\n",
    "                         'RAAE FV TRAIN POLY PRED': raae_train_polynomial_pred,\n",
    "                         'RAAE FV TRAIN LSTSQ': raae_train_lstsq,\n",
    "                         'RMAE FV TRAIN PRED': rmae_train_pred,\n",
    "                         'RMAE FV TRAIN POLY': rmae_train_polynomial,\n",
    "                         'RMAE FV TRAIN POLY PRED': rmae_train_polynomial_pred,\n",
    "                         'RMAE FV TRAIN LSTSQ': rmae_train_lstsq,\n",
    "                         'FD FV TRAIN PRED': fd_train_pred,   \n",
    "                         'FD FV TRAIN POLY': fd_train_polynomial,   \n",
    "                         'FD FV TRAIN POLY PRED': fd_train_polynomial_pred,   \n",
    "                         'FD FV TRAIN LSTSQ': fd_train_lstsq,   \n",
    "                         'DTW FV TRAIN PRED': dtw_train_pred, \n",
    "                         'DTW FV TRAIN POLY': dtw_train_polynomial, \n",
    "                         'DTW FV TRAIN POLY PRED': dtw_train_polynomial_pred, \n",
    "                         'DTW FV TRAIN LSTSQ': dtw_train_lstsq, \n",
    "        },{\n",
    "                         'MAE FV VALID PRED': mae_valid_pred,\n",
    "                         'MAE FV VALID POLY': mae_valid_polynomial,\n",
    "                         'MAE FV VALID POLY PRED': mae_valid_polynomial_pred,\n",
    "                         'MAE FV VALID LSTSQ': mae_valid_lstsq,\n",
    "                         'RMSE FV VALID PRED': rmse_valid_pred,\n",
    "                         'RMSE FV VALID POLY': rmse_valid_polynomial,\n",
    "                         'RMSE FV VALID POLY PRED': rmse_valid_polynomial_pred,\n",
    "                         'RMSE FV VALID LSTSQ': rmse_valid_lstsq,\n",
    "                         'MAPE FV VALID PRED': mape_valid_pred,\n",
    "                         'MAPE FV VALID POLY': mape_valid_polynomial,\n",
    "                         'MAPE FV VALID POLY PRED': mape_valid_polynomial_pred,\n",
    "                         'MAPE FV VALID LSTSQ': mape_valid_lstsq,\n",
    "                         'R2 FV VALID PRED': r2_valid_pred,\n",
    "                         'R2 FV VALID POLY': r2_valid_polynomial,\n",
    "                         'R2 FV VALID POLY PRED': r2_valid_polynomial_pred,\n",
    "                         'R2 FV VALID LSTSQ': r2_valid_lstsq,\n",
    "                         'RAAE FV PRED VALID': raae_valid_pred,\n",
    "                         'RAAE FV VALID POLY': raae_valid_polynomial,\n",
    "                         'RAAE FV VALID POLY PRED': raae_valid_polynomial_pred,\n",
    "                         'RAAE FV VALID LSTSQ': raae_valid_lstsq,\n",
    "                         'RMAE FV VALID PRED': rmae_valid_pred,\n",
    "                         'RMAE FV VALID POLY': rmae_valid_polynomial,\n",
    "                         'RMAE FV VALID POLY PRED': rmae_valid_polynomial_pred,\n",
    "                         'RMAE FV VALID LSTSQ': rmae_valid_lstsq,\n",
    "                         'FD FV VALID PRED': fd_valid_pred,   \n",
    "                         'FD FV VALID POLY': fd_valid_polynomial,   \n",
    "                         'FD FV VALID POLY PRED': fd_valid_polynomial_pred,   \n",
    "                         'FD FV VALID LSTSQ': fd_valid_lstsq,   \n",
    "                         'DTW FV VALID PRED': dtw_valid_pred, \n",
    "                         'DTW FV VALID POLY': dtw_valid_polynomial, \n",
    "                         'DTW FV VALID POLY PRED': dtw_valid_polynomial_pred, \n",
    "                         'DTW FV VALID LSTSQ': dtw_valid_lstsq, \n",
    "        },{\n",
    "                         'MAE FV TEST PRED': mae_test_pred,\n",
    "                         'MAE FV TEST POLY': mae_test_polynomial,\n",
    "                         'MAE FV TEST POLY PRED': mae_test_polynomial_pred,\n",
    "                         'MAE FV TEST LSTSQ': mae_test_lstsq,\n",
    "                         'RMSE FV TEST PRED': rmse_test_pred,\n",
    "                         'RMSE FV TEST POLY': rmse_test_polynomial,\n",
    "                         'RMSE FV TEST POLY PRED': rmse_test_polynomial_pred,\n",
    "                         'RMSE FV TEST LSTSQ': rmse_test_lstsq,\n",
    "                         'MAPE FV TEST PRED': mape_test_pred,\n",
    "                         'MAPE FV TEST POLY': mape_test_polynomial,\n",
    "                         'MAPE FV TEST POLY PRED': mape_test_polynomial_pred,\n",
    "                         'MAPE FV TEST LSTSQ': mape_test_lstsq,\n",
    "                         'R2 FV TEST PRED': r2_test_pred,\n",
    "                         'R2 FV TEST POLY': r2_test_polynomial,\n",
    "                         'R2 FV TEST POLY PRED': r2_test_polynomial_pred,\n",
    "                         'R2 FV TEST LSTSQ': r2_test_lstsq,\n",
    "                         'RAAE FV TEST PRED': raae_test_pred,\n",
    "                         'RAAE FV TEST POLY': raae_test_polynomial,\n",
    "                         'RAAE FV TEST POLY PRED': raae_test_polynomial_pred,\n",
    "                         'RAAE FV TEST LSTSQ': raae_test_lstsq,\n",
    "                         'RMAE FV TEST PRED': rmae_test_pred,\n",
    "                         'RMAE FV TEST POLY': rmae_test_polynomial,\n",
    "                         'RMAE FV TEST POLY PRED': rmae_test_polynomial_pred,        \n",
    "                         'RMAE FV TEST LSTSQ': rmae_test_lstsq,\n",
    "                         'FD FV TEST PRED': fd_test_pred,    \n",
    "                         'FD FV TEST POLY': fd_test_polynomial,    \n",
    "                         'FD FV TEST POLY PRED': fd_test_polynomial_pred, \n",
    "                         'FD FV TEST LSTSQ': fd_test_lstsq,    \n",
    "                         'DTW FV TEST PRED': dtw_test_pred,\n",
    "                         'DTW FV TEST POLY': dtw_test_polynomial,\n",
    "                         'DTW FV TEST POLY PRED': dtw_test_polynomial_pred, \n",
    "                         'DTW FV TEST LSTSQ': dtw_test_lstsq,\n",
    "        },{\n",
    "                         'STD FV TRAIN REAL': std_train_real, \n",
    "                         'STD FV VALID REAL': std_valid_real, \n",
    "                         'STD FV VALID PRED': std_valid_pred,\n",
    "                         'STD FV VALID POLY': std_valid_polynomial, \n",
    "                         'STD FV VALID LSTSQ': std_valid_lstsq, \n",
    "                         'STD FV TEST REAL': std_test_real,\n",
    "                         'STD FV TEST PRED': std_test_pred, \n",
    "                         'STD FV TEST POLY': std_test_polynomial, \n",
    "                         'STD FV TEST LSTSQ': std_test_lstsq, \n",
    "        },{\n",
    "                         'MEAN FV TRAIN REAL': mean_train_real, \n",
    "                         'MEAN FV VALID REAL': mean_valid_real, \n",
    "                         'MEAN FV VALID PRED': mean_valid_pred,\n",
    "                         'MEAN FV VALID POLY': mean_valid_polynomial, \n",
    "                         'MEAN FV VALID LSTSQ': mean_valid_lstsq, \n",
    "                         'MEAN FV TEST REAL': mean_test_real,\n",
    "                         'MEAN FV TEST PRED': mean_test_pred, \n",
    "                         'MEAN FV TEST POLY': mean_test_polynomial, \n",
    "                         'MEAN FV TEST LSTSQ': mean_test_lstsq, \n",
    "        }]        \n",
    "                  \n",
    "            result_dict_list.append(result_dict_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save != None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str((i+1)*each_epochs_save).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                for i, value in enumerate(polynomial.values):\n",
    "                    if i == 0:\n",
    "                        text_file.write(str(value))  \n",
    "                    else:\n",
    "                        text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list, history, #polynomial_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_pred_list, polynomial_lstsq_true_list), result_dict_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T14:29:08.226362Z",
     "start_time": "2020-10-06T14:29:08.063992Z"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "polynomial = X_data_list[0][0]\n",
    "\n",
    "X_data = X_data_list[0][1].values\n",
    "y_data = y_data_list[0][1].values\n",
    "\n",
    "X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "  \n",
    "\n",
    "term_list_all = []\n",
    "for constant, term in zip(polynomial.values, list(polynomial.index)):\n",
    "    term_list = [int(value_mult) for value_mult in term]\n",
    "    term_list_all.append(term_list)\n",
    "\n",
    "\n",
    "terms_matrix = []\n",
    "for unknowns in X_train:\n",
    "    terms = []\n",
    "    for term_multipliers in term_list_all:\n",
    "        term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "\n",
    "        terms.append(term_value)\n",
    "    terms_matrix.append(np.array(terms))\n",
    "\n",
    "terms_matrix = np.array(terms_matrix)\n",
    "\n",
    "\n",
    "polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train.ravel(), rcond=-1)#[::-1]\n",
    "\n",
    "y_train_lstsq = []\n",
    "for entry in X_train:\n",
    "    print(entry)\n",
    "    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial_lstsq_true, entry)\n",
    "    y_train_lstsq.append(true_function_value_lstsq)     \n",
    "\n",
    "y_train_lstsq = np.array(y_train_lstsq).reshape(len(y_train_lstsq), 1)  \n",
    "\n",
    "y_train_poly = []\n",
    "for entry in X_train:\n",
    "    true_function_value_lstsq, _ = calcualate_function_with_data(polynomial.values, entry)\n",
    "    y_train_poly.append(true_function_value_lstsq)     \n",
    "\n",
    "y_train_poly = np.array(y_train_poly).reshape(len(y_train_lstsq), 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:34:24.210145Z",
     "start_time": "2020-10-06T14:29:08.228579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    4.005, max:   17.321, cur:    4.005)\n",
      "\tvalidation       \t (min:    4.618, max:   16.758, cur:    4.618)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    0.586, max:    1.541, cur:    0.586)\n",
      "\tvalidation       \t (min:    0.471, max:    1.063, cur:    0.471)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    5.526, max:   20.786, cur:    5.534)\n",
      "\tvalidation       \t (min:    6.177, max:   20.255, cur:    6.177)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(X_data[1].values, y_data[1].values, X_data[0], return_history=True, each_epochs_save=each_epochs_save, printing=True) for X_data, y_data in zip(X_data_list_split, y_data_list_split))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:36:25.392406Z",
     "start_time": "2020-10-07T12:34:24.238387Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    dict_list_train = [clf[1][0] for clf in clf_list]\n",
    "    dict_list_valid = [clf[1][1] for clf in clf_list]\n",
    "    dict_list_test = [clf[1][2] for clf in clf_list]\n",
    "    dict_list_stds = [clf[1][3] for clf in clf_list]\n",
    "    dict_list_means = [clf[1][4] for clf in clf_list]\n",
    "\n",
    "    dict_list_train_mean = pd.DataFrame(dict_list_train, columns=dict_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((dict_list_train_mean.values[0::4], dict_list_train_mean.values[1::4], dict_list_train_mean.values[2::4], dict_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_dict_list_by_epochs = [[] for i in range(epochs//each_epochs_save)]\n",
    "    for scores_dict_list in scores_list:   \n",
    "        for index, scores_dict in enumerate(scores_dict_list):\n",
    "            scores_dict_list_by_epochs[index].append(scores_dict)\n",
    "            \n",
    "        \n",
    "    for index, scores_dict_list_single_epoch in enumerate(scores_dict_list_by_epochs):\n",
    "        index = (index+1)*each_epochs_save\n",
    "        dict_list_train = [dict_list[0] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_valid = [dict_list[1] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_test = [dict_list[2] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_stds = [dict_list[3] for dict_list in scores_dict_list_single_epoch]\n",
    "        dict_list_means = [dict_list[4] for dict_list in scores_dict_list_single_epoch]\n",
    "        \n",
    "        dict_list_train_mean = pd.DataFrame(dict_list_train, columns=dict_list_train[0].keys()).mean()  \n",
    "        dict_list_valid_mean = pd.DataFrame(dict_list_valid, columns=dict_list_valid[0].keys()).mean()  \n",
    "        dict_list_test_mean = pd.DataFrame(dict_list_test, columns=dict_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(dict_list_stds, columns=dict_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(dict_list_means, columns=dict_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1*each_epochs_save:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((dict_list_train_mean.values[0::4], dict_list_train_mean.values[1::4], dict_list_train_mean.values[2::4], dict_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((dict_list_train_mean.values[0::4], dict_list_train_mean.values[1::4], dict_list_train_mean.values[2::4], dict_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((dict_list_valid_mean.values[0::4], dict_list_valid_mean.values[1::4], dict_list_valid_mean.values[2::4], dict_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((dict_list_test_mean.values[0::4], dict_list_test_mean.values[1::4], dict_list_test_mean.values[2::4], dict_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:36:25.416166Z",
     "start_time": "2020-10-07T12:36:25.394773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E5</th>\n",
       "      <th>TRAIN POLY E5</th>\n",
       "      <th>TRAIN POLY PRED E5</th>\n",
       "      <th>TRAIN LSTSQ E5</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E15</th>\n",
       "      <th>TRAIN POLY E15</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E195</th>\n",
       "      <th>TRAIN POLY E195</th>\n",
       "      <th>TRAIN POLY PRED E195</th>\n",
       "      <th>TRAIN LSTSQ E195</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>9.969</td>\n",
       "      <td>9.969</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.686</td>\n",
       "      <td>9.687</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.343</td>\n",
       "      <td>9.344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.207</td>\n",
       "      <td>4.253</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.161</td>\n",
       "      <td>4.209</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.808</td>\n",
       "      <td>12.808</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.518</td>\n",
       "      <td>12.518</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.165</td>\n",
       "      <td>12.165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.005</td>\n",
       "      <td>5.977</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.948</td>\n",
       "      <td>5.918</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.265</td>\n",
       "      <td>1.273</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.508</td>\n",
       "      <td>1.520</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.799</td>\n",
       "      <td>1.817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.882</td>\n",
       "      <td>3.791</td>\n",
       "      <td>1.684</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.877</td>\n",
       "      <td>3.781</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.896</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.534</td>\n",
       "      <td>3.533</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.516</td>\n",
       "      <td>3.515</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.489</td>\n",
       "      <td>3.488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.383</td>\n",
       "      <td>2.313</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.368</td>\n",
       "      <td>2.296</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>22.081</td>\n",
       "      <td>22.084</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.626</td>\n",
       "      <td>21.630</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.051</td>\n",
       "      <td>21.052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.334</td>\n",
       "      <td>9.308</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.261</td>\n",
       "      <td>9.236</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>90.637</td>\n",
       "      <td>90.654</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>87.832</td>\n",
       "      <td>87.852</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>84.426</td>\n",
       "      <td>84.447</td>\n",
       "      <td>...</td>\n",
       "      <td>3.845</td>\n",
       "      <td>0.000</td>\n",
       "      <td>34.833</td>\n",
       "      <td>35.475</td>\n",
       "      <td>3.966</td>\n",
       "      <td>0.000</td>\n",
       "      <td>34.472</td>\n",
       "      <td>35.132</td>\n",
       "      <td>4.086</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E5  TRAIN POLY E5  TRAIN POLY PRED E5  TRAIN LSTSQ E5  \\\n",
       "MAE FV           9.969          9.969               0.012           0.000   \n",
       "RMSE FV         12.808         12.808               0.016           0.000   \n",
       "MAPE FV          1.265          1.273               0.270           0.000   \n",
       "R2 FV           -0.355         -0.355               0.982           1.000   \n",
       "RAAE FV          0.896          0.896               0.098           0.000   \n",
       "RMAE FV          3.534          3.533               0.485           0.000   \n",
       "FD FV           22.081         22.084               0.026           0.000   \n",
       "DTW FV          90.637         90.654               0.089           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.686           9.687                0.014            0.000   \n",
       "RMSE FV          12.518          12.518                0.018            0.000   \n",
       "MAPE FV           1.508           1.520                0.651            0.000   \n",
       "R2 FV            -0.286          -0.286                0.993            1.000   \n",
       "RAAE FV           0.870           0.870                0.061            0.000   \n",
       "RMAE FV           3.516           3.515                0.338            0.000   \n",
       "FD FV            21.626          21.630                0.027            0.000   \n",
       "DTW FV           87.832          87.852                0.099            0.000   \n",
       "\n",
       "         TRAIN PRED E15  TRAIN POLY E15  ...  TRAIN POLY PRED E190  \\\n",
       "MAE FV            9.343           9.344  ...                 0.410   \n",
       "RMSE FV          12.165          12.165  ...                 0.527   \n",
       "MAPE FV           1.799           1.817  ...                 0.916   \n",
       "R2 FV            -0.205          -0.205  ...                 0.995   \n",
       "RAAE FV           0.837           0.838  ...                 0.054   \n",
       "RMAE FV           3.489           3.488  ...                 0.278   \n",
       "FD FV            21.051          21.052  ...                 0.891   \n",
       "DTW FV           84.426          84.447  ...                 3.845   \n",
       "\n",
       "         TRAIN LSTSQ E190  TRAIN PRED E195  TRAIN POLY E195  \\\n",
       "MAE FV              0.000            4.207            4.253   \n",
       "RMSE FV             0.000            6.005            5.977   \n",
       "MAPE FV             0.000            3.882            3.791   \n",
       "R2 FV               1.000            0.685            0.687   \n",
       "RAAE FV             0.000            0.386            0.391   \n",
       "RMAE FV             0.000            2.383            2.313   \n",
       "FD FV               0.000            9.334            9.308   \n",
       "DTW FV              0.000           34.833           35.475   \n",
       "\n",
       "         TRAIN POLY PRED E195  TRAIN LSTSQ E195  TRAIN PRED E200  \\\n",
       "MAE FV                  0.424             0.000            4.161   \n",
       "RMSE FV                 0.544             0.000            5.948   \n",
       "MAPE FV                 1.684             0.000            3.877   \n",
       "R2 FV                   0.994             1.000            0.690   \n",
       "RAAE FV                 0.055             0.000            0.382   \n",
       "RMAE FV                 0.284             0.000            2.368   \n",
       "FD FV                   0.919             0.000            9.261   \n",
       "DTW FV                  3.966             0.000           34.472   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             4.209                 0.437             0.000  \n",
       "RMSE FV            5.918                 0.560             0.000  \n",
       "MAPE FV            3.781                 0.555             0.000  \n",
       "R2 FV              0.693                 0.994             1.000  \n",
       "RAAE FV            0.387                 0.057             0.000  \n",
       "RMAE FV            2.296                 0.289             0.000  \n",
       "FD FV              9.236                 0.947             0.000  \n",
       "DTW FV            35.132                 4.086             0.000  \n",
       "\n",
       "[8 rows x 160 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:36:25.436056Z",
     "start_time": "2020-10-07T12:36:25.417701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E5</th>\n",
       "      <th>VALID POLY E5</th>\n",
       "      <th>VALID POLY PRED E5</th>\n",
       "      <th>VALID LSTSQ E5</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E15</th>\n",
       "      <th>VALID POLY E15</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E195</th>\n",
       "      <th>VALID POLY E195</th>\n",
       "      <th>VALID POLY PRED E195</th>\n",
       "      <th>VALID LSTSQ E195</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.368</td>\n",
       "      <td>10.367</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.093</td>\n",
       "      <td>10.093</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.760</td>\n",
       "      <td>9.761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.625</td>\n",
       "      <td>4.639</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.580</td>\n",
       "      <td>4.593</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>13.343</td>\n",
       "      <td>13.343</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.060</td>\n",
       "      <td>13.059</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.715</td>\n",
       "      <td>12.715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.542</td>\n",
       "      <td>6.472</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.485</td>\n",
       "      <td>6.411</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.179</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.336</td>\n",
       "      <td>1.335</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.538</td>\n",
       "      <td>1.541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.071</td>\n",
       "      <td>2.913</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.030</td>\n",
       "      <td>2.878</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.977</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.890</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.072</td>\n",
       "      <td>3.071</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.054</td>\n",
       "      <td>3.054</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.029</td>\n",
       "      <td>3.029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.025</td>\n",
       "      <td>1.963</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.012</td>\n",
       "      <td>1.947</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>32.130</td>\n",
       "      <td>32.117</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>31.736</td>\n",
       "      <td>31.732</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.000</td>\n",
       "      <td>31.226</td>\n",
       "      <td>31.232</td>\n",
       "      <td>...</td>\n",
       "      <td>1.786</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.158</td>\n",
       "      <td>16.822</td>\n",
       "      <td>1.842</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.050</td>\n",
       "      <td>16.687</td>\n",
       "      <td>1.899</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>112.316</td>\n",
       "      <td>112.300</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.000</td>\n",
       "      <td>109.655</td>\n",
       "      <td>109.638</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.000</td>\n",
       "      <td>106.402</td>\n",
       "      <td>106.414</td>\n",
       "      <td>...</td>\n",
       "      <td>5.777</td>\n",
       "      <td>0.000</td>\n",
       "      <td>54.372</td>\n",
       "      <td>53.801</td>\n",
       "      <td>5.965</td>\n",
       "      <td>0.000</td>\n",
       "      <td>53.946</td>\n",
       "      <td>53.337</td>\n",
       "      <td>6.153</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E5  VALID POLY E5  VALID POLY PRED E5  VALID LSTSQ E5  \\\n",
       "MAE FV          10.368         10.367               0.014           0.000   \n",
       "RMSE FV         13.343         13.343               0.019           0.000   \n",
       "MAPE FV          1.179          1.180               0.334           0.000   \n",
       "R2 FV           -0.338         -0.338               0.977           1.000   \n",
       "RAAE FV          0.890          0.890               0.108           0.000   \n",
       "RMAE FV          3.072          3.071               0.504           0.000   \n",
       "FD FV           32.130         32.117               0.065           0.000   \n",
       "DTW FV         112.316        112.300               0.223           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV           10.093          10.093                0.015            0.000   \n",
       "RMSE FV          13.060          13.059                0.020            0.000   \n",
       "MAPE FV           1.336           1.335                0.176            0.000   \n",
       "R2 FV            -0.274          -0.274                0.991            1.000   \n",
       "RAAE FV           0.865           0.865                0.067            0.000   \n",
       "RMAE FV           3.054           3.054                0.300            0.000   \n",
       "FD FV            31.736          31.732                0.072            0.000   \n",
       "DTW FV          109.655         109.638                0.229            0.000   \n",
       "\n",
       "         VALID PRED E15  VALID POLY E15  ...  VALID POLY PRED E190  \\\n",
       "MAE FV            9.760           9.761  ...                 0.444   \n",
       "RMSE FV          12.715          12.715  ...                 0.582   \n",
       "MAPE FV           1.538           1.541  ...                 0.635   \n",
       "R2 FV            -0.199          -0.199  ...                 0.994   \n",
       "RAAE FV           0.835           0.835  ...                 0.057   \n",
       "RMAE FV           3.029           3.029  ...                 0.267   \n",
       "FD FV            31.226          31.232  ...                 1.786   \n",
       "DTW FV          106.402         106.414  ...                 5.777   \n",
       "\n",
       "         VALID LSTSQ E190  VALID PRED E195  VALID POLY E195  \\\n",
       "MAE FV              0.000            4.625            4.639   \n",
       "RMSE FV             0.000            6.542            6.472   \n",
       "MAPE FV             0.000            3.071            2.913   \n",
       "R2 FV               1.000            0.659            0.666   \n",
       "RAAE FV             0.000            0.405            0.407   \n",
       "RMAE FV             0.000            2.025            1.963   \n",
       "FD FV               0.000           17.158           16.822   \n",
       "DTW FV              0.000           54.372           53.801   \n",
       "\n",
       "         VALID POLY PRED E195  VALID LSTSQ E195  VALID PRED E200  \\\n",
       "MAE FV                  0.458             0.000            4.580   \n",
       "RMSE FV                 0.600             0.000            6.485   \n",
       "MAPE FV                 0.669             0.000            3.030   \n",
       "R2 FV                   0.993             1.000            0.665   \n",
       "RAAE FV                 0.059             0.000            0.401   \n",
       "RMAE FV                 0.272             0.000            2.012   \n",
       "FD FV                   1.842             0.000           17.050   \n",
       "DTW FV                  5.965             0.000           53.946   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             4.593                 0.472             0.000  \n",
       "RMSE FV            6.411                 0.619             0.000  \n",
       "MAPE FV            2.878                 0.686             0.000  \n",
       "R2 FV              0.672                 0.993             1.000  \n",
       "RAAE FV            0.403                 0.060             0.000  \n",
       "RMAE FV            1.947                 0.278             0.000  \n",
       "FD FV             16.687                 1.899             0.000  \n",
       "DTW FV            53.337                 6.153             0.000  \n",
       "\n",
       "[8 rows x 160 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:36:25.455967Z",
     "start_time": "2020-10-07T12:36:25.437454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E5</th>\n",
       "      <th>TEST POLY E5</th>\n",
       "      <th>TEST POLY PRED E5</th>\n",
       "      <th>TEST LSTSQ E5</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E15</th>\n",
       "      <th>TEST POLY E15</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E195</th>\n",
       "      <th>TEST POLY E195</th>\n",
       "      <th>TEST POLY PRED E195</th>\n",
       "      <th>TEST LSTSQ E195</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.351</td>\n",
       "      <td>10.351</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.074</td>\n",
       "      <td>10.074</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.738</td>\n",
       "      <td>9.739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.524</td>\n",
       "      <td>4.539</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.479</td>\n",
       "      <td>4.493</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>13.182</td>\n",
       "      <td>13.182</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.892</td>\n",
       "      <td>12.892</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.539</td>\n",
       "      <td>12.538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.267</td>\n",
       "      <td>6.211</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.210</td>\n",
       "      <td>6.151</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.903</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.024</td>\n",
       "      <td>3.023</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.006</td>\n",
       "      <td>3.005</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.981</td>\n",
       "      <td>2.980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.964</td>\n",
       "      <td>1.905</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.950</td>\n",
       "      <td>1.890</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>27.526</td>\n",
       "      <td>27.525</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.080</td>\n",
       "      <td>27.079</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.522</td>\n",
       "      <td>26.522</td>\n",
       "      <td>...</td>\n",
       "      <td>1.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.477</td>\n",
       "      <td>13.218</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.373</td>\n",
       "      <td>13.104</td>\n",
       "      <td>1.293</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.741</td>\n",
       "      <td>102.717</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.952</td>\n",
       "      <td>99.928</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.547</td>\n",
       "      <td>96.547</td>\n",
       "      <td>...</td>\n",
       "      <td>4.823</td>\n",
       "      <td>0.000</td>\n",
       "      <td>46.322</td>\n",
       "      <td>46.262</td>\n",
       "      <td>4.982</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.907</td>\n",
       "      <td>45.840</td>\n",
       "      <td>5.139</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E5  TEST POLY E5  TEST POLY PRED E5  TEST LSTSQ E5  \\\n",
       "MAE FV         10.351        10.351              0.013          0.000   \n",
       "RMSE FV        13.182        13.182              0.016          0.000   \n",
       "MAPE FV           inf           inf              0.329          0.000   \n",
       "R2 FV          -0.352        -0.352              0.982          1.000   \n",
       "RAAE FV         0.903         0.903              0.100          0.000   \n",
       "RMAE FV         3.024         3.023              0.468          0.000   \n",
       "FD FV          27.526        27.525              0.041          0.000   \n",
       "DTW FV        102.741       102.717              0.121          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV          10.074         10.074               0.014           0.000   \n",
       "RMSE FV         12.892         12.892               0.018           0.000   \n",
       "MAPE FV            inf            inf               0.146           0.000   \n",
       "R2 FV           -0.285         -0.285               0.993           1.000   \n",
       "RAAE FV          0.877          0.877               0.062           0.000   \n",
       "RMAE FV          3.006          3.005               0.286           0.000   \n",
       "FD FV           27.080         27.079               0.043           0.000   \n",
       "DTW FV          99.952         99.928               0.143           0.000   \n",
       "\n",
       "         TEST PRED E15  TEST POLY E15  ...  TEST POLY PRED E190  \\\n",
       "MAE FV           9.738          9.739  ...                0.429   \n",
       "RMSE FV         12.539         12.538  ...                0.553   \n",
       "MAPE FV            inf            inf  ...                0.608   \n",
       "R2 FV           -0.206         -0.206  ...                0.994   \n",
       "RAAE FV          0.847          0.847  ...                0.055   \n",
       "RMAE FV          2.981          2.980  ...                0.250   \n",
       "FD FV           26.522         26.522  ...                1.218   \n",
       "DTW FV          96.547         96.547  ...                4.823   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E195  TEST POLY E195  TEST POLY PRED E195  \\\n",
       "MAE FV             0.000           4.524           4.539                0.443   \n",
       "RMSE FV            0.000           6.267           6.211                0.571   \n",
       "MAPE FV            0.000             inf             inf                0.710   \n",
       "R2 FV              1.000           0.675           0.681                0.994   \n",
       "RAAE FV            0.000           0.403           0.405                0.056   \n",
       "RMAE FV            0.000           1.964           1.905                0.256   \n",
       "FD FV              0.000          13.477          13.218                1.255   \n",
       "DTW FV             0.000          46.322          46.262                4.982   \n",
       "\n",
       "         TEST LSTSQ E195  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           4.479           4.493                0.456   \n",
       "RMSE FV            0.000           6.210           6.151                0.588   \n",
       "MAPE FV            0.000             inf             inf                0.531   \n",
       "R2 FV              1.000           0.681           0.687                0.994   \n",
       "RAAE FV            0.000           0.399           0.401                0.058   \n",
       "RMAE FV            0.000           1.950           1.890                0.261   \n",
       "FD FV              0.000          13.373          13.104                1.293   \n",
       "DTW FV             0.000          45.907          45.840                5.139   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 160 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:36:25.476020Z",
     "start_time": "2020-10-07T12:36:25.457380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E5</th>\n",
       "      <th>E10</th>\n",
       "      <th>E15</th>\n",
       "      <th>E20</th>\n",
       "      <th>E25</th>\n",
       "      <th>E30</th>\n",
       "      <th>E35</th>\n",
       "      <th>E40</th>\n",
       "      <th>E45</th>\n",
       "      <th>E50</th>\n",
       "      <th>...</th>\n",
       "      <th>E155</th>\n",
       "      <th>E160</th>\n",
       "      <th>E165</th>\n",
       "      <th>E170</th>\n",
       "      <th>E175</th>\n",
       "      <th>E180</th>\n",
       "      <th>E185</th>\n",
       "      <th>E190</th>\n",
       "      <th>E195</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL</th>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>...</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL</th>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>...</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.939</td>\n",
       "      <td>2.530</td>\n",
       "      <td>3.128</td>\n",
       "      <td>3.707</td>\n",
       "      <td>...</td>\n",
       "      <td>7.904</td>\n",
       "      <td>7.949</td>\n",
       "      <td>7.992</td>\n",
       "      <td>8.034</td>\n",
       "      <td>8.075</td>\n",
       "      <td>8.117</td>\n",
       "      <td>8.158</td>\n",
       "      <td>8.198</td>\n",
       "      <td>8.239</td>\n",
       "      <td>8.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID POLY</th>\n",
       "      <td>0.138</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.939</td>\n",
       "      <td>2.530</td>\n",
       "      <td>3.127</td>\n",
       "      <td>3.706</td>\n",
       "      <td>...</td>\n",
       "      <td>7.904</td>\n",
       "      <td>7.948</td>\n",
       "      <td>7.991</td>\n",
       "      <td>8.034</td>\n",
       "      <td>8.076</td>\n",
       "      <td>8.117</td>\n",
       "      <td>8.158</td>\n",
       "      <td>8.199</td>\n",
       "      <td>8.240</td>\n",
       "      <td>8.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID LSTSQ</th>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>...</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL</th>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>...</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.416</td>\n",
       "      <td>1.965</td>\n",
       "      <td>2.563</td>\n",
       "      <td>3.166</td>\n",
       "      <td>3.750</td>\n",
       "      <td>...</td>\n",
       "      <td>7.957</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.043</td>\n",
       "      <td>8.084</td>\n",
       "      <td>8.126</td>\n",
       "      <td>8.166</td>\n",
       "      <td>8.207</td>\n",
       "      <td>8.247</td>\n",
       "      <td>8.287</td>\n",
       "      <td>8.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST POLY</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.418</td>\n",
       "      <td>1.967</td>\n",
       "      <td>2.565</td>\n",
       "      <td>3.170</td>\n",
       "      <td>3.755</td>\n",
       "      <td>...</td>\n",
       "      <td>7.966</td>\n",
       "      <td>8.009</td>\n",
       "      <td>8.051</td>\n",
       "      <td>8.092</td>\n",
       "      <td>8.133</td>\n",
       "      <td>8.173</td>\n",
       "      <td>8.213</td>\n",
       "      <td>8.253</td>\n",
       "      <td>8.292</td>\n",
       "      <td>8.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST LSTSQ</th>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>...</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "      <td>11.567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       E5    E10    E15    E20    E25    E30    E35    E40  \\\n",
       "STD FV TRAIN REAL  11.220 11.220 11.220 11.220 11.220 11.220 11.220 11.220   \n",
       "STD FV VALID REAL  11.754 11.754 11.754 11.754 11.754 11.754 11.754 11.754   \n",
       "STD FV VALID PRED   0.140  0.242  0.396  0.621  0.950  1.396  1.939  2.530   \n",
       "STD FV VALID POLY   0.138  0.242  0.396  0.621  0.949  1.396  1.939  2.530   \n",
       "STD FV VALID LSTSQ 11.754 11.754 11.754 11.754 11.754 11.754 11.754 11.754   \n",
       "STD FV TEST REAL   11.567 11.567 11.567 11.567 11.567 11.567 11.567 11.567   \n",
       "STD FV TEST PRED    0.139  0.245  0.401  0.631  0.964  1.416  1.965  2.563   \n",
       "STD FV TEST POLY    0.140  0.246  0.403  0.632  0.965  1.418  1.967  2.565   \n",
       "STD FV TEST LSTSQ  11.567 11.567 11.567 11.567 11.567 11.567 11.567 11.567   \n",
       "\n",
       "                      E45    E50  ...   E155   E160   E165   E170   E175  \\\n",
       "STD FV TRAIN REAL  11.220 11.220  ... 11.220 11.220 11.220 11.220 11.220   \n",
       "STD FV VALID REAL  11.754 11.754  ... 11.754 11.754 11.754 11.754 11.754   \n",
       "STD FV VALID PRED   3.128  3.707  ...  7.904  7.949  7.992  8.034  8.075   \n",
       "STD FV VALID POLY   3.127  3.706  ...  7.904  7.948  7.991  8.034  8.076   \n",
       "STD FV VALID LSTSQ 11.754 11.754  ... 11.754 11.754 11.754 11.754 11.754   \n",
       "STD FV TEST REAL   11.567 11.567  ... 11.567 11.567 11.567 11.567 11.567   \n",
       "STD FV TEST PRED    3.166  3.750  ...  7.957  8.000  8.043  8.084  8.126   \n",
       "STD FV TEST POLY    3.170  3.755  ...  7.966  8.009  8.051  8.092  8.133   \n",
       "STD FV TEST LSTSQ  11.567 11.567  ... 11.567 11.567 11.567 11.567 11.567   \n",
       "\n",
       "                     E180   E185   E190   E195   E200  \n",
       "STD FV TRAIN REAL  11.220 11.220 11.220 11.220 11.220  \n",
       "STD FV VALID REAL  11.754 11.754 11.754 11.754 11.754  \n",
       "STD FV VALID PRED   8.117  8.158  8.198  8.239  8.280  \n",
       "STD FV VALID POLY   8.117  8.158  8.199  8.240  8.281  \n",
       "STD FV VALID LSTSQ 11.754 11.754 11.754 11.754 11.754  \n",
       "STD FV TEST REAL   11.567 11.567 11.567 11.567 11.567  \n",
       "STD FV TEST PRED    8.166  8.207  8.247  8.287  8.327  \n",
       "STD FV TEST POLY    8.173  8.213  8.253  8.292  8.332  \n",
       "STD FV TEST LSTSQ  11.567 11.567 11.567 11.567 11.567  \n",
       "\n",
       "[9 rows x 40 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:36:25.495998Z",
     "start_time": "2020-10-07T12:36:25.477573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E5</th>\n",
       "      <th>E10</th>\n",
       "      <th>E15</th>\n",
       "      <th>E20</th>\n",
       "      <th>E25</th>\n",
       "      <th>E30</th>\n",
       "      <th>E35</th>\n",
       "      <th>E40</th>\n",
       "      <th>E45</th>\n",
       "      <th>E50</th>\n",
       "      <th>...</th>\n",
       "      <th>E155</th>\n",
       "      <th>E160</th>\n",
       "      <th>E165</th>\n",
       "      <th>E170</th>\n",
       "      <th>E175</th>\n",
       "      <th>E180</th>\n",
       "      <th>E185</th>\n",
       "      <th>E190</th>\n",
       "      <th>E195</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL</th>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL</th>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED</th>\n",
       "      <td>0.193</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID POLY</th>\n",
       "      <td>0.193</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID LSTSQ</th>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL</th>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED</th>\n",
       "      <td>0.202</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST POLY</th>\n",
       "      <td>0.201</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.209</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST LSTSQ</th>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        E5    E10    E15    E20    E25    E30    E35    E40  \\\n",
       "MEAN FV TRAIN REAL  -0.079 -0.079 -0.079 -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV VALID REAL  -0.102 -0.102 -0.102 -0.102 -0.102 -0.102 -0.102 -0.102   \n",
       "MEAN FV VALID PRED   0.193  0.216  0.268  0.350  0.429  0.448  0.401  0.327   \n",
       "MEAN FV VALID POLY   0.193  0.215  0.267  0.349  0.429  0.447  0.401  0.326   \n",
       "MEAN FV VALID LSTSQ -0.102 -0.102 -0.102 -0.102 -0.102 -0.102 -0.102 -0.102   \n",
       "MEAN FV TEST REAL   -0.095 -0.095 -0.095 -0.095 -0.095 -0.095 -0.095 -0.095   \n",
       "MEAN FV TEST PRED    0.202  0.225  0.279  0.363  0.443  0.464  0.418  0.343   \n",
       "MEAN FV TEST POLY    0.201  0.224  0.278  0.362  0.443  0.463  0.417  0.342   \n",
       "MEAN FV TEST LSTSQ  -0.095 -0.095 -0.095 -0.095 -0.095 -0.095 -0.095 -0.095   \n",
       "\n",
       "                       E45    E50  ...   E155   E160   E165   E170   E175  \\\n",
       "MEAN FV TRAIN REAL  -0.079 -0.079  ... -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV VALID REAL  -0.102 -0.102  ... -0.102 -0.102 -0.102 -0.102 -0.102   \n",
       "MEAN FV VALID PRED   0.254  0.194  ... -0.016 -0.016 -0.016 -0.016 -0.016   \n",
       "MEAN FV VALID POLY   0.254  0.193  ... -0.015 -0.015 -0.015 -0.015 -0.015   \n",
       "MEAN FV VALID LSTSQ -0.102 -0.102  ... -0.102 -0.102 -0.102 -0.102 -0.102   \n",
       "MEAN FV TEST REAL   -0.095 -0.095  ... -0.095 -0.095 -0.095 -0.095 -0.095   \n",
       "MEAN FV TEST PRED    0.270  0.210  ... -0.007 -0.007 -0.007 -0.007 -0.007   \n",
       "MEAN FV TEST POLY    0.269  0.209  ... -0.007 -0.008 -0.008 -0.008 -0.008   \n",
       "MEAN FV TEST LSTSQ  -0.095 -0.095  ... -0.095 -0.095 -0.095 -0.095 -0.095   \n",
       "\n",
       "                      E180   E185   E190   E195   E200  \n",
       "MEAN FV TRAIN REAL  -0.079 -0.079 -0.079 -0.079 -0.079  \n",
       "MEAN FV VALID REAL  -0.102 -0.102 -0.102 -0.102 -0.102  \n",
       "MEAN FV VALID PRED  -0.016 -0.016 -0.017 -0.017 -0.017  \n",
       "MEAN FV VALID POLY  -0.015 -0.016 -0.016 -0.016 -0.016  \n",
       "MEAN FV VALID LSTSQ -0.102 -0.102 -0.102 -0.102 -0.102  \n",
       "MEAN FV TEST REAL   -0.095 -0.095 -0.095 -0.095 -0.095  \n",
       "MEAN FV TEST PRED   -0.008 -0.008 -0.008 -0.009 -0.009  \n",
       "MEAN FV TEST POLY   -0.008 -0.008 -0.009 -0.009 -0.010  \n",
       "MEAN FV TEST LSTSQ  -0.095 -0.095 -0.095 -0.095 -0.095  \n",
       "\n",
       "[9 rows x 40 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:05:39.952258Z",
     "start_time": "2020-10-07T12:36:25.497603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7517e18ebafb41cebecef75c63ea2d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e89035bd4cb4346ae8d7a25d40e94c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_train_real, pred_evaluation_dataset_train, pred_evaluation_dataset_train_polynomial, X_train_data, pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data = [clf[2] for clf in clf_list]\n",
    "\n",
    "    pred_evaluation_dataset_train_real = pred_evaluation_dataset_train_real.ravel()\n",
    "    pred_evaluation_dataset_valid_real = pred_evaluation_dataset_valid_real.ravel()\n",
    "    pred_evaluation_dataset_test_real = pred_evaluation_dataset_test_real.ravel()\n",
    "    pred_evaluation_dataset_train = pred_evaluation_dataset_train.ravel()\n",
    "    pred_evaluation_dataset_valid = pred_evaluation_dataset_valid.ravel()\n",
    "    pred_evaluation_dataset_test = pred_evaluation_dataset_test.ravel()\n",
    "    pred_evaluation_dataset_train_polynomial = pred_evaluation_dataset_train_polynomial.ravel()\n",
    "    pred_evaluation_dataset_valid_polynomial = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "    pred_evaluation_dataset_test_polynomial = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_train_real, columns=[str(train_data) for train_data in X_train_data])\n",
    "    pred_evaluation_dataset_train_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_train_df = pd.DataFrame(pred_evaluation_dataset_train, columns=[str(train_data) for train_data in X_vtrain_data])\n",
    "    pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test, columns=[str(test_data) for test_data in X_test_data])\n",
    "    pred_evaluation_dataset_train_polynomial_df = pd.DataFrame(pred_evaluation_dataset_train_polynomial, columns=[str(train_data) for train_data in X_train_data])\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial, columns=[str(test_data) for test_data in X_test_data])    \n",
    "    \n",
    "    pred_evaluation_dataset_train_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_real_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "    pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "    pred_evaluation_dataset_train_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "    pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "    pred_evaluation_dataset_train_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_polynomial_df], axis=1)\n",
    "    pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "    pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "       \n",
    "    path_pred_evaluation_dataset_train_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_train_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    pred_evaluation_dataset_train_real_df.to_csv(path_pred_evaluation_dataset_train_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)\n",
    "    pred_evaluation_dataset_train_df.to_csv(path_pred_evaluation_dataset_train, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)\n",
    "    pred_evaluation_dataset_train_polynomial_df.to_csv(path_pred_evaluation_dataset_train_polynomial, sep=',', index=False)\n",
    "    pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "    pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    polynomials = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_pred_lists = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_true_lists = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomials_df = pd.DataFrame(polynomials)\n",
    "    \n",
    "    pred_evaluation_dataset_train_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][0]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_train_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_train_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][4]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_valid_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_real_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][8]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]))) for i in range(epochs//each_epochs_save)]\n",
    "    pred_evaluation_dataset_test_polynomial_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]))) for i in range(epochs//each_epochs_save)]\n",
    "    \n",
    "    for i, pred_evaluation_dataset_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (pred_evaluation_dataset_train_real, pred_evaluation_dataset_train, pred_evaluation_dataset_train_polynomial, X_train_data, pred_evaluation_dataset_valid_real, pred_evaluation_dataset_valid, pred_evaluation_dataset_valid_polynomial, X_valid_data, pred_evaluation_dataset_test_real, pred_evaluation_dataset_test, pred_evaluation_dataset_test_polynomial, X_test_data) in enumerate(pred_evaluation_dataset_per_epoch):\n",
    "            pred_evaluation_dataset_train_real_list[index][i] = pred_evaluation_dataset_train_real.ravel()\n",
    "            pred_evaluation_dataset_train_list[index][i] = pred_evaluation_dataset_train.ravel()\n",
    "            pred_evaluation_dataset_train_polynomial_list[index][i] = pred_evaluation_dataset_train_polynomial.ravel()\n",
    "            \n",
    "            pred_evaluation_dataset_valid_real_list[index][i] = pred_evaluation_dataset_valid_real.ravel()\n",
    "            pred_evaluation_dataset_valid_list[index][i] = pred_evaluation_dataset_valid.ravel()\n",
    "            pred_evaluation_dataset_valid_polynomial_list[index][i] = pred_evaluation_dataset_valid_polynomial.ravel()\n",
    "            \n",
    "            pred_evaluation_dataset_test_real_list[index][i] = pred_evaluation_dataset_test_real.ravel()\n",
    "            pred_evaluation_dataset_test_list[index][i] = pred_evaluation_dataset_test.ravel()\n",
    "            pred_evaluation_dataset_test_polynomial_list[index][i] = pred_evaluation_dataset_test_polynomial.ravel()\n",
    "    \n",
    "    for index, (pred_evaluation_dataset_train_real_by_epoch, pred_evaluation_dataset_train_by_epoch, pred_evaluation_dataset_train_polynomial_by_epoch, pred_evaluation_dataset_valid_real_by_epoch, pred_evaluation_dataset_valid_by_epoch, pred_evaluation_dataset_valid_polynomial_by_epoch, pred_evaluation_dataset_test_real_by_epoch, pred_evaluation_dataset_test_by_epoch, pred_evaluation_dataset_test_polynomial_by_epoch) in tqdm(enumerate(zip(pred_evaluation_dataset_train_real_list, pred_evaluation_dataset_train_list, pred_evaluation_dataset_train_polynomial_list, pred_evaluation_dataset_valid_real_list, pred_evaluation_dataset_valid_list, pred_evaluation_dataset_valid_polynomial_list, pred_evaluation_dataset_test_real_list, pred_evaluation_dataset_test_list, pred_evaluation_dataset_test_polynomial_list)), total=len(pred_evaluation_dataset_valid_list)):\n",
    "        \n",
    "        pred_evaluation_dataset_train_real_df = pd.DataFrame(pred_evaluation_dataset_train_real_by_epoch, columns=[str(train_data) for train_data in X_train_data])\n",
    "        pred_evaluation_dataset_train_df = pd.DataFrame(pred_evaluation_dataset_train_by_epoch, columns=[str(train_data) for train_data in X_train_data])\n",
    "        pred_evaluation_dataset_train_polynomial_df = pd.DataFrame(pred_evaluation_dataset_train_polynomial_by_epoch, columns=[str(train_data) for train_data in X_train_data])  \n",
    "        pred_evaluation_dataset_valid_real_df = pd.DataFrame(pred_evaluation_dataset_valid_real_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_df = pd.DataFrame(pred_evaluation_dataset_valid_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.DataFrame(pred_evaluation_dataset_valid_polynomial_by_epoch, columns=[str(valid_data) for valid_data in X_valid_data])  \n",
    "        pred_evaluation_dataset_test_real_df = pd.DataFrame(pred_evaluation_dataset_test_real_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_df = pd.DataFrame(pred_evaluation_dataset_test_by_epoch, columns=[str(test_data) for test_data in X_test_data])\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.DataFrame(pred_evaluation_dataset_test_polynomial_by_epoch, columns=[str(test_data) for test_data in X_test_data])   \n",
    "        \n",
    "        pred_evaluation_dataset_train_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_real_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_real_df], axis=1)\n",
    "        pred_evaluation_dataset_test_real_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_real_df], axis=1)\n",
    "        pred_evaluation_dataset_train_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_df], axis=1)\n",
    "        pred_evaluation_dataset_test_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_df], axis=1)\n",
    "        pred_evaluation_dataset_train_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_train_polynomial_df], axis=1)\n",
    "        pred_evaluation_dataset_valid_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_valid_polynomial_df], axis=1)\n",
    "        pred_evaluation_dataset_test_polynomial_df = pd.concat([polynomials_df, pred_evaluation_dataset_test_polynomial_df], axis=1)\n",
    "\n",
    "        path_pred_evaluation_dataset_train_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_real = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_real_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_train_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_train_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_valid_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_valid_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "        path_pred_evaluation_dataset_test_polynomial = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/pred_evaluation_dataset_test_polynomial_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        pred_evaluation_dataset_train_real_df.to_csv(path_pred_evaluation_dataset_train_real, sep=',', index=False)\n",
    "        pred_evaluation_dataset_valid_real_df.to_csv(path_pred_evaluation_dataset_valid_real, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_real_df.to_csv(path_pred_evaluation_dataset_test_real, sep=',', index=False)         \n",
    "        pred_evaluation_dataset_train_df.to_csv(path_pred_evaluation_dataset_train, sep=',', index=False)\n",
    "        pred_evaluation_dataset_valid_df.to_csv(path_pred_evaluation_dataset_valid, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_df.to_csv(path_pred_evaluation_dataset_test, sep=',', index=False)    \n",
    "        pred_evaluation_dataset_train_polynomial_df.to_csv(path_pred_evaluation_dataset_train_polynomial, sep=',', index=False)\n",
    "        pred_evaluation_dataset_valid_polynomial_df.to_csv(path_pred_evaluation_dataset_valid_polynomial, sep=',', index=False)\n",
    "        pred_evaluation_dataset_test_polynomial_df.to_csv(path_pred_evaluation_dataset_test_polynomial, sep=',', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:05:39.976398Z",
     "start_time": "2020-10-07T16:05:39.954787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>[ 0.97 -0.07  0.48  0.99]</th>\n",
       "      <th>[ 0.17  0.43  0.98 -0.34]</th>\n",
       "      <th>[ 0.74 -0.63 -1.    0.86]</th>\n",
       "      <th>[ 0.47 -0.83 -0.17 -0.62]</th>\n",
       "      <th>[ 0.47 -0.76 -0.33  0.35]</th>\n",
       "      <th>[-0.86 -0.47  0.93  0.06]</th>\n",
       "      <th>[ 0.69 -0.97  0.81  0.9 ]</th>\n",
       "      <th>[ 0.13  0.65 -0.8  -0.85]</th>\n",
       "      <th>[-0.26  0.87  0.21  0.04]</th>\n",
       "      <th>[ 0.41 -0.89 -0.68  0.3 ]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.006</td>\n",
       "      <td>-17.558</td>\n",
       "      <td>-4.561</td>\n",
       "      <td>0.662</td>\n",
       "      <td>-12.496</td>\n",
       "      <td>9.220</td>\n",
       "      <td>-65.443</td>\n",
       "      <td>7.035</td>\n",
       "      <td>-2.213</td>\n",
       "      <td>-9.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795</td>\n",
       "      <td>-10.012</td>\n",
       "      <td>2.290</td>\n",
       "      <td>-15.570</td>\n",
       "      <td>4.532</td>\n",
       "      <td>0.417</td>\n",
       "      <td>18.573</td>\n",
       "      <td>-39.182</td>\n",
       "      <td>-21.942</td>\n",
       "      <td>2.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>9.451</td>\n",
       "      <td>-26.598</td>\n",
       "      <td>-12.140</td>\n",
       "      <td>-27.922</td>\n",
       "      <td>-15.232</td>\n",
       "      <td>-34.171</td>\n",
       "      <td>-27.133</td>\n",
       "      <td>-30.753</td>\n",
       "      <td>-9.239</td>\n",
       "      <td>-22.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.541</td>\n",
       "      <td>8.875</td>\n",
       "      <td>-12.981</td>\n",
       "      <td>10.153</td>\n",
       "      <td>-1.975</td>\n",
       "      <td>14.043</td>\n",
       "      <td>5.669</td>\n",
       "      <td>24.789</td>\n",
       "      <td>6.662</td>\n",
       "      <td>-15.405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.886</td>\n",
       "      <td>11.196</td>\n",
       "      <td>11.486</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-1.788</td>\n",
       "      <td>6.771</td>\n",
       "      <td>-27.256</td>\n",
       "      <td>6.467</td>\n",
       "      <td>-7.982</td>\n",
       "      <td>1.365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   [ 0.97 -0.07  0.48  0.99]  [ 0.17  0.43  0.98 -0.34]  \\\n",
       "0                    -15.006                    -17.558   \n",
       "1                      0.795                    -10.012   \n",
       "2                      9.451                    -26.598   \n",
       "3                     21.541                      8.875   \n",
       "4                     -3.886                     11.196   \n",
       "\n",
       "   [ 0.74 -0.63 -1.    0.86]  [ 0.47 -0.83 -0.17 -0.62]  \\\n",
       "0                     -4.561                      0.662   \n",
       "1                      2.290                    -15.570   \n",
       "2                    -12.140                    -27.922   \n",
       "3                    -12.981                     10.153   \n",
       "4                     11.486                     -0.436   \n",
       "\n",
       "   [ 0.47 -0.76 -0.33  0.35]  [-0.86 -0.47  0.93  0.06]  \\\n",
       "0                    -12.496                      9.220   \n",
       "1                      4.532                      0.417   \n",
       "2                    -15.232                    -34.171   \n",
       "3                     -1.975                     14.043   \n",
       "4                     -1.788                      6.771   \n",
       "\n",
       "   [ 0.69 -0.97  0.81  0.9 ]  [ 0.13  0.65 -0.8  -0.85]  \\\n",
       "0                    -65.443                      7.035   \n",
       "1                     18.573                    -39.182   \n",
       "2                    -27.133                    -30.753   \n",
       "3                      5.669                     24.789   \n",
       "4                    -27.256                      6.467   \n",
       "\n",
       "   [-0.26  0.87  0.21  0.04]  [ 0.41 -0.89 -0.68  0.3 ]  \n",
       "0                     -2.213                     -9.881  \n",
       "1                    -21.942                      2.468  \n",
       "2                     -9.239                    -22.558  \n",
       "3                      6.662                    -15.405  \n",
       "4                     -7.982                      1.365  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_evaluation_dataset_test_real_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:05:40.036902Z",
     "start_time": "2020-10-07T16:05:39.981650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>[ 0.97 -0.07  0.48  0.99]</th>\n",
       "      <th>[ 0.17  0.43  0.98 -0.34]</th>\n",
       "      <th>[ 0.74 -0.63 -1.    0.86]</th>\n",
       "      <th>[ 0.47 -0.83 -0.17 -0.62]</th>\n",
       "      <th>[ 0.47 -0.76 -0.33  0.35]</th>\n",
       "      <th>[-0.86 -0.47  0.93  0.06]</th>\n",
       "      <th>[ 0.69 -0.97  0.81  0.9 ]</th>\n",
       "      <th>[ 0.13  0.65 -0.8  -0.85]</th>\n",
       "      <th>[-0.26  0.87  0.21  0.04]</th>\n",
       "      <th>[ 0.41 -0.89 -0.68  0.3 ]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.910</td>\n",
       "      <td>-5.964</td>\n",
       "      <td>-5.910</td>\n",
       "      <td>-5.107</td>\n",
       "      <td>-11.877</td>\n",
       "      <td>-5.914</td>\n",
       "      <td>-35.326</td>\n",
       "      <td>11.273</td>\n",
       "      <td>5.185</td>\n",
       "      <td>-8.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500</td>\n",
       "      <td>-15.578</td>\n",
       "      <td>2.135</td>\n",
       "      <td>-18.568</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-7.569</td>\n",
       "      <td>3.781</td>\n",
       "      <td>-34.049</td>\n",
       "      <td>-16.448</td>\n",
       "      <td>-2.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802</td>\n",
       "      <td>-22.284</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-24.407</td>\n",
       "      <td>-8.572</td>\n",
       "      <td>-29.424</td>\n",
       "      <td>-13.341</td>\n",
       "      <td>-18.193</td>\n",
       "      <td>-9.013</td>\n",
       "      <td>-9.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.124</td>\n",
       "      <td>11.877</td>\n",
       "      <td>-10.001</td>\n",
       "      <td>6.843</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>10.776</td>\n",
       "      <td>9.342</td>\n",
       "      <td>6.808</td>\n",
       "      <td>7.201</td>\n",
       "      <td>-5.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.806</td>\n",
       "      <td>2.794</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>8.224</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.888</td>\n",
       "      <td>-6.882</td>\n",
       "      <td>8.062</td>\n",
       "      <td>-0.905</td>\n",
       "      <td>3.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   [ 0.97 -0.07  0.48  0.99]  [ 0.17  0.43  0.98 -0.34]  \\\n",
       "0                    -19.910                     -5.964   \n",
       "1                      2.500                    -15.578   \n",
       "2                      0.802                    -22.284   \n",
       "3                      7.124                     11.877   \n",
       "4                     -4.806                      2.794   \n",
       "\n",
       "   [ 0.74 -0.63 -1.    0.86]  [ 0.47 -0.83 -0.17 -0.62]  \\\n",
       "0                     -5.910                     -5.107   \n",
       "1                      2.135                    -18.568   \n",
       "2                      0.588                    -24.407   \n",
       "3                    -10.001                      6.843   \n",
       "4                     -0.862                      8.224   \n",
       "\n",
       "   [ 0.47 -0.76 -0.33  0.35]  [-0.86 -0.47  0.93  0.06]  \\\n",
       "0                    -11.877                     -5.914   \n",
       "1                     -0.298                     -7.569   \n",
       "2                     -8.572                    -29.424   \n",
       "3                     -0.418                     10.776   \n",
       "4                      0.361                     -0.888   \n",
       "\n",
       "   [ 0.69 -0.97  0.81  0.9 ]  [ 0.13  0.65 -0.8  -0.85]  \\\n",
       "0                    -35.326                     11.273   \n",
       "1                      3.781                    -34.049   \n",
       "2                    -13.341                    -18.193   \n",
       "3                      9.342                      6.808   \n",
       "4                     -6.882                      8.062   \n",
       "\n",
       "   [-0.26  0.87  0.21  0.04]  [ 0.41 -0.89 -0.68  0.3 ]  \n",
       "0                      5.185                     -8.917  \n",
       "1                    -16.448                     -2.819  \n",
       "2                     -9.013                     -9.164  \n",
       "3                      7.201                     -5.710  \n",
       "4                     -0.905                      3.005  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_evaluation_dataset_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:05:40.056179Z",
     "start_time": "2020-10-07T16:05:40.038762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>0030</th>\n",
       "      <th>...</th>\n",
       "      <th>[ 0.97 -0.07  0.48  0.99]</th>\n",
       "      <th>[ 0.17  0.43  0.98 -0.34]</th>\n",
       "      <th>[ 0.74 -0.63 -1.    0.86]</th>\n",
       "      <th>[ 0.47 -0.83 -0.17 -0.62]</th>\n",
       "      <th>[ 0.47 -0.76 -0.33  0.35]</th>\n",
       "      <th>[-0.86 -0.47  0.93  0.06]</th>\n",
       "      <th>[ 0.69 -0.97  0.81  0.9 ]</th>\n",
       "      <th>[ 0.13  0.65 -0.8  -0.85]</th>\n",
       "      <th>[-0.26  0.87  0.21  0.04]</th>\n",
       "      <th>[ 0.41 -0.89 -0.68  0.3 ]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.954</td>\n",
       "      <td>-5.869</td>\n",
       "      <td>-6.449</td>\n",
       "      <td>-5.628</td>\n",
       "      <td>-10.799</td>\n",
       "      <td>-5.864</td>\n",
       "      <td>-38.147</td>\n",
       "      <td>10.218</td>\n",
       "      <td>5.350</td>\n",
       "      <td>-8.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>9.500</td>\n",
       "      <td>...</td>\n",
       "      <td>2.129</td>\n",
       "      <td>-15.376</td>\n",
       "      <td>2.026</td>\n",
       "      <td>-17.989</td>\n",
       "      <td>-1.284</td>\n",
       "      <td>-8.053</td>\n",
       "      <td>3.436</td>\n",
       "      <td>-33.868</td>\n",
       "      <td>-16.422</td>\n",
       "      <td>-3.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-22.034</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-24.306</td>\n",
       "      <td>-8.665</td>\n",
       "      <td>-30.693</td>\n",
       "      <td>-13.235</td>\n",
       "      <td>-18.194</td>\n",
       "      <td>-9.059</td>\n",
       "      <td>-9.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.353</td>\n",
       "      <td>11.807</td>\n",
       "      <td>-9.368</td>\n",
       "      <td>6.857</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>10.477</td>\n",
       "      <td>9.955</td>\n",
       "      <td>6.923</td>\n",
       "      <td>6.151</td>\n",
       "      <td>-5.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.309</td>\n",
       "      <td>2.869</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>8.066</td>\n",
       "      <td>0.488</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-7.239</td>\n",
       "      <td>7.659</td>\n",
       "      <td>-1.254</td>\n",
       "      <td>2.987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0000   0001   0002   0003   0010   0011   0012   0020   0021   0030  ...  \\\n",
       "0  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500  8.800 -7.400  ...   \n",
       "1 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100 -4.500  9.500  ...   \n",
       "2 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500 -0.300 -2.900  ...   \n",
       "3  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200 -9.200 -2.000  ...   \n",
       "4  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300 -8.800 -7.200  ...   \n",
       "\n",
       "   [ 0.97 -0.07  0.48  0.99]  [ 0.17  0.43  0.98 -0.34]  \\\n",
       "0                    -19.954                     -5.869   \n",
       "1                      2.129                    -15.376   \n",
       "2                      0.749                    -22.034   \n",
       "3                      8.353                     11.807   \n",
       "4                     -4.309                      2.869   \n",
       "\n",
       "   [ 0.74 -0.63 -1.    0.86]  [ 0.47 -0.83 -0.17 -0.62]  \\\n",
       "0                     -6.449                     -5.628   \n",
       "1                      2.026                    -17.989   \n",
       "2                      0.531                    -24.306   \n",
       "3                     -9.368                      6.857   \n",
       "4                     -0.755                      8.066   \n",
       "\n",
       "   [ 0.47 -0.76 -0.33  0.35]  [-0.86 -0.47  0.93  0.06]  \\\n",
       "0                    -10.799                     -5.864   \n",
       "1                     -1.284                     -8.053   \n",
       "2                     -8.665                    -30.693   \n",
       "3                     -0.823                     10.477   \n",
       "4                      0.488                     -0.580   \n",
       "\n",
       "   [ 0.69 -0.97  0.81  0.9 ]  [ 0.13  0.65 -0.8  -0.85]  \\\n",
       "0                    -38.147                     10.218   \n",
       "1                      3.436                    -33.868   \n",
       "2                    -13.235                    -18.194   \n",
       "3                      9.955                      6.923   \n",
       "4                     -7.239                      7.659   \n",
       "\n",
       "   [-0.26  0.87  0.21  0.04]  [ 0.41 -0.89 -0.68  0.3 ]  \n",
       "0                      5.350                     -8.617  \n",
       "1                    -16.422                     -3.265  \n",
       "2                     -9.059                     -9.281  \n",
       "3                      6.151                     -5.514  \n",
       "4                     -1.254                      2.987  \n",
       "\n",
       "[5 rows x 285 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_evaluation_dataset_test_polynomial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:05:40.059584Z",
     "start_time": "2020-10-07T16:05:40.057706Z"
    }
   },
   "outputs": [],
   "source": [
    "#variable_values = []\n",
    "#for column in pred_evaluation_dataset_df.columns:\n",
    "#    variable_values.append(np.array(column[1:-1].split()).astype('float'))\n",
    "#variable_values = np.array(variable_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:05:40.064551Z",
     "start_time": "2020-10-07T16:05:40.061155Z"
    }
   },
   "outputs": [],
   "source": [
    "#fv_with_vv = []\n",
    "#for function_values in tqdm(pred_evaluation_dataset_df.values):\n",
    "#    fv_with_vv.append(np.array([np.append(vv, fv) for fv, vv in zip(function_values, variable_values)]))\n",
    "#fv_with_vv = np.array(fv_with_vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:06:06.311389Z",
     "start_time": "2020-10-07T16:05:40.066088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0c91e195b141798e5787982821deb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:07:22.155926Z",
     "start_time": "2020-10-07T16:06:06.313282Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:07:23.147727Z",
     "start_time": "2020-10-07T16:07:22.160032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.208</td>\n",
       "      <td>10.156</td>\n",
       "      <td>10.104</td>\n",
       "      <td>10.052</td>\n",
       "      <td>9.999</td>\n",
       "      <td>9.946</td>\n",
       "      <td>9.892</td>\n",
       "      <td>9.837</td>\n",
       "      <td>9.780</td>\n",
       "      <td>9.721</td>\n",
       "      <td>...</td>\n",
       "      <td>4.252</td>\n",
       "      <td>4.243</td>\n",
       "      <td>4.234</td>\n",
       "      <td>4.225</td>\n",
       "      <td>4.216</td>\n",
       "      <td>4.206</td>\n",
       "      <td>4.198</td>\n",
       "      <td>4.188</td>\n",
       "      <td>4.179</td>\n",
       "      <td>4.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.347</td>\n",
       "      <td>2.323</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.277</td>\n",
       "      <td>2.253</td>\n",
       "      <td>2.229</td>\n",
       "      <td>2.205</td>\n",
       "      <td>2.180</td>\n",
       "      <td>2.154</td>\n",
       "      <td>2.127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.296</td>\n",
       "      <td>4.291</td>\n",
       "      <td>4.286</td>\n",
       "      <td>4.282</td>\n",
       "      <td>4.277</td>\n",
       "      <td>4.273</td>\n",
       "      <td>4.268</td>\n",
       "      <td>4.261</td>\n",
       "      <td>4.252</td>\n",
       "      <td>4.244</td>\n",
       "      <td>...</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.286</td>\n",
       "      <td>2.281</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.264</td>\n",
       "      <td>2.253</td>\n",
       "      <td>2.245</td>\n",
       "      <td>2.239</td>\n",
       "      <td>2.228</td>\n",
       "      <td>2.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.491</td>\n",
       "      <td>8.455</td>\n",
       "      <td>8.421</td>\n",
       "      <td>8.385</td>\n",
       "      <td>8.349</td>\n",
       "      <td>8.314</td>\n",
       "      <td>8.280</td>\n",
       "      <td>8.243</td>\n",
       "      <td>8.204</td>\n",
       "      <td>8.165</td>\n",
       "      <td>...</td>\n",
       "      <td>3.909</td>\n",
       "      <td>3.900</td>\n",
       "      <td>3.891</td>\n",
       "      <td>3.883</td>\n",
       "      <td>3.874</td>\n",
       "      <td>3.865</td>\n",
       "      <td>3.857</td>\n",
       "      <td>3.849</td>\n",
       "      <td>3.839</td>\n",
       "      <td>3.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.944</td>\n",
       "      <td>9.896</td>\n",
       "      <td>9.850</td>\n",
       "      <td>9.802</td>\n",
       "      <td>9.753</td>\n",
       "      <td>9.706</td>\n",
       "      <td>9.656</td>\n",
       "      <td>9.608</td>\n",
       "      <td>9.557</td>\n",
       "      <td>9.506</td>\n",
       "      <td>...</td>\n",
       "      <td>4.242</td>\n",
       "      <td>4.233</td>\n",
       "      <td>4.224</td>\n",
       "      <td>4.215</td>\n",
       "      <td>4.206</td>\n",
       "      <td>4.196</td>\n",
       "      <td>4.188</td>\n",
       "      <td>4.179</td>\n",
       "      <td>4.170</td>\n",
       "      <td>4.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.626</td>\n",
       "      <td>11.561</td>\n",
       "      <td>11.499</td>\n",
       "      <td>11.433</td>\n",
       "      <td>11.368</td>\n",
       "      <td>11.305</td>\n",
       "      <td>11.242</td>\n",
       "      <td>11.171</td>\n",
       "      <td>11.100</td>\n",
       "      <td>11.031</td>\n",
       "      <td>...</td>\n",
       "      <td>4.590</td>\n",
       "      <td>4.581</td>\n",
       "      <td>4.571</td>\n",
       "      <td>4.561</td>\n",
       "      <td>4.552</td>\n",
       "      <td>4.541</td>\n",
       "      <td>4.532</td>\n",
       "      <td>4.522</td>\n",
       "      <td>4.512</td>\n",
       "      <td>4.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.535</td>\n",
       "      <td>21.366</td>\n",
       "      <td>21.198</td>\n",
       "      <td>21.030</td>\n",
       "      <td>20.859</td>\n",
       "      <td>20.686</td>\n",
       "      <td>20.558</td>\n",
       "      <td>20.427</td>\n",
       "      <td>20.291</td>\n",
       "      <td>20.149</td>\n",
       "      <td>...</td>\n",
       "      <td>6.486</td>\n",
       "      <td>6.481</td>\n",
       "      <td>6.473</td>\n",
       "      <td>6.467</td>\n",
       "      <td>6.461</td>\n",
       "      <td>6.456</td>\n",
       "      <td>6.450</td>\n",
       "      <td>6.443</td>\n",
       "      <td>6.437</td>\n",
       "      <td>6.430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     50000.000     50000.000     50000.000     50000.000     50000.000   \n",
       "mean         10.208        10.156        10.104        10.052         9.999   \n",
       "std           2.347         2.323         2.300         2.277         2.253   \n",
       "min           4.296         4.291         4.286         4.282         4.277   \n",
       "25%           8.491         8.455         8.421         8.385         8.349   \n",
       "50%           9.944         9.896         9.850         9.802         9.753   \n",
       "75%          11.626        11.561        11.499        11.433        11.368   \n",
       "max          21.535        21.366        21.198        21.030        20.859   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     50000.000     50000.000     50000.000     50000.000      50000.000   \n",
       "mean          9.946         9.892         9.837         9.780          9.721   \n",
       "std           2.229         2.205         2.180         2.154          2.127   \n",
       "min           4.273         4.268         4.261         4.252          4.244   \n",
       "25%           8.314         8.280         8.243         8.204          8.165   \n",
       "50%           9.706         9.656         9.608         9.557          9.506   \n",
       "75%          11.305        11.242        11.171        11.100         11.031   \n",
       "max          20.686        20.558        20.427        20.291         20.149   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       50000.000       50000.000       50000.000       50000.000   \n",
       "mean   ...           4.252           4.243           4.234           4.225   \n",
       "std    ...           0.503           0.502           0.501           0.500   \n",
       "min    ...           2.294           2.286           2.281           2.269   \n",
       "25%    ...           3.909           3.900           3.891           3.883   \n",
       "50%    ...           4.242           4.233           4.224           4.215   \n",
       "75%    ...           4.590           4.581           4.571           4.561   \n",
       "max    ...           6.486           6.481           6.473           6.467   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       50000.000       50000.000       50000.000       50000.000   \n",
       "mean            4.216           4.206           4.198           4.188   \n",
       "std             0.499           0.499           0.498           0.497   \n",
       "min             2.264           2.253           2.245           2.239   \n",
       "25%             3.874           3.865           3.857           3.849   \n",
       "50%             4.206           4.196           4.188           4.179   \n",
       "75%             4.552           4.541           4.532           4.522   \n",
       "max             6.461           6.456           6.450           6.443   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       50000.000       50000.000  \n",
       "mean            4.179           4.170  \n",
       "std             0.496           0.495  \n",
       "min             2.228           2.222  \n",
       "25%             3.839           3.831  \n",
       "50%             4.170           4.161  \n",
       "75%             4.512           4.503  \n",
       "max             6.437           6.430  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:07:24.121852Z",
     "start_time": "2020-10-07T16:07:23.149558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.571</td>\n",
       "      <td>10.520</td>\n",
       "      <td>10.470</td>\n",
       "      <td>10.419</td>\n",
       "      <td>10.368</td>\n",
       "      <td>10.316</td>\n",
       "      <td>10.262</td>\n",
       "      <td>10.208</td>\n",
       "      <td>10.152</td>\n",
       "      <td>10.093</td>\n",
       "      <td>...</td>\n",
       "      <td>4.661</td>\n",
       "      <td>4.652</td>\n",
       "      <td>4.643</td>\n",
       "      <td>4.634</td>\n",
       "      <td>4.625</td>\n",
       "      <td>4.616</td>\n",
       "      <td>4.607</td>\n",
       "      <td>4.598</td>\n",
       "      <td>4.589</td>\n",
       "      <td>4.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.420</td>\n",
       "      <td>2.398</td>\n",
       "      <td>2.376</td>\n",
       "      <td>2.354</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.310</td>\n",
       "      <td>2.286</td>\n",
       "      <td>2.262</td>\n",
       "      <td>2.238</td>\n",
       "      <td>2.212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.059</td>\n",
       "      <td>4.055</td>\n",
       "      <td>4.050</td>\n",
       "      <td>4.045</td>\n",
       "      <td>4.041</td>\n",
       "      <td>4.037</td>\n",
       "      <td>4.033</td>\n",
       "      <td>4.029</td>\n",
       "      <td>4.025</td>\n",
       "      <td>4.022</td>\n",
       "      <td>...</td>\n",
       "      <td>2.447</td>\n",
       "      <td>2.441</td>\n",
       "      <td>2.438</td>\n",
       "      <td>2.436</td>\n",
       "      <td>2.429</td>\n",
       "      <td>2.425</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.419</td>\n",
       "      <td>2.416</td>\n",
       "      <td>2.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.790</td>\n",
       "      <td>8.756</td>\n",
       "      <td>8.721</td>\n",
       "      <td>8.687</td>\n",
       "      <td>8.651</td>\n",
       "      <td>8.617</td>\n",
       "      <td>8.578</td>\n",
       "      <td>8.541</td>\n",
       "      <td>8.504</td>\n",
       "      <td>8.465</td>\n",
       "      <td>...</td>\n",
       "      <td>4.253</td>\n",
       "      <td>4.244</td>\n",
       "      <td>4.236</td>\n",
       "      <td>4.228</td>\n",
       "      <td>4.219</td>\n",
       "      <td>4.211</td>\n",
       "      <td>4.202</td>\n",
       "      <td>4.193</td>\n",
       "      <td>4.185</td>\n",
       "      <td>4.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.316</td>\n",
       "      <td>10.269</td>\n",
       "      <td>10.225</td>\n",
       "      <td>10.179</td>\n",
       "      <td>10.133</td>\n",
       "      <td>10.083</td>\n",
       "      <td>10.033</td>\n",
       "      <td>9.982</td>\n",
       "      <td>9.931</td>\n",
       "      <td>9.876</td>\n",
       "      <td>...</td>\n",
       "      <td>4.645</td>\n",
       "      <td>4.636</td>\n",
       "      <td>4.627</td>\n",
       "      <td>4.618</td>\n",
       "      <td>4.610</td>\n",
       "      <td>4.600</td>\n",
       "      <td>4.590</td>\n",
       "      <td>4.581</td>\n",
       "      <td>4.573</td>\n",
       "      <td>4.564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.054</td>\n",
       "      <td>11.989</td>\n",
       "      <td>11.926</td>\n",
       "      <td>11.863</td>\n",
       "      <td>11.797</td>\n",
       "      <td>11.732</td>\n",
       "      <td>11.666</td>\n",
       "      <td>11.596</td>\n",
       "      <td>11.526</td>\n",
       "      <td>11.456</td>\n",
       "      <td>...</td>\n",
       "      <td>5.054</td>\n",
       "      <td>5.044</td>\n",
       "      <td>5.035</td>\n",
       "      <td>5.026</td>\n",
       "      <td>5.016</td>\n",
       "      <td>5.006</td>\n",
       "      <td>4.996</td>\n",
       "      <td>4.987</td>\n",
       "      <td>4.978</td>\n",
       "      <td>4.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.868</td>\n",
       "      <td>21.755</td>\n",
       "      <td>21.642</td>\n",
       "      <td>21.529</td>\n",
       "      <td>21.416</td>\n",
       "      <td>21.301</td>\n",
       "      <td>21.183</td>\n",
       "      <td>21.061</td>\n",
       "      <td>20.935</td>\n",
       "      <td>20.801</td>\n",
       "      <td>...</td>\n",
       "      <td>7.340</td>\n",
       "      <td>7.336</td>\n",
       "      <td>7.325</td>\n",
       "      <td>7.312</td>\n",
       "      <td>7.299</td>\n",
       "      <td>7.283</td>\n",
       "      <td>7.277</td>\n",
       "      <td>7.267</td>\n",
       "      <td>7.251</td>\n",
       "      <td>7.239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         50000.000         50000.000         50000.000         50000.000   \n",
       "mean             10.571            10.520            10.470            10.419   \n",
       "std               2.420             2.398             2.376             2.354   \n",
       "min               4.059             4.055             4.050             4.045   \n",
       "25%               8.790             8.756             8.721             8.687   \n",
       "50%              10.316            10.269            10.225            10.179   \n",
       "75%              12.054            11.989            11.926            11.863   \n",
       "max              21.868            21.755            21.642            21.529   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         50000.000         50000.000         50000.000         50000.000   \n",
       "mean             10.368            10.316            10.262            10.208   \n",
       "std               2.332             2.310             2.286             2.262   \n",
       "min               4.041             4.037             4.033             4.029   \n",
       "25%               8.651             8.617             8.578             8.541   \n",
       "50%              10.133            10.083            10.033             9.982   \n",
       "75%              11.797            11.732            11.666            11.596   \n",
       "max              21.416            21.301            21.183            21.061   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         50000.000          50000.000  ...           50000.000   \n",
       "mean             10.152             10.093  ...               4.661   \n",
       "std               2.238              2.212  ...               0.594   \n",
       "min               4.025              4.022  ...               2.447   \n",
       "25%               8.504              8.465  ...               4.253   \n",
       "50%               9.931              9.876  ...               4.645   \n",
       "75%              11.526             11.456  ...               5.054   \n",
       "max              20.935             20.801  ...               7.340   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           50000.000           50000.000           50000.000   \n",
       "mean                4.652               4.643               4.634   \n",
       "std                 0.594               0.593               0.592   \n",
       "min                 2.441               2.438               2.436   \n",
       "25%                 4.244               4.236               4.228   \n",
       "50%                 4.636               4.627               4.618   \n",
       "75%                 5.044               5.035               5.026   \n",
       "max                 7.336               7.325               7.312   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           50000.000           50000.000           50000.000   \n",
       "mean                4.625               4.616               4.607   \n",
       "std                 0.591               0.590               0.589   \n",
       "min                 2.429               2.425               2.420   \n",
       "25%                 4.219               4.211               4.202   \n",
       "50%                 4.610               4.600               4.590   \n",
       "75%                 5.016               5.006               4.996   \n",
       "max                 7.299               7.283               7.277   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           50000.000           50000.000           50000.000  \n",
       "mean                4.598               4.589               4.580  \n",
       "std                 0.588               0.587               0.586  \n",
       "min                 2.419               2.416               2.412  \n",
       "25%                 4.193               4.185               4.177  \n",
       "50%                 4.581               4.573               4.564  \n",
       "75%                 4.987               4.978               4.968  \n",
       "max                 7.267               7.251               7.239  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:07:25.111374Z",
     "start_time": "2020-10-07T16:07:24.123649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.184</td>\n",
       "      <td>1.174</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.251</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.329</td>\n",
       "      <td>1.375</td>\n",
       "      <td>1.427</td>\n",
       "      <td>1.492</td>\n",
       "      <td>...</td>\n",
       "      <td>3.884</td>\n",
       "      <td>3.879</td>\n",
       "      <td>3.907</td>\n",
       "      <td>3.877</td>\n",
       "      <td>4.011</td>\n",
       "      <td>3.877</td>\n",
       "      <td>3.869</td>\n",
       "      <td>3.898</td>\n",
       "      <td>3.873</td>\n",
       "      <td>4.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.178</td>\n",
       "      <td>7.956</td>\n",
       "      <td>8.336</td>\n",
       "      <td>9.753</td>\n",
       "      <td>12.986</td>\n",
       "      <td>12.354</td>\n",
       "      <td>12.357</td>\n",
       "      <td>13.548</td>\n",
       "      <td>15.559</td>\n",
       "      <td>19.928</td>\n",
       "      <td>...</td>\n",
       "      <td>113.931</td>\n",
       "      <td>115.173</td>\n",
       "      <td>119.209</td>\n",
       "      <td>118.250</td>\n",
       "      <td>149.777</td>\n",
       "      <td>118.038</td>\n",
       "      <td>118.772</td>\n",
       "      <td>123.346</td>\n",
       "      <td>123.166</td>\n",
       "      <td>155.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.957</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.013</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.011</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.186</td>\n",
       "      <td>1.183</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.031</td>\n",
       "      <td>1.023</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.069</td>\n",
       "      <td>...</td>\n",
       "      <td>1.607</td>\n",
       "      <td>1.605</td>\n",
       "      <td>1.600</td>\n",
       "      <td>1.597</td>\n",
       "      <td>1.594</td>\n",
       "      <td>1.593</td>\n",
       "      <td>1.589</td>\n",
       "      <td>1.587</td>\n",
       "      <td>1.583</td>\n",
       "      <td>1.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.070</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.067</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.092</td>\n",
       "      <td>1.110</td>\n",
       "      <td>1.129</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.171</td>\n",
       "      <td>1.195</td>\n",
       "      <td>...</td>\n",
       "      <td>2.301</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.291</td>\n",
       "      <td>2.284</td>\n",
       "      <td>2.279</td>\n",
       "      <td>2.279</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.268</td>\n",
       "      <td>2.262</td>\n",
       "      <td>2.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1618.814</td>\n",
       "      <td>1636.724</td>\n",
       "      <td>1745.504</td>\n",
       "      <td>2031.010</td>\n",
       "      <td>2740.249</td>\n",
       "      <td>2310.348</td>\n",
       "      <td>2319.988</td>\n",
       "      <td>2461.135</td>\n",
       "      <td>2828.894</td>\n",
       "      <td>3839.143</td>\n",
       "      <td>...</td>\n",
       "      <td>24008.457</td>\n",
       "      <td>24418.008</td>\n",
       "      <td>25239.396</td>\n",
       "      <td>25150.988</td>\n",
       "      <td>32476.291</td>\n",
       "      <td>25006.771</td>\n",
       "      <td>25290.840</td>\n",
       "      <td>26244.438</td>\n",
       "      <td>26326.258</td>\n",
       "      <td>33848.141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       50000.000       50000.000       50000.000       50000.000   \n",
       "mean            1.184           1.174           1.176           1.206   \n",
       "std             8.178           7.956           8.336           9.753   \n",
       "min             0.957           0.941           0.924           0.907   \n",
       "25%             1.013           1.005           1.000           0.999   \n",
       "50%             1.031           1.023           1.020           1.022   \n",
       "75%             1.070           1.064           1.067           1.078   \n",
       "max          1618.814        1636.724        1745.504        2031.010   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       50000.000       50000.000       50000.000       50000.000   \n",
       "mean            1.251           1.292           1.329           1.375   \n",
       "std            12.986          12.354          12.357          13.548   \n",
       "min             0.886           0.861           0.839           0.814   \n",
       "25%             0.999           1.000           1.002           1.005   \n",
       "50%             1.026           1.033           1.040           1.049   \n",
       "75%             1.092           1.110           1.129           1.150   \n",
       "max          2740.249        2310.348        2319.988        2461.135   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       50000.000        50000.000  ...         50000.000   \n",
       "mean            1.427            1.492  ...             3.884   \n",
       "std            15.559           19.928  ...           113.931   \n",
       "min             0.790            0.765  ...             0.215   \n",
       "25%             1.007            1.011  ...             1.192   \n",
       "50%             1.058            1.069  ...             1.607   \n",
       "75%             1.171            1.195  ...             2.301   \n",
       "max          2828.894         3839.143  ...         24008.457   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         50000.000         50000.000         50000.000         50000.000   \n",
       "mean              3.879             3.907             3.877             4.011   \n",
       "std             115.173           119.209           118.250           149.777   \n",
       "min               0.215             0.216             0.212             0.212   \n",
       "25%               1.192             1.187             1.186             1.183   \n",
       "50%               1.605             1.600             1.597             1.594   \n",
       "75%               2.294             2.291             2.284             2.279   \n",
       "max           24418.008         25239.396         25150.988         32476.291   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         50000.000         50000.000         50000.000         50000.000   \n",
       "mean              3.877             3.869             3.898             3.873   \n",
       "std             118.038           118.772           123.346           123.166   \n",
       "min               0.213             0.213             0.215             0.210   \n",
       "25%               1.182             1.180             1.178             1.176   \n",
       "50%               1.593             1.589             1.587             1.583   \n",
       "75%               2.279             2.272             2.268             2.262   \n",
       "max           25006.771         25290.840         26244.438         26326.258   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         50000.000  \n",
       "mean              4.011  \n",
       "std             155.654  \n",
       "min               0.210  \n",
       "25%               1.172  \n",
       "50%               1.580  \n",
       "75%               2.257  \n",
       "max           33848.141  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:07:26.085891Z",
     "start_time": "2020-10-07T16:07:25.113263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "      <td>50000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.128</td>\n",
       "      <td>1.126</td>\n",
       "      <td>1.137</td>\n",
       "      <td>1.156</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.235</td>\n",
       "      <td>1.268</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.336</td>\n",
       "      <td>...</td>\n",
       "      <td>3.099</td>\n",
       "      <td>3.097</td>\n",
       "      <td>3.091</td>\n",
       "      <td>3.090</td>\n",
       "      <td>3.080</td>\n",
       "      <td>3.073</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.054</td>\n",
       "      <td>3.049</td>\n",
       "      <td>3.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.557</td>\n",
       "      <td>5.079</td>\n",
       "      <td>4.717</td>\n",
       "      <td>4.349</td>\n",
       "      <td>4.088</td>\n",
       "      <td>3.913</td>\n",
       "      <td>3.864</td>\n",
       "      <td>4.060</td>\n",
       "      <td>4.241</td>\n",
       "      <td>4.531</td>\n",
       "      <td>...</td>\n",
       "      <td>60.267</td>\n",
       "      <td>60.861</td>\n",
       "      <td>60.475</td>\n",
       "      <td>60.994</td>\n",
       "      <td>59.744</td>\n",
       "      <td>59.153</td>\n",
       "      <td>58.377</td>\n",
       "      <td>56.961</td>\n",
       "      <td>56.868</td>\n",
       "      <td>55.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.001</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.988</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.028</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.021</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.014</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.019</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.037</td>\n",
       "      <td>...</td>\n",
       "      <td>1.419</td>\n",
       "      <td>1.417</td>\n",
       "      <td>1.414</td>\n",
       "      <td>1.412</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.406</td>\n",
       "      <td>1.404</td>\n",
       "      <td>1.401</td>\n",
       "      <td>1.399</td>\n",
       "      <td>1.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.048</td>\n",
       "      <td>1.045</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.087</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.139</td>\n",
       "      <td>1.159</td>\n",
       "      <td>...</td>\n",
       "      <td>2.110</td>\n",
       "      <td>2.106</td>\n",
       "      <td>2.103</td>\n",
       "      <td>2.099</td>\n",
       "      <td>2.096</td>\n",
       "      <td>2.094</td>\n",
       "      <td>2.090</td>\n",
       "      <td>2.086</td>\n",
       "      <td>2.083</td>\n",
       "      <td>2.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1205.164</td>\n",
       "      <td>1089.926</td>\n",
       "      <td>990.303</td>\n",
       "      <td>874.117</td>\n",
       "      <td>765.461</td>\n",
       "      <td>654.541</td>\n",
       "      <td>549.000</td>\n",
       "      <td>500.005</td>\n",
       "      <td>417.013</td>\n",
       "      <td>446.463</td>\n",
       "      <td>...</td>\n",
       "      <td>12713.266</td>\n",
       "      <td>12865.476</td>\n",
       "      <td>12774.354</td>\n",
       "      <td>12898.829</td>\n",
       "      <td>12604.125</td>\n",
       "      <td>12469.521</td>\n",
       "      <td>12293.026</td>\n",
       "      <td>11959.801</td>\n",
       "      <td>11941.788</td>\n",
       "      <td>11629.843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           50000.000           50000.000           50000.000   \n",
       "mean                1.128               1.126               1.137   \n",
       "std                 5.557               5.079               4.717   \n",
       "min                 0.923               0.902               0.888   \n",
       "25%                 1.001               0.995               0.992   \n",
       "50%                 1.014               1.008               1.007   \n",
       "75%                 1.048               1.045               1.050   \n",
       "max              1205.164            1089.926             990.303   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           50000.000           50000.000           50000.000   \n",
       "mean                1.156               1.180               1.206   \n",
       "std                 4.349               4.088               3.913   \n",
       "min                 0.872               0.849               0.828   \n",
       "25%                 0.989               0.988               0.988   \n",
       "50%                 1.008               1.010               1.014   \n",
       "75%                 1.060               1.073               1.087   \n",
       "max               874.117             765.461             654.541   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           50000.000           50000.000           50000.000   \n",
       "mean                1.235               1.268               1.301   \n",
       "std                 3.864               4.060               4.241   \n",
       "min                 0.799               0.769               0.736   \n",
       "25%                 0.987               0.987               0.988   \n",
       "50%                 1.019               1.024               1.030   \n",
       "75%                 1.103               1.120               1.139   \n",
       "max               549.000             500.005             417.013   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            50000.000  ...             50000.000             50000.000   \n",
       "mean                 1.336  ...                 3.099                 3.097   \n",
       "std                  4.531  ...                60.267                60.861   \n",
       "min                  0.704  ...                 0.227                 0.227   \n",
       "25%                  0.988  ...                 1.031                 1.030   \n",
       "50%                  1.037  ...                 1.419                 1.417   \n",
       "75%                  1.159  ...                 2.110                 2.106   \n",
       "max                446.463  ...             12713.266             12865.476   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             50000.000             50000.000             50000.000   \n",
       "mean                  3.091                 3.090                 3.080   \n",
       "std                  60.475                60.994                59.744   \n",
       "min                   0.226                 0.225                 0.225   \n",
       "25%                   1.028                 1.026                 1.025   \n",
       "50%                   1.414                 1.412                 1.409   \n",
       "75%                   2.103                 2.099                 2.096   \n",
       "max               12774.354             12898.829             12604.125   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             50000.000             50000.000             50000.000   \n",
       "mean                  3.073                 3.065                 3.054   \n",
       "std                  59.153                58.377                56.961   \n",
       "min                   0.224                 0.224                 0.223   \n",
       "25%                   1.022                 1.021                 1.020   \n",
       "50%                   1.406                 1.404                 1.401   \n",
       "75%                   2.094                 2.090                 2.086   \n",
       "max               12469.521             12293.026             11959.801   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             50000.000             50000.000  \n",
       "mean                  3.049                 3.039  \n",
       "std                  56.868                55.554  \n",
       "min                   0.223                 0.223  \n",
       "25%                   1.018                 1.017  \n",
       "50%                   1.399                 1.397  \n",
       "75%                   2.083                 2.081  \n",
       "max               11941.788             11629.843  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:07:28.409992Z",
     "start_time": "2020-10-07T16:07:26.087695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T16:07:30.626524Z",
     "start_time": "2020-10-07T16:07:28.412139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
