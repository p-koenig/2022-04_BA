{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:49.401456Z",
     "start_time": "2021-01-19T08:20:49.393851Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:49.453857Z",
     "start_time": "2021-01-19T08:20:49.410083Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3  \n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "n_jobs = 1#-3\n",
    "\n",
    "trials = 200\n",
    "\n",
    "data_size = 10000 #for loading lambda models\n",
    "\n",
    "#specify interpretation net structure\n",
    "optimizer = 'adam'\n",
    "dropout = 0\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "interpretation_network_layers = [2048]\n",
    "\n",
    "random_evaluation_dataset_size = 500\n",
    "\n",
    "#lambda net specifications for loading (need to be set according to lambda net training to load correct weights)\n",
    "epochs_lambda = 200\n",
    "batch_lambda = 64\n",
    "lambda_network_layers = [5*sparsity]\n",
    "optimizer_lambda = '_' + 'SGD'\n",
    "\n",
    "\n",
    "lambda_dataset_size = 1000\n",
    "\n",
    "#set if multi_epoch_analysis should be performed\n",
    "multi_epoch_analysis = False\n",
    "each_epochs_save_lambda = 20\n",
    "epoch_start = 0 #use to skip first epochs in multi_epoch_analysis\n",
    "\n",
    "#set if samples analysis should be performed\n",
    "samples_list = None#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "\n",
    "evaluate_with_real_function = True\n",
    "consider_labels_training = False\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 1\n",
    "\n",
    "inet_holdout_seed_evaluation = False\n",
    "seed_in_inet_training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:49.469236Z",
     "start_time": "2021-01-19T08:20:49.456627Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    gpu_numbers = '2'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "structure = '_' + layers_str + str(epochs_lambda) + 'e' + str(batch_lambda) + 'b' + optimizer_lambda\n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure\n",
    "\n",
    "interpretation_network_string = 'drop' + str(dropout) + 'e' + str(epochs) + 'b' + str(batch_size) + '_' + str(interpretation_network_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:53.843294Z",
     "start_time": "2021-01-19T08:20:49.471339Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "import autokeras as ak\n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('./data/plotting/' + interpretation_network_string + filename + '/')\n",
    "    os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.444539Z",
     "start_time": "2021-01-19T08:20:53.845770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.470522Z",
     "start_time": "2021-01-19T08:20:56.448491Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#General Utility Functions\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "\n",
    "def return_float_tensor_representation(some_representation, dtype=tf.float32):\n",
    "    if tf.is_tensor(some_representation):\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "    else:\n",
    "        some_representation = tf.convert_to_tensor(some_representation)\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "        \n",
    "    if not tf.is_tensor(some_representation):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(dtype) + ':' + str(some_representation))\n",
    "     \n",
    "    return some_representation\n",
    "\n",
    "\n",
    "def return_numpy_representation(some_representation):\n",
    "    if isinstance(some_representation, pd.DataFrame):\n",
    "        some_representation = some_representation.values\n",
    "        \n",
    "    if isinstance(some_representation, list):\n",
    "        some_representation = np.array(some_representation)\n",
    "    \n",
    "    if not isinstance(some_representation, np.ndarray):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(np.ndarray) + ':' + str(some_representation))\n",
    "    \n",
    "    return some_representation\n",
    "\n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict\n",
    "\n",
    "def return_callbacks_from_string(callback_string_list):\n",
    "    callbacks = [] if len(callback_string_list) > 0 else None\n",
    "    #if 'plot_losses_callback' in callback_string_list:\n",
    "        #callbacks.append(PlotLossesCallback())\n",
    "    if 'reduce_lr_loss' in callback_string_list:\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=epochs/10, verbose=0, min_delta=0, mode='min') #epsilon\n",
    "        callbacks.append(reduce_lr_loss)\n",
    "    if 'early_stopping' in callback_string_list:\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0, verbose=0, mode='min')\n",
    "        callbacks.append(earlyStopping)\n",
    "        \n",
    "    #if not multi_epoch_analysis and samples_list == None: \n",
    "        #callbacks.append(TQDMNotebookCallback())\n",
    "        \n",
    "    return callbacks\n",
    "\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def generate_random_x_values(size, x_max, x_min, x_step, numnber_of_variables, seed=42):\n",
    "    \n",
    "    if random.seed != None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    x_values_list = []\n",
    "    \n",
    "    for j in range(size):\n",
    "        values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))\n",
    "        while arreq_in_list(values, x_values_list):\n",
    "                values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))         \n",
    "        x_values_list.append(values)\n",
    "    \n",
    "    return np.array(x_values_list)\n",
    "\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.522146Z",
     "start_time": "2021-01-19T08:20:56.472120Z"
    },
    "code_folding": [
     136
    ]
   },
   "outputs": [],
   "source": [
    "class LambdaNetDataset():\n",
    "    lambda_net_list = None\n",
    "    \n",
    "    weight_list = None\n",
    "    \n",
    "    train_settings_list = None\n",
    "    index_list = None\n",
    "    \n",
    "    target_polynomial_list = None\n",
    "    lstsq_lambda_pred_polynomial_list = None\n",
    "    lstsq_target_polynomial_list = None    \n",
    "        \n",
    "    test_data_list = None\n",
    "    \n",
    "    def __init__(self, lambda_net_list):\n",
    "        self.lambda_net_list = lambda_net_list\n",
    "        \n",
    "        self.weight_list = [lambda_net.weights for lambda_net in lambda_net_list]\n",
    "        \n",
    "        self.train_settings_list = {}\n",
    "        for key in lambda_net_list[0].train_settings.keys():\n",
    "            self.train_settings_list[key] = []   \n",
    "        for lambda_net in lambda_net_list:\n",
    "            for key in lambda_net.train_settings.keys():\n",
    "                self.train_settings_list[key].append(lambda_net.train_settings[key])\n",
    "        \n",
    "        self.index_list = [lambda_net.index for lambda_net in lambda_net_list]\n",
    "        \n",
    "        self.target_polynomial_list = [lambda_net.target_polynomial for lambda_net in lambda_net_list]\n",
    "        self.lstsq_lambda_pred_polynomial_list = [lambda_net.lstsq_lambda_pred_polynomial for lambda_net in lambda_net_list]\n",
    "        self.lstsq_target_polynomial_list = [lambda_net.lstsq_target_polynomial for lambda_net in lambda_net_list]\n",
    "      \n",
    "        self.test_data_list = [lambda_net.test_data for lambda_net in lambda_net_list]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.as_pandas().head())\n",
    "    def __str__(self):\n",
    "        return str(self.as_pandas().head())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lambda_net_list)\n",
    "    \n",
    "    \n",
    "    def make_prediction_on_dataset(self, evaluation_dataset):  \n",
    "        assert(evaluation_dataset.shape[1] == n)\n",
    "        lambda_network_preds_list = []\n",
    "        \n",
    "        for weights in self.weight_list:\n",
    "            lambda_network_preds = weights_to_pred(weights, evaluation_dataset)\n",
    "            lambda_network_preds_list.append(lambda_network_preds)\n",
    "        \n",
    "        return np.array(lambda_network_preds_list)\n",
    "    \n",
    "    def make_prediction_on_test_data(self):\n",
    "        lambda_network_preds_list = []\n",
    "        for lambda_net in self.lambda_net_list:\n",
    "            lambda_network_preds = lambda_net.make_prediction_on_test_data()\n",
    "            lambda_network_preds_list.append(lambda_network_preds)\n",
    "            \n",
    "        return np.array(lambda_network_preds_list)\n",
    "                \n",
    "        \n",
    "    def return_target_poly_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape + ' but required (x, ' + str(n) + ')'))     \n",
    "        target_poly_fvs_list = parallel_fv_calculation_from_polynomial(self.target_polynomial_list, [evaluation_dataset for _ in range(len(self.target_polynomial_list))])\n",
    "            \n",
    "        return np.array(target_poly_fvs_list)\n",
    "    \n",
    "    def return_target_poly_fvs_on_test_data(self):        \n",
    "        target_poly_fvs_list = parallel_fv_calculation_from_polynomial(self.target_polynomial_list, self.test_data_list)\n",
    "        \n",
    "        return np.array(target_poly_fvs_list)\n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape + ' but required (x, ' + str(n) + ')'))    \n",
    "        lstsq_lambda_pred_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_lambda_pred_polynomial_list, [evaluation_dataset for _ in range(len(self.target_polynomial_list))])\n",
    "            \n",
    "        return np.array(lstsq_lambda_pred_polynomial_fvs_list)\n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_lambda_pred_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_lambda_pred_polynomial_list, self.test_data_list)\n",
    "            \n",
    "        return np.array(lstsq_lambda_pred_polynomial_fvs_list)\n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape + ' but required (x, ' + str(n) + ')'))\n",
    "        lstsq_target_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_target_polynomial_list, [evaluation_dataset for _ in range(len(self.target_polynomial_list))])\n",
    "            \n",
    "        return np.array(lstsq_target_polynomial_fvs_list)\n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_target_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_target_polynomial_list, self.test_data_list)\n",
    "            \n",
    "        return np.array(lstsq_target_polynomial_fvs_list)\n",
    "    \n",
    "    def as_pandas(self):  \n",
    "        lambda_dataframe = pd.DataFrame(data=[lambda_net.as_array() for lambda_net in self.lambda_net_list], \n",
    "                                columns=self.lambda_net_list[0].return_column_names(), \n",
    "                                index=[lambda_net.index for lambda_net in self.lambda_net_list])\n",
    "        lambda_dataframe['seed'] = lambda_dataframe['seed'].astype(int)\n",
    "        \n",
    "        return lambda_dataframe\n",
    "\n",
    "    \n",
    "    def get_lambda_nets_by_seed(self, seed_list):\n",
    "        lambda_nets_by_seed = []\n",
    "        for lambda_net in self.lambda_net_list:\n",
    "            if lambda_net.train_settings['seed'] in seed_list:\n",
    "                lambda_nets_by_seed.append(lambda_net)\n",
    "    \n",
    "        return LambdaNetDataset(lambda_nets_by_seed)\n",
    "    \n",
    "    def get_lambda_nets_by_lambda_index(self, lambda_index_list):\n",
    "        lambda_nets_by_lambda_index = []\n",
    "        for lambda_net in self.lambda_net_list:\n",
    "            if lambda_net.index in lambda_index_list:\n",
    "                lambda_nets_by_lambda_index.append(lambda_net)\n",
    "    \n",
    "        return LambdaNetDataset(lambda_nets_by_lambda_index) \n",
    "    \n",
    "    def get_lambda_net_by_lambda_index(self, lambda_index):\n",
    "        for lambda_net in self.lambda_net_list:\n",
    "            if lambda_net.index in lambda_index:\n",
    "                return lambda_net\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    def sample(self, size, seed=42):\n",
    "        \n",
    "        assert(isinstance(size, int) or isinstance(size, float), 'Wrong sample size specified')\n",
    "        \n",
    "        random.seed(seed)\n",
    "        \n",
    "        sample_lambda_net_list = None\n",
    "        if isinstance(size, int):\n",
    "            sample_lambda_net_list = random.sample(self.lambda_net_list, size)\n",
    "        elif isinstance(size, float):\n",
    "            size = int(np.round(len(self.lambda_net_list)*size))\n",
    "            sample_lambda_net_list = random.sample(self.lambda_net_list, size)\n",
    "            \n",
    "        return LambdaNetDataset(sample_lambda_net_list)\n",
    "    \n",
    "\n",
    "class LambdaNet():\n",
    "    weights = None\n",
    "    model = None\n",
    "    \n",
    "    train_settings = None\n",
    "    index = None\n",
    "    \n",
    "    target_polynomial = None\n",
    "    lstsq_lambda_pred_polynomial = None\n",
    "    lstsq_target_polynomial = None\n",
    "    \n",
    "    test_data = None\n",
    "    \n",
    "    def __init__(self, line):\n",
    "        assert(isinstance(line, np.ndarray), 'line is no array: ' + str(line))\n",
    "        \n",
    "        self.index = int(line[0])\n",
    "        try:\n",
    "            self.train_settings = {'seed': int(line[1])}\n",
    "        except ValueError:\n",
    "            self.train_settings = {'seed': -1}\n",
    "            \n",
    "        self.target_polynomial = line[range(2, nCr(n+d, d)+2)].astype(float)\n",
    "        self.lstsq_lambda_pred_polynomial = line[range(nCr(n+d, d)+2, nCr(n+d, d)*2+2)].astype(float)\n",
    "        self.lstsq_target_polynomial = line[range(nCr(n+d, d)*2+2, nCr(n+d, d)*3+2)].astype(float)\n",
    "        assert(self.target_polynomial.shape[0] == sparsity, 'target polynomial has incorrect shape ' + str(self.target_polynomial.shape[0]) + ' but should be ' + str(sparsity))\n",
    "        assert(self.lstsq_lambda_pred_polynomial.shape[0] == sparsity, 'lstsq lambda pred polynomial has incorrect shape ' + str(self.lstsq_lambda_pred_polynomial.shape[0]) + ' but should be ' + str(sparsity))\n",
    "        assert(self.lstsq_target_polynomial.shape[0] == sparsity, 'lstsq target polynomial has incorrect shape ' + str(self.lstsq_target_polynomial.shape[0]) + ' but should be ' + str(sparsity))    \n",
    "        \n",
    "        self.weights = line[nCr(n+d, d)*3+2:].astype(float)\n",
    "        assert(self.weights.shape[0] == number_of_lambda_weights, 'weights have incorrect shape ' + str(self.weights.shape[0]) + ' but should be ' + str(number_of_lambda_weights))\n",
    "        \n",
    "        directory = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "        path = directory + 'lambda_' + str(self.index) + '_test_data.npy'\n",
    "        \n",
    "        self.test_data = np.load(path)\n",
    "        assert(self.test_data.shape[1] == n, 'test data has wrong shape ' + str(self.test_data.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.weights)\n",
    "    def __str__(self):\n",
    "        return str(self.weights)\n",
    "        \n",
    "    def make_prediction_on_dataset(self, evaluation_dataset):  \n",
    "        assert(evaluation_dataset.shape[1] == n) \n",
    "        lambda_network_preds = weights_to_pred(self.weights, evaluation_dataset)\n",
    "        \n",
    "        return lambda_network_preds\n",
    "    \n",
    "    def make_prediction_on_test_data(self):        \n",
    "        lambda_network_preds = weights_to_pred(self.weights, self.test_data)\n",
    "        \n",
    "        return lambda_network_preds               \n",
    "        \n",
    "    def return_target_poly_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        target_poly_fvs = parallel_fv_calculation_from_polynomial([self.target_polynomial], [evaluation_dataset])\n",
    "    \n",
    "        return target_poly_fvs\n",
    "    \n",
    "    def return_target_poly_fvs_on_test_data(self):\n",
    "        target_poly_fvs = parallel_fv_calculation_from_polynomial([self.target_polynomial], [self.test_data])\n",
    "    \n",
    "        return target_poly_fvs    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        lstsq_lambda_pred_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_lambda_pred_polynomial], [evaluation_dataset])\n",
    "    \n",
    "        return lstsq_lambda_pred_polynomial_fvs\n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_lambda_pred_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_lambda_pred_polynomial], [self.test_data])\n",
    "    \n",
    "        return lstsq_lambda_pred_polynomial_fvs     \n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        lstsq_target_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_target_polynomial], [evaluation_dataset])\n",
    "    \n",
    "        return lstsq_target_polynomial_fvs\n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_target_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_target_polynomial], [self.test_data])\n",
    "    \n",
    "        return lstsq_target_polynomial_fvs  \n",
    "    \n",
    "    def as_pandas(self): \n",
    "        columns = return_column_names(self)\n",
    "        data = as_array(self)\n",
    "        \n",
    "        df = pd.DataFrame(data=data, columns=columns, index=[self.index])\n",
    "        df['seed'] = df['seed'].astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def as_array(self):\n",
    "        data = np.hstack([self.train_settings['seed'], self.target_polynomial, self.lstsq_lambda_pred_polynomial, self.lstsq_target_polynomial, self.weights])\n",
    "        return data\n",
    "    \n",
    "    def return_column_names(self):\n",
    "        target_polynomial_identifiers = [monomial_identifiers + str('-target') for monomial_identifiers in list_of_monomial_identifiers]\n",
    "        lstsq_lambda_pred_polynomial_identifiers = [monomial_identifiers + str('-lstsq_lambda') for monomial_identifiers in list_of_monomial_identifiers]\n",
    "        lstsq_target_polynomial_identifiers = [monomial_identifiers + str('-lstsq_target') for monomial_identifiers in list_of_monomial_identifiers]\n",
    "\n",
    "        weight_identifiers = ['wb_' + str(i) for i in range(self.weights.shape[0])]\n",
    "        \n",
    "        columns = list(flatten(['seed', target_polynomial_identifiers, lstsq_lambda_pred_polynomial_identifiers, lstsq_target_polynomial_identifiers, weight_identifiers]))\n",
    "                \n",
    "        return columns \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def split_LambdaNetDataset(dataset, test_split, random_seed=RANDOM_SEED):\n",
    "    assert(isinstance(dataset, LambdaNetDataset))\n",
    "    \n",
    "    lambda_nets_list = dataset.lambda_net_list\n",
    "    \n",
    "    if isinstance(test_split, int) or isinstance(test_split, float):\n",
    "        lambda_nets_train_list, lambda_nets_test_list = train_test_split(lambda_nets_list, test_size=test_split, random_state=random_seed)     \n",
    "    elif isinstance(test_split, list):\n",
    "        lambda_nets_test_list = [lambda_nets_list[i] for i in test_split]\n",
    "        lambda_nets_train_list = list(set(lambda_nets_list) - set(lambda_nets_test_list))\n",
    "        #lambda_nets_train_list = lambda_nets_list.copy()\n",
    "        #for i in sorted(test_split, reverse=True):\n",
    "        #    del lambda_nets_train_list[i]           \n",
    "    assert(len(lambda_nets_list) == len(lambda_nets_train_list) + len(lambda_nets_test_list))\n",
    "    \n",
    "    return LambdaNetDataset(lambda_nets_train_list), LambdaNetDataset(lambda_nets_test_list)\n",
    "                                                                                                 \n",
    "def generate_base_model(): #without dropout\n",
    "    base_model = Sequential()\n",
    "\n",
    "    base_model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=n))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        base_model.add(Dense(neurons, activation='relu'))\n",
    "\n",
    "    base_model.add(Dense(1))\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "def shape_flat_weights(flat_weights, target_weights):\n",
    "    \n",
    "    shaped_weights =[]\n",
    "    start = 0\n",
    "    for el in target_weights:\n",
    "        target_shape = el.shape\n",
    "        size = len(list(flatten(el)))\n",
    "        shaped_el = np.reshape(flat_weights[start:start+size], target_shape)\n",
    "        shaped_weights.append(shaped_el)\n",
    "        start += size\n",
    "\n",
    "    return shaped_weights\n",
    "\n",
    "def weights_to_pred(weights, x, base_model=None):\n",
    "\n",
    "    if base_model is None:\n",
    "        base_model = generate_base_model()\n",
    "    else:\n",
    "        base_model = keras.models.clone_model(base_model)\n",
    "    \n",
    "    # Shape weights (flat) into correct model structure\n",
    "    shaped_weights = shape_flat_weights(weights, base_model.get_weights())\n",
    "    \n",
    "    # Make prediction\n",
    "    base_model.set_weights(shaped_weights)\n",
    "    y = base_model.predict(x).ravel()\n",
    "    return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.575584Z",
     "start_time": "2021-01-19T08:20:56.524626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1741b6f938bb4eac83ba58635cf0ff4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555e3de13a3148c581cc2f63a9bcdbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n",
    "\n",
    "\n",
    "layers_with_input_output = list(flatten([[n], lambda_network_layers, [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.599446Z",
     "start_time": "2021-01-19T08:20:56.578754Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with lambda-net prediction based (predictions made in loss function)\n",
    "\n",
    "\n",
    "def mean_absolute_error_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers, base_model):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)    \n",
    "    \n",
    "    model_lambda_placeholder = keras.models.clone_model(base_model)  \n",
    "    \n",
    "    weights_structure = base_model.get_weights()\n",
    "    dims = [np_arrays.shape for np_arrays in weights_structure]\n",
    "    \n",
    "    def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        if seed_in_inet_training:\n",
    "            network_parameters = polynomial_true_with_lambda_fv[:,sparsity+1:]\n",
    "            polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "        else:\n",
    "            network_parameters = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "            polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "            \n",
    "        network_parameters = return_float_tensor_representation(network_parameters)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        \n",
    "        assert(polynomial_true.shape[1] == sparsity)\n",
    "        assert(polynomial_pred.shape[1] == sparsity)   \n",
    "        assert(network_parameters.shape[1] == number_of_lambda_weights)   \n",
    "        \n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder), (polynomial_pred, network_parameters), fn_output_signature=tf.float32))\n",
    "    return mean_absolute_error_tf_fv_lambda_extended\n",
    "\n",
    "def calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder):\n",
    "\n",
    "    def calculate_mae_fv_lambda(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        #polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[0]\n",
    "        network_parameters = input_list[1]\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        #CALCULATE LAMBDA FV HERE FOR EVALUATION DATASET\n",
    "        # build models\n",
    "        start = 0\n",
    "        layers = []\n",
    "        for i in range(len(dims)//2):\n",
    "            \n",
    "            # set weights of layer\n",
    "            index = i*2\n",
    "            size = np.product(dims[index])\n",
    "            weights_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[0].assign(weights_tf_true)\n",
    "            start += size\n",
    "            \n",
    "            # set biases of layer\n",
    "            index += 1\n",
    "            size = np.product(dims[index])\n",
    "            biases_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[1].assign(biases_tf_true)\n",
    "            start += size\n",
    "\n",
    "        \n",
    "        lambda_fv = tf.keras.backend.flatten(model_lambda_placeholder(evaluation_dataset))\n",
    "        \n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_lambda\n",
    "\n",
    "\n",
    "\n",
    "#Manual TF Loss function for fv comparison of real and predicted polynomial\n",
    "\n",
    "def mean_absolute_error_tf_fv_poly_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)        \n",
    "    \n",
    "    @tf.function()\n",
    "    def mean_absolute_error_tf_fv_poly_extended(polynomial_true, polynomial_pred):\n",
    "\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        \n",
    "        assert(polynomial_true.shape[1] == sparsity, 'Shape of True Polynomial: ' + str(polynomial_true.shape))\n",
    "        assert(polynomial_pred.shape[1] == sparsity, 'Shape of True Polynomial: ' + str(polynomial_pred.shape))       \n",
    "        \n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_mae_fv_poly_wrapper(evaluation_dataset, list_of_monomial_identifiers), (polynomial_true, polynomial_pred), fn_output_signature=tf.float32))\n",
    "    return mean_absolute_error_tf_fv_poly_extended\n",
    "\n",
    "def calculate_mae_fv_poly_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "\n",
    "    def calculate_mae_fv_poly(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[1]\n",
    "        \n",
    "        polynomial_true_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_true), (evaluation_dataset))\n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "        \n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (polynomial_true_fv_list, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_poly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#GENERAL LOSS UTILITY FUNCTIONS\n",
    "def calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred):\n",
    "\n",
    "\n",
    "    def calculate_fv_from_data(evaluation_entry):\n",
    "\n",
    "\n",
    "        value_without_coefficient = tf.vectorized_map(calculate_value_without_coefficient_wrapper(evaluation_entry), (list_of_monomial_identifiers))\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        \n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)     \n",
    "        \n",
    "        return polynomial_pred_fv\n",
    "    return calculate_fv_from_data\n",
    "\n",
    "\n",
    "#calculate intermediate term (without coefficient multiplication)\n",
    "def calculate_value_without_coefficient_wrapper(evaluation_entry):\n",
    "    def calculate_value_without_coefficient(coefficient_multiplier_term):      \n",
    "   \n",
    "        return tf.math.reduce_prod(tf.vectorized_map(lambda x: x[0]**x[1], (evaluation_entry, coefficient_multiplier_term)))\n",
    "    return calculate_value_without_coefficient\n",
    "\n",
    "#calculate MAE at the end ---> general:REPLACE FUNCTION WITH LOSS CALL OR LAMBDA\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BASIC COEFFICIENT-BASED LOSS IF X_DATA IS APPENDED\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    \n",
    "    if seed_in_inet_training:\n",
    "        assert(polynomial_true_with_lambda_fv.shape[1] == sparsity+number_of_lambda_weights+1)\n",
    "    else:\n",
    "        assert(polynomial_true_with_lambda_fv.shape[1] == sparsity+number_of_lambda_weights)\n",
    "    \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]    \n",
    "    \n",
    "    assert(polynomial_true.shape[1] == sparsity)\n",
    "    assert(polynomial_pred.shape[1] == sparsity)\n",
    "    \n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.622228Z",
     "start_time": "2021-01-19T08:20:56.600802Z"
    }
   },
   "outputs": [],
   "source": [
    "def r2_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers, base_model):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)    \n",
    "    \n",
    "    model_lambda_placeholder = keras.models.clone_model(base_model)  \n",
    "    \n",
    "    weights_structure = base_model.get_weights()\n",
    "    dims = [np_arrays.shape for np_arrays in weights_structure]\n",
    "    \n",
    "    def r2_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        if seed_in_inet_training:\n",
    "            network_parameters = polynomial_true_with_lambda_fv[:,sparsity+1:]\n",
    "            polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "        else:\n",
    "            network_parameters = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "            polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "            \n",
    "        network_parameters = return_float_tensor_representation(network_parameters)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        \n",
    "        assert(polynomial_true.shape[1] == sparsity)\n",
    "        assert(polynomial_pred.shape[1] == sparsity)   \n",
    "        assert(network_parameters.shape[1] == number_of_lambda_weights)   \n",
    "        \n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_r2_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder), (polynomial_pred, network_parameters), fn_output_signature=tf.float32))\n",
    "    return r2_tf_fv_lambda_extended\n",
    "\n",
    "def calculate_r2_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder):\n",
    "\n",
    "    def calculate_r2_fv_lambda(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        #polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[0]\n",
    "        network_parameters = input_list[1]\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        #CALCULATE LAMBDA FV HERE FOR EVALUATION DATASET\n",
    "        # build models\n",
    "        start = 0\n",
    "        layers = []\n",
    "        for i in range(len(dims)//2):\n",
    "            \n",
    "            # set weights of layer\n",
    "            index = i*2\n",
    "            size = np.product(dims[index])\n",
    "            weights_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[0].assign(weights_tf_true)\n",
    "            start += size\n",
    "            \n",
    "            # set biases of layer\n",
    "            index += 1\n",
    "            size = np.product(dims[index])\n",
    "            biases_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[1].assign(biases_tf_true)\n",
    "            start += size\n",
    "\n",
    "        \n",
    "        lambda_fv = tf.keras.backend.flatten(model_lambda_placeholder(evaluation_dataset))\n",
    "        \n",
    "        return r2_keras_loss(lambda_fv, polynomial_pred_fv_list)\n",
    "    \n",
    "    return calculate_r2_fv_lambda\n",
    "\n",
    "\n",
    "\n",
    "#Manual TF Loss function for fv comparison of real and predicted polynomial\n",
    "\n",
    "def r2_tf_fv_poly_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)        \n",
    "    \n",
    "    @tf.function()\n",
    "    def r2_tf_fv_poly_extended(polynomial_true, polynomial_pred):\n",
    "\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        \n",
    "        assert(polynomial_true.shape[1] == sparsity, 'Shape of True Polynomial: ' + str(polynomial_true.shape))\n",
    "        assert(polynomial_pred.shape[1] == sparsity, 'Shape of True Polynomial: ' + str(polynomial_pred.shape))       \n",
    "        \n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_r2_fv_poly_wrapper(evaluation_dataset, list_of_monomial_identifiers), (polynomial_true, polynomial_pred), fn_output_signature=tf.float32))\n",
    "    return r2_tf_fv_poly_extended\n",
    "\n",
    "def calculate_r2_fv_poly_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "\n",
    "    def calculate_r2_fv_poly(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[1]\n",
    "        \n",
    "        polynomial_true_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_true), (evaluation_dataset))\n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "        \n",
    "        return r2_keras_loss(polynomial_true_fv_list, polynomial_pred_fv_list)\n",
    "    \n",
    "    return calculate_r2_fv_poly\n",
    "\n",
    "\n",
    "\n",
    "#calculate MAE at the end ---> general:REPLACE FUNCTION WITH LOSS CALL OR LAMBDA\n",
    "def calculate_r2_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return r2_keras(true_fv, pred_fv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BASIC COEFFICIENT-BASED LOSS IF X_DATA IS APPENDED\n",
    "def r2_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    \n",
    "    if seed_in_inet_training:\n",
    "        assert(polynomial_true_with_lambda_fv.shape[1] == sparsity+number_of_lambda_weights+1)\n",
    "    else:\n",
    "        assert(polynomial_true_with_lambda_fv.shape[1] == sparsity+number_of_lambda_weights)\n",
    "    \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]    \n",
    "    \n",
    "    assert(polynomial_true.shape[1] == sparsity)\n",
    "    assert(polynomial_pred.shape[1] == sparsity)\n",
    "    \n",
    "    return r2_keras(polynomial_true, polynomial_pred)\n",
    "\n",
    "def r2_keras(y_true, y_pred, epsilon=K.epsilon()):\n",
    "    \n",
    "    y_true_cleared = tf.boolean_mask(y_true, tf.not_equal(return_float_tensor_representation(0), y_true))\n",
    "    y_pred_cleared = tf.boolean_mask(y_pred, tf.not_equal(return_float_tensor_representation(0), y_true))\n",
    "\n",
    "    epsilon = 1e-5\n",
    "    SS_res =  K.sum(K.square(y_true_cleared - y_pred_cleared)) \n",
    "    SS_tot = K.sum(K.square(y_true_cleared - K.mean(y_true_cleared))) \n",
    "    return ( 1 - SS_res/(SS_tot + epsilon) )\n",
    "\n",
    "def r2_keras_loss(y_true, y_pred, epsilon=K.epsilon()):\n",
    "    \n",
    "    #y_true = tf.boolean_mask(y_true, tf.not_equal(return_float_tensor_representation(0), y_true))\n",
    "    #y_pred = tf.boolean_mask(y_pred, tf.not_equal(return_float_tensor_representation(0), y_true))\n",
    "\n",
    "    #epsilon = 1e-5\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return  - ( 1 - SS_res/(SS_tot + epsilon) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.634491Z",
     "start_time": "2021-01-19T08:20:56.623690Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Basic Keras/TF Loss functions\n",
    "def root_mean_squared_error(y_true, y_pred):   \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "        \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)           \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))      \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)        \n",
    "    epsilon = return_float_tensor_representation(epsilon)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.645886Z",
     "start_time": "2021-01-19T08:20:56.635676Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual calculations for comparison of polynomials based on function values (no TF!)\n",
    "\n",
    "def calcualate_function_value(coefficient_list, lambda_input_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0   \n",
    "        \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        value_without_coefficient = [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multipliers, lambda_input_entry)]\n",
    "\n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, value_without_coefficient)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_function_values_from_polynomial(polynomial, lambda_input_data):        \n",
    "    function_value_list = []\n",
    "        \n",
    "    for lambda_input_entry in lambda_input_data:\n",
    "        function_value = calcualate_function_value(polynomial, lambda_input_entry)\n",
    "        function_value_list.append(function_value)\n",
    "\n",
    "    return np.array(function_value_list)\n",
    "\n",
    "\n",
    "def parallel_fv_calculation_from_polynomial(polynomial_list, lambda_input_list):\n",
    "    \n",
    "    polynomial_list = return_numpy_representation(polynomial_list)\n",
    "    lambda_input_list = return_numpy_representation(lambda_input_list)\n",
    "    \n",
    "    assert(polynomial_list.shape[0] == lambda_input_list.shape[0])\n",
    "    assert(polynomial_list.shape[1] == sparsity)\n",
    "    assert(lambda_input_list.shape[2] == n)\n",
    "    \n",
    "    n_jobs_parallel_fv = 10 if polynomial_list.shape[0] > 10 else polynomial_list.shape[0]\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs_parallel_fv, verbose=0, backend='threading')\n",
    "    polynomial_true_fv = parallel(delayed(calculate_function_values_from_polynomial)(polynomial, lambda_inputs) for polynomial, lambda_inputs in zip(polynomial_list, lambda_input_list))  \n",
    "    del parallel   \n",
    "    \n",
    "\n",
    "    return np.array(polynomial_true_fv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.666465Z",
     "start_time": "2021-01-19T08:20:56.647292Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Standard Metrics (no TF!)\n",
    "\n",
    "def mean_absolute_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)      \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(true_values-pred_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))  \n",
    "\n",
    "def root_mean_squared_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)         \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sqrt(np.mean((true_values-pred_values)**2)))\n",
    "    \n",
    "    return np.mean(np.array(result_list)) \n",
    "\n",
    "def mean_absolute_percentage_error_function_values(y_true, y_pred, epsilon=10e-3):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred) \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(((true_values-pred_values)/(true_values+epsilon)))))\n",
    "\n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def r2_score_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(r2_score(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_absolute_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sum(np.abs(true_values-pred_values))/(true_values.shape[0]*np.std(true_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_maximum_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.max(true_values-pred_values)/np.std(true_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_area_between_two_curves_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "      \n",
    "    assert(number_of_variables==1)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(area_between_two_curves(true_values, pred_values))\n",
    " \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_dtw_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "\n",
    "    result_list_single = []\n",
    "    result_list_array = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_single_value, result_single_array = dtw(true_values, pred_values)\n",
    "        result_list_single.append(result_single_value)\n",
    "        result_list_array.append(result_single_array)\n",
    "    \n",
    "    return np.mean(np.array(result_list_single)), np.mean(np.array(result_list_array), axis=1)\n",
    "\n",
    "def mean_frechet_dist_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(frechet_dist(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.679941Z",
     "start_time": "2021-01-19T08:20:56.667847Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_interpretation_net(y_data_real, \n",
    "                                y_data_pred, \n",
    "                                polynomial_true_fv, \n",
    "                                polynomial_pred_inet_fv):\n",
    "    \n",
    "    if type(y_data_real) != type(None) and type(y_data_pred) != type(None):\n",
    "        y_data_real = return_numpy_representation(y_data_real)\n",
    "        y_data_pred = return_numpy_representation(y_data_pred)     \n",
    "        \n",
    "        assert(y_data_real.shape[1] == sparsity)\n",
    "        assert(y_data_pred.shape[1] == sparsity)\n",
    "        \n",
    "        mae_coeff = np.round(mean_absolute_error(y_data_real, y_data_pred), 4)\n",
    "        rmse_coeff = np.round(root_mean_squared_error(y_data_real, y_data_pred), 4)\n",
    "        mape_coeff = np.round(mean_absolute_percentage_error_keras(y_data_real, y_data_pred), 4)\n",
    "        accuracy_coeff = np.round(accuracy_single(y_data_real, y_data_pred), 4)\n",
    "        accuracy_multi_coeff = np.round(accuracy_multilabel(y_data_real, y_data_pred), 4)\n",
    "    else:\n",
    "        mae_coeff = np.nan\n",
    "        rmse_coeff = np.nan\n",
    "        mape_coeff = np.nan\n",
    "        accuracy_coeff = np.nan\n",
    "        accuracy_multi_coeff = np.nan\n",
    "        \n",
    "    polynomial_true_fv = return_numpy_representation(polynomial_true_fv)\n",
    "    polynomial_pred_inet_fv = return_numpy_representation(polynomial_pred_inet_fv)\n",
    "    \n",
    "    mae_fv = np.round(mean_absolute_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmse_fv = np.round(root_mean_squared_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    mape_fv = np.round(mean_absolute_percentage_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    r2_fv = np.round(r2_score_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    raae_fv = np.round(relative_absolute_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmae_fv = np.round(relative_maximum_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4) \n",
    "\n",
    "    std_fv = np.std(mae_fv)\n",
    "    mean_fv = np.mean(mae_fv)\n",
    "\n",
    "    return pd.Series(data=[mae_coeff,\n",
    "                          rmse_coeff,\n",
    "                          mape_coeff,\n",
    "                          accuracy_coeff,\n",
    "                          accuracy_multi_coeff,\n",
    "                          \n",
    "                          mae_fv,\n",
    "                          rmse_fv,\n",
    "                          mape_fv,\n",
    "                          r2_fv,\n",
    "                          raae_fv,\n",
    "                          rmae_fv,\n",
    "                          \n",
    "                          std_fv,\n",
    "                          mean_fv],\n",
    "                     index=['MAE',\n",
    "                           'RMSE',\n",
    "                           'MAPE',\n",
    "                           'Accuracy',\n",
    "                           'Accuracy Multilabel',\n",
    "                           \n",
    "                           'MAE FV',\n",
    "                           'RMSE FV',\n",
    "                           'MAPE FV',\n",
    "                           'R2 FV',\n",
    "                           'RAAE FV',\n",
    "                           'RMAE FV',\n",
    "                            \n",
    "                           'STD FV ERROR',\n",
    "                           'MEAN FV ERROR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:20:56.690645Z",
     "start_time": "2021-01-19T08:20:56.681300Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3) + filename + '.txt'\n",
    "\n",
    "    \n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "    \n",
    "    lambda_nets = []\n",
    "    for _, row in weight_data.iterrows():\n",
    "        lambda_net = LambdaNet(row.values)\n",
    "        lambda_nets.append(lambda_net)\n",
    "      \n",
    "    if data_size < len(lambda_nets):\n",
    "        random.seed(RANDOM_SEED)\n",
    "        lambda_nets = random.sample(lambda_nets, data_size)\n",
    "    \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:22:35.154037Z",
     "start_time": "2021-01-19T08:20:56.691959Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.7s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "\n",
    "parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:22:38.830754Z",
     "start_time": "2021-01-19T08:22:35.156036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0000-target</th>\n",
       "      <th>0001-target</th>\n",
       "      <th>0002-target</th>\n",
       "      <th>0003-target</th>\n",
       "      <th>0010-target</th>\n",
       "      <th>0011-target</th>\n",
       "      <th>0012-target</th>\n",
       "      <th>0020-target</th>\n",
       "      <th>0021-target</th>\n",
       "      <th>0030-target</th>\n",
       "      <th>0100-target</th>\n",
       "      <th>0101-target</th>\n",
       "      <th>0102-target</th>\n",
       "      <th>0110-target</th>\n",
       "      <th>0111-target</th>\n",
       "      <th>0120-target</th>\n",
       "      <th>0200-target</th>\n",
       "      <th>0201-target</th>\n",
       "      <th>0210-target</th>\n",
       "      <th>0300-target</th>\n",
       "      <th>1000-target</th>\n",
       "      <th>1001-target</th>\n",
       "      <th>1002-target</th>\n",
       "      <th>1010-target</th>\n",
       "      <th>1011-target</th>\n",
       "      <th>1020-target</th>\n",
       "      <th>1100-target</th>\n",
       "      <th>1101-target</th>\n",
       "      <th>1110-target</th>\n",
       "      <th>1200-target</th>\n",
       "      <th>2000-target</th>\n",
       "      <th>2001-target</th>\n",
       "      <th>2010-target</th>\n",
       "      <th>2100-target</th>\n",
       "      <th>3000-target</th>\n",
       "      <th>0000-lstsq_lambda</th>\n",
       "      <th>0001-lstsq_lambda</th>\n",
       "      <th>0002-lstsq_lambda</th>\n",
       "      <th>0003-lstsq_lambda</th>\n",
       "      <th>0010-lstsq_lambda</th>\n",
       "      <th>0011-lstsq_lambda</th>\n",
       "      <th>0012-lstsq_lambda</th>\n",
       "      <th>0020-lstsq_lambda</th>\n",
       "      <th>0021-lstsq_lambda</th>\n",
       "      <th>0030-lstsq_lambda</th>\n",
       "      <th>0100-lstsq_lambda</th>\n",
       "      <th>0101-lstsq_lambda</th>\n",
       "      <th>0102-lstsq_lambda</th>\n",
       "      <th>0110-lstsq_lambda</th>\n",
       "      <th>0111-lstsq_lambda</th>\n",
       "      <th>0120-lstsq_lambda</th>\n",
       "      <th>0200-lstsq_lambda</th>\n",
       "      <th>0201-lstsq_lambda</th>\n",
       "      <th>0210-lstsq_lambda</th>\n",
       "      <th>0300-lstsq_lambda</th>\n",
       "      <th>1000-lstsq_lambda</th>\n",
       "      <th>1001-lstsq_lambda</th>\n",
       "      <th>1002-lstsq_lambda</th>\n",
       "      <th>1010-lstsq_lambda</th>\n",
       "      <th>1011-lstsq_lambda</th>\n",
       "      <th>1020-lstsq_lambda</th>\n",
       "      <th>1100-lstsq_lambda</th>\n",
       "      <th>1101-lstsq_lambda</th>\n",
       "      <th>1110-lstsq_lambda</th>\n",
       "      <th>1200-lstsq_lambda</th>\n",
       "      <th>2000-lstsq_lambda</th>\n",
       "      <th>2001-lstsq_lambda</th>\n",
       "      <th>2010-lstsq_lambda</th>\n",
       "      <th>2100-lstsq_lambda</th>\n",
       "      <th>3000-lstsq_lambda</th>\n",
       "      <th>0000-lstsq_target</th>\n",
       "      <th>0001-lstsq_target</th>\n",
       "      <th>0002-lstsq_target</th>\n",
       "      <th>0003-lstsq_target</th>\n",
       "      <th>0010-lstsq_target</th>\n",
       "      <th>0011-lstsq_target</th>\n",
       "      <th>0012-lstsq_target</th>\n",
       "      <th>0020-lstsq_target</th>\n",
       "      <th>0021-lstsq_target</th>\n",
       "      <th>0030-lstsq_target</th>\n",
       "      <th>0100-lstsq_target</th>\n",
       "      <th>0101-lstsq_target</th>\n",
       "      <th>0102-lstsq_target</th>\n",
       "      <th>0110-lstsq_target</th>\n",
       "      <th>0111-lstsq_target</th>\n",
       "      <th>0120-lstsq_target</th>\n",
       "      <th>0200-lstsq_target</th>\n",
       "      <th>0201-lstsq_target</th>\n",
       "      <th>0210-lstsq_target</th>\n",
       "      <th>0300-lstsq_target</th>\n",
       "      <th>1000-lstsq_target</th>\n",
       "      <th>1001-lstsq_target</th>\n",
       "      <th>1002-lstsq_target</th>\n",
       "      <th>1010-lstsq_target</th>\n",
       "      <th>1011-lstsq_target</th>\n",
       "      <th>1020-lstsq_target</th>\n",
       "      <th>1100-lstsq_target</th>\n",
       "      <th>1101-lstsq_target</th>\n",
       "      <th>1110-lstsq_target</th>\n",
       "      <th>1200-lstsq_target</th>\n",
       "      <th>2000-lstsq_target</th>\n",
       "      <th>2001-lstsq_target</th>\n",
       "      <th>2010-lstsq_target</th>\n",
       "      <th>2100-lstsq_target</th>\n",
       "      <th>3000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "      <th>wb_176</th>\n",
       "      <th>wb_177</th>\n",
       "      <th>wb_178</th>\n",
       "      <th>wb_179</th>\n",
       "      <th>wb_180</th>\n",
       "      <th>wb_181</th>\n",
       "      <th>wb_182</th>\n",
       "      <th>wb_183</th>\n",
       "      <th>wb_184</th>\n",
       "      <th>wb_185</th>\n",
       "      <th>wb_186</th>\n",
       "      <th>wb_187</th>\n",
       "      <th>wb_188</th>\n",
       "      <th>wb_189</th>\n",
       "      <th>wb_190</th>\n",
       "      <th>wb_191</th>\n",
       "      <th>wb_192</th>\n",
       "      <th>wb_193</th>\n",
       "      <th>wb_194</th>\n",
       "      <th>wb_195</th>\n",
       "      <th>wb_196</th>\n",
       "      <th>wb_197</th>\n",
       "      <th>wb_198</th>\n",
       "      <th>wb_199</th>\n",
       "      <th>wb_200</th>\n",
       "      <th>wb_201</th>\n",
       "      <th>wb_202</th>\n",
       "      <th>wb_203</th>\n",
       "      <th>wb_204</th>\n",
       "      <th>wb_205</th>\n",
       "      <th>wb_206</th>\n",
       "      <th>wb_207</th>\n",
       "      <th>wb_208</th>\n",
       "      <th>wb_209</th>\n",
       "      <th>wb_210</th>\n",
       "      <th>wb_211</th>\n",
       "      <th>wb_212</th>\n",
       "      <th>wb_213</th>\n",
       "      <th>wb_214</th>\n",
       "      <th>wb_215</th>\n",
       "      <th>wb_216</th>\n",
       "      <th>wb_217</th>\n",
       "      <th>wb_218</th>\n",
       "      <th>wb_219</th>\n",
       "      <th>wb_220</th>\n",
       "      <th>wb_221</th>\n",
       "      <th>wb_222</th>\n",
       "      <th>wb_223</th>\n",
       "      <th>wb_224</th>\n",
       "      <th>wb_225</th>\n",
       "      <th>wb_226</th>\n",
       "      <th>wb_227</th>\n",
       "      <th>wb_228</th>\n",
       "      <th>wb_229</th>\n",
       "      <th>wb_230</th>\n",
       "      <th>wb_231</th>\n",
       "      <th>wb_232</th>\n",
       "      <th>wb_233</th>\n",
       "      <th>wb_234</th>\n",
       "      <th>wb_235</th>\n",
       "      <th>wb_236</th>\n",
       "      <th>wb_237</th>\n",
       "      <th>wb_238</th>\n",
       "      <th>wb_239</th>\n",
       "      <th>wb_240</th>\n",
       "      <th>wb_241</th>\n",
       "      <th>wb_242</th>\n",
       "      <th>wb_243</th>\n",
       "      <th>wb_244</th>\n",
       "      <th>wb_245</th>\n",
       "      <th>wb_246</th>\n",
       "      <th>wb_247</th>\n",
       "      <th>wb_248</th>\n",
       "      <th>wb_249</th>\n",
       "      <th>wb_250</th>\n",
       "      <th>wb_251</th>\n",
       "      <th>wb_252</th>\n",
       "      <th>wb_253</th>\n",
       "      <th>wb_254</th>\n",
       "      <th>wb_255</th>\n",
       "      <th>wb_256</th>\n",
       "      <th>wb_257</th>\n",
       "      <th>wb_258</th>\n",
       "      <th>wb_259</th>\n",
       "      <th>wb_260</th>\n",
       "      <th>wb_261</th>\n",
       "      <th>wb_262</th>\n",
       "      <th>wb_263</th>\n",
       "      <th>wb_264</th>\n",
       "      <th>wb_265</th>\n",
       "      <th>wb_266</th>\n",
       "      <th>wb_267</th>\n",
       "      <th>wb_268</th>\n",
       "      <th>wb_269</th>\n",
       "      <th>wb_270</th>\n",
       "      <th>wb_271</th>\n",
       "      <th>wb_272</th>\n",
       "      <th>wb_273</th>\n",
       "      <th>wb_274</th>\n",
       "      <th>wb_275</th>\n",
       "      <th>wb_276</th>\n",
       "      <th>wb_277</th>\n",
       "      <th>wb_278</th>\n",
       "      <th>wb_279</th>\n",
       "      <th>wb_280</th>\n",
       "      <th>wb_281</th>\n",
       "      <th>wb_282</th>\n",
       "      <th>wb_283</th>\n",
       "      <th>wb_284</th>\n",
       "      <th>wb_285</th>\n",
       "      <th>wb_286</th>\n",
       "      <th>wb_287</th>\n",
       "      <th>wb_288</th>\n",
       "      <th>wb_289</th>\n",
       "      <th>wb_290</th>\n",
       "      <th>wb_291</th>\n",
       "      <th>wb_292</th>\n",
       "      <th>wb_293</th>\n",
       "      <th>wb_294</th>\n",
       "      <th>wb_295</th>\n",
       "      <th>wb_296</th>\n",
       "      <th>wb_297</th>\n",
       "      <th>wb_298</th>\n",
       "      <th>wb_299</th>\n",
       "      <th>wb_300</th>\n",
       "      <th>wb_301</th>\n",
       "      <th>wb_302</th>\n",
       "      <th>wb_303</th>\n",
       "      <th>wb_304</th>\n",
       "      <th>wb_305</th>\n",
       "      <th>wb_306</th>\n",
       "      <th>wb_307</th>\n",
       "      <th>wb_308</th>\n",
       "      <th>wb_309</th>\n",
       "      <th>wb_310</th>\n",
       "      <th>wb_311</th>\n",
       "      <th>wb_312</th>\n",
       "      <th>wb_313</th>\n",
       "      <th>wb_314</th>\n",
       "      <th>wb_315</th>\n",
       "      <th>wb_316</th>\n",
       "      <th>wb_317</th>\n",
       "      <th>wb_318</th>\n",
       "      <th>wb_319</th>\n",
       "      <th>wb_320</th>\n",
       "      <th>wb_321</th>\n",
       "      <th>wb_322</th>\n",
       "      <th>wb_323</th>\n",
       "      <th>wb_324</th>\n",
       "      <th>wb_325</th>\n",
       "      <th>wb_326</th>\n",
       "      <th>wb_327</th>\n",
       "      <th>wb_328</th>\n",
       "      <th>wb_329</th>\n",
       "      <th>wb_330</th>\n",
       "      <th>wb_331</th>\n",
       "      <th>wb_332</th>\n",
       "      <th>wb_333</th>\n",
       "      <th>wb_334</th>\n",
       "      <th>wb_335</th>\n",
       "      <th>wb_336</th>\n",
       "      <th>wb_337</th>\n",
       "      <th>wb_338</th>\n",
       "      <th>wb_339</th>\n",
       "      <th>wb_340</th>\n",
       "      <th>wb_341</th>\n",
       "      <th>wb_342</th>\n",
       "      <th>wb_343</th>\n",
       "      <th>wb_344</th>\n",
       "      <th>wb_345</th>\n",
       "      <th>wb_346</th>\n",
       "      <th>wb_347</th>\n",
       "      <th>wb_348</th>\n",
       "      <th>wb_349</th>\n",
       "      <th>wb_350</th>\n",
       "      <th>wb_351</th>\n",
       "      <th>wb_352</th>\n",
       "      <th>wb_353</th>\n",
       "      <th>wb_354</th>\n",
       "      <th>wb_355</th>\n",
       "      <th>wb_356</th>\n",
       "      <th>wb_357</th>\n",
       "      <th>wb_358</th>\n",
       "      <th>wb_359</th>\n",
       "      <th>wb_360</th>\n",
       "      <th>wb_361</th>\n",
       "      <th>wb_362</th>\n",
       "      <th>wb_363</th>\n",
       "      <th>wb_364</th>\n",
       "      <th>wb_365</th>\n",
       "      <th>wb_366</th>\n",
       "      <th>wb_367</th>\n",
       "      <th>wb_368</th>\n",
       "      <th>wb_369</th>\n",
       "      <th>wb_370</th>\n",
       "      <th>wb_371</th>\n",
       "      <th>wb_372</th>\n",
       "      <th>wb_373</th>\n",
       "      <th>wb_374</th>\n",
       "      <th>wb_375</th>\n",
       "      <th>wb_376</th>\n",
       "      <th>wb_377</th>\n",
       "      <th>wb_378</th>\n",
       "      <th>wb_379</th>\n",
       "      <th>wb_380</th>\n",
       "      <th>wb_381</th>\n",
       "      <th>wb_382</th>\n",
       "      <th>wb_383</th>\n",
       "      <th>wb_384</th>\n",
       "      <th>wb_385</th>\n",
       "      <th>wb_386</th>\n",
       "      <th>wb_387</th>\n",
       "      <th>wb_388</th>\n",
       "      <th>wb_389</th>\n",
       "      <th>wb_390</th>\n",
       "      <th>wb_391</th>\n",
       "      <th>wb_392</th>\n",
       "      <th>wb_393</th>\n",
       "      <th>wb_394</th>\n",
       "      <th>wb_395</th>\n",
       "      <th>wb_396</th>\n",
       "      <th>wb_397</th>\n",
       "      <th>wb_398</th>\n",
       "      <th>wb_399</th>\n",
       "      <th>wb_400</th>\n",
       "      <th>wb_401</th>\n",
       "      <th>wb_402</th>\n",
       "      <th>wb_403</th>\n",
       "      <th>wb_404</th>\n",
       "      <th>wb_405</th>\n",
       "      <th>wb_406</th>\n",
       "      <th>wb_407</th>\n",
       "      <th>wb_408</th>\n",
       "      <th>wb_409</th>\n",
       "      <th>wb_410</th>\n",
       "      <th>wb_411</th>\n",
       "      <th>wb_412</th>\n",
       "      <th>wb_413</th>\n",
       "      <th>wb_414</th>\n",
       "      <th>wb_415</th>\n",
       "      <th>wb_416</th>\n",
       "      <th>wb_417</th>\n",
       "      <th>wb_418</th>\n",
       "      <th>wb_419</th>\n",
       "      <th>wb_420</th>\n",
       "      <th>wb_421</th>\n",
       "      <th>wb_422</th>\n",
       "      <th>wb_423</th>\n",
       "      <th>wb_424</th>\n",
       "      <th>wb_425</th>\n",
       "      <th>wb_426</th>\n",
       "      <th>wb_427</th>\n",
       "      <th>wb_428</th>\n",
       "      <th>wb_429</th>\n",
       "      <th>wb_430</th>\n",
       "      <th>wb_431</th>\n",
       "      <th>wb_432</th>\n",
       "      <th>wb_433</th>\n",
       "      <th>wb_434</th>\n",
       "      <th>wb_435</th>\n",
       "      <th>wb_436</th>\n",
       "      <th>wb_437</th>\n",
       "      <th>wb_438</th>\n",
       "      <th>wb_439</th>\n",
       "      <th>wb_440</th>\n",
       "      <th>wb_441</th>\n",
       "      <th>wb_442</th>\n",
       "      <th>wb_443</th>\n",
       "      <th>wb_444</th>\n",
       "      <th>wb_445</th>\n",
       "      <th>wb_446</th>\n",
       "      <th>wb_447</th>\n",
       "      <th>wb_448</th>\n",
       "      <th>wb_449</th>\n",
       "      <th>wb_450</th>\n",
       "      <th>wb_451</th>\n",
       "      <th>wb_452</th>\n",
       "      <th>wb_453</th>\n",
       "      <th>wb_454</th>\n",
       "      <th>wb_455</th>\n",
       "      <th>wb_456</th>\n",
       "      <th>wb_457</th>\n",
       "      <th>wb_458</th>\n",
       "      <th>wb_459</th>\n",
       "      <th>wb_460</th>\n",
       "      <th>wb_461</th>\n",
       "      <th>wb_462</th>\n",
       "      <th>wb_463</th>\n",
       "      <th>wb_464</th>\n",
       "      <th>wb_465</th>\n",
       "      <th>wb_466</th>\n",
       "      <th>wb_467</th>\n",
       "      <th>wb_468</th>\n",
       "      <th>wb_469</th>\n",
       "      <th>wb_470</th>\n",
       "      <th>wb_471</th>\n",
       "      <th>wb_472</th>\n",
       "      <th>wb_473</th>\n",
       "      <th>wb_474</th>\n",
       "      <th>wb_475</th>\n",
       "      <th>wb_476</th>\n",
       "      <th>wb_477</th>\n",
       "      <th>wb_478</th>\n",
       "      <th>wb_479</th>\n",
       "      <th>wb_480</th>\n",
       "      <th>wb_481</th>\n",
       "      <th>wb_482</th>\n",
       "      <th>wb_483</th>\n",
       "      <th>wb_484</th>\n",
       "      <th>wb_485</th>\n",
       "      <th>wb_486</th>\n",
       "      <th>wb_487</th>\n",
       "      <th>wb_488</th>\n",
       "      <th>wb_489</th>\n",
       "      <th>wb_490</th>\n",
       "      <th>wb_491</th>\n",
       "      <th>wb_492</th>\n",
       "      <th>wb_493</th>\n",
       "      <th>wb_494</th>\n",
       "      <th>wb_495</th>\n",
       "      <th>wb_496</th>\n",
       "      <th>wb_497</th>\n",
       "      <th>wb_498</th>\n",
       "      <th>wb_499</th>\n",
       "      <th>wb_500</th>\n",
       "      <th>wb_501</th>\n",
       "      <th>wb_502</th>\n",
       "      <th>wb_503</th>\n",
       "      <th>wb_504</th>\n",
       "      <th>wb_505</th>\n",
       "      <th>wb_506</th>\n",
       "      <th>wb_507</th>\n",
       "      <th>wb_508</th>\n",
       "      <th>wb_509</th>\n",
       "      <th>wb_510</th>\n",
       "      <th>wb_511</th>\n",
       "      <th>wb_512</th>\n",
       "      <th>wb_513</th>\n",
       "      <th>wb_514</th>\n",
       "      <th>wb_515</th>\n",
       "      <th>wb_516</th>\n",
       "      <th>wb_517</th>\n",
       "      <th>wb_518</th>\n",
       "      <th>wb_519</th>\n",
       "      <th>wb_520</th>\n",
       "      <th>wb_521</th>\n",
       "      <th>wb_522</th>\n",
       "      <th>wb_523</th>\n",
       "      <th>wb_524</th>\n",
       "      <th>wb_525</th>\n",
       "      <th>wb_526</th>\n",
       "      <th>wb_527</th>\n",
       "      <th>wb_528</th>\n",
       "      <th>wb_529</th>\n",
       "      <th>wb_530</th>\n",
       "      <th>wb_531</th>\n",
       "      <th>wb_532</th>\n",
       "      <th>wb_533</th>\n",
       "      <th>wb_534</th>\n",
       "      <th>wb_535</th>\n",
       "      <th>wb_536</th>\n",
       "      <th>wb_537</th>\n",
       "      <th>wb_538</th>\n",
       "      <th>wb_539</th>\n",
       "      <th>wb_540</th>\n",
       "      <th>wb_541</th>\n",
       "      <th>wb_542</th>\n",
       "      <th>wb_543</th>\n",
       "      <th>wb_544</th>\n",
       "      <th>wb_545</th>\n",
       "      <th>wb_546</th>\n",
       "      <th>wb_547</th>\n",
       "      <th>wb_548</th>\n",
       "      <th>wb_549</th>\n",
       "      <th>wb_550</th>\n",
       "      <th>wb_551</th>\n",
       "      <th>wb_552</th>\n",
       "      <th>wb_553</th>\n",
       "      <th>wb_554</th>\n",
       "      <th>wb_555</th>\n",
       "      <th>wb_556</th>\n",
       "      <th>wb_557</th>\n",
       "      <th>wb_558</th>\n",
       "      <th>wb_559</th>\n",
       "      <th>wb_560</th>\n",
       "      <th>wb_561</th>\n",
       "      <th>wb_562</th>\n",
       "      <th>wb_563</th>\n",
       "      <th>wb_564</th>\n",
       "      <th>wb_565</th>\n",
       "      <th>wb_566</th>\n",
       "      <th>wb_567</th>\n",
       "      <th>wb_568</th>\n",
       "      <th>wb_569</th>\n",
       "      <th>wb_570</th>\n",
       "      <th>wb_571</th>\n",
       "      <th>wb_572</th>\n",
       "      <th>wb_573</th>\n",
       "      <th>wb_574</th>\n",
       "      <th>wb_575</th>\n",
       "      <th>wb_576</th>\n",
       "      <th>wb_577</th>\n",
       "      <th>wb_578</th>\n",
       "      <th>wb_579</th>\n",
       "      <th>wb_580</th>\n",
       "      <th>wb_581</th>\n",
       "      <th>wb_582</th>\n",
       "      <th>wb_583</th>\n",
       "      <th>wb_584</th>\n",
       "      <th>wb_585</th>\n",
       "      <th>wb_586</th>\n",
       "      <th>wb_587</th>\n",
       "      <th>wb_588</th>\n",
       "      <th>wb_589</th>\n",
       "      <th>wb_590</th>\n",
       "      <th>wb_591</th>\n",
       "      <th>wb_592</th>\n",
       "      <th>wb_593</th>\n",
       "      <th>wb_594</th>\n",
       "      <th>wb_595</th>\n",
       "      <th>wb_596</th>\n",
       "      <th>wb_597</th>\n",
       "      <th>wb_598</th>\n",
       "      <th>wb_599</th>\n",
       "      <th>wb_600</th>\n",
       "      <th>wb_601</th>\n",
       "      <th>wb_602</th>\n",
       "      <th>wb_603</th>\n",
       "      <th>wb_604</th>\n",
       "      <th>wb_605</th>\n",
       "      <th>wb_606</th>\n",
       "      <th>wb_607</th>\n",
       "      <th>wb_608</th>\n",
       "      <th>wb_609</th>\n",
       "      <th>wb_610</th>\n",
       "      <th>wb_611</th>\n",
       "      <th>wb_612</th>\n",
       "      <th>wb_613</th>\n",
       "      <th>wb_614</th>\n",
       "      <th>wb_615</th>\n",
       "      <th>wb_616</th>\n",
       "      <th>wb_617</th>\n",
       "      <th>wb_618</th>\n",
       "      <th>wb_619</th>\n",
       "      <th>wb_620</th>\n",
       "      <th>wb_621</th>\n",
       "      <th>wb_622</th>\n",
       "      <th>wb_623</th>\n",
       "      <th>wb_624</th>\n",
       "      <th>wb_625</th>\n",
       "      <th>wb_626</th>\n",
       "      <th>wb_627</th>\n",
       "      <th>wb_628</th>\n",
       "      <th>wb_629</th>\n",
       "      <th>wb_630</th>\n",
       "      <th>wb_631</th>\n",
       "      <th>wb_632</th>\n",
       "      <th>wb_633</th>\n",
       "      <th>wb_634</th>\n",
       "      <th>wb_635</th>\n",
       "      <th>wb_636</th>\n",
       "      <th>wb_637</th>\n",
       "      <th>wb_638</th>\n",
       "      <th>wb_639</th>\n",
       "      <th>wb_640</th>\n",
       "      <th>wb_641</th>\n",
       "      <th>wb_642</th>\n",
       "      <th>wb_643</th>\n",
       "      <th>wb_644</th>\n",
       "      <th>wb_645</th>\n",
       "      <th>wb_646</th>\n",
       "      <th>wb_647</th>\n",
       "      <th>wb_648</th>\n",
       "      <th>wb_649</th>\n",
       "      <th>wb_650</th>\n",
       "      <th>wb_651</th>\n",
       "      <th>wb_652</th>\n",
       "      <th>wb_653</th>\n",
       "      <th>wb_654</th>\n",
       "      <th>wb_655</th>\n",
       "      <th>wb_656</th>\n",
       "      <th>wb_657</th>\n",
       "      <th>wb_658</th>\n",
       "      <th>wb_659</th>\n",
       "      <th>wb_660</th>\n",
       "      <th>wb_661</th>\n",
       "      <th>wb_662</th>\n",
       "      <th>wb_663</th>\n",
       "      <th>wb_664</th>\n",
       "      <th>wb_665</th>\n",
       "      <th>wb_666</th>\n",
       "      <th>wb_667</th>\n",
       "      <th>wb_668</th>\n",
       "      <th>wb_669</th>\n",
       "      <th>wb_670</th>\n",
       "      <th>wb_671</th>\n",
       "      <th>wb_672</th>\n",
       "      <th>wb_673</th>\n",
       "      <th>wb_674</th>\n",
       "      <th>wb_675</th>\n",
       "      <th>wb_676</th>\n",
       "      <th>wb_677</th>\n",
       "      <th>wb_678</th>\n",
       "      <th>wb_679</th>\n",
       "      <th>wb_680</th>\n",
       "      <th>wb_681</th>\n",
       "      <th>wb_682</th>\n",
       "      <th>wb_683</th>\n",
       "      <th>wb_684</th>\n",
       "      <th>wb_685</th>\n",
       "      <th>wb_686</th>\n",
       "      <th>wb_687</th>\n",
       "      <th>wb_688</th>\n",
       "      <th>wb_689</th>\n",
       "      <th>wb_690</th>\n",
       "      <th>wb_691</th>\n",
       "      <th>wb_692</th>\n",
       "      <th>wb_693</th>\n",
       "      <th>wb_694</th>\n",
       "      <th>wb_695</th>\n",
       "      <th>wb_696</th>\n",
       "      <th>wb_697</th>\n",
       "      <th>wb_698</th>\n",
       "      <th>wb_699</th>\n",
       "      <th>wb_700</th>\n",
       "      <th>wb_701</th>\n",
       "      <th>wb_702</th>\n",
       "      <th>wb_703</th>\n",
       "      <th>wb_704</th>\n",
       "      <th>wb_705</th>\n",
       "      <th>wb_706</th>\n",
       "      <th>wb_707</th>\n",
       "      <th>wb_708</th>\n",
       "      <th>wb_709</th>\n",
       "      <th>wb_710</th>\n",
       "      <th>wb_711</th>\n",
       "      <th>wb_712</th>\n",
       "      <th>wb_713</th>\n",
       "      <th>wb_714</th>\n",
       "      <th>wb_715</th>\n",
       "      <th>wb_716</th>\n",
       "      <th>wb_717</th>\n",
       "      <th>wb_718</th>\n",
       "      <th>wb_719</th>\n",
       "      <th>wb_720</th>\n",
       "      <th>wb_721</th>\n",
       "      <th>wb_722</th>\n",
       "      <th>wb_723</th>\n",
       "      <th>wb_724</th>\n",
       "      <th>wb_725</th>\n",
       "      <th>wb_726</th>\n",
       "      <th>wb_727</th>\n",
       "      <th>wb_728</th>\n",
       "      <th>wb_729</th>\n",
       "      <th>wb_730</th>\n",
       "      <th>wb_731</th>\n",
       "      <th>wb_732</th>\n",
       "      <th>wb_733</th>\n",
       "      <th>wb_734</th>\n",
       "      <th>wb_735</th>\n",
       "      <th>wb_736</th>\n",
       "      <th>wb_737</th>\n",
       "      <th>wb_738</th>\n",
       "      <th>wb_739</th>\n",
       "      <th>wb_740</th>\n",
       "      <th>wb_741</th>\n",
       "      <th>wb_742</th>\n",
       "      <th>wb_743</th>\n",
       "      <th>wb_744</th>\n",
       "      <th>wb_745</th>\n",
       "      <th>wb_746</th>\n",
       "      <th>wb_747</th>\n",
       "      <th>wb_748</th>\n",
       "      <th>wb_749</th>\n",
       "      <th>wb_750</th>\n",
       "      <th>wb_751</th>\n",
       "      <th>wb_752</th>\n",
       "      <th>wb_753</th>\n",
       "      <th>wb_754</th>\n",
       "      <th>wb_755</th>\n",
       "      <th>wb_756</th>\n",
       "      <th>wb_757</th>\n",
       "      <th>wb_758</th>\n",
       "      <th>wb_759</th>\n",
       "      <th>wb_760</th>\n",
       "      <th>wb_761</th>\n",
       "      <th>wb_762</th>\n",
       "      <th>wb_763</th>\n",
       "      <th>wb_764</th>\n",
       "      <th>wb_765</th>\n",
       "      <th>wb_766</th>\n",
       "      <th>wb_767</th>\n",
       "      <th>wb_768</th>\n",
       "      <th>wb_769</th>\n",
       "      <th>wb_770</th>\n",
       "      <th>wb_771</th>\n",
       "      <th>wb_772</th>\n",
       "      <th>wb_773</th>\n",
       "      <th>wb_774</th>\n",
       "      <th>wb_775</th>\n",
       "      <th>wb_776</th>\n",
       "      <th>wb_777</th>\n",
       "      <th>wb_778</th>\n",
       "      <th>wb_779</th>\n",
       "      <th>wb_780</th>\n",
       "      <th>wb_781</th>\n",
       "      <th>wb_782</th>\n",
       "      <th>wb_783</th>\n",
       "      <th>wb_784</th>\n",
       "      <th>wb_785</th>\n",
       "      <th>wb_786</th>\n",
       "      <th>wb_787</th>\n",
       "      <th>wb_788</th>\n",
       "      <th>wb_789</th>\n",
       "      <th>wb_790</th>\n",
       "      <th>wb_791</th>\n",
       "      <th>wb_792</th>\n",
       "      <th>wb_793</th>\n",
       "      <th>wb_794</th>\n",
       "      <th>wb_795</th>\n",
       "      <th>wb_796</th>\n",
       "      <th>wb_797</th>\n",
       "      <th>wb_798</th>\n",
       "      <th>wb_799</th>\n",
       "      <th>wb_800</th>\n",
       "      <th>wb_801</th>\n",
       "      <th>wb_802</th>\n",
       "      <th>wb_803</th>\n",
       "      <th>wb_804</th>\n",
       "      <th>wb_805</th>\n",
       "      <th>wb_806</th>\n",
       "      <th>wb_807</th>\n",
       "      <th>wb_808</th>\n",
       "      <th>wb_809</th>\n",
       "      <th>wb_810</th>\n",
       "      <th>wb_811</th>\n",
       "      <th>wb_812</th>\n",
       "      <th>wb_813</th>\n",
       "      <th>wb_814</th>\n",
       "      <th>wb_815</th>\n",
       "      <th>wb_816</th>\n",
       "      <th>wb_817</th>\n",
       "      <th>wb_818</th>\n",
       "      <th>wb_819</th>\n",
       "      <th>wb_820</th>\n",
       "      <th>wb_821</th>\n",
       "      <th>wb_822</th>\n",
       "      <th>wb_823</th>\n",
       "      <th>wb_824</th>\n",
       "      <th>wb_825</th>\n",
       "      <th>wb_826</th>\n",
       "      <th>wb_827</th>\n",
       "      <th>wb_828</th>\n",
       "      <th>wb_829</th>\n",
       "      <th>wb_830</th>\n",
       "      <th>wb_831</th>\n",
       "      <th>wb_832</th>\n",
       "      <th>wb_833</th>\n",
       "      <th>wb_834</th>\n",
       "      <th>wb_835</th>\n",
       "      <th>wb_836</th>\n",
       "      <th>wb_837</th>\n",
       "      <th>wb_838</th>\n",
       "      <th>wb_839</th>\n",
       "      <th>wb_840</th>\n",
       "      <th>wb_841</th>\n",
       "      <th>wb_842</th>\n",
       "      <th>wb_843</th>\n",
       "      <th>wb_844</th>\n",
       "      <th>wb_845</th>\n",
       "      <th>wb_846</th>\n",
       "      <th>wb_847</th>\n",
       "      <th>wb_848</th>\n",
       "      <th>wb_849</th>\n",
       "      <th>wb_850</th>\n",
       "      <th>wb_851</th>\n",
       "      <th>wb_852</th>\n",
       "      <th>wb_853</th>\n",
       "      <th>wb_854</th>\n",
       "      <th>wb_855</th>\n",
       "      <th>wb_856</th>\n",
       "      <th>wb_857</th>\n",
       "      <th>wb_858</th>\n",
       "      <th>wb_859</th>\n",
       "      <th>wb_860</th>\n",
       "      <th>wb_861</th>\n",
       "      <th>wb_862</th>\n",
       "      <th>wb_863</th>\n",
       "      <th>wb_864</th>\n",
       "      <th>wb_865</th>\n",
       "      <th>wb_866</th>\n",
       "      <th>wb_867</th>\n",
       "      <th>wb_868</th>\n",
       "      <th>wb_869</th>\n",
       "      <th>wb_870</th>\n",
       "      <th>wb_871</th>\n",
       "      <th>wb_872</th>\n",
       "      <th>wb_873</th>\n",
       "      <th>wb_874</th>\n",
       "      <th>wb_875</th>\n",
       "      <th>wb_876</th>\n",
       "      <th>wb_877</th>\n",
       "      <th>wb_878</th>\n",
       "      <th>wb_879</th>\n",
       "      <th>wb_880</th>\n",
       "      <th>wb_881</th>\n",
       "      <th>wb_882</th>\n",
       "      <th>wb_883</th>\n",
       "      <th>wb_884</th>\n",
       "      <th>wb_885</th>\n",
       "      <th>wb_886</th>\n",
       "      <th>wb_887</th>\n",
       "      <th>wb_888</th>\n",
       "      <th>wb_889</th>\n",
       "      <th>wb_890</th>\n",
       "      <th>wb_891</th>\n",
       "      <th>wb_892</th>\n",
       "      <th>wb_893</th>\n",
       "      <th>wb_894</th>\n",
       "      <th>wb_895</th>\n",
       "      <th>wb_896</th>\n",
       "      <th>wb_897</th>\n",
       "      <th>wb_898</th>\n",
       "      <th>wb_899</th>\n",
       "      <th>wb_900</th>\n",
       "      <th>wb_901</th>\n",
       "      <th>wb_902</th>\n",
       "      <th>wb_903</th>\n",
       "      <th>wb_904</th>\n",
       "      <th>wb_905</th>\n",
       "      <th>wb_906</th>\n",
       "      <th>wb_907</th>\n",
       "      <th>wb_908</th>\n",
       "      <th>wb_909</th>\n",
       "      <th>wb_910</th>\n",
       "      <th>wb_911</th>\n",
       "      <th>wb_912</th>\n",
       "      <th>wb_913</th>\n",
       "      <th>wb_914</th>\n",
       "      <th>wb_915</th>\n",
       "      <th>wb_916</th>\n",
       "      <th>wb_917</th>\n",
       "      <th>wb_918</th>\n",
       "      <th>wb_919</th>\n",
       "      <th>wb_920</th>\n",
       "      <th>wb_921</th>\n",
       "      <th>wb_922</th>\n",
       "      <th>wb_923</th>\n",
       "      <th>wb_924</th>\n",
       "      <th>wb_925</th>\n",
       "      <th>wb_926</th>\n",
       "      <th>wb_927</th>\n",
       "      <th>wb_928</th>\n",
       "      <th>wb_929</th>\n",
       "      <th>wb_930</th>\n",
       "      <th>wb_931</th>\n",
       "      <th>wb_932</th>\n",
       "      <th>wb_933</th>\n",
       "      <th>wb_934</th>\n",
       "      <th>wb_935</th>\n",
       "      <th>wb_936</th>\n",
       "      <th>wb_937</th>\n",
       "      <th>wb_938</th>\n",
       "      <th>wb_939</th>\n",
       "      <th>wb_940</th>\n",
       "      <th>wb_941</th>\n",
       "      <th>wb_942</th>\n",
       "      <th>wb_943</th>\n",
       "      <th>wb_944</th>\n",
       "      <th>wb_945</th>\n",
       "      <th>wb_946</th>\n",
       "      <th>wb_947</th>\n",
       "      <th>wb_948</th>\n",
       "      <th>wb_949</th>\n",
       "      <th>wb_950</th>\n",
       "      <th>wb_951</th>\n",
       "      <th>wb_952</th>\n",
       "      <th>wb_953</th>\n",
       "      <th>wb_954</th>\n",
       "      <th>wb_955</th>\n",
       "      <th>wb_956</th>\n",
       "      <th>wb_957</th>\n",
       "      <th>wb_958</th>\n",
       "      <th>wb_959</th>\n",
       "      <th>wb_960</th>\n",
       "      <th>wb_961</th>\n",
       "      <th>wb_962</th>\n",
       "      <th>wb_963</th>\n",
       "      <th>wb_964</th>\n",
       "      <th>wb_965</th>\n",
       "      <th>wb_966</th>\n",
       "      <th>wb_967</th>\n",
       "      <th>wb_968</th>\n",
       "      <th>wb_969</th>\n",
       "      <th>wb_970</th>\n",
       "      <th>wb_971</th>\n",
       "      <th>wb_972</th>\n",
       "      <th>wb_973</th>\n",
       "      <th>wb_974</th>\n",
       "      <th>wb_975</th>\n",
       "      <th>wb_976</th>\n",
       "      <th>wb_977</th>\n",
       "      <th>wb_978</th>\n",
       "      <th>wb_979</th>\n",
       "      <th>wb_980</th>\n",
       "      <th>wb_981</th>\n",
       "      <th>wb_982</th>\n",
       "      <th>wb_983</th>\n",
       "      <th>wb_984</th>\n",
       "      <th>wb_985</th>\n",
       "      <th>wb_986</th>\n",
       "      <th>wb_987</th>\n",
       "      <th>wb_988</th>\n",
       "      <th>wb_989</th>\n",
       "      <th>wb_990</th>\n",
       "      <th>wb_991</th>\n",
       "      <th>wb_992</th>\n",
       "      <th>wb_993</th>\n",
       "      <th>wb_994</th>\n",
       "      <th>wb_995</th>\n",
       "      <th>wb_996</th>\n",
       "      <th>wb_997</th>\n",
       "      <th>wb_998</th>\n",
       "      <th>wb_999</th>\n",
       "      <th>wb_1000</th>\n",
       "      <th>wb_1001</th>\n",
       "      <th>wb_1002</th>\n",
       "      <th>wb_1003</th>\n",
       "      <th>wb_1004</th>\n",
       "      <th>wb_1005</th>\n",
       "      <th>wb_1006</th>\n",
       "      <th>wb_1007</th>\n",
       "      <th>wb_1008</th>\n",
       "      <th>wb_1009</th>\n",
       "      <th>wb_1010</th>\n",
       "      <th>wb_1011</th>\n",
       "      <th>wb_1012</th>\n",
       "      <th>wb_1013</th>\n",
       "      <th>wb_1014</th>\n",
       "      <th>wb_1015</th>\n",
       "      <th>wb_1016</th>\n",
       "      <th>wb_1017</th>\n",
       "      <th>wb_1018</th>\n",
       "      <th>wb_1019</th>\n",
       "      <th>wb_1020</th>\n",
       "      <th>wb_1021</th>\n",
       "      <th>wb_1022</th>\n",
       "      <th>wb_1023</th>\n",
       "      <th>wb_1024</th>\n",
       "      <th>wb_1025</th>\n",
       "      <th>wb_1026</th>\n",
       "      <th>wb_1027</th>\n",
       "      <th>wb_1028</th>\n",
       "      <th>wb_1029</th>\n",
       "      <th>wb_1030</th>\n",
       "      <th>wb_1031</th>\n",
       "      <th>wb_1032</th>\n",
       "      <th>wb_1033</th>\n",
       "      <th>wb_1034</th>\n",
       "      <th>wb_1035</th>\n",
       "      <th>wb_1036</th>\n",
       "      <th>wb_1037</th>\n",
       "      <th>wb_1038</th>\n",
       "      <th>wb_1039</th>\n",
       "      <th>wb_1040</th>\n",
       "      <th>wb_1041</th>\n",
       "      <th>wb_1042</th>\n",
       "      <th>wb_1043</th>\n",
       "      <th>wb_1044</th>\n",
       "      <th>wb_1045</th>\n",
       "      <th>wb_1046</th>\n",
       "      <th>wb_1047</th>\n",
       "      <th>wb_1048</th>\n",
       "      <th>wb_1049</th>\n",
       "      <th>wb_1050</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>2746317213</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-6.800</td>\n",
       "      <td>9.100</td>\n",
       "      <td>9.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>4.400</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-6.400</td>\n",
       "      <td>4.400</td>\n",
       "      <td>8.200</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>5.400</td>\n",
       "      <td>5.800</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-8.500</td>\n",
       "      <td>-8.400</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>8.300</td>\n",
       "      <td>5.000</td>\n",
       "      <td>6.400</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-3.100</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>5.800</td>\n",
       "      <td>1.300</td>\n",
       "      <td>6.700</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>9.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>-8.085</td>\n",
       "      <td>2.082</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-2.825</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.273</td>\n",
       "      <td>-0.666</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.074</td>\n",
       "      <td>1.883</td>\n",
       "      <td>4.288</td>\n",
       "      <td>0.274</td>\n",
       "      <td>2.274</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-1.897</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>9.427</td>\n",
       "      <td>1.968</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>1.826</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-5.150</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-2.148</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.897</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-6.800</td>\n",
       "      <td>9.100</td>\n",
       "      <td>9.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>4.400</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-6.400</td>\n",
       "      <td>4.400</td>\n",
       "      <td>8.200</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>5.400</td>\n",
       "      <td>5.800</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-8.500</td>\n",
       "      <td>-8.400</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>8.300</td>\n",
       "      <td>5.000</td>\n",
       "      <td>6.400</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>-3.100</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>5.800</td>\n",
       "      <td>1.300</td>\n",
       "      <td>6.700</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>9.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.721</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.618</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.767</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.634</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.521</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>2746317213</td>\n",
       "      <td>5.400</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>8.800</td>\n",
       "      <td>3.400</td>\n",
       "      <td>7.300</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.600</td>\n",
       "      <td>6.300</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>-2.200</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>6.600</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>5.800</td>\n",
       "      <td>5.100</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>3.900</td>\n",
       "      <td>9.600</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>4.700</td>\n",
       "      <td>-7.800</td>\n",
       "      <td>10.494</td>\n",
       "      <td>-10.606</td>\n",
       "      <td>3.991</td>\n",
       "      <td>0.896</td>\n",
       "      <td>5.701</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>1.680</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.965</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.232</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-4.026</td>\n",
       "      <td>1.896</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-4.109</td>\n",
       "      <td>-1.277</td>\n",
       "      <td>0.337</td>\n",
       "      <td>2.378</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>2.603</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.084</td>\n",
       "      <td>5.400</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>8.800</td>\n",
       "      <td>3.400</td>\n",
       "      <td>7.300</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.800</td>\n",
       "      <td>2.600</td>\n",
       "      <td>6.300</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>-2.200</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>6.600</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>5.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>5.800</td>\n",
       "      <td>5.100</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>3.900</td>\n",
       "      <td>9.600</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>4.700</td>\n",
       "      <td>-7.800</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.545</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.267</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.673</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.864</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.988</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.889</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.557</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.622</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.685</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>2746317213</td>\n",
       "      <td>-3.200</td>\n",
       "      <td>5.200</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>3.400</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-5.800</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.400</td>\n",
       "      <td>2.200</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>2.700</td>\n",
       "      <td>3.700</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>5.700</td>\n",
       "      <td>-7.900</td>\n",
       "      <td>-8.700</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>-9.600</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>8.000</td>\n",
       "      <td>-5.800</td>\n",
       "      <td>5.200</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>5.800</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-5.400</td>\n",
       "      <td>9.100</td>\n",
       "      <td>-2.800</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>9.000</td>\n",
       "      <td>-1.900</td>\n",
       "      <td>-5.502</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-1.602</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>4.139</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-2.157</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>2.342</td>\n",
       "      <td>-1.689</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.852</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-2.544</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>1.864</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-1.644</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>1.569</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-3.200</td>\n",
       "      <td>5.200</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>3.400</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-5.800</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.400</td>\n",
       "      <td>2.200</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>2.700</td>\n",
       "      <td>3.700</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>5.700</td>\n",
       "      <td>-7.900</td>\n",
       "      <td>-8.700</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>-9.600</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>8.000</td>\n",
       "      <td>-5.800</td>\n",
       "      <td>5.200</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>5.800</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-5.400</td>\n",
       "      <td>9.100</td>\n",
       "      <td>-2.800</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>9.000</td>\n",
       "      <td>-1.900</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.477</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.852</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.406</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.335</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.611</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-1.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>2746317213</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-1.800</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-4.800</td>\n",
       "      <td>0.100</td>\n",
       "      <td>7.800</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>-3.600</td>\n",
       "      <td>6.700</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>4.100</td>\n",
       "      <td>7.400</td>\n",
       "      <td>5.500</td>\n",
       "      <td>-7.000</td>\n",
       "      <td>-6.600</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>1.500</td>\n",
       "      <td>4.100</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>4.700</td>\n",
       "      <td>7.100</td>\n",
       "      <td>-6.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>5.400</td>\n",
       "      <td>1.900</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>7.899</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-5.985</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.771</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-15.641</td>\n",
       "      <td>-1.359</td>\n",
       "      <td>0.011</td>\n",
       "      <td>4.343</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.963</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.852</td>\n",
       "      <td>5.860</td>\n",
       "      <td>0.731</td>\n",
       "      <td>2.487</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>2.109</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-1.800</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-4.800</td>\n",
       "      <td>0.100</td>\n",
       "      <td>7.800</td>\n",
       "      <td>-2.400</td>\n",
       "      <td>-3.600</td>\n",
       "      <td>6.700</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>4.100</td>\n",
       "      <td>7.400</td>\n",
       "      <td>5.500</td>\n",
       "      <td>-7.000</td>\n",
       "      <td>-6.600</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>1.500</td>\n",
       "      <td>4.100</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>4.700</td>\n",
       "      <td>7.100</td>\n",
       "      <td>-6.600</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>5.400</td>\n",
       "      <td>1.900</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.323</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.493</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.129</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.378</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.775</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.455</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.587</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.657</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.073</td>\n",
       "      <td>1.029</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.675</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.927</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.368</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.395</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>2746317213</td>\n",
       "      <td>-6.300</td>\n",
       "      <td>4.600</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-7.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.100</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.300</td>\n",
       "      <td>8.100</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>4.600</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>2.700</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>3.400</td>\n",
       "      <td>7.000</td>\n",
       "      <td>-3.900</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>-7.600</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>6.100</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.200</td>\n",
       "      <td>-3.700</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>7.100</td>\n",
       "      <td>-3.368</td>\n",
       "      <td>2.171</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.042</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-4.062</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.564</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>3.475</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>4.344</td>\n",
       "      <td>2.647</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-2.697</td>\n",
       "      <td>-0.807</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>2.752</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>2.448</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.621</td>\n",
       "      <td>-6.300</td>\n",
       "      <td>4.600</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>-7.800</td>\n",
       "      <td>2.100</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.100</td>\n",
       "      <td>7.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.300</td>\n",
       "      <td>8.100</td>\n",
       "      <td>1.700</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>4.600</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>2.700</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>3.400</td>\n",
       "      <td>7.000</td>\n",
       "      <td>-3.900</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>-7.600</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>6.100</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.200</td>\n",
       "      <td>-3.700</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>7.100</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.516</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.486</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.500</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.451</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-0.384</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.248</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.615</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.739</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>-0.475</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.608</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.579</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.708</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.442</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.674</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.650</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.576</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.671</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.542</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.446</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0000-target  0001-target  0002-target  0003-target  \\\n",
       "6252  2746317213        0.800       -6.800        9.100        9.100   \n",
       "4684  2746317213        5.400       -9.800        8.800        3.400   \n",
       "1731  2746317213       -3.200        5.200       -8.000       -5.700   \n",
       "4742  2746317213        9.400       -1.800       -2.300       -4.800   \n",
       "4521  2746317213       -6.300        4.600       -0.900       -7.800   \n",
       "\n",
       "      0010-target  0011-target  0012-target  0020-target  0021-target  \\\n",
       "6252       -0.100        4.400        1.800       -4.300       -6.100   \n",
       "4684        7.300        1.700       -1.700        2.500        1.800   \n",
       "1731        3.400        1.100        1.300       -5.800        2.900   \n",
       "4742        0.100        7.800       -2.400       -3.600        6.700   \n",
       "4521        2.100       -9.800        8.200        2.100        7.000   \n",
       "\n",
       "      0030-target  0100-target  0101-target  0102-target  0110-target  \\\n",
       "6252       -6.400        4.400        8.200       -6.500        5.400   \n",
       "4684        2.600        6.300        1.800       -2.400       -2.200   \n",
       "1731        2.400        2.200       -3.500        2.700        3.700   \n",
       "4742       -2.900        4.100        7.400        5.500       -7.000   \n",
       "4521        0.500        1.300        8.100        1.700       -1.100   \n",
       "\n",
       "      0111-target  0120-target  0200-target  0201-target  0210-target  \\\n",
       "6252        5.800       -5.100       -8.500       -8.400       -5.300   \n",
       "4684       -0.200       -5.700        6.600       -9.900       -0.200   \n",
       "1731       -0.200        5.700       -7.900       -8.700       -2.500   \n",
       "4742       -6.600       -2.600        1.500        4.100       -1.000   \n",
       "4521        4.600       -0.600        2.700        1.800      -10.000   \n",
       "\n",
       "      0300-target  1000-target  1001-target  1002-target  1010-target  \\\n",
       "6252       -5.100        8.300        5.000        6.400        1.700   \n",
       "4684      -10.000        6.500        5.000       -8.900       -8.000   \n",
       "1731       -9.600        1.300       -0.300        8.000       -5.800   \n",
       "4742       -3.800       -9.200       -6.500       -9.800        4.700   \n",
       "4521       -2.100        3.400        7.000       -3.900       -5.700   \n",
       "\n",
       "      1011-target  1020-target  1100-target  1101-target  1110-target  \\\n",
       "6252       -8.800       -3.100       -6.200        5.800        1.300   \n",
       "4684      -10.000       -8.600        5.800        5.100       -1.000   \n",
       "1731        5.200       -0.400        5.800       -0.300       -2.000   \n",
       "4742        7.100       -6.600        0.000       -2.300       -3.500   \n",
       "4521       -7.600       -0.700        6.100        0.800       -5.200   \n",
       "\n",
       "      1200-target  2000-target  2001-target  2010-target  2100-target  \\\n",
       "6252        6.700       -3.500       -1.400        9.700        9.300   \n",
       "4684        3.900        9.600       -0.600       -9.900        4.700   \n",
       "1731       -5.400        9.100       -2.800       -7.500        9.000   \n",
       "4742        5.400        1.900       -0.100       -9.800       -8.000   \n",
       "4521       -6.500        8.200       -3.700        1.300       -3.800   \n",
       "\n",
       "      3000-target  0000-lstsq_lambda  0001-lstsq_lambda  0002-lstsq_lambda  \\\n",
       "6252       -3.800             -0.911             -8.085              2.082   \n",
       "4684       -7.800             10.494            -10.606              3.991   \n",
       "1731       -1.900             -5.502             -0.466             -1.602   \n",
       "4742       -7.400              7.899              0.186              0.850   \n",
       "4521        7.100             -3.368              2.171             -0.198   \n",
       "\n",
       "      0003-lstsq_lambda  0010-lstsq_lambda  0011-lstsq_lambda  \\\n",
       "6252              0.381             -2.825              0.081   \n",
       "4684              0.896              5.701             -0.741   \n",
       "1731             -0.264              4.139              0.875   \n",
       "4742              0.085             -5.985              0.525   \n",
       "4521              0.042              1.990             -4.062   \n",
       "\n",
       "      0012-lstsq_lambda  0020-lstsq_lambda  0021-lstsq_lambda  \\\n",
       "6252              0.273             -0.666              0.177   \n",
       "4684             -0.387              1.680              0.689   \n",
       "1731              0.011             -2.157              0.218   \n",
       "4742              0.038              0.848              0.194   \n",
       "4521              0.113             -0.564             -0.255   \n",
       "\n",
       "      0030-lstsq_lambda  0100-lstsq_lambda  0101-lstsq_lambda  \\\n",
       "6252              0.074              1.883              4.288   \n",
       "4684             -0.150             -0.965              0.900   \n",
       "1731             -0.302              2.342             -1.689   \n",
       "4742             -0.025              0.570              0.541   \n",
       "4521             -0.173             -0.762              3.475   \n",
       "\n",
       "      0102-lstsq_lambda  0110-lstsq_lambda  0111-lstsq_lambda  \\\n",
       "6252              0.274              2.274             -0.065   \n",
       "4684              0.126             -1.312             -0.037   \n",
       "1731             -0.359              0.852             -0.198   \n",
       "4742              0.045             -0.771             -0.054   \n",
       "4521              0.150              0.562              0.649   \n",
       "\n",
       "      0120-lstsq_lambda  0200-lstsq_lambda  0201-lstsq_lambda  \\\n",
       "6252              0.155             -1.897              0.197   \n",
       "4684              0.021              1.232              0.206   \n",
       "1731              0.111             -2.544             -0.518   \n",
       "4742             -0.057              1.031              0.012   \n",
       "4521              0.093             -0.090             -0.016   \n",
       "\n",
       "      0210-lstsq_lambda  0300-lstsq_lambda  1000-lstsq_lambda  \\\n",
       "6252              0.089             -0.142              9.427   \n",
       "4684             -0.203              0.022             -4.026   \n",
       "1731             -0.658             -0.432              1.864   \n",
       "4742              0.084             -0.043            -15.641   \n",
       "4521             -0.280             -0.189              4.344   \n",
       "\n",
       "      1001-lstsq_lambda  1002-lstsq_lambda  1010-lstsq_lambda  \\\n",
       "6252              1.968             -0.581              1.826   \n",
       "4684              1.896              0.245             -4.109   \n",
       "1731              0.502              0.079             -1.644   \n",
       "4742             -1.359              0.011              4.343   \n",
       "4521              2.647              0.208             -2.697   \n",
       "\n",
       "      1011-lstsq_lambda  1020-lstsq_lambda  1100-lstsq_lambda  \\\n",
       "6252             -0.717             -0.521             -5.150   \n",
       "4684             -1.277              0.337              2.378   \n",
       "1731              0.158             -0.152              1.569   \n",
       "4742              0.594              0.963             -0.842   \n",
       "4521             -0.807             -0.177              2.752   \n",
       "\n",
       "      1101-lstsq_lambda  1110-lstsq_lambda  1200-lstsq_lambda  \\\n",
       "6252             -0.019             -0.008              0.105   \n",
       "4684              0.141             -0.118             -0.084   \n",
       "1731              0.089              0.076              0.003   \n",
       "4742             -0.298             -0.451              0.852   \n",
       "4521              0.579             -0.096             -0.144   \n",
       "\n",
       "      2000-lstsq_lambda  2001-lstsq_lambda  2010-lstsq_lambda  \\\n",
       "6252             -2.148              0.462              0.505   \n",
       "4684              2.603              0.458             -0.259   \n",
       "1731              0.536              0.133             -0.289   \n",
       "4742              5.860              0.731              2.487   \n",
       "4521              2.448              0.543             -0.676   \n",
       "\n",
       "      2100-lstsq_lambda  3000-lstsq_lambda  0000-lstsq_target  \\\n",
       "6252              0.897             -0.677              0.800   \n",
       "4684              0.183              0.084              5.400   \n",
       "1731              0.165              0.045             -3.200   \n",
       "4742             -0.079              2.109              9.400   \n",
       "4521              0.273              0.621             -6.300   \n",
       "\n",
       "      0001-lstsq_target  0002-lstsq_target  0003-lstsq_target  \\\n",
       "6252             -6.800              9.100              9.100   \n",
       "4684             -9.800              8.800              3.400   \n",
       "1731              5.200             -8.000             -5.700   \n",
       "4742             -1.800             -2.300             -4.800   \n",
       "4521              4.600             -0.900             -7.800   \n",
       "\n",
       "      0010-lstsq_target  0011-lstsq_target  0012-lstsq_target  \\\n",
       "6252             -0.100              4.400              1.800   \n",
       "4684              7.300              1.700             -1.700   \n",
       "1731              3.400              1.100              1.300   \n",
       "4742              0.100              7.800             -2.400   \n",
       "4521              2.100             -9.800              8.200   \n",
       "\n",
       "      0020-lstsq_target  0021-lstsq_target  0030-lstsq_target  \\\n",
       "6252             -4.300             -6.100             -6.400   \n",
       "4684              2.500              1.800              2.600   \n",
       "1731             -5.800              2.900              2.400   \n",
       "4742             -3.600              6.700             -2.900   \n",
       "4521              2.100              7.000              0.500   \n",
       "\n",
       "      0100-lstsq_target  0101-lstsq_target  0102-lstsq_target  \\\n",
       "6252              4.400              8.200             -6.500   \n",
       "4684              6.300              1.800             -2.400   \n",
       "1731              2.200             -3.500              2.700   \n",
       "4742              4.100              7.400              5.500   \n",
       "4521              1.300              8.100              1.700   \n",
       "\n",
       "      0110-lstsq_target  0111-lstsq_target  0120-lstsq_target  \\\n",
       "6252              5.400              5.800             -5.100   \n",
       "4684             -2.200             -0.200             -5.700   \n",
       "1731              3.700             -0.200              5.700   \n",
       "4742             -7.000             -6.600             -2.600   \n",
       "4521             -1.100              4.600             -0.600   \n",
       "\n",
       "      0200-lstsq_target  0201-lstsq_target  0210-lstsq_target  \\\n",
       "6252             -8.500             -8.400             -5.300   \n",
       "4684              6.600             -9.900             -0.200   \n",
       "1731             -7.900             -8.700             -2.500   \n",
       "4742              1.500              4.100             -1.000   \n",
       "4521              2.700              1.800            -10.000   \n",
       "\n",
       "      0300-lstsq_target  1000-lstsq_target  1001-lstsq_target  \\\n",
       "6252             -5.100              8.300              5.000   \n",
       "4684            -10.000              6.500              5.000   \n",
       "1731             -9.600              1.300             -0.300   \n",
       "4742             -3.800             -9.200             -6.500   \n",
       "4521             -2.100              3.400              7.000   \n",
       "\n",
       "      1002-lstsq_target  1010-lstsq_target  1011-lstsq_target  \\\n",
       "6252              6.400              1.700             -8.800   \n",
       "4684             -8.900             -8.000            -10.000   \n",
       "1731              8.000             -5.800              5.200   \n",
       "4742             -9.800              4.700              7.100   \n",
       "4521             -3.900             -5.700             -7.600   \n",
       "\n",
       "      1020-lstsq_target  1100-lstsq_target  1101-lstsq_target  \\\n",
       "6252             -3.100             -6.200              5.800   \n",
       "4684             -8.600              5.800              5.100   \n",
       "1731             -0.400              5.800             -0.300   \n",
       "4742             -6.600             -0.000             -2.300   \n",
       "4521             -0.700              6.100              0.800   \n",
       "\n",
       "      1110-lstsq_target  1200-lstsq_target  2000-lstsq_target  \\\n",
       "6252              1.300              6.700             -3.500   \n",
       "4684             -1.000              3.900              9.600   \n",
       "1731             -2.000             -5.400              9.100   \n",
       "4742             -3.500              5.400              1.900   \n",
       "4521             -5.200             -6.500              8.200   \n",
       "\n",
       "      2001-lstsq_target  2010-lstsq_target  2100-lstsq_target  \\\n",
       "6252             -1.400              9.700              9.300   \n",
       "4684             -0.600             -9.900              4.700   \n",
       "1731             -2.800             -7.500              9.000   \n",
       "4742             -0.100             -9.800             -8.000   \n",
       "4521             -3.700              1.300             -3.800   \n",
       "\n",
       "      3000-lstsq_target   wb_0   wb_1   wb_2   wb_3   wb_4   wb_5   wb_6  \\\n",
       "6252             -3.800  0.025 -0.064 -0.074 -0.422  0.123  0.235 -0.033   \n",
       "4684             -7.800 -0.217 -0.075 -0.191 -0.073 -0.139  0.017 -0.056   \n",
       "1731             -1.900  0.059 -0.145  0.085 -0.114  0.134  0.104  0.005   \n",
       "4742             -7.400 -0.170 -0.184 -0.241 -0.082 -0.473 -0.338 -0.085   \n",
       "4521              7.100  0.029 -0.083  0.021 -0.084  0.129  0.264  0.004   \n",
       "\n",
       "       wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  \\\n",
       "6252 -0.001 -0.402  0.185  0.071  0.073 -0.051  0.046  0.017 -0.393 -0.045   \n",
       "4684  0.157 -0.084 -0.348  0.024 -0.477 -0.061 -0.216 -0.246 -0.070 -0.126   \n",
       "1731  0.164 -0.109  0.334  0.054  0.280 -0.144  0.052  0.180  0.034  0.070   \n",
       "4742 -0.623 -0.138 -0.280 -0.322 -0.403 -0.314 -0.153 -0.391 -0.063 -0.049   \n",
       "4521  0.164 -0.061  0.459  0.223  0.353 -0.111  0.038  0.221 -0.077 -0.063   \n",
       "\n",
       "      wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  \\\n",
       "6252  0.071  0.298  0.047 -0.276 -0.013  0.303 -0.054 -0.316  0.393  0.031   \n",
       "4684  0.246  0.056 -0.020 -0.070  0.003  0.127 -0.193 -0.151 -0.515 -0.082   \n",
       "1731  0.014  0.259  0.014 -0.051 -0.064  0.208  0.063  0.093  0.224  0.010   \n",
       "4742 -0.480 -0.081 -0.265 -0.097 -0.135 -0.635 -0.060 -0.080 -0.302 -0.246   \n",
       "4521  0.148  0.435  0.100 -0.038 -0.022  0.465  0.002 -0.016  0.376  0.039   \n",
       "\n",
       "      wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  \\\n",
       "6252 -0.266 -0.360 -0.403 -0.065 -0.146 -0.387 -0.144  0.302 -0.399  0.221   \n",
       "4684 -0.067 -0.085 -0.081 -0.043 -0.032 -0.078 -0.173  0.138 -0.163 -0.455   \n",
       "1731  0.053  0.060 -0.114 -0.000 -0.021  0.017  0.055  0.350  0.084  0.169   \n",
       "4742 -0.070 -0.076 -0.066 -0.163 -0.019 -0.064 -0.111 -0.602 -0.094 -0.217   \n",
       "4521 -0.065 -0.099 -0.079 -0.015 -0.034 -0.090 -0.007  0.516 -0.015  0.221   \n",
       "\n",
       "      wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  \\\n",
       "6252  0.137  0.153  0.098 -0.073 -0.011 -0.067 -0.044  0.037  0.191 -0.352   \n",
       "4684 -0.271 -0.720 -0.306 -0.045  0.112 -0.193 -0.034  0.196  0.139 -0.067   \n",
       "1731  0.349  0.131  0.260 -0.080  0.082  0.027 -0.117  0.196  0.373  0.033   \n",
       "4742 -0.323 -0.364 -0.515 -0.194 -0.475 -0.046 -0.247 -0.711 -0.428 -0.081   \n",
       "4521  0.486 -0.222  0.326 -0.099  0.074 -0.016 -0.103  0.198  0.499 -0.045   \n",
       "\n",
       "      wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  \\\n",
       "6252 -0.345 -0.542 -0.284 -0.119  0.197  0.197 -0.026  0.217 -0.201 -0.004   \n",
       "4684 -0.086 -0.116 -0.054 -0.042 -0.189  0.150 -0.020 -0.519 -0.050 -0.022   \n",
       "1731 -0.021 -0.234 -0.081  0.050  0.094  0.222 -0.097  0.123 -0.010  0.056   \n",
       "4742 -0.079 -0.084 -0.070 -0.027 -0.210 -0.574 -0.180 -0.398 -0.103 -0.327   \n",
       "4521 -0.084 -0.119 -0.051 -0.026  0.172  0.417 -0.072  0.007 -0.036  0.032   \n",
       "\n",
       "      wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  \\\n",
       "6252  0.161  0.310  0.042  0.101 -0.471 -0.463 -0.017 -0.254  0.038 -0.353   \n",
       "4684 -0.379 -0.194  0.024  0.118 -0.081 -0.061 -0.274 -0.074  0.043 -0.070   \n",
       "1731  0.095  0.130  0.030  0.193 -0.034 -0.174  0.067 -0.078  0.014 -0.057   \n",
       "4742 -0.189 -0.400 -0.247 -0.382 -0.076 -0.107 -0.085 -0.187 -0.360 -0.070   \n",
       "4521  0.105  0.271  0.082  0.380 -0.109 -0.078  0.003 -0.065  0.031 -0.089   \n",
       "\n",
       "      wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  \\\n",
       "6252  0.137 -0.309 -0.294  0.366 -0.340 -0.147  0.257  0.184 -0.413 -0.355   \n",
       "4684 -0.224 -0.056 -0.175  0.134 -0.087 -0.045  0.130  0.119 -0.133 -0.191   \n",
       "1731  0.131 -0.057  0.105  0.319  0.014 -0.053  0.192  0.083 -0.246  0.161   \n",
       "4742 -0.126 -0.077 -0.177 -0.427 -0.087 -0.088 -0.687 -0.596 -0.077 -0.085   \n",
       "4521  0.206 -0.080 -0.015  0.495 -0.115 -0.048  0.421  0.237 -0.155 -0.110   \n",
       "\n",
       "      wb_77  wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  \\\n",
       "6252 -0.102  0.078 -0.536 -0.006 -0.035 -0.407  0.226 -0.289  0.091 -0.002   \n",
       "4684 -0.078 -0.395 -0.086 -0.167 -0.018 -0.078  0.136 -0.073  0.089  0.017   \n",
       "1731 -0.057  0.084 -0.042  0.056  0.044 -0.074  0.144 -0.071  0.209 -0.038   \n",
       "4742 -0.084 -0.276 -0.076 -0.211 -0.389 -0.098 -0.635 -0.121 -0.264 -0.175   \n",
       "4521 -0.039  0.061 -0.062  0.018  0.042 -0.117  0.388 -0.065  0.338  0.018   \n",
       "\n",
       "      wb_87  wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  \\\n",
       "6252 -0.448  0.271  0.070  0.087 -0.013 -0.272 -0.278  0.129 -0.567  0.001   \n",
       "4684 -0.088 -0.394  0.088 -0.495  0.102 -0.101 -0.144  0.334 -0.105 -0.443   \n",
       "1731 -0.027  0.134  0.020  0.099 -0.101 -0.143  0.070  0.268 -0.074  0.075   \n",
       "4742 -0.075 -0.259 -0.546 -0.238 -0.356 -0.050 -0.084 -0.806 -0.088 -0.241   \n",
       "4521 -0.032  0.224  0.061 -0.065 -0.030 -0.107 -0.022  0.336 -0.057 -0.146   \n",
       "\n",
       "      wb_97  wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  \\\n",
       "6252  0.237  0.319 -0.407   0.140  -0.270  -0.245  -0.033  -0.394  -0.236   \n",
       "4684  0.056 -0.321 -0.074   0.246  -0.049  -0.055  -0.085  -0.145  -0.059   \n",
       "1731  0.271  0.133 -0.054   0.225  -0.047  -0.149   0.011   0.061   0.007   \n",
       "4742 -0.152 -0.381 -0.078  -0.560  -0.068  -0.051  -0.230  -0.108  -0.051   \n",
       "4521  0.443  0.256 -0.087   0.460  -0.071  -0.056  -0.003  -0.040  -0.074   \n",
       "\n",
       "      wb_106  wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  \\\n",
       "6252   0.333  -0.577  -0.496  -0.293  -0.001   0.056   0.304  -0.146   0.024   \n",
       "4684   0.124  -0.102  -0.085  -0.073  -0.006  -0.063   0.128  -0.161  -0.653   \n",
       "1731   0.189  -0.099  -0.221  -0.071   0.066   0.039   0.259   0.116   0.101   \n",
       "4742  -0.834  -0.087  -0.084  -0.127  -0.205  -0.259  -0.480  -0.146  -0.296   \n",
       "4521   0.400  -0.097  -0.108  -0.065   0.079   0.109   0.481  -0.051  -0.161   \n",
       "\n",
       "      wb_115  wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  \\\n",
       "6252  -0.027  -0.204  -0.454  -0.441  -0.239   0.339   0.164  -0.353  -0.233   \n",
       "4684  -0.008  -0.087  -0.078  -0.070  -0.093   0.120   0.263  -0.064  -0.055   \n",
       "1731   0.035   0.094  -0.068  -0.053  -0.137   0.294   0.292  -0.049  -0.157   \n",
       "4742  -0.245  -0.071  -0.079  -0.065  -0.092  -0.214  -0.513  -0.051  -0.057   \n",
       "4521   0.014  -0.041  -0.102  -0.042  -0.042   0.461   0.415  -0.071  -0.061   \n",
       "\n",
       "      wb_124  wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  \\\n",
       "6252   0.128  -0.257   0.005   0.221  -0.026   0.076  -0.216  -0.085   0.101   \n",
       "4684  -0.098  -0.086  -0.416   0.373  -0.019  -0.234  -0.134  -0.030   0.041   \n",
       "1731   0.308  -0.106   0.089   0.271   0.040   0.158  -0.277  -0.045   0.073   \n",
       "4742  -0.318  -0.054  -0.255  -0.616  -0.269  -0.234  -0.121  -0.103  -0.399   \n",
       "4521   0.436  -0.076  -0.174   0.395   0.026   0.215  -0.101  -0.038   0.198   \n",
       "\n",
       "      wb_133  wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  \\\n",
       "6252   0.022  -0.013   0.100   0.086  -0.043   0.297   0.250  -0.439  -0.075   \n",
       "4684  -0.293   0.004   0.080   0.013  -0.183  -0.254  -0.475  -0.090  -0.054   \n",
       "1731   0.047   0.008   0.174   0.109   0.071   0.127   0.103  -0.073  -0.092   \n",
       "4742  -0.262  -0.133  -0.580  -0.343  -0.148  -0.495  -0.356  -0.088  -0.132   \n",
       "4521   0.065   0.008   0.207   0.259  -0.041   0.229   0.099  -0.122  -0.047   \n",
       "\n",
       "      wb_142  wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  \\\n",
       "6252   0.004   0.099   0.269  -0.370   0.086  -0.427   0.077   0.300  -0.066   \n",
       "4684  -0.540  -0.342   0.120  -0.077  -0.138  -0.075   0.116   0.522  -0.061   \n",
       "1731   0.128   0.061   0.371   0.049   0.080  -0.095   0.117   0.378  -0.121   \n",
       "4742  -0.474  -0.156  -0.307  -0.078  -0.408  -0.066  -0.319  -0.845  -0.067   \n",
       "4521  -0.226   0.033   0.481  -0.107   0.052  -0.074   0.288   0.500  -0.032   \n",
       "\n",
       "      wb_151  wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  \\\n",
       "6252  -0.354   0.146   0.075  -0.078   0.162  -0.235   0.017   0.095  -0.502   \n",
       "4684  -0.081  -0.365   0.063  -0.056   0.371  -0.064   0.049  -0.454  -0.087   \n",
       "1731   0.057   0.081   0.064   0.030   0.254  -0.065   0.056   0.072  -0.041   \n",
       "4742  -0.075  -0.263  -0.284  -0.149  -0.557  -0.077  -0.604  -0.171  -0.077   \n",
       "4521  -0.018   0.100   0.171  -0.044   0.365  -0.053   0.055  -0.024  -0.094   \n",
       "\n",
       "      wb_160  wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  \\\n",
       "6252   0.225   0.223   0.116  -0.441   0.034   0.108   0.089  -0.171   0.201   \n",
       "4684  -0.703   0.121  -0.040  -0.080   0.036   0.324   0.059  -0.130   0.146   \n",
       "1731   0.149   0.295   0.171  -0.066   0.208   0.048   0.125  -0.271   0.139   \n",
       "4742  -0.343  -0.391  -0.533  -0.086  -0.466  -0.596  -0.333  -0.125  -0.504   \n",
       "4521  -0.085   0.382   0.192  -0.126   0.234   0.245   0.293  -0.114   0.385   \n",
       "\n",
       "      wb_169  wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  wb_176  wb_177  \\\n",
       "6252   0.042  -0.354   0.039   0.099   0.197   0.070  -0.078   0.137  -0.229   \n",
       "4684   0.234  -0.116   0.085  -0.692   0.468  -0.408  -0.166   0.062  -0.177   \n",
       "1731   0.069   0.100  -0.010   0.144   0.299   0.095  -0.082   0.210  -0.169   \n",
       "4742  -0.433  -0.081  -0.287  -0.350  -0.691  -0.300  -0.058   0.101  -0.063   \n",
       "4521   0.099  -0.077   0.174  -0.121   0.451  -0.161  -0.058   0.212  -0.124   \n",
       "\n",
       "      wb_178  wb_179  wb_180  wb_181  wb_182  wb_183  wb_184  wb_185  wb_186  \\\n",
       "6252  -0.093  -0.233  -0.107   0.015  -0.122   0.022  -0.210   0.111  -0.128   \n",
       "4684   0.116  -0.235  -0.107   0.043   0.144   0.037  -0.081   0.290  -0.033   \n",
       "1731   0.172  -0.005  -0.041   0.069   0.141   0.072   0.093   0.140   0.106   \n",
       "4742   0.134  -0.140  -0.131   0.071   0.260   0.116   0.042   0.157   0.086   \n",
       "4521   0.197  -0.124   0.029   0.152   0.161   0.400   0.126   0.239   0.111   \n",
       "\n",
       "      wb_187  wb_188  wb_189  wb_190  wb_191  wb_192  wb_193  wb_194  wb_195  \\\n",
       "6252   0.143  -0.033  -0.033  -0.571  -0.068   0.198  -0.152   0.105   0.055   \n",
       "4684   0.057  -0.097   0.073  -0.084  -0.103   0.342  -0.006   0.218   0.048   \n",
       "1731   0.176  -0.056   0.126  -0.521  -0.151   0.196   0.071   0.125   0.212   \n",
       "4742   0.141  -0.055   0.198  -0.078  -0.105   0.329  -0.003   0.124   0.124   \n",
       "4521   0.053  -0.038   0.154  -0.141  -0.070   0.180   0.176   0.200   0.320   \n",
       "\n",
       "      wb_196  wb_197  wb_198  wb_199  wb_200  wb_201  wb_202  wb_203  wb_204  \\\n",
       "6252   0.183  -0.159  -0.007  -0.351  -0.258   0.143  -0.246  -0.359  -0.079   \n",
       "4684   0.178  -0.191  -0.089  -0.088  -0.237   0.216  -0.030  -0.054   0.128   \n",
       "1731   0.233  -0.033  -0.065  -0.141   0.029   0.172  -0.398  -0.571   0.266   \n",
       "4742   0.128  -0.160  -0.001  -0.055  -0.344   0.132  -0.027  -0.051   0.108   \n",
       "4521   0.176   0.208   0.067  -0.094   0.027   0.203   0.217   0.246   0.131   \n",
       "\n",
       "      wb_205  wb_206  wb_207  wb_208  wb_209  wb_210  wb_211  wb_212  wb_213  \\\n",
       "6252   0.041  -0.033  -0.427  -0.106  -0.123  -0.507  -0.128   0.018  -0.176   \n",
       "4684   0.058   0.043  -0.098  -0.049   0.105  -0.118  -0.062   0.122  -0.372   \n",
       "1731   0.095   0.068  -0.565   0.006   0.092  -0.340   0.052   0.131  -0.197   \n",
       "4742   0.064   0.056  -0.095   0.006   0.228  -0.101  -0.039   0.193  -0.300   \n",
       "4521   0.044   0.052  -0.071  -0.017   0.292  -0.143   0.049   0.212  -0.235   \n",
       "\n",
       "      wb_214  wb_215  wb_216  wb_217  wb_218  wb_219  wb_220  wb_221  wb_222  \\\n",
       "6252  -0.262  -0.076  -0.074   0.069   0.187  -0.243   0.107  -0.573  -0.113   \n",
       "4684  -0.252   0.051   0.116   0.021   0.134   0.031   0.212  -0.118  -0.012   \n",
       "1731   0.065   0.105   0.148   0.066   0.211   0.056   0.122  -0.511  -0.264   \n",
       "4742   0.034   0.084   0.219   0.147   0.148   0.138   0.286  -0.139   0.004   \n",
       "4521   0.047   0.017   0.147   0.163   0.117   0.005   0.352  -0.238   0.345   \n",
       "\n",
       "      wb_223  wb_224  wb_225  wb_226  wb_227  wb_228  wb_229  wb_230  wb_231  \\\n",
       "6252  -0.115  -0.043  -0.170  -0.124  -0.067   0.219  -0.144  -0.267  -0.174   \n",
       "4684   0.117   0.051  -0.054  -0.118   0.057   0.152  -0.314  -0.026  -0.207   \n",
       "1731   0.371   0.058  -0.212  -0.018   0.045   0.280  -0.237  -0.024  -0.221   \n",
       "4742   0.067   0.095  -0.036  -0.185   0.127   0.135  -0.258  -0.006  -0.128   \n",
       "4521   0.333   0.217  -0.035  -0.006   0.241   0.154  -0.200  -0.101  -0.149   \n",
       "\n",
       "      wb_232  wb_233  wb_234  wb_235  wb_236  wb_237  wb_238  wb_239  wb_240  \\\n",
       "6252  -0.018  -0.178   0.131   0.111  -0.632  -0.065  -0.060   0.027  -0.111   \n",
       "4684  -0.013  -0.203   0.189   0.232  -0.075   0.133  -0.097   0.134  -0.083   \n",
       "1731   0.043  -0.085   0.159   0.108  -0.451   0.330  -0.048   0.227  -0.162   \n",
       "4742  -0.022  -0.254   0.208   0.173  -0.068   0.108  -0.045   0.119  -0.119   \n",
       "4521   0.075  -0.024   0.149   0.296  -0.110   0.307  -0.012   0.068  -0.097   \n",
       "\n",
       "      wb_241  wb_242  wb_243  wb_244  wb_245  wb_246  wb_247  wb_248  wb_249  \\\n",
       "6252  -0.377  -0.051  -0.201  -0.366  -0.192  -0.345   0.058  -0.145  -0.130   \n",
       "4684  -0.032   0.003   0.000  -0.108  -0.077  -0.083   0.103  -0.156  -0.120   \n",
       "1731  -0.229   0.046  -0.066  -0.147   0.065  -0.536   0.161  -0.031  -0.111   \n",
       "4742  -0.029   0.016   0.049  -0.042  -0.036  -0.079   0.088  -0.085  -0.143   \n",
       "4521   0.009   0.064   0.039  -0.129   0.232   0.165   0.080   0.192   0.022   \n",
       "\n",
       "      wb_250  wb_251  wb_252  wb_253  wb_254  wb_255  wb_256  wb_257  wb_258  \\\n",
       "6252  -0.153  -0.497   0.101  -0.104  -0.578  -0.065  -0.164  -0.371  -0.119   \n",
       "4684   0.044  -0.198   0.060  -0.251  -0.033  -0.086  -0.062  -0.023  -0.175   \n",
       "1731   0.323  -0.600   0.064  -0.167  -0.165  -0.036  -0.047  -0.167  -0.065   \n",
       "4742   0.038  -0.061   0.011  -0.154  -0.029   0.004   0.016   0.003  -0.107   \n",
       "4521   0.278  -0.099   0.285  -0.107  -0.136  -0.051  -0.001   0.043   0.174   \n",
       "\n",
       "      wb_259  wb_260  wb_261  wb_262  wb_263  wb_264  wb_265  wb_266  wb_267  \\\n",
       "6252  -0.166   0.090   0.136  -0.564  -0.187  -0.161  -0.114   0.192  -0.317   \n",
       "4684   0.058   0.125   0.149  -0.082  -0.196  -0.094  -0.127   0.155  -0.014   \n",
       "1731   0.120   0.080   0.125  -0.220  -0.011  -0.204   0.014   0.191  -0.016   \n",
       "4742   0.061   0.128   0.107  -0.069  -0.256  -0.172  -0.064   0.219  -0.042   \n",
       "4521   0.024   0.240   0.120  -0.149  -0.018  -0.134  -0.097   0.189   0.047   \n",
       "\n",
       "      wb_268  wb_269  wb_270  wb_271  wb_272  wb_273  wb_274  wb_275  wb_276  \\\n",
       "6252  -0.273  -0.241  -0.479  -0.118  -0.052  -0.185  -0.415   0.066  -0.220   \n",
       "4684  -0.055   0.078   0.015  -0.213   0.063  -0.206  -0.010   0.419  -0.007   \n",
       "1731  -0.049   0.072  -0.080  -0.059   0.081  -0.073  -0.232   0.096  -0.081   \n",
       "4742  -0.015   0.174   0.015  -0.087   0.051  -0.294  -0.018   0.250   0.024   \n",
       "4521  -0.072   0.101  -0.030  -0.142   0.202  -0.037   0.003   0.323   0.018   \n",
       "\n",
       "      wb_277  wb_278  wb_279  wb_280  wb_281  wb_282  wb_283  wb_284  wb_285  \\\n",
       "6252   0.070   0.080  -0.276  -0.212  -0.215  -0.195   0.002  -0.139  -0.036   \n",
       "4684   0.088   0.070   0.010  -0.043  -0.226   0.130   0.123   0.071   0.017   \n",
       "1731   0.259   0.119   0.056  -0.352  -0.078   0.117   0.343   0.138   0.056   \n",
       "4742   0.065   0.082   0.038  -0.035  -0.239   0.104   0.070   0.070   0.099   \n",
       "4521   0.229   0.075  -0.033   0.171   0.073   0.139   0.353   0.031   0.059   \n",
       "\n",
       "      wb_286  wb_287  wb_288  wb_289  wb_290  wb_291  wb_292  wb_293  wb_294  \\\n",
       "6252   0.129  -0.145  -0.221  -0.128  -0.103  -0.209  -0.444  -0.415   0.150   \n",
       "4684   0.224  -0.102  -0.195  -0.195  -0.032  -0.046  -0.026  -0.016   0.137   \n",
       "1731   0.147   0.038  -0.516  -0.034  -0.048  -0.369  -0.169  -0.116   0.419   \n",
       "4742   0.133  -0.060  -0.224  -0.067   0.008  -0.040  -0.021  -0.010   0.156   \n",
       "4521   0.213   0.246  -0.110  -0.164   0.063   0.195  -0.004  -0.051   0.364   \n",
       "\n",
       "      wb_295  wb_296  wb_297  wb_298  wb_299  wb_300  wb_301  wb_302  wb_303  \\\n",
       "6252  -0.173   0.086  -0.254   0.086   0.054   0.113  -0.129  -0.086  -0.116   \n",
       "4684  -0.067   0.440   0.042   0.084   0.167   0.168  -0.258   0.186  -0.047   \n",
       "1731   0.063   0.112   0.003   0.262   0.117   0.386  -0.111   0.073  -0.049   \n",
       "4742  -0.069   0.328   0.011   0.069   0.235   0.170  -0.148   0.204  -0.012   \n",
       "4521   0.209   0.314   0.023   0.237   0.209   0.293  -0.168   0.227   0.014   \n",
       "\n",
       "      wb_304  wb_305  wb_306  wb_307  wb_308  wb_309  wb_310  wb_311  wb_312  \\\n",
       "6252   0.065   0.293   0.065  -0.027   0.112  -0.050  -0.240   0.122  -0.139   \n",
       "4684   0.085   0.159   0.091  -0.208   0.073  -0.006  -0.127   0.298  -0.187   \n",
       "1731   0.112   0.623   0.127  -0.104   0.177  -0.005   0.014   0.149  -0.243   \n",
       "4742   0.136   0.143   0.071  -0.050   0.173   0.023  -0.094   0.191  -0.169   \n",
       "4521   0.177   0.539   0.069   0.060   0.162   0.034  -0.054   0.245  -0.112   \n",
       "\n",
       "      wb_313  wb_314  wb_315  wb_316  wb_317  wb_318  wb_319  wb_320  wb_321  \\\n",
       "6252  -0.150  -0.133  -0.298   0.024  -0.224   0.018  -0.100  -0.438  -0.155   \n",
       "4684  -0.215  -0.229  -0.016  -0.016  -0.403  -0.024   0.149  -0.054  -0.275   \n",
       "1731  -0.168  -0.163  -0.198   0.005  -0.230  -0.004   0.112  -0.538  -0.272   \n",
       "4742  -0.220  -0.234  -0.013   0.018  -0.287  -0.037   0.254  -0.059  -0.231   \n",
       "4521  -0.098  -0.143   0.227   0.172  -0.269   0.020   0.297   0.032  -0.165   \n",
       "\n",
       "      wb_322  wb_323  wb_324  wb_325  wb_326  wb_327  wb_328  wb_329  wb_330  \\\n",
       "6252  -0.220   0.061  -0.183   0.199  -0.552  -0.064   0.018  -0.076   0.074   \n",
       "4684   0.083   0.282   0.196   0.112  -0.127  -0.130  -0.105  -0.078   0.367   \n",
       "1731   0.116   0.078   0.101   0.330  -0.470  -0.094  -0.036  -0.322   0.103   \n",
       "4742   0.037   0.146   0.259   0.113  -0.123  -0.123   0.006  -0.117   0.363   \n",
       "4521   0.107   0.214   0.261   0.278  -0.167  -0.065   0.095   0.085   0.257   \n",
       "\n",
       "      wb_331  wb_332  wb_333  wb_334  wb_335  wb_336  wb_337  wb_338  wb_339  \\\n",
       "6252  -0.348  -0.219  -0.028  -0.628  -0.222  -0.097  -0.247  -0.333  -0.178   \n",
       "4684  -0.036  -0.123  -0.119  -0.058  -0.215   0.148  -0.199  -0.015  -0.019   \n",
       "1731  -0.042  -0.193  -0.060  -0.320  -0.014   0.087   0.016  -0.234   0.061   \n",
       "4742  -0.033  -0.140  -0.100  -0.048  -0.173   0.284  -0.113  -0.008   0.115   \n",
       "4521  -0.103  -0.121  -0.049  -0.142  -0.156   0.218  -0.082   0.212   0.060   \n",
       "\n",
       "      wb_340  wb_341  wb_342  wb_343  wb_344  wb_345  wb_346  wb_347  wb_348  \\\n",
       "6252   0.026   0.110   0.328  -0.086   0.022  -0.458   0.079  -0.180   0.018   \n",
       "4684   0.188   0.252   0.152  -0.166   0.173  -0.116   0.169  -0.213   0.378   \n",
       "1731   0.068   0.120   0.477  -0.028   0.128  -0.484   0.078   0.013   0.109   \n",
       "4742   0.183   0.167   0.133  -0.063   0.253  -0.048   0.123  -0.102   0.400   \n",
       "4521   0.149   0.264   0.471   0.193   0.137  -0.064   0.146  -0.170   0.294   \n",
       "\n",
       "      wb_349  wb_350  wb_351  wb_352  wb_353  wb_354  wb_355  wb_356  wb_357  \\\n",
       "6252  -0.145   0.125  -0.326   0.125   0.254  -0.131   0.045   0.013  -0.087   \n",
       "4684  -0.292   0.208  -0.158   0.222   0.081   0.020   0.236  -0.002  -0.161   \n",
       "1731  -0.233   0.151  -0.522   0.148   0.115  -0.052   0.063   0.024  -0.046   \n",
       "4742  -0.234   0.070  -0.183   0.068   0.193  -0.244   0.033  -0.082  -0.295   \n",
       "4521  -0.194   0.116  -0.322   0.142   0.021  -0.039  -0.038  -0.111  -0.011   \n",
       "\n",
       "      wb_358  wb_359  wb_360  wb_361  wb_362  wb_363  wb_364  wb_365  wb_366  \\\n",
       "6252   0.078  -0.113   0.111  -0.019  -0.380   0.102   0.013   0.150   0.046   \n",
       "4684   0.007   0.360   0.129   0.545  -0.184   0.167   0.353  -0.002   0.067   \n",
       "1731  -0.185  -0.072   0.077  -0.012  -0.533   0.124   0.022  -0.128   0.035   \n",
       "4742   0.120  -0.116  -0.006  -0.122  -0.136   0.058  -0.144   0.023  -0.024   \n",
       "4521  -0.397  -0.069  -0.102   0.048  -0.215   0.104   0.070  -0.103   0.045   \n",
       "\n",
       "      wb_367  wb_368  wb_369  wb_370  wb_371  wb_372  wb_373  wb_374  wb_375  \\\n",
       "6252  -0.097   0.007   0.104   0.067  -0.018  -0.045   0.084   0.274  -0.074   \n",
       "4684  -0.173   0.307   0.144   0.036   0.002   0.023   0.116   0.181   0.436   \n",
       "1731  -0.093  -0.040   0.087  -0.050  -0.048  -0.095   0.089   0.272   0.006   \n",
       "4742  -0.262   0.071   0.014   0.176  -0.081  -0.150   0.070   0.135  -0.047   \n",
       "4521  -0.108  -0.159  -0.014  -0.287  -0.092  -0.293   0.003   0.188   0.004   \n",
       "\n",
       "      wb_376  wb_377  wb_378  wb_379  wb_380  wb_381  wb_382  wb_383  wb_384  \\\n",
       "6252   0.149   0.016   0.006   0.220   0.080   0.099  -0.104   0.173  -0.099   \n",
       "4684   0.236  -0.016  -0.041   0.057   0.129   0.048  -0.131   0.222  -0.012   \n",
       "1731   0.152  -0.108  -0.166  -0.028   0.051   0.083  -0.330   0.232  -0.119   \n",
       "4742   0.033   0.016  -0.017   0.097   0.040   0.093  -0.096   0.106  -0.242   \n",
       "4521   0.099  -0.293  -0.381   0.087   0.101   0.036  -0.370   0.162  -0.335   \n",
       "\n",
       "      wb_385  wb_386  wb_387  wb_388  wb_389  wb_390  wb_391  wb_392  wb_393  \\\n",
       "6252   0.290   0.042   0.189   0.040  -0.152  -0.127  -0.059   0.162  -0.229   \n",
       "4684   0.186   0.429   0.518   0.355   0.134  -0.053  -0.121   0.149  -0.047   \n",
       "1731   0.187   0.034  -0.029   0.226  -0.081  -0.185  -0.017   0.144  -0.234   \n",
       "4742   0.138  -0.005  -0.080  -0.052  -0.278  -0.016  -0.230   0.249  -0.062   \n",
       "4521   0.216   0.074  -0.061   0.255  -0.029  -0.041  -0.028   0.027  -0.075   \n",
       "\n",
       "      wb_394  wb_395  wb_396  wb_397  wb_398  wb_399  wb_400  wb_401  wb_402  \\\n",
       "6252  -0.160   0.124  -0.040  -0.038   0.127   0.124   0.065  -0.023  -0.069   \n",
       "4684  -0.272   0.248  -0.112  -0.070  -0.130   0.034   0.037   0.180  -0.144   \n",
       "1731  -0.102  -0.068  -0.284  -0.329  -0.659   0.006   0.034   0.014  -0.096   \n",
       "4742  -0.413  -0.170  -0.059  -0.045  -0.017   0.133   0.038  -0.040  -0.214   \n",
       "4521  -0.083  -0.269  -0.173  -0.469  -0.293  -0.147   0.013  -0.005  -0.280   \n",
       "\n",
       "      wb_403  wb_404  wb_405  wb_406  wb_407  wb_408  wb_409  wb_410  wb_411  \\\n",
       "6252  -0.128   0.067   0.021  -0.059   0.151  -0.005   0.062   0.123   0.204   \n",
       "4684  -0.009   0.294  -0.010   0.048   0.400   0.294   0.084   0.218   0.011   \n",
       "1731  -0.122   0.164  -0.126  -0.050   0.121   0.060   0.030   0.032  -0.163   \n",
       "4742  -0.056   0.017   0.019  -0.031   0.051   0.002  -0.122  -0.037   0.033   \n",
       "4521  -0.052   0.144   0.054  -0.027   0.159   0.003   0.029  -0.214  -0.097   \n",
       "\n",
       "      wb_412  wb_413  wb_414  wb_415  wb_416  wb_417  wb_418  wb_419  wb_420  \\\n",
       "6252   0.188   0.181   0.151  -0.103   0.128   0.086   0.199   0.239  -0.096   \n",
       "4684   0.004   0.198   0.142  -0.122   0.000   0.284   0.031   0.217   0.083   \n",
       "1731  -0.224   0.228   0.032  -0.143  -0.154   0.007  -0.013   0.224  -0.092   \n",
       "4742   0.080   0.122   0.060  -0.230   0.012  -0.002   0.092   0.109  -0.202   \n",
       "4521  -0.172   0.159   0.167  -0.116  -0.120   0.001  -0.035   0.210  -0.294   \n",
       "\n",
       "      wb_421  wb_422  wb_423  wb_424  wb_425  wb_426  wb_427  wb_428  wb_429  \\\n",
       "6252  -0.121   0.065  -0.084  -0.106  -0.168   0.269  -0.138   0.141   0.269   \n",
       "4684  -0.146   0.080  -0.094  -0.145  -0.213   0.180  -0.089   0.267  -0.016   \n",
       "1731  -0.360   0.019  -0.121  -0.130  -0.852   0.170  -0.232   0.173  -0.094   \n",
       "4742  -0.115   0.033  -0.256  -0.287  -0.073   0.210  -0.171   0.070   0.036   \n",
       "4521  -0.425   0.067  -0.286  -0.171  -0.436   0.140  -0.359   0.148   0.203   \n",
       "\n",
       "      wb_430  wb_431  wb_432  wb_433  wb_434  wb_435  wb_436  wb_437  wb_438  \\\n",
       "6252   0.154  -0.094   0.203  -0.062   0.117   0.104  -0.042   0.274  -0.053   \n",
       "4684   0.243  -0.082   0.022  -0.069   0.004   0.230   0.015   0.057   0.304   \n",
       "1731   0.159  -0.020  -0.084  -0.112  -0.133  -0.008  -0.031  -0.023   0.027   \n",
       "4742   0.079  -0.275   0.078  -0.213   0.019  -0.010  -0.027   0.073  -0.046   \n",
       "4521   0.150  -0.065  -0.098  -0.267   0.093  -0.177  -0.007   0.206   0.026   \n",
       "\n",
       "      wb_439  wb_440  wb_441  wb_442  wb_443  wb_444  wb_445  wb_446  wb_447  \\\n",
       "6252  -0.136   0.005  -0.205  -0.255   0.248  -0.168   0.419  -0.035   0.133   \n",
       "4684  -0.181   0.362  -0.180  -0.185   0.175  -0.345   0.105   0.225   0.356   \n",
       "1731  -0.198   0.089  -0.343  -0.619   0.244  -0.119   0.086   0.125  -0.029   \n",
       "4742  -0.345  -0.039  -0.307  -0.095   0.126  -0.393   0.131  -0.064   0.054   \n",
       "4521  -0.165   0.188  -0.262  -0.384   0.182  -0.151   0.275   0.177  -0.139   \n",
       "\n",
       "      wb_448  wb_449  wb_450  wb_451  wb_452  wb_453  wb_454  wb_455  wb_456  \\\n",
       "6252  -0.022   0.078   0.012   0.189   0.013   0.151   0.315  -0.056  -0.134   \n",
       "4684   0.306  -0.122  -0.112   0.029  -0.048   0.234   0.201  -0.081  -0.084   \n",
       "1731   0.056  -0.288  -0.076  -0.009  -0.299   0.124   0.238  -0.206  -0.140   \n",
       "4742  -0.031  -0.017  -0.198   0.074  -0.007   0.064   0.147  -0.056  -0.377   \n",
       "4521   0.017  -0.200  -0.300  -0.019  -0.194   0.153   0.249  -0.296  -0.233   \n",
       "\n",
       "      wb_457  wb_458  wb_459  wb_460  wb_461  wb_462  wb_463  wb_464  wb_465  \\\n",
       "6252   0.363   0.087   0.122  -0.023   0.143  -0.025   0.034  -0.008  -0.065   \n",
       "4684   0.072  -0.084   0.017   0.031   0.253   0.116   0.034   0.430  -0.066   \n",
       "1731   0.080  -0.548  -0.117  -0.003   0.137  -0.069  -0.035   0.229  -0.031   \n",
       "4742   0.133  -0.011   0.023  -0.091   0.020  -0.108  -0.106  -0.048  -0.212   \n",
       "4521   0.136  -0.325   0.104   0.016   0.026  -0.299  -0.113   0.268  -0.124   \n",
       "\n",
       "      wb_466  wb_467  wb_468  wb_469  wb_470  wb_471  wb_472  wb_473  wb_474  \\\n",
       "6252   0.031   0.336   0.321   0.028  -0.059   0.010   0.168  -0.038   0.192   \n",
       "4684  -0.007   0.043   0.045   0.034   0.176  -0.130  -0.049  -0.058   0.404   \n",
       "1731  -0.041  -0.021   0.007  -0.130  -0.076  -0.070  -0.154  -0.326  -0.029   \n",
       "4742   0.015   0.061   0.065   0.174  -0.044  -0.240   0.015  -0.015  -0.113   \n",
       "4521  -0.205   0.016   0.200  -0.273  -0.263  -0.265   0.063  -0.247  -0.086   \n",
       "\n",
       "      wb_475  wb_476  wb_477  wb_478  wb_479  wb_480  wb_481  wb_482  wb_483  \\\n",
       "6252   0.092  -0.035  -0.098  -0.067   0.181  -0.334   0.034   0.037   0.146   \n",
       "4684   0.104   0.197  -0.290  -0.056   0.377  -0.136   0.058   0.167   0.377   \n",
       "1731  -0.013   0.115  -0.102  -0.010   0.060  -0.531   0.005   0.032   0.156   \n",
       "4742   0.189  -0.070  -0.279  -0.192  -0.046  -0.099   0.007  -0.007  -0.067   \n",
       "4521  -0.070   0.177  -0.264  -0.061   0.068  -0.615   0.050  -0.074   0.176   \n",
       "\n",
       "      wb_484  wb_485  wb_486  wb_487  wb_488  wb_489  wb_490  wb_491  wb_492  \\\n",
       "6252  -0.032  -0.143   0.119   0.054   0.090   0.073   0.135  -0.204  -0.089   \n",
       "4684  -0.040  -0.176   0.191   0.108   0.389   0.350  -0.008  -0.136   0.241   \n",
       "1731  -0.021  -0.082   0.069   0.065   0.144   0.155  -0.206  -0.441   0.096   \n",
       "4742  -0.107  -0.360  -0.069  -0.025   0.045   0.047   0.002  -0.223  -0.155   \n",
       "4521  -0.060  -0.089  -0.119   0.049   0.094   0.144  -0.342  -0.298   0.209   \n",
       "\n",
       "      wb_493  wb_494  wb_495  wb_496  wb_497  wb_498  wb_499  wb_500  wb_501  \\\n",
       "6252   0.141  -0.006   0.078  -0.056   0.181   0.046  -0.185  -0.103   0.129   \n",
       "4684   0.309   0.278  -0.009   0.059  -0.054  -0.015  -0.395  -0.030   0.043   \n",
       "1731   0.176  -0.078  -0.122  -0.001  -0.273  -0.009  -0.131  -0.165  -0.059   \n",
       "4742   0.073  -0.163   0.026  -0.124   0.021  -0.074  -0.367  -0.057   0.050   \n",
       "4521   0.173  -0.243  -0.207  -0.056  -0.027  -0.183  -0.317  -0.254   0.112   \n",
       "\n",
       "      wb_502  wb_503  wb_504  wb_505  wb_506  wb_507  wb_508  wb_509  wb_510  \\\n",
       "6252   0.127   0.058  -0.121  -0.086  -0.036  -0.136   0.156   0.094   0.033   \n",
       "4684   0.310   0.192  -0.101  -0.237  -0.081  -0.180   0.327  -0.118   0.515   \n",
       "1731   0.162   0.067  -0.161  -0.081  -0.286  -0.143   0.221  -0.315   0.118   \n",
       "4742   0.091   0.029  -0.162  -0.278  -0.019  -0.425   0.081  -0.007  -0.018   \n",
       "4521   0.148  -0.058  -0.225  -0.235   0.026  -0.138   0.202  -0.160   0.250   \n",
       "\n",
       "      wb_511  wb_512  wb_513  wb_514  wb_515  wb_516  wb_517  wb_518  wb_519  \\\n",
       "6252  -0.088  -0.144   0.017  -0.116  -0.158   0.122  -0.406  -0.008  -0.035   \n",
       "4684   0.070  -0.067  -0.047  -0.098  -0.263   0.215  -0.168   0.085  -0.192   \n",
       "1731  -0.081  -0.072  -0.332  -0.068  -0.177   0.071  -0.696  -0.040  -0.057   \n",
       "4742  -0.252  -0.284  -0.032  -0.247  -0.274  -0.013  -0.159  -0.084  -0.214   \n",
       "4521  -0.226  -0.068  -0.388  -0.034  -0.192  -0.147  -0.605  -0.241  -0.059   \n",
       "\n",
       "      wb_520  wb_521  wb_522  wb_523  wb_524  wb_525  wb_526  wb_527  wb_528  \\\n",
       "6252   0.230  -0.074  -0.047  -0.126  -0.019   0.061   0.059   0.109   0.171   \n",
       "4684   0.129  -0.059   0.460  -0.330   0.191   0.079   0.030   0.146   0.019   \n",
       "1731   0.103  -0.083   0.100  -0.109   0.105   0.077   0.181   0.179   0.029   \n",
       "4742   0.156  -0.085  -0.070  -0.366  -0.082   0.151  -0.000   0.223  -0.094   \n",
       "4521   0.080  -0.126   0.248  -0.296   0.150   0.104  -0.212   0.217   0.120   \n",
       "\n",
       "      wb_529  wb_530  wb_531  wb_532  wb_533  wb_534  wb_535  wb_536  wb_537  \\\n",
       "6252  -0.446   0.048  -0.179  -0.670   0.076  -0.542   0.237  -0.619   0.153   \n",
       "4684  -0.538  -0.043  -0.260  -0.854  -0.056  -0.606   0.154  -0.637   0.142   \n",
       "1731  -0.054   0.090  -0.252  -0.134  -0.178  -0.025   0.157  -0.053   0.301   \n",
       "4742  -0.133   0.186  -0.095  -0.146  -0.136  -0.098   0.306  -0.089   0.172   \n",
       "4521  -0.087   0.086  -0.197  -0.112  -0.422   0.092   0.227  -0.006   0.079   \n",
       "\n",
       "      wb_538  wb_539  wb_540  wb_541  wb_542  wb_543  wb_544  wb_545  wb_546  \\\n",
       "6252   0.051  -0.447   0.201  -0.051  -0.029  -0.040   0.194   0.019  -0.041   \n",
       "4684   0.015  -0.453   0.012  -0.097  -0.057  -0.193   0.141  -0.082  -0.058   \n",
       "1731   0.055  -0.077  -0.091  -0.203   0.005   0.008   0.183  -0.304  -0.038   \n",
       "4742   0.128  -0.056  -0.004  -0.091   0.045   0.133   0.272  -0.160  -0.011   \n",
       "4521   0.058  -0.030   0.147  -0.103   0.109   0.198   0.177  -0.362  -0.139   \n",
       "\n",
       "      wb_547  wb_548  wb_549  wb_550  wb_551  wb_552  wb_553  wb_554  wb_555  \\\n",
       "6252   0.039  -0.131   0.192  -0.246   0.161   0.061   0.101   0.180   0.110   \n",
       "4684  -0.023  -0.173   0.101  -0.480   0.114  -0.072  -0.069   0.100   0.154   \n",
       "1731   0.091  -0.336   0.165  -0.017   0.157  -0.315  -0.349   0.340   0.235   \n",
       "4742   0.181  -0.142   0.099  -0.106   0.264  -0.092  -0.086   0.097   0.192   \n",
       "4521   0.288  -0.154   0.291   0.016   0.131  -0.260  -0.266   0.205   0.139   \n",
       "\n",
       "      wb_556  wb_557  wb_558  wb_559  wb_560  wb_561  wb_562  wb_563  wb_564  \\\n",
       "6252   0.059   0.198   0.094  -0.105   0.242  -0.191  -0.182  -0.313  -0.709   \n",
       "4684   0.011   0.029   0.132  -0.173   0.111  -0.352  -0.515  -0.296  -0.738   \n",
       "1731   0.030  -0.103   0.120  -0.006   0.156  -0.021  -0.012  -0.137  -0.050   \n",
       "4742  -0.035  -0.003   0.161   0.061   0.125  -0.018   0.007  -0.092  -0.183   \n",
       "4521   0.062   0.016   0.188   0.359   0.357  -0.022   0.148  -0.341   0.013   \n",
       "\n",
       "      wb_565  wb_566  wb_567  wb_568  wb_569  wb_570  wb_571  wb_572  wb_573  \\\n",
       "6252   0.226  -0.515  -0.089   0.114  -0.721   0.165   0.268   0.059   0.271   \n",
       "4684   0.164  -0.673  -0.127   0.156  -0.846  -0.187   0.107  -0.082   0.118   \n",
       "1731   0.428  -0.144  -0.277   0.343  -0.097   0.007   0.080  -0.338   0.351   \n",
       "4742   0.193  -0.122  -0.187   0.209  -0.225   0.206   0.088  -0.090   0.068   \n",
       "4521   0.176  -0.165  -0.145   0.123  -0.078   0.369   0.194  -0.412  -0.092   \n",
       "\n",
       "      wb_574  wb_575  wb_576  wb_577  wb_578  wb_579  wb_580  wb_581  wb_582  \\\n",
       "6252   0.106   0.043  -0.089   0.076   0.042  -0.049   0.219   0.126  -0.026   \n",
       "4684  -0.008  -0.015  -0.204   0.013   0.086  -0.108   0.125   0.140  -0.236   \n",
       "1731  -0.030  -0.078   0.006   0.038   0.148   0.009   0.355   0.180  -0.001   \n",
       "4742  -0.107  -0.024  -0.010   0.136   0.143   0.100   0.126   0.142   0.045   \n",
       "4521  -0.114   0.010   0.012   0.304   0.020  -0.084   0.227   0.150  -0.043   \n",
       "\n",
       "      wb_583  wb_584  wb_585  wb_586  wb_587  wb_588  wb_589  wb_590  wb_591  \\\n",
       "6252  -0.063  -0.048   0.285   0.228   0.181   0.021   0.149  -0.091   0.151   \n",
       "4684  -0.218  -0.154   0.087   0.011   0.062   0.007   0.202  -0.157  -0.006   \n",
       "1731   0.054  -0.033   0.083  -0.047   0.128   0.003   0.406  -0.028  -0.029   \n",
       "4742   0.118   0.015   0.324  -0.002  -0.024   0.071   0.239  -0.060  -0.015   \n",
       "4521   0.030   0.010   0.323   0.159  -0.060   0.034   0.239  -0.005   0.066   \n",
       "\n",
       "      wb_592  wb_593  wb_594  wb_595  wb_596  wb_597  wb_598  wb_599  wb_600  \\\n",
       "6252  -0.056   0.132   0.224  -0.172   0.165   0.060   0.025  -0.069   0.315   \n",
       "4684  -0.220  -0.005   0.179  -0.196  -0.033   0.084  -0.021  -0.159   0.147   \n",
       "1731  -0.003  -0.011   0.292   0.001  -0.189   0.187   0.078   0.057   0.382   \n",
       "4742   0.003  -0.065   0.227   0.025  -0.061   0.116   0.136   0.030   0.059   \n",
       "4521   0.035   0.057   0.341   0.293  -0.198   0.100   0.268   0.119  -0.198   \n",
       "\n",
       "      wb_601  wb_602  wb_603  wb_604  wb_605  wb_606  wb_607  wb_608  wb_609  \\\n",
       "6252   0.119  -0.083   0.053   0.428   0.112  -0.517   0.148   0.095   0.229   \n",
       "4684  -0.065  -0.178   0.023   0.116   0.181  -0.572  -0.021   0.045   0.141   \n",
       "1731  -0.284  -0.344   0.049   0.198   0.182  -0.260  -0.062   0.117   0.461   \n",
       "4742  -0.142  -0.175   0.134   0.100   0.245  -0.189  -0.068   0.175   0.156   \n",
       "4521  -0.053  -0.356   0.048   0.416   0.182  -0.166   0.045   0.252   0.226   \n",
       "\n",
       "      wb_610  wb_611  wb_612  wb_613  wb_614  wb_615  wb_616  wb_617  wb_618  \\\n",
       "6252   0.221   0.060   0.326  -0.190  -0.178  -0.344  -0.018   0.299   0.185   \n",
       "4684  -0.047   0.080   0.104  -0.335  -0.265  -0.339  -0.114   0.107   0.123   \n",
       "1731   0.028   0.182   0.154  -0.019  -0.033  -0.070   0.047   0.269   0.200   \n",
       "4742   0.245   0.158   0.090  -0.076  -0.085  -0.063  -0.061   0.045   0.126   \n",
       "4521   0.260   0.102   0.390  -0.020  -0.004  -0.198  -0.223  -0.055   0.280   \n",
       "\n",
       "      wb_619  wb_620  wb_621  wb_622  wb_623  wb_624  wb_625  wb_626  wb_627  \\\n",
       "6252  -0.618   0.358  -0.377   0.032  -0.112   0.277   0.245   0.132   0.058   \n",
       "4684  -0.761   0.091  -0.319  -0.202  -0.300   0.078   0.108  -0.004   0.041   \n",
       "1731  -0.054   0.174  -0.157   0.014   0.022   0.084   0.071  -0.004   0.109   \n",
       "4742  -0.162   0.076  -0.109   0.193   0.047   0.021   0.310  -0.047   0.005   \n",
       "4521   0.091   0.414  -0.234   0.204   0.001   0.102   0.383   0.061  -0.134   \n",
       "\n",
       "      wb_628  wb_629  wb_630  wb_631  wb_632  wb_633  wb_634  wb_635  wb_636  \\\n",
       "6252   0.162   0.249   0.093  -0.191   0.271   0.188   0.222  -0.241   0.179   \n",
       "4684   0.204   0.178  -0.039  -0.228   0.092   0.086   0.146  -0.288   0.089   \n",
       "1731   0.268   0.362  -0.170   0.040   0.201   0.237   0.462  -0.047   0.123   \n",
       "4742   0.274   0.185  -0.045   0.031   0.058   0.025   0.165  -0.046   0.261   \n",
       "4521   0.175   0.355  -0.186   0.183   0.295  -0.209   0.230  -0.031   0.164   \n",
       "\n",
       "      wb_637  wb_638  wb_639  wb_640  wb_641  wb_642  wb_643  wb_644  wb_645  \\\n",
       "6252   0.026   0.020  -0.499  -0.329   0.026   0.279   0.303  -0.012  -0.112   \n",
       "4684  -0.052  -0.124  -0.460  -0.352  -0.118   0.017   0.060  -0.082  -0.168   \n",
       "1731   0.040  -0.257  -0.211  -0.250  -0.366   0.054   0.129  -0.166   0.003   \n",
       "4742   0.234  -0.182  -0.108  -0.151  -0.126   0.010   0.060  -0.163   0.079   \n",
       "4521   0.313  -0.086  -0.315  -0.148  -0.241   0.170   0.326  -0.351   0.267   \n",
       "\n",
       "      wb_646  wb_647  wb_648  wb_649  wb_650  wb_651  wb_652  wb_653  wb_654  \\\n",
       "6252   0.136   0.246   0.031  -0.043   0.040  -0.343  -0.099  -0.365  -0.065   \n",
       "4684  -0.027   0.086   0.035  -0.357  -0.009  -0.289  -0.230  -0.400  -0.307   \n",
       "1731   0.008   0.200   0.091  -0.005  -0.015  -0.188  -0.004  -0.218  -0.016   \n",
       "4742   0.196   0.063  -0.014   0.054  -0.081  -0.112   0.014  -0.148   0.011   \n",
       "4521   0.349   0.202  -0.204   0.164  -0.133  -0.252   0.282  -0.129   0.035   \n",
       "\n",
       "      wb_655  wb_656  wb_657  wb_658  wb_659  wb_660  wb_661  wb_662  wb_663  \\\n",
       "6252  -0.055   0.061   0.219  -0.197  -0.169  -0.525   0.203   0.016   0.075   \n",
       "4684  -0.082   0.080   0.248  -0.323  -0.185  -0.644   0.075  -0.033  -0.072   \n",
       "1731  -0.075   0.168   0.191  -0.136  -0.109  -0.051   0.086  -0.057   0.124   \n",
       "4742  -0.130   0.114   0.301  -0.046  -0.068  -0.167   0.248  -0.038   0.252   \n",
       "4521  -0.595   0.080   0.163  -0.106  -0.074  -0.017   0.230  -0.064   0.083   \n",
       "\n",
       "      wb_664  wb_665  wb_666  wb_667  wb_668  wb_669  wb_670  wb_671  wb_672  \\\n",
       "6252  -0.024   0.136  -0.001  -0.534   0.007  -0.060   0.124   0.002   0.267   \n",
       "4684  -0.154  -0.037  -0.106  -0.451  -0.128  -0.290  -0.039   0.034   0.095   \n",
       "1731   0.033  -0.118  -0.024  -0.266  -0.010  -0.007  -0.268   0.038   0.265   \n",
       "4742   0.118  -0.046  -0.146  -0.173   0.061   0.083  -0.065   0.033   0.067   \n",
       "4521  -0.029  -0.218  -0.217  -0.347  -0.051   0.299  -0.018   0.029   0.154   \n",
       "\n",
       "      wb_673  wb_674  wb_675  wb_676  wb_677  wb_678  wb_679  wb_680  wb_681  \\\n",
       "6252   0.199  -0.334  -0.051   0.229   0.064   0.201  -0.038   0.005   0.243   \n",
       "4684   0.099  -0.531  -0.104   0.081  -0.058   0.205  -0.123  -0.111   0.114   \n",
       "1731   0.081  -0.025  -0.104   0.059   0.063   0.148  -0.172  -0.004   0.308   \n",
       "4742   0.229  -0.053  -0.077   0.078   0.183   0.270  -0.149   0.118   0.123   \n",
       "4521   0.250   0.312  -0.300   0.311   0.038   0.149  -0.155   0.281   0.243   \n",
       "\n",
       "      wb_682  wb_683  wb_684  wb_685  wb_686  wb_687  wb_688  wb_689  wb_690  \\\n",
       "6252  -0.470  -0.042   0.375  -0.376  -0.185  -0.533   0.205  -0.590  -0.002   \n",
       "4684  -0.531  -0.120   0.105  -0.418  -0.276  -0.662   0.006  -0.731  -0.134   \n",
       "1731  -0.194  -0.049   0.150  -0.066  -0.017  -0.055   0.001  -0.050   0.042   \n",
       "4742  -0.199   0.033   0.065  -0.063  -0.005  -0.159  -0.009  -0.134   0.002   \n",
       "4521  -0.123  -0.109   0.207  -0.277   0.236  -0.044  -0.203   0.001   0.172   \n",
       "\n",
       "      wb_691  wb_692  wb_693  wb_694  wb_695  wb_696  wb_697  wb_698  wb_699  \\\n",
       "6252   0.255  -0.042   0.152  -0.201   0.136   0.105  -0.530  -0.017  -0.244   \n",
       "4684   0.094  -0.005   0.104  -0.313  -0.038   0.088  -0.505  -0.157  -0.236   \n",
       "1731   0.111   0.114   0.121  -0.058  -0.202   0.183  -0.098  -0.001  -0.115   \n",
       "4742   0.306  -0.067   0.281  -0.041  -0.095   0.186  -0.109   0.136  -0.096   \n",
       "4521   0.263  -0.512   0.259   0.001   0.033   0.169  -0.301   0.340  -0.234   \n",
       "\n",
       "      wb_700  wb_701  wb_702  wb_703  wb_704  wb_705  wb_706  wb_707  wb_708  \\\n",
       "6252  -0.020   0.054  -0.014   0.187   0.039   0.073   0.027   0.063   0.128   \n",
       "4684  -0.043  -0.020  -0.038  -0.107   0.067   0.312  -0.012   0.115  -0.148   \n",
       "1731   0.012   0.064   0.006   0.158  -0.046  -0.081   0.026  -0.075   0.239   \n",
       "4742  -0.009  -0.039  -0.014  -0.022   0.197   0.091   0.034   0.324  -0.037   \n",
       "4521   0.006   0.083   0.023   0.203   0.001  -0.044   0.016  -0.032   0.118   \n",
       "\n",
       "      wb_709  wb_710  wb_711  wb_712  wb_713  wb_714  wb_715  wb_716  wb_717  \\\n",
       "6252   0.147  -0.001   0.110   0.046  -0.002   0.089  -0.128  -0.011   0.031   \n",
       "4684   0.403   0.097   0.310  -0.016   0.010   0.268  -0.124  -0.015   0.065   \n",
       "1731  -0.026  -0.024  -0.040   0.038   0.004  -0.035   0.124   0.033  -0.020   \n",
       "4742   0.196  -0.071   0.253  -0.040  -0.002   0.192  -0.126   0.009   0.073   \n",
       "4521  -0.083  -0.014  -0.063   0.067   0.003   0.000   0.181   0.005  -0.034   \n",
       "\n",
       "      wb_718  wb_719  wb_720  wb_721  wb_722  wb_723  wb_724  wb_725  wb_726  \\\n",
       "6252   0.111   0.004   0.104   0.000   0.101  -0.005   0.023   0.157   0.002   \n",
       "4684   0.406   0.075  -0.122   0.001   0.319  -0.010  -0.048   0.326   0.079   \n",
       "1731  -0.018  -0.005   0.082   0.011  -0.068   0.015   0.038  -0.054  -0.015   \n",
       "4742  -0.017  -0.057  -0.009   0.033   0.255   0.002  -0.052   0.150  -0.052   \n",
       "4521  -0.071  -0.021   0.061   0.047  -0.075   0.041   0.071  -0.074  -0.013   \n",
       "\n",
       "      wb_727  wb_728  wb_729  wb_730  wb_731  wb_732  wb_733  wb_734  wb_735  \\\n",
       "6252  -0.093  -0.137   0.176   0.075   0.068  -0.110   0.064   0.144  -0.028   \n",
       "4684  -0.124  -0.149  -0.068  -0.012  -0.033  -0.065  -0.050   0.346  -0.053   \n",
       "1731   0.106   0.145  -0.045  -0.003   0.037   0.120   0.053  -0.021   0.017   \n",
       "4742  -0.108  -0.149  -0.075  -0.034   0.000  -0.104  -0.026   0.245  -0.061   \n",
       "4521   0.062   0.107   0.187   0.044   0.069   0.217   0.060  -0.080   0.070   \n",
       "\n",
       "      wb_736  wb_737  wb_738  wb_739  wb_740  wb_741  wb_742  wb_743  wb_744  \\\n",
       "6252   0.044   0.263  -0.051   0.071   0.140   0.054  -0.018   0.044   0.044   \n",
       "4684   0.182   0.386  -0.009   0.151  -0.012   0.093   0.023  -0.007   0.110   \n",
       "1731  -0.040  -0.033  -0.036  -0.031  -0.020  -0.035   0.016   0.002  -0.063   \n",
       "4742   0.152   0.128   0.200   0.332  -0.045   0.235   0.024  -0.053   0.415   \n",
       "4521  -0.030  -0.072   0.014  -0.061   0.001  -0.013  -0.013   0.035  -0.025   \n",
       "\n",
       "      wb_745  wb_746  wb_747  wb_748  wb_749  wb_750  wb_751  wb_752  wb_753  \\\n",
       "6252  -0.068  -0.075  -0.014   0.175   0.115  -0.041   0.076   0.051   0.026   \n",
       "4684   0.410  -0.062  -0.136  -0.091  -0.087  -0.020   0.155   0.154   0.015   \n",
       "1731  -0.026   0.110   0.093   0.074   0.163   0.030  -0.039  -0.035  -0.028   \n",
       "4742  -0.028  -0.066  -0.149  -0.165  -0.007  -0.029   0.082   0.167  -0.041   \n",
       "4521  -0.074   0.172   0.079   0.209   0.114   0.057  -0.039  -0.065   0.068   \n",
       "\n",
       "      wb_754  wb_755  wb_756  wb_757  wb_758  wb_759  wb_760  wb_761  wb_762  \\\n",
       "6252   0.037   0.035   0.021   0.031   0.117   0.029  -0.003  -0.139   0.178   \n",
       "4684   0.027  -0.023   0.047   0.141   0.311   0.112   0.206  -0.141  -0.100   \n",
       "1731  -0.040  -0.016  -0.035  -0.054  -0.082  -0.012  -0.041   0.175   0.033   \n",
       "4742   0.172  -0.022   0.089   0.076   0.160   0.058  -0.081  -0.141  -0.083   \n",
       "4521   0.038   0.009   0.018   0.039  -0.058   0.006  -0.040   0.224   0.172   \n",
       "\n",
       "      wb_763  wb_764  wb_765  wb_766  wb_767  wb_768  wb_769  wb_770  wb_771  \\\n",
       "6252  -0.013   0.172  -0.019  -0.081   0.028   0.046   0.040   0.165  -0.087   \n",
       "4684   0.001  -0.014   0.039  -0.094   0.140  -0.083  -0.047   0.391  -0.073   \n",
       "1731  -0.005  -0.047   0.020   0.205  -0.023   0.207   0.053  -0.013   0.122   \n",
       "4742  -0.014  -0.053   0.040  -0.092   0.076  -0.024  -0.036   0.149  -0.103   \n",
       "4521   0.003   0.118  -0.015   0.224  -0.031   0.168   0.072  -0.077   0.142   \n",
       "\n",
       "      wb_772  wb_773  wb_774  wb_775  wb_776  wb_777  wb_778  wb_779  wb_780  \\\n",
       "6252   0.074   0.094   0.067   0.189  -0.091   0.036  -0.009   0.064  -0.005   \n",
       "4684   0.006   0.242   0.122  -0.046  -0.070  -0.052  -0.014  -0.132  -0.047   \n",
       "1731  -0.030  -0.059  -0.042   0.089   0.082  -0.000  -0.033   0.361   0.028   \n",
       "4742  -0.034   0.196   0.113  -0.167  -0.058  -0.001   0.067  -0.160  -0.030   \n",
       "4521   0.071  -0.068  -0.044   0.169   0.150   0.034   0.008   0.095   0.029   \n",
       "\n",
       "      wb_781  wb_782  wb_783  wb_784  wb_785  wb_786  wb_787  wb_788  wb_789  \\\n",
       "6252   0.015  -0.043   0.055   0.139  -0.006   0.014  -0.001   0.096  -0.007   \n",
       "4684   0.041  -0.112   0.205  -0.036   0.250   0.047  -0.093   0.196   0.081   \n",
       "1731   0.027   0.282  -0.070  -0.029  -0.025  -0.021   0.274  -0.060   0.020   \n",
       "4742   0.232  -0.066   0.162  -0.035  -0.058  -0.039  -0.113   0.135   0.071   \n",
       "4521  -0.013   0.231  -0.064   0.098  -0.049  -0.019   0.074  -0.049  -0.035   \n",
       "\n",
       "      wb_790  wb_791  wb_792  wb_793  wb_794  wb_795  wb_796  wb_797  wb_798  \\\n",
       "6252   0.008   0.013   0.027   0.048   0.081   0.132   0.021   0.031   0.125   \n",
       "4684   0.109   0.025  -0.010  -0.045   0.139  -0.146   0.006   0.414   0.246   \n",
       "1731  -0.054   0.047   0.037   0.065  -0.042   0.324  -0.011  -0.024  -0.082   \n",
       "4742   0.188   0.033  -0.108  -0.047   0.493  -0.152   0.167  -0.024   0.137   \n",
       "4521   0.006   0.029   0.192   0.077  -0.045   0.158   0.004  -0.078  -0.050   \n",
       "\n",
       "      wb_799  wb_800  wb_801  wb_802  wb_803  wb_804  wb_805  wb_806  wb_807  \\\n",
       "6252  -0.017  -0.059   0.024   0.105   0.033   0.139  -0.068   0.137   0.242   \n",
       "4684  -0.023   0.092  -0.076  -0.055  -0.024  -0.051  -0.065   0.312  -0.120   \n",
       "1731   0.234  -0.056   0.186   0.035   0.005   0.068   0.100  -0.072   0.243   \n",
       "4742  -0.113  -0.044  -0.031  -0.077  -0.042  -0.056  -0.092   0.175  -0.136   \n",
       "4521   0.285  -0.073   0.145   0.122   0.030   0.130   0.074  -0.078   0.248   \n",
       "\n",
       "      wb_808  wb_809  wb_810  wb_811  wb_812  wb_813  wb_814  wb_815  wb_816  \\\n",
       "6252   0.128   0.150   0.028   0.011   0.111  -0.053   0.032   0.006  -0.077   \n",
       "4684  -0.096  -0.038   0.056   0.097   0.352  -0.024   0.041   0.025  -0.076   \n",
       "1731   0.071  -0.034  -0.023  -0.013  -0.028   0.086  -0.023   0.024   0.087   \n",
       "4742  -0.140  -0.039   0.094  -0.050   0.074  -0.006   0.229   0.129  -0.073   \n",
       "4521   0.161   0.104  -0.000  -0.025  -0.069   0.058   0.012  -0.020   0.054   \n",
       "\n",
       "      wb_817  wb_818  wb_819  wb_820  wb_821  wb_822  wb_823  wb_824  wb_825  \\\n",
       "6252   0.046   0.093   0.109   0.142  -0.047   0.100   0.122   0.159   0.106   \n",
       "4684  -0.136  -0.131  -0.075   0.388   0.103  -0.062  -0.057   0.345  -0.035   \n",
       "1731   0.316   0.281   0.064  -0.012  -0.025   0.133   0.041  -0.026   0.002   \n",
       "4742  -0.130  -0.131  -0.010   0.085  -0.006  -0.103  -0.072   0.071  -0.005   \n",
       "4521   0.225   0.107   0.082  -0.070  -0.061   0.124   0.095  -0.072   0.073   \n",
       "\n",
       "      wb_826  wb_827  wb_828  wb_829  wb_830  wb_831  wb_832  wb_833  wb_834  \\\n",
       "6252   0.008   0.118   0.010   0.129   0.159   0.066  -0.023   0.134   0.005   \n",
       "4684  -0.021   0.099   0.029   0.221  -0.089   0.010   0.074   0.204   0.014   \n",
       "1731  -0.000  -0.023   0.026  -0.030   0.111  -0.025  -0.032   0.002   0.004   \n",
       "4742   0.167   0.207   0.143   0.094  -0.120  -0.026   0.011   0.110   0.077   \n",
       "4521   0.010  -0.064  -0.010   0.023   0.126   0.048  -0.027   0.017  -0.011   \n",
       "\n",
       "      wb_835  wb_836  wb_837  wb_838  wb_839  wb_840  wb_841  wb_842  wb_843  \\\n",
       "6252   0.035  -0.008  -0.018   0.087   0.062  -0.025   0.041   0.009   0.034   \n",
       "4684   0.087   0.149  -0.025   0.298   0.103  -0.130  -0.028  -0.022   0.072   \n",
       "1731  -0.037  -0.017   0.015  -0.063  -0.044   0.297  -0.004   0.023  -0.013   \n",
       "4742   0.168  -0.050   0.044   0.141   0.136  -0.127   0.030   0.280   0.046   \n",
       "4521  -0.013  -0.016   0.013   0.002   0.017   0.121   0.045   0.015   0.008   \n",
       "\n",
       "      wb_844  wb_845  wb_846  wb_847  wb_848  wb_849  wb_850  wb_851  wb_852  \\\n",
       "6252   0.129  -0.144  -0.029   0.156  -0.015   0.179   0.036  -0.100   0.068   \n",
       "4684   0.443  -0.135   0.011  -0.086   0.078   0.203  -0.045  -0.056   0.084   \n",
       "1731  -0.026   0.129  -0.016   0.071  -0.040  -0.036   0.056   0.049  -0.024   \n",
       "4742   0.017  -0.118   0.103  -0.124  -0.045   0.378  -0.016  -0.065   0.022   \n",
       "4521  -0.075   0.217  -0.017   0.236  -0.042  -0.077   0.059   0.020   0.010   \n",
       "\n",
       "      wb_853  wb_854  wb_855  wb_856  wb_857  wb_858  wb_859  wb_860  wb_861  \\\n",
       "6252  -0.009   0.007   0.055   0.004  -0.008   0.020  -0.054  -0.038   0.158   \n",
       "4684   0.068  -0.015   0.077  -0.018   0.085   0.046  -0.063   0.181   0.305   \n",
       "1731  -0.023   0.076  -0.034   0.015   0.021  -0.027   0.263  -0.079  -0.024   \n",
       "4742  -0.036  -0.028   0.079  -0.041   0.114   0.067  -0.147   0.234   0.061   \n",
       "4521  -0.014   0.030  -0.059   0.007  -0.003   0.008   0.261  -0.002  -0.055   \n",
       "\n",
       "      wb_862  wb_863  wb_864  wb_865  wb_866  wb_867  wb_868  wb_869  wb_870  \\\n",
       "6252   0.050  -0.086   0.039   0.034  -0.005   0.139   0.034   0.057  -0.085   \n",
       "4684   0.095  -0.120   0.075   0.059   0.168  -0.067   0.193   0.077  -0.065   \n",
       "1731  -0.038   0.241  -0.027  -0.026  -0.030   0.108  -0.089  -0.024   0.081   \n",
       "4742   0.200  -0.123   0.285   0.207  -0.070  -0.087   0.054   0.161  -0.065   \n",
       "4521  -0.005   0.136  -0.034  -0.040  -0.021   0.125  -0.056  -0.025   0.164   \n",
       "\n",
       "      wb_871  wb_872  wb_873  wb_874  wb_875  wb_876  wb_877  wb_878  wb_879  \\\n",
       "6252  -0.003   0.033   0.095  -0.028  -0.065  -0.321  -0.213  -0.531   0.518   \n",
       "4684   0.043   0.130   0.080  -0.034   0.322  -0.053   0.316  -0.065   0.592   \n",
       "1731  -0.004  -0.080  -0.038   0.012  -0.129  -0.584  -0.223  -0.220   0.089   \n",
       "4742  -0.031   0.275   0.065   0.169   0.194   0.216   0.276  -0.196   0.584   \n",
       "4521  -0.029  -0.004  -0.079   0.016  -0.079  -0.417  -0.208  -0.262   0.152   \n",
       "\n",
       "      wb_880  wb_881  wb_882  wb_883  wb_884  wb_885  wb_886  wb_887  wb_888  \\\n",
       "6252   0.213   0.107   0.640  -0.409   0.625   0.190   0.608  -0.368  -0.005   \n",
       "4684   0.367   0.223   0.864  -0.104   0.876   0.291   0.988  -0.060   0.261   \n",
       "1731   0.021  -0.216   0.085  -0.342   0.335  -0.062   0.217  -0.608  -0.090   \n",
       "4742   0.380   0.079   0.775  -0.209   0.355   0.421   0.457   0.332   0.174   \n",
       "4521   0.225  -0.227   0.037  -0.700   0.481   0.345   0.311  -0.119  -0.033   \n",
       "\n",
       "      wb_889  wb_890  wb_891  wb_892  wb_893  wb_894  wb_895  wb_896  wb_897  \\\n",
       "6252   0.404  -0.732  -0.021  -0.126   0.339   0.135  -0.257  -0.122   0.273   \n",
       "4684   0.648  -0.068   0.172   0.418   0.536   0.225  -0.039   0.120   0.315   \n",
       "1731   0.103  -0.538  -0.245  -0.091   0.251  -0.116  -0.348  -0.204   0.102   \n",
       "4742   0.455  -0.061   0.106   0.611   0.135   0.346  -0.231   0.147   0.701   \n",
       "4521   0.181  -0.261   0.103   0.198   0.528   0.196  -0.540  -0.202   0.615   \n",
       "\n",
       "      wb_898  wb_899  wb_900  wb_901  wb_902  wb_903  wb_904  wb_905  wb_906  \\\n",
       "6252   0.021  -0.547   0.530   0.099  -0.345  -0.502  -0.490  -0.041  -0.184   \n",
       "4684   0.249   0.198   0.902   0.262  -0.044  -0.060  -0.018   0.131  -0.004   \n",
       "1731  -0.320  -0.308   0.153  -0.133  -0.508  -0.681  -0.402  -0.196  -0.089   \n",
       "4742  -0.045   0.062   0.462   0.302  -0.044  -0.055   0.008   0.205  -0.084   \n",
       "4521  -0.055  -0.312   0.341   0.100  -0.429  -0.512  -0.260  -0.072  -0.085   \n",
       "\n",
       "      wb_907  wb_908  wb_909  wb_910  wb_911  wb_912  wb_913  wb_914  wb_915  \\\n",
       "6252  -0.601  -0.195   0.342  -0.714   0.286   0.323   0.312   0.767  -0.252   \n",
       "4684  -0.073   0.255   0.385   0.203   0.726   0.846   0.900   0.849   0.007   \n",
       "1731  -0.648  -0.193   0.339  -0.365   0.105   0.297  -0.261   0.248  -0.448   \n",
       "4742  -0.022   0.116   0.711   0.094   0.220   0.334   0.463   0.682   0.223   \n",
       "4521  -0.405  -0.169   0.754  -0.388   0.186   0.509   0.477   0.305  -0.092   \n",
       "\n",
       "      wb_916  wb_917  wb_918  wb_919  wb_920  wb_921  wb_922  wb_923  wb_924  \\\n",
       "6252   0.476  -0.078  -0.246   0.749   0.235  -0.699  -0.317  -0.624  -0.334   \n",
       "4684   0.672   0.197   0.040   0.891   0.537  -0.069  -0.037  -0.170  -0.064   \n",
       "1731  -0.005  -0.255  -0.432   0.139   0.346  -0.565  -0.514  -0.847  -0.165   \n",
       "4742   0.587  -0.289   0.299   0.933   0.544   0.006  -0.049  -0.079  -0.181   \n",
       "4521  -0.067  -0.106  -0.048   0.083   0.739  -0.337  -0.696  -0.475  -0.295   \n",
       "\n",
       "      wb_925  wb_926  wb_927  wb_928  wb_929  wb_930  wb_931  wb_932  wb_933  \\\n",
       "6252  -0.214   0.242   0.137  -0.199   0.159  -0.376  -0.118   0.150   0.328   \n",
       "4684   0.044   0.370   0.181   0.059   0.644   0.016   0.177   0.591   0.520   \n",
       "1731  -0.225   0.036   0.161  -0.312  -0.222  -0.351  -0.228   0.009  -0.007   \n",
       "4742  -0.010   0.277   0.632   0.219   0.462   0.090   0.340   0.134   0.476   \n",
       "4521  -0.026   0.148   0.608  -0.094   0.137  -0.216  -0.102   0.130   0.202   \n",
       "\n",
       "      wb_934  wb_935  wb_936  wb_937  wb_938  wb_939  wb_940  wb_941  wb_942  \\\n",
       "6252  -0.008   0.267  -0.843  -0.542  -0.103  -0.265   0.095  -0.555   0.152   \n",
       "4684   0.231   0.343  -0.094  -0.106   0.311   0.123   0.162  -0.101   0.434   \n",
       "1731  -0.051   0.104  -0.490  -0.427  -0.180  -0.395  -0.156  -0.342   0.100   \n",
       "4742   0.312   0.491  -0.090  -0.114   0.050   0.207   0.421  -0.100   0.110   \n",
       "4521   0.065   0.579  -0.292  -0.373  -0.011  -0.195   0.019  -0.268   0.198   \n",
       "\n",
       "      wb_943  wb_944  wb_945  wb_946  wb_947  wb_948  wb_949  wb_950  wb_951  \\\n",
       "6252  -0.435  -0.522   0.469  -0.493  -0.125   0.220   0.177  -0.564  -0.634   \n",
       "4684  -0.079   0.261   0.452  -0.037   0.059   0.232   0.223  -0.225   0.228   \n",
       "1731  -0.214  -0.339   0.315  -0.652  -0.206   0.085  -0.039  -1.002  -0.660   \n",
       "4742  -0.130   0.205   0.480   0.025   0.092   0.738   0.657  -0.074  -0.114   \n",
       "4521  -0.190  -0.356   0.679  -0.488  -0.074   0.563   0.249  -0.565   0.037   \n",
       "\n",
       "      wb_952  wb_953  wb_954  wb_955  wb_956  wb_957  wb_958  wb_959  wb_960  \\\n",
       "6252  -0.101  -0.028  -0.918  -0.051   0.521  -0.600   0.159  -0.408   0.227   \n",
       "4684   0.120   0.502  -0.051   0.302   0.554  -0.111   0.205   0.011   0.341   \n",
       "1731  -0.376  -0.172  -0.412  -0.159  -0.202  -0.339   0.018  -0.469   0.165   \n",
       "4742   0.171   0.297  -0.079   0.271   0.533  -0.130   0.678   0.122   0.354   \n",
       "4521  -0.549  -0.019  -0.456  -0.146  -0.040  -0.269   0.514  -0.211   0.498   \n",
       "\n",
       "      wb_961  wb_962  wb_963  wb_964  wb_965  wb_966  wb_967  wb_968  wb_969  \\\n",
       "6252  -0.069  -0.814   0.371   0.182   0.337  -0.199  -0.552  -0.462   0.669   \n",
       "4684   0.110   0.029   0.645   0.279   0.701   0.202  -0.177   0.188   0.889   \n",
       "1731  -0.179  -0.334   0.062  -0.187  -0.001  -0.360  -0.672  -0.274   0.229   \n",
       "4742   0.223  -0.011   0.373   0.641   0.275   0.483  -0.047   0.073   1.029   \n",
       "4521   0.079  -0.432   0.187   0.050   0.250  -0.341  -0.419  -0.295   0.336   \n",
       "\n",
       "      wb_970  wb_971  wb_972  wb_973  wb_974  wb_975  wb_976  wb_977  wb_978  \\\n",
       "6252  -0.904   0.362   0.242   0.362  -0.643   0.176  -0.412  -0.260   0.022   \n",
       "4684  -0.005   0.605   0.569   0.595  -0.145   0.460  -0.068  -0.077   0.228   \n",
       "1731  -0.319  -0.151   0.248   0.004  -0.441   0.131  -0.197  -0.424  -0.211   \n",
       "4742  -0.046   0.288   0.214   0.468  -0.118   0.675  -0.097  -0.031   0.289   \n",
       "4521  -0.473   0.317   0.533   0.190  -0.364   0.708  -0.161  -0.338  -0.058   \n",
       "\n",
       "      wb_979  wb_980  wb_981  wb_982  wb_983  wb_984  wb_985  wb_986  wb_987  \\\n",
       "6252  -0.590  -0.316   0.409  -0.762  -0.531  -0.397   0.226   0.142   0.311   \n",
       "4684   0.178  -0.022   0.398  -0.049  -0.153   0.019   0.279   0.289   0.369   \n",
       "1731  -0.367  -0.434   0.069  -0.275  -0.710  -0.468   0.033  -0.070   0.214   \n",
       "4742   0.089  -0.014   0.927  -0.070  -0.098   0.132   0.249   0.321   0.524   \n",
       "4521  -0.379  -0.381   0.442  -0.378  -0.537  -0.216   0.044   0.183   0.674   \n",
       "\n",
       "      wb_988  wb_989  wb_990  wb_991  wb_992  wb_993  wb_994  wb_995  wb_996  \\\n",
       "6252  -0.191   0.448   0.321  -0.254  -0.760  -0.737  -0.228   0.413   0.106   \n",
       "4684   0.208   0.891   0.331   0.035  -0.101  -0.049   0.021   0.466   0.496   \n",
       "1731  -0.562  -0.208  -0.216  -0.510  -0.345  -0.294  -0.450   0.294   0.244   \n",
       "4742   0.279   0.303   0.351  -0.003  -0.101  -0.062  -0.222   0.236   0.649   \n",
       "4521  -0.013   0.397  -0.146  -0.338  -0.269  -0.370  -0.545   0.626   0.650   \n",
       "\n",
       "      wb_997  wb_998  wb_999  wb_1000  wb_1001  wb_1002  wb_1003  wb_1004  \\\n",
       "6252  -0.522  -0.260   0.205   -0.232    0.327    0.246    0.365    0.166   \n",
       "4684  -0.070  -0.080   0.636    0.038    0.577    0.539    0.384    0.557   \n",
       "1731  -0.262  -0.445   0.263   -0.338   -0.201    0.246   -0.184    0.079   \n",
       "4742  -0.054  -0.046   0.368   -0.166    0.322    0.715    0.364    0.219   \n",
       "4521  -0.226  -0.400   0.481   -0.271    0.353    0.576   -0.051    0.218   \n",
       "\n",
       "      wb_1005  wb_1006  wb_1007  wb_1008  wb_1009  wb_1010  wb_1011  wb_1012  \\\n",
       "6252   -0.443   -0.070    0.135    0.148    0.166    0.582    0.177   -0.109   \n",
       "4684   -0.006    0.056    0.311    0.556    0.177    0.670    0.327    0.263   \n",
       "1731   -0.829   -0.177   -0.109   -0.086   -0.087    0.127    0.014   -0.243   \n",
       "4742   -0.009    0.117    0.457    0.220    0.188    0.710    0.420    0.204   \n",
       "4521   -0.986   -0.042    0.181    0.065   -0.075    0.171    0.385    0.089   \n",
       "\n",
       "      wb_1013  wb_1014  wb_1015  wb_1016  wb_1017  wb_1018  wb_1019  wb_1020  \\\n",
       "6252    0.242    0.209   -0.555   -0.138    0.532    0.068    0.282   -0.591   \n",
       "4684    0.531    0.622   -0.130    0.057    0.809    0.456    0.608   -0.086   \n",
       "1731   -0.106   -0.133   -0.422   -0.415   -0.300   -0.093    0.366   -0.611   \n",
       "4742    0.552    0.411   -0.128    0.243    0.615    0.102    0.411   -0.085   \n",
       "4521    0.067    0.076   -0.485   -0.373    0.477    0.087    0.671   -0.285   \n",
       "\n",
       "      wb_1021  wb_1022  wb_1023  wb_1024  wb_1025  wb_1026  wb_1027  wb_1028  \\\n",
       "6252    0.037   -0.580    0.159    0.509   -0.179   -0.691    0.113    0.145   \n",
       "4684    0.255   -0.100    0.287    0.864    0.063    0.048    0.468    0.264   \n",
       "1731   -0.220   -0.388    0.039    0.362   -0.372   -0.448   -0.093   -0.057   \n",
       "4742    0.460   -0.066    0.395    1.008    0.027    0.040    0.297    0.356   \n",
       "4521    0.002   -0.274    0.447    0.692   -0.458   -0.325    0.011    0.187   \n",
       "\n",
       "      wb_1029  wb_1030  wb_1031  wb_1032  wb_1033  wb_1034  wb_1035  wb_1036  \\\n",
       "6252   -0.020    0.016   -0.462    0.486    0.043   -0.880    0.443    0.331   \n",
       "4684    0.085    0.551   -0.052    0.536    0.555   -0.131    0.982    0.440   \n",
       "1731   -0.372    0.202   -0.399   -0.218   -0.162   -0.520    0.004    0.293   \n",
       "4742    0.240    0.704    0.040    0.752    0.130   -0.106    0.399    0.533   \n",
       "4521   -0.239    0.542   -0.225    0.018    0.147   -0.378    0.352    0.535   \n",
       "\n",
       "      wb_1037  wb_1038  wb_1039  wb_1040  wb_1041  wb_1042  wb_1043  wb_1044  \\\n",
       "6252    0.600   -0.589    0.612    0.001    0.229   -0.505    0.174    0.137   \n",
       "4684    0.685   -0.127    0.728    0.439    0.320   -0.073    0.249    0.444   \n",
       "1731    0.134   -0.468    0.186   -0.046    0.029   -0.861    0.050    0.001   \n",
       "4742    0.648   -0.128    0.607    0.684    0.436    0.055    0.552    0.544   \n",
       "4521    0.170   -0.513    0.201    0.334    0.446   -0.900    0.521    0.066   \n",
       "\n",
       "      wb_1045  wb_1046  wb_1047  wb_1048  wb_1049  wb_1050  \n",
       "6252   -0.612   -0.031    0.529    0.086    0.236   -0.148  \n",
       "4684    0.101    0.159    0.979    0.667    0.560    1.896  \n",
       "1731   -0.513   -0.152    0.016    0.240   -0.239   -1.525  \n",
       "4742   -0.087    0.342    0.421    0.857    0.398    0.988  \n",
       "4521   -0.074    0.271    0.386    0.665    0.332   -0.974  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:22:45.232453Z",
     "start_time": "2021-01-19T08:22:38.832382Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6c80d62ac9b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambda_net_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[1;32m   9802\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9804\u001b[0;31m         \u001b[0mldesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescribe_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9805\u001b[0m         \u001b[0;31m# set a convenient order for rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9806\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   9802\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9804\u001b[0;31m         \u001b[0mldesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescribe_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9805\u001b[0m         \u001b[0;31m# set a convenient order for rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9806\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdescribe_1d\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   9781\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdescribe_categorical_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9782\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_numeric_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9783\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdescribe_numeric_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9784\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_timedelta64_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9785\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdescribe_numeric_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdescribe_numeric_1d\u001b[0;34m(series)\u001b[0m\n\u001b[1;32m   9747\u001b[0m                           formatted_percentiles + ['max'])\n\u001b[1;32m   9748\u001b[0m             d = ([series.count(), series.mean(), series.std(), series.min()] +\n\u001b[0;32m-> 9749\u001b[0;31m                  series.quantile(percentiles).tolist() + [series.max()])\n\u001b[0m\u001b[1;32m   9750\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstat_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mquantile\u001b[0;34m(self, q, interpolation)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2060\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2061\u001b[0m             return self._constructor(result,\n\u001b[1;32m   2062\u001b[0m                                      \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFloat64Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5078\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5079\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5080\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5081\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5082\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Series.name must be a hashable type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/masterthesos/lib/python3.6/site-packages/pandas/core/dtypes/inference.py\u001b[0m in \u001b[0;36mis_hashable\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m     \"\"\"Return True if hash(obj) will succeed, False otherwise.\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T08:22:45.280097Z",
     "start_time": "2021-01-19T08:22:45.235453Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "for lambda_net_dataset in lambda_net_dataset_list:\n",
    "    \n",
    "    \n",
    "    if inet_holdout_seed_evaluation:\n",
    "        complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "        random.seed(RANDOM_SEED)\n",
    "        test_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/0.9)))\n",
    "        lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "        \n",
    "        random.seed(RANDOM_SEED)\n",
    "        valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/0.9)))\n",
    "        lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "        train_seeds = complete_seed_list\n",
    "        lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "        \n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "        \n",
    "        del lambda_net_dataset\n",
    "    else:\n",
    "        lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=0.1)\n",
    "        lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "    \n",
    "        del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "        \n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.364Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.366Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.369Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.396Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.430Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.453Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.461Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.463Z"
    },
    "code_folding": [
     227
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn_and_pred(lambda_net_train_dataset,\n",
    "                      lambda_net_valid_dataset,\n",
    "                      lambda_net_test_dataset, \n",
    "                      callback_names=[], \n",
    "                      return_model=False):       \n",
    "   \n",
    "    global optimizer\n",
    "\n",
    "    ############################## DATA PREPARATION ###############################\n",
    "\n",
    "    if seed_in_inet_training:\n",
    "        normalizer = Normalizer().fit([np.array(lambda_net_train_dataset.train_settings_list['seed'])])\n",
    "        train_seed_list = normalizer.transform([np.array(lambda_net_train_dataset.train_settings_list['seed'])])[0]\n",
    "        valid_seed_list = normalizer.transform([np.array(lambda_net_valid_dataset.train_settings_list['seed'])])[0]\n",
    "        test_seed_list = normalizer.transform([np.array(lambda_net_test_dataset.train_settings_list['seed'])])[0]\n",
    "\n",
    "        X_train = np.hstack([np.expand_dims(train_seed_list, axis=1), np.array(lambda_net_train_dataset.weight_list)])\n",
    "        X_valid = np.hstack([np.expand_dims(valid_seed_list, axis=1), np.array(lambda_net_valid_dataset.weight_list)])\n",
    "        X_test = np.hstack([np.expand_dims(test_seed_list, axis=1), np.array(lambda_net_test_dataset.weight_list)])\n",
    "    else:   #normalize if included in training   \n",
    "        X_train = np.array(lambda_net_train_dataset.weight_list)\n",
    "        X_valid = np.array(lambda_net_valid_dataset.weight_list)\n",
    "        X_test = np.array(lambda_net_test_dataset.weight_list) \n",
    "        \n",
    "    if evaluate_with_real_function: #target polynomial as inet target\n",
    "        y_train = np.array(lambda_net_train_dataset.target_polynomial_list)\n",
    "        y_valid = np.array(lambda_net_valid_dataset.target_polynomial_list)\n",
    "        y_test = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "    else: #lstsq lambda pred polynomial as inet target\n",
    "        y_train = np.array(lambda_net_train_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "        y_valid = np.array(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "        y_test = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "    base_model = generate_base_model()\n",
    "    if seed_in_inet_training:\n",
    "        pass\n",
    "    else:\n",
    "        shaped_weights_train_list = []\n",
    "        for train_data in tqdm(X_train):\n",
    "            shaped_weights_train = shape_flat_weights(train_data, base_model.get_weights())\n",
    "            shaped_weights_train_list.append(shaped_weights_train)\n",
    "            \n",
    "        max_size = 0\n",
    "        for weights in shaped_weights_train:\n",
    "            max_size = max(max_size, max(weights.shape))      \n",
    "    \n",
    "        shaped_weights_valid_list = []\n",
    "        for valid_data in tqdm(X_valid):\n",
    "            shaped_weights_valid = shape_flat_weights(valid_data, base_model.get_weights())\n",
    "            shaped_weights_valid_list.append(shaped_weights_valid)\n",
    "          \n",
    "        shaped_weights_test_list = []\n",
    "        for test_data in tqdm(X_test):\n",
    "            shaped_weights_test = shape_flat_weights(test_data, base_model.get_weights())\n",
    "            shaped_weights_test_list.append(shaped_weights_test)\n",
    "         \n",
    "        if False: #one sequence for biases and one sequence for weights per layer (padded to maximum size)\n",
    "            X_train_list = []\n",
    "            for shaped_weights_train in tqdm(shaped_weights_train_list):\n",
    "                padded_network_parameters_train_list = []\n",
    "                for layer_weights, biases in pairwise(shaped_weights_train):\n",
    "                    padded_weights_train_list = []\n",
    "                    for weights in layer_weights:\n",
    "                        padded_weights_train = np.pad(weights, (int(np.floor((max_size-weights.shape[0])/2)), int(np.ceil((max_size-weights.shape[0])/2))), 'constant')\n",
    "                        padded_weights_train_list.append(padded_weights_train)\n",
    "                    padded_biases_train = np.pad(biases, (int(np.floor((max_size-biases.shape[0])/2)), int(np.ceil((max_size-biases.shape[0])/2))), 'constant')\n",
    "                    padded_network_parameters_train_list.append(padded_biases_train)\n",
    "                    padded_network_parameters_train_list.extend(padded_weights_train_list)   \n",
    "                X_train_list.append(padded_network_parameters_train_list)\n",
    "            X_train = np.array(X_train_list)    \n",
    "\n",
    "\n",
    "            X_valid_list = []\n",
    "            for shaped_weights_train in tqdm(shaped_weights_valid_list):\n",
    "                padded_network_parameters_valid_list = []\n",
    "                for layer_weights, biases in pairwise(shaped_weights_valid):\n",
    "                    padded_weights_valid_list = []\n",
    "                    for weights in layer_weights:\n",
    "                        padded_weights_valid = np.pad(weights, (int(np.floor((max_size-weights.shape[0])/2)), int(np.ceil((max_size-weights.shape[0])/2))), 'constant')\n",
    "                        padded_weights_valid_list.append(padded_weights_valid)\n",
    "                    padded_biases_valid = np.pad(biases, (int(np.floor((max_size-biases.shape[0])/2)), int(np.ceil((max_size-biases.shape[0])/2))), 'constant')\n",
    "                    padded_network_parameters_valid_list.append(padded_biases_valid)\n",
    "                    padded_network_parameters_valid_list.extend(padded_weights_valid_list)    \n",
    "                X_valid_list.append(padded_network_parameters_valid_list)\n",
    "            X_valid = np.array(X_valid_list) \n",
    "\n",
    "            X_test_list = []\n",
    "            for shaped_weights_train in tqdm(shaped_weights_test_list):        \n",
    "                padded_network_parameters_test_list = []\n",
    "                for layer_weights, biases in pairwise(shaped_weights_test):\n",
    "                    padded_weights_test_list = []\n",
    "                    for weights in layer_weights:\n",
    "                        padded_weights_test = np.pad(weights, (int(np.floor((max_size-weights.shape[0])/2)), int(np.ceil((max_size-weights.shape[0])/2))), 'constant')\n",
    "                        padded_weights_test_list.append(padded_weights_test)\n",
    "                    padded_biases_test = np.pad(biases, (int(np.floor((max_size-biases.shape[0])/2)), int(np.ceil((max_size-biases.shape[0])/2))), 'constant')\n",
    "                    padded_network_parameters_test_list.append(padded_biases_test)\n",
    "                    padded_network_parameters_test_list.extend(padded_weights_test_list)    \n",
    "                X_test_list.append(padded_network_parameters_test_list)\n",
    "            X_test = np.array(X_test_list)   \n",
    "        elif True: #each path from input bias to output bias combines in one sequence for biases and one sequence for weights per layer\n",
    "            lambda_net_structure = list(flatten([n, lambda_network_layers, 1]))                    \n",
    "            number_of_paths = reduce(lambda x, y: x * y, lambda_net_structure)\n",
    "                        \n",
    "            X_train_list = []\n",
    "            for shaped_weights in tqdm(shaped_weights_train_list):        \n",
    "                network_parameters_sequence_list = np.array([]).reshape(number_of_paths, 0)    \n",
    "                for layer_index, (weights, biases) in zip(range(1, len(lambda_net_structure)), pairwise(shaped_weights)):\n",
    "\n",
    "                    layer_neurons = lambda_net_structure[layer_index]    \n",
    "                    previous_layer_neurons = lambda_net_structure[layer_index-1]\n",
    "\n",
    "                    assert(biases.shape[0] == layer_neurons)\n",
    "                    assert(weights.shape[0]*weights.shape[1] == previous_layer_neurons*layer_neurons)\n",
    "\n",
    "                    bias_multiplier = number_of_paths//layer_neurons\n",
    "                    weight_multiplier = number_of_paths//(previous_layer_neurons * layer_neurons)\n",
    "\n",
    "                    extended_bias_list = []\n",
    "                    for bias in biases:\n",
    "                        extended_bias = np.tile(bias, (bias_multiplier,1))\n",
    "                        extended_bias_list.extend(extended_bias)\n",
    "\n",
    "\n",
    "                    extended_weights_list = []\n",
    "                    for weight in weights.flatten():\n",
    "                        extended_weights = np.tile(weight, (weight_multiplier,1))\n",
    "                        extended_weights_list.extend(extended_weights)      \n",
    "\n",
    "                    network_parameters_sequence = np.concatenate([extended_weights_list, extended_bias_list], axis=1)\n",
    "                    network_parameters_sequence_list = np.hstack([network_parameters_sequence_list, network_parameters_sequence])\n",
    "\n",
    "\n",
    "                number_of_paths = network_parameters_sequence_list.shape[0]\n",
    "                number_of_unique_paths = np.unique(network_parameters_sequence_list, axis=0).shape[0]\n",
    "                number_of_nonUnique_paths = number_of_paths-number_of_unique_paths\n",
    "                \n",
    "                if number_of_nonUnique_paths > 0:\n",
    "                    print(\"Number of non-unique rows: \" + str(number_of_nonUnique_paths))\n",
    "                    print(network_parameters_sequence_list)\n",
    "                    \n",
    "                X_train_list.append(network_parameters_sequence_list)\n",
    "            X_train = np.array(X_train_list)\n",
    "\n",
    "            X_valid_list = []\n",
    "            for shaped_weights in tqdm(shaped_weights_valid_list):        \n",
    "                network_parameters_sequence_list = np.array([]).reshape(number_of_paths, 0)    \n",
    "                for layer_index, (weights, biases) in zip(range(1, len(lambda_net_structure)), pairwise(shaped_weights)):\n",
    "\n",
    "                    layer_neurons = lambda_net_structure[layer_index]    \n",
    "                    previous_layer_neurons = lambda_net_structure[layer_index-1]\n",
    "\n",
    "                    assert(biases.shape[0] == layer_neurons)\n",
    "                    assert(weights.shape[0]*weights.shape[1] == previous_layer_neurons*layer_neurons)\n",
    "\n",
    "                    bias_multiplier = number_of_paths//layer_neurons\n",
    "                    weight_multiplier = number_of_paths//(previous_layer_neurons * layer_neurons)\n",
    "\n",
    "                    extended_bias_list = []\n",
    "                    for bias in biases:\n",
    "                        extended_bias = np.tile(bias, (bias_multiplier,1))\n",
    "                        extended_bias_list.extend(extended_bias)\n",
    "\n",
    "\n",
    "                    extended_weights_list = []\n",
    "                    for weight in weights.flatten():\n",
    "                        extended_weights = np.tile(weight, (weight_multiplier,1))\n",
    "                        extended_weights_list.extend(extended_weights)      \n",
    "\n",
    "                    network_parameters_sequence = np.concatenate([extended_weights_list, extended_bias_list], axis=1)\n",
    "                    network_parameters_sequence_list = np.hstack([network_parameters_sequence_list, network_parameters_sequence])\n",
    "\n",
    "\n",
    "                number_of_paths = network_parameters_sequence_list.shape[0]\n",
    "                number_of_unique_paths = np.unique(network_parameters_sequence_list, axis=0).shape[0]\n",
    "                number_of_nonUnique_paths = number_of_paths-number_of_unique_paths\n",
    "                \n",
    "                if number_of_nonUnique_paths > 0:\n",
    "                    print(\"Number of non-unique rows: \" + str(number_of_nonUnique_paths))\n",
    "                    print(network_parameters_sequence_list)\n",
    "                    \n",
    "                X_valid_list.append(network_parameters_sequence_list)\n",
    "            X_valid = np.array(X_valid_list)\n",
    "            \n",
    "            X_test_list = []\n",
    "            for shaped_weights in tqdm(shaped_weights_test_list):        \n",
    "                network_parameters_sequence_list = np.array([]).reshape(number_of_paths, 0)   \n",
    "                for layer_index, (weights, biases) in zip(range(1, len(lambda_net_structure)), pairwise(shaped_weights)):\n",
    "\n",
    "                    layer_neurons = lambda_net_structure[layer_index]    \n",
    "                    previous_layer_neurons = lambda_net_structure[layer_index-1]\n",
    "\n",
    "                    assert(biases.shape[0] == layer_neurons)\n",
    "                    assert(weights.shape[0]*weights.shape[1] == previous_layer_neurons*layer_neurons)\n",
    "\n",
    "                    bias_multiplier = number_of_paths//layer_neurons\n",
    "                    weight_multiplier = number_of_paths//(previous_layer_neurons * layer_neurons)\n",
    "\n",
    "                    extended_bias_list = []\n",
    "                    for bias in biases:\n",
    "                        extended_bias = np.tile(bias, (bias_multiplier,1))\n",
    "                        extended_bias_list.extend(extended_bias)\n",
    "\n",
    "\n",
    "                    extended_weights_list = []\n",
    "                    for weight in weights.flatten():\n",
    "                        extended_weights = np.tile(weight, (weight_multiplier,1))\n",
    "                        extended_weights_list.extend(extended_weights)      \n",
    "\n",
    "                    network_parameters_sequence = np.concatenate([extended_weights_list, extended_bias_list], axis=1)\n",
    "                    network_parameters_sequence_list = np.hstack([network_parameters_sequence_list, network_parameters_sequence])\n",
    "\n",
    "\n",
    "                number_of_paths = network_parameters_sequence_list.shape[0]\n",
    "                number_of_unique_paths = np.unique(network_parameters_sequence_list, axis=0).shape[0]\n",
    "                number_of_nonUnique_paths = number_of_paths-number_of_unique_paths\n",
    "                \n",
    "                if number_of_nonUnique_paths > 0:\n",
    "                    print(\"Number of non-unique rows: \" + str(number_of_nonUnique_paths))\n",
    "                    print(network_parameters_sequence_list)\n",
    "                    \n",
    "                X_test_list.append(network_parameters_sequence_list)\n",
    "            X_test = np.array(X_test_list)\n",
    "            \n",
    "            if False:\n",
    "                X_train = np.transpose(X_train, (0, 2, 1))\n",
    "                X_valid = np.transpose(X_valid, (0, 2, 1))\n",
    "                X_test = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "            if False: #generate subsequences for cnn-lstm\n",
    "                subsequences = 2 #for each bias+weights\n",
    "                timesteps = X_train.shape[1]//subsequences\n",
    "                \n",
    "                X_train = X_train.reshape((X_train.shape[0], subsequences, timesteps, X_train.shape[2]))\n",
    "                \n",
    "                X_valid = X_valid.reshape((X_valid.shape[0], subsequences, timesteps, X_valid.shape[2]))\n",
    "\n",
    "                X_test = X_test.reshape((X_test.shape[0], subsequences, timesteps, X_test.shape[2]))        \n",
    "        \n",
    "    ############################## OBJECTIVE SPECIFICATION AND LOSS FUNCTION ADJUSTMENTS ###############################\n",
    "        \n",
    "    if consider_labels_training: #coefficient-based evaluation\n",
    "        loss_function = r2_tf_fv\n",
    "        metrics = [mean_absolute_error_tf_fv, 'mean_absolute_error']\n",
    "        #loss_function = mean_absolute_error_tf_fv\n",
    "        #metrics = [r2_tf_fv, 'mean_absolute_error']\n",
    "        valid_data = (X_valid, y_valid)\n",
    "        y_train_model = y_train\n",
    "    else: #fv-based evaluation\n",
    "        if evaluate_with_real_function: #based on in-loss fv calculation of real and predicted polynomial\n",
    "            random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "            list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "            loss_function = r2_tf_fv_poly_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers)\n",
    "            metrics = [mean_absolute_error_tf_fv_poly_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers), 'mean_absolute_error']\n",
    "            #loss_function = mean_absolute_error_tf_fv_poly_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers)\n",
    "            #metrics = [r2_tf_fv_poly_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers), 'mean_absolute_error']\n",
    "            valid_data = (X_valid, y_valid)\n",
    "            y_train_model = y_train\n",
    "        else: #in-loss prediction of lambda-nets\n",
    "            base_model = generate_base_model()\n",
    "            random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "            list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "            loss_function = r2_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "            metrics = [mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model), mean_absolute_error_extended]\n",
    "            #loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "            #metrics = [r2_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model), mean_absolute_error_extended]\n",
    "            y_train_model = np.hstack((y_train, X_train))   \n",
    "            valid_data = (X_valid, np.hstack((y_valid, X_valid)))   \n",
    "            \n",
    "            \n",
    "            \n",
    "#TODO ADD ALTERNATIVE: FV COMPARISON WITH LSTSQ POLYNOMIAL INSTEAD OF DIRECTLY LAMBDA NET PREDS\n",
    "        \n",
    "    ############################## BUILD MODEL ###############################\n",
    "        \n",
    "    from tensorflow.keras.utils import CustomObjectScope\n",
    "\n",
    "    with CustomObjectScope({'custom_loss': loss_function}):    \n",
    "        if True: #CNN\n",
    "            input_node = ak.Input()\n",
    "            output_node = ak.ConvBlock()(input_node)\n",
    "            output_node = ak.DenseBlock()(output_node)\n",
    "            output_node = ak.RegressionHead()(output_node)\n",
    "            \n",
    "            directory = './data/autokeras/automodel/CNN-noT'\n",
    "        if False: #LSTM\n",
    "            input_node = ak.Input()\n",
    "            output_node = ak.RNNBlock()(input_node)\n",
    "            output_node = ak.DenseBlock()(output_node)\n",
    "            output_node = ak.RegressionHead()(output_node)\n",
    "            \n",
    "            directory = './data/autokeras/automodel/LSTM-noT'\n",
    "        elif False: #CNN-LSTM\n",
    "            input_node = ak.Input()\n",
    "            output_node = ak.ConvBlock()(input_node)\n",
    "            output_node = ak.RNNBlock()(output_node)\n",
    "            output_node = ak.DenseBlock()(output_node)\n",
    "            output_node = ak.RegressionHead()(output_node)  \n",
    "            \n",
    "            directory = './data/autokeras/automodel/CNN-LSTM-noT'\n",
    "        elif False: #CNN-LSTM-parallel                              \n",
    "            input_node = ak.Input()\n",
    "            output_node1 = ak.ConvBlock()(input_node)\n",
    "            output_node2 = ak.RNNBlock()(input_node)\n",
    "            output_node = ak.Merge()([output_node1, output_node2])\n",
    "            output_node = ak.DenseBlock()(output_node)\n",
    "            output_node = ak.RegressionHead()(output_node)  \n",
    "            \n",
    "            directory = './data/autokeras/automodel/CNN-LSTM-parallel-noT'\n",
    "        \n",
    "        auto_model = ak.AutoModel(inputs=input_node, \n",
    "                            outputs=output_node,\n",
    "                            #output_dim=sparsity,\n",
    "                            loss='custom_loss',\n",
    "                            overwrite=True,\n",
    "                            max_trials=trials,\n",
    "                            directory=directory,\n",
    "                            seed=RANDOM_SEED)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        \n",
    "\n",
    "        ############################## PREDICTION ###############################\n",
    "        auto_model.fit(\n",
    "            x=X_train,\n",
    "            y=y_train_model,\n",
    "            validation_data=valid_data,\n",
    "            epochs=100\n",
    "            )\n",
    "\n",
    "\n",
    "        results_summary = auto_model.tuner.results_summary()\n",
    "        print(results_summary)\n",
    "        model = auto_model.export_model()\n",
    "        print(model.summary())\n",
    "        \n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    pred_list = [y_valid_pred, y_test_pred]\n",
    "              \n",
    "        \n",
    "    ############################## FUNCTION VALUE CALCULATION ###############################\n",
    "    \n",
    "    lambda_test_data_preds_valid = lambda_net_valid_dataset.make_prediction_on_test_data()\n",
    "    lambda_test_data_preds_test = lambda_net_test_dataset.make_prediction_on_test_data() \n",
    "              \n",
    "    target_poly_test_data_fvs_valid = lambda_net_valid_dataset.return_target_poly_fvs_on_test_data()\n",
    "    target_poly_test_data_fvs_test = lambda_net_test_dataset.return_target_poly_fvs_on_test_data() \n",
    "                \n",
    "    lstsq_lambda_pred_polynomial_test_data_fvs_valid = lambda_net_valid_dataset.return_lstsq_lambda_pred_polynomial_fvs_on_test_data()\n",
    "    lstsq_lambda_pred_polynomial_test_data_fvs_test = lambda_net_test_dataset.return_lstsq_lambda_pred_polynomial_fvs_on_test_data() \n",
    "             \n",
    "    lstsq_target_polynomial_test_data_fvs_valid = lambda_net_valid_dataset.return_lstsq_target_polynomial_fvs_on_test_data()\n",
    "    lstsq_target_polynomial_test_data_fvs_test = lambda_net_test_dataset.return_lstsq_target_polynomial_fvs_on_test_data() \n",
    "        \n",
    "    inet_poly_test_data_fvs_valid = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_net_valid_dataset.test_data_list)\n",
    "    inet_poly_test_data_fvs_test = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_net_test_dataset.test_data_list) \n",
    "    \n",
    "    \n",
    "    function_values_valid = [lambda_test_data_preds_valid, \n",
    "                            target_poly_test_data_fvs_valid, \n",
    "                            lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                            lstsq_target_polynomial_test_data_fvs_valid,\n",
    "                            inet_poly_test_data_fvs_valid]\n",
    "    \n",
    "    function_values_test = [lambda_test_data_preds_test, \n",
    "                            target_poly_test_data_fvs_test, \n",
    "                            lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                            lstsq_target_polynomial_test_data_fvs_test,\n",
    "                            inet_poly_test_data_fvs_test]\n",
    "    \n",
    "    function_values = [function_values_valid, function_values_test]    \n",
    "    \n",
    "    \n",
    "    ############################## EVALUATION ###############################\n",
    "    \n",
    "    #evaluate inet poly against target polynomial on fv-basis\n",
    "    scores_inetPoly_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred,\n",
    "                                                                                   lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                   inet_poly_test_data_fvs_valid, \n",
    "                                                                                   target_poly_test_data_fvs_valid)  \n",
    "    scores_inetPoly_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                  lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                  inet_poly_test_data_fvs_test, \n",
    "                                                                                  target_poly_test_data_fvs_test)\n",
    "\n",
    "    #evaluate inet poly against lambda-net preds on fv-basis\n",
    "    scores_inetPoly_VS_predLambda_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred, \n",
    "                                                                                   None, \n",
    "                                                                                   inet_poly_test_data_fvs_valid, \n",
    "                                                                                   lambda_test_data_preds_valid)\n",
    "    scores_inetPoly_VS_predLambda_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                  None, \n",
    "                                                                                  inet_poly_test_data_fvs_test, \n",
    "                                                                                  lambda_test_data_preds_test)       \n",
    "        \n",
    "    #evaluate inet poly against lstsq target poly on fv-basis\n",
    "    scores_inetPoly_VS_lstsqTarget_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred, \n",
    "                                                                                    lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                    inet_poly_test_data_fvs_valid, \n",
    "                                                                                    lstsq_target_polynomial_test_data_fvs_valid)\n",
    "    scores_inetPoly_VS_lstsqTarget_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                   lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                   inet_poly_test_data_fvs_test, \n",
    "                                                                                   lstsq_target_polynomial_test_data_fvs_test)  \n",
    "\n",
    "    #evaluate inet poly against lstsq lambda poly on fv-basis\n",
    "    scores_inetPoly_VS_lstsqLambda_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred, \n",
    "                                                                                    lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                    inet_poly_test_data_fvs_valid, \n",
    "                                                                                    lstsq_lambda_pred_polynomial_test_data_fvs_valid)\n",
    "    scores_inetPoly_VS_lstsqLambda_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                   lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                   inet_poly_test_data_fvs_test, \n",
    "                                                                                   lstsq_lambda_pred_polynomial_test_data_fvs_test)     \n",
    "      \n",
    "    #evaluate lstsq lambda pred poly against lambda-net preds on fv-basis\n",
    "    scores_lstsqLambda_VS_predLambda_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                      None, \n",
    "                                                                                      lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                                                                                      lambda_test_data_preds_valid)\n",
    "    scores_lstsqLambda_VS_predLambda_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                     None, \n",
    "                                                                                     lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                                                                                     lambda_test_data_preds_test)\n",
    "    \n",
    "    #evaluate lstsq lambda pred poly against lstsq target poly on fv-basis\n",
    "    scores_lstsqLambda_VS_lstsqTarget_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                       lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                       lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                                                                                       lstsq_target_polynomial_test_data_fvs_valid)\n",
    "    scores_lstsqLambda_VS_lstsqTarget_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                      lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                      lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                                                                                      lstsq_target_polynomial_test_data_fvs_test)    \n",
    "    \n",
    "    #evaluate lstsq lambda pred poly against target poly on fv-basis\n",
    "    scores_lstsqLambda_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                      lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                      lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                                                                                      target_poly_test_data_fvs_valid)\n",
    "    scores_lstsqLambda_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                     lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                     lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                                                                                     target_poly_test_data_fvs_test)    \n",
    "    \n",
    "    #evaluate lambda-net preds against lstsq target poly on fv-basis\n",
    "    scores_predLambda_VS_lstsqTarget_test_data_fv_valid = evaluate_interpretation_net(None, \n",
    "                                                                                      lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                      lambda_test_data_preds_valid, \n",
    "                                                                                      lstsq_target_polynomial_test_data_fvs_valid)\n",
    "    scores_predLambda_VS_lstsqTarget_test_data_fv_test = evaluate_interpretation_net(None, \n",
    "                                                                                     lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                     lambda_test_data_preds_test, \n",
    "                                                                                     lstsq_target_polynomial_test_data_fvs_test)\n",
    "        \n",
    "    #evaluate lambda-net preds against target poly on fv-basis\n",
    "    scores_predLambda_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(None, \n",
    "                                                                                     lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                     lambda_test_data_preds_valid, \n",
    "                                                                                     target_poly_test_data_fvs_valid)\n",
    "    scores_predLambda_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(None, \n",
    "                                                                                    lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                    lambda_test_data_preds_test, \n",
    "                                                                                    target_poly_test_data_fvs_test)\n",
    "      \n",
    "    #evaluate lstsq target poly against target poly on fv-basis\n",
    "    scores_lstsqTarget_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                      lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                      lstsq_target_polynomial_test_data_fvs_valid, \n",
    "                                                                                      target_poly_test_data_fvs_valid)\n",
    "    scores_lstsqTarget_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                     lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                     lstsq_target_polynomial_test_data_fvs_test, \n",
    "                                                                                     target_poly_test_data_fvs_test)\n",
    "        \n",
    "    scores_dict = pd.DataFrame(data=[scores_inetPoly_VS_targetPoly_test_data_fv_valid, \n",
    "                                     scores_inetPoly_VS_targetPoly_test_data_fv_test, \n",
    "                                     scores_inetPoly_VS_predLambda_test_data_fv_valid,\n",
    "                                     scores_inetPoly_VS_predLambda_test_data_fv_test,\n",
    "                                     scores_inetPoly_VS_lstsqTarget_test_data_fv_valid,\n",
    "                                     scores_inetPoly_VS_lstsqTarget_test_data_fv_test,\n",
    "                                     scores_inetPoly_VS_lstsqLambda_test_data_fv_valid,\n",
    "                                     scores_inetPoly_VS_lstsqLambda_test_data_fv_test,\n",
    "                                     scores_lstsqLambda_VS_predLambda_test_data_fv_valid,\n",
    "                                     scores_lstsqLambda_VS_predLambda_test_data_fv_test,\n",
    "                                     scores_lstsqLambda_VS_lstsqTarget_test_data_fv_valid,\n",
    "                                     scores_lstsqLambda_VS_lstsqTarget_test_data_fv_test,\n",
    "                                     scores_lstsqLambda_VS_targetPoly_test_data_fv_valid,\n",
    "                                     scores_lstsqLambda_VS_targetPoly_test_data_fv_test,\n",
    "                                     scores_predLambda_VS_lstsqTarget_test_data_fv_valid,\n",
    "                                     scores_predLambda_VS_lstsqTarget_test_data_fv_test,\n",
    "                                     scores_predLambda_VS_targetPoly_test_data_fv_valid,\n",
    "                                     scores_predLambda_VS_targetPoly_test_data_fv_test,\n",
    "                                     scores_lstsqTarget_VS_targetPoly_test_data_fv_valid,\n",
    "                                     scores_lstsqTarget_VS_targetPoly_test_data_fv_test],\n",
    "                               index=['inetPoly_VS_targetPoly_valid', \n",
    "                                      'inetPoly_VS_targetPoly_test', \n",
    "                                      'inetPoly_VS_predLambda_valid',\n",
    "                                      'inetPoly_VS_predLambda_test',\n",
    "                                      'inetPoly_VS_lstsqTarget_valid',\n",
    "                                      'inetPoly_VS_lstsqTarget_test',\n",
    "                                      'inetPoly_VS_lstsqLambda_valid',\n",
    "                                      'inetPoly_VS_lstsqLambda_test',\n",
    "                                      'lstsqLambda_VS_predLambda_valid',\n",
    "                                      'lstsqLambda_VS_predLambda_test',\n",
    "                                      'lstsqLambda_VS_lstsqTarget_valid',\n",
    "                                      'lstsqLambda_VS_lstsqTarget_test',\n",
    "                                      'lstsqLambda_VS_targetPoly_valid',\n",
    "                                      'lstsqLambda_VS_targetPoly_test',\n",
    "                                      'predLambda_VS_lstsqTarget_valid',\n",
    "                                      'predLambda_VS_lstsqTarget_test',\n",
    "                                      'predLambda_VS_targetPoly_valid',\n",
    "                                      'predLambda_VS_targetPoly_test',\n",
    "                                      'lstsqTarget_VS_targetPoly_valid',\n",
    "                                      'lstsqTarget_VS_targetPoly_test'])\n",
    "    \n",
    "\n",
    "    if return_model or n_jobs==1:\n",
    "        #return history.history, scores_dict, function_values, pred_list, model    \n",
    "        return results_summary, scores_dict, function_values, pred_list, model         \n",
    "    else: \n",
    "        #return history.history, scores_dict, function_values, pred_list   \n",
    "        return results_summary, scores_dict, function_values, pred_list       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.466Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if samples_list == None: \n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, \n",
    "                            verbose=11, \n",
    "                            backend='loky')(delayed(train_nn_and_pred)(lambda_net_train_dataset,\n",
    "                                                                       lambda_net_valid_dataset,\n",
    "                                                                       lambda_net_test_dataset, \n",
    "                                                                       callback_names=['early_stopping']) for lambda_net_train_dataset,\n",
    "                                                                                                              lambda_net_valid_dataset,\n",
    "                                                                                                              lambda_net_test_dataset  in zip(lambda_net_train_dataset_list,\n",
    "                                                                                                                                              lambda_net_valid_dataset_list,\n",
    "                                                                                                                                              lambda_net_test_dataset_list))      \n",
    "    results_summary_list = [result[0] for result in results_list]\n",
    "        \n",
    "    scores_list = [result[1] for result in results_list]\n",
    "    \n",
    "    function_values_complete_list = [result[2] for result in results_list]\n",
    "    function_values_valid_list = [function_values[0] for function_values in function_values_complete_list]\n",
    "    function_values_test_list = [function_values[1] for function_values in function_values_complete_list]\n",
    "\n",
    "    inet_preds_list = [result[3] for result in results_list]\n",
    "\n",
    "else:\n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, verbose=11, backend='loky')(delayed(train_nn_and_pred)(lambda_net_train_dataset.sample(samples),\n",
    "                                                                                                  lambda_net_valid_dataset,\n",
    "                                                                                                  lambda_net_test_dataset, \n",
    "                                                                                                  callback_names=['early_stopping']) for samples in samples_list)     \n",
    "    \n",
    "    results_summary_list = [result[0] for result in results_list]\n",
    "    \n",
    "    scores_list = [result[1] for result in results_list]\n",
    "    \n",
    "    function_values_complete_list = [result[2] for result in results_list]\n",
    "    function_values_valid_list = [function_values[0] for function_values in fulocnction_values_complete_list]\n",
    "    function_values_test_list = [function_values[1] for function_values in function_values_complete_list]\n",
    "\n",
    "    inet_preds_list = [result[3] for result in results_list]\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.468Z"
    }
   },
   "outputs": [],
   "source": [
    "if n_jobs==1:\n",
    "    print(results_list[-1][4].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.470Z"
    }
   },
   "outputs": [],
   "source": [
    "if n_jobs==1:\n",
    "    print(results_list[-1][4].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.472Z"
    }
   },
   "outputs": [],
   "source": [
    "print(results_summary_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.474Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.476Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.478Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(history['loss'])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.480Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "if samples_list == None:\n",
    "    x_axis_steps = [(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]\n",
    "    x_max = epochs_lambda\n",
    "else:\n",
    "    x_axis_steps = samples_list\n",
    "    x_max = samples_list[-1]\n",
    "    \n",
    "if evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(length_plt//2, 2, figsize=(30,20))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        inetPoly_VS_targetPoly_test = []\n",
    "        #inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        #inetPoly_VS_lstsqLambda_test = []\n",
    "        #lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        predLambda_VS_targetPoly_test = []\n",
    "        lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_targetPoly_test.append(scores[metric].loc['inetPoly_VS_targetPoly_test'])\n",
    "            predLambda_VS_targetPoly_test.append(scores[metric].loc['predLambda_VS_targetPoly_test'])\n",
    "            lstsqLambda_VS_targetPoly_test.append(scores[metric].loc['lstsqLambda_VS_targetPoly_test'])     \n",
    "            lstsqTarget_VS_targetPoly_test.append(scores[metric].loc['lstsqTarget_VS_targetPoly_test'])\n",
    "        \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_targetPoly_test, predLambda_VS_targetPoly_test, lstsqLambda_VS_targetPoly_test, lstsqTarget_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test'])\n",
    "\n",
    "        ax[index//2, index%2].set_title(metric)\n",
    "        sns.set(font_scale = 1.25)\n",
    "        p = sns.lineplot(data=plot_df, ax=ax[index//2, index%2])\n",
    "        p.set_yticklabels(p.get_yticks(), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)        \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_REAL_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else:\n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(length_plt//2, 2, figsize=(30,20))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        #inetPoly_VS_targetPoly_test = []\n",
    "        inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        inetPoly_VS_lstsqLambda_test = []\n",
    "        lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        #lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        predLambda_VS_targetPoly_test = []\n",
    "        #lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_lstsqLambda_test.append(scores[metric].loc['inetPoly_VS_lstsqLambda_test'])\n",
    "            inetPoly_VS_predLambda_test.append(scores[metric].loc['inetPoly_VS_predLambda_test'])\n",
    "            lstsqLambda_VS_predLambda_test.append(scores[metric].loc['lstsqLambda_VS_predLambda_test'])     \n",
    "            predLambda_VS_targetPoly_test.append(scores[metric].loc['predLambda_VS_targetPoly_test'])     \n",
    "            \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_predLambda_test, inetPoly_VS_lstsqLambda_test, lstsqLambda_VS_predLambda_test, predLambda_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test'])\n",
    "\n",
    "        ax[index//2, index%2].set_title(metric)\n",
    "        sns.set(font_scale = 1.25)\n",
    "        p = sns.lineplot(data=plot_df, ax=ax[index//2, index%2])\n",
    "        p.set_yticklabels(p.get_yticks(), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)  \n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else: \n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.482Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "plot_metric_list = ['MAE FV']\n",
    "\n",
    "if samples_list == None:\n",
    "    x_axis_steps = [(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]\n",
    "    x_max = epochs_lambda\n",
    "else:\n",
    "    x_axis_steps = samples_list\n",
    "    x_max = samples_list[-1]\n",
    "    \n",
    "if evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        inetPoly_VS_targetPoly_test = []\n",
    "        #inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        #inetPoly_VS_lstsqLambda_test = []\n",
    "        #lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        predLambda_VS_targetPoly_test = []\n",
    "        lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_targetPoly_test.append(scores[metric].loc['inetPoly_VS_targetPoly_test'])\n",
    "            predLambda_VS_targetPoly_test.append(scores[metric].loc['predLambda_VS_targetPoly_test'])\n",
    "            lstsqLambda_VS_targetPoly_test.append(scores[metric].loc['lstsqLambda_VS_targetPoly_test'])     \n",
    "            lstsqTarget_VS_targetPoly_test.append(scores[metric].loc['lstsqTarget_VS_targetPoly_test'])\n",
    "        \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_targetPoly_test, predLambda_VS_targetPoly_test, lstsqLambda_VS_targetPoly_test, lstsqTarget_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test'])\n",
    "\n",
    "        ax.set_title(metric)\n",
    "        sns.set(font_scale = 1.25)\n",
    "        p = sns.lineplot(data=plot_df, ax=ax)\n",
    "        p.set_yticklabels(p.get_yticks(), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_REAL_' + metric + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else:\n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        #inetPoly_VS_targetPoly_test = []\n",
    "        inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        inetPoly_VS_lstsqLambda_test = []\n",
    "        lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        #lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        predLambda_VS_targetPoly_test = []\n",
    "        #lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_lstsqLambda_test.append(scores[metric].loc['inetPoly_VS_lstsqLambda_test'])\n",
    "            inetPoly_VS_predLambda_test.append(scores[metric].loc['inetPoly_VS_predLambda_test'])\n",
    "            lstsqLambda_VS_predLambda_test.append(scores[metric].loc['lstsqLambda_VS_predLambda_test'])     \n",
    "            predLambda_VS_targetPoly_test.append(scores[metric].loc['predLambda_VS_targetPoly_test'])     \n",
    "            \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_predLambda_test, inetPoly_VS_lstsqLambda_test, lstsqLambda_VS_predLambda_test, predLambda_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test'])\n",
    "\n",
    "        ax.set_title(metric)\n",
    "        sns.set(font_scale = 1.25)\n",
    "        p = sns.lineplot(data=plot_df, ax=ax)\n",
    "        p.set_yticklabels(p.get_yticks(), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_MODEL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else: \n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.484Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "plot_metric_list = ['R2 FV']\n",
    "\n",
    "if samples_list == None:\n",
    "    x_axis_steps = [(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]\n",
    "    x_max = epochs_lambda\n",
    "else:\n",
    "    x_axis_steps = samples_list\n",
    "    x_max = samples_list[-1]\n",
    "    \n",
    "if evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        inetPoly_VS_targetPoly_test = []\n",
    "        #inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        #inetPoly_VS_lstsqLambda_test = []\n",
    "        #lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        predLambda_VS_targetPoly_test = []\n",
    "        lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_targetPoly_test.append(scores[metric].loc['inetPoly_VS_targetPoly_test'])\n",
    "            predLambda_VS_targetPoly_test.append(scores[metric].loc['predLambda_VS_targetPoly_test'])\n",
    "            lstsqLambda_VS_targetPoly_test.append(scores[metric].loc['lstsqLambda_VS_targetPoly_test'])     \n",
    "            lstsqTarget_VS_targetPoly_test.append(scores[metric].loc['lstsqTarget_VS_targetPoly_test'])\n",
    "        \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_targetPoly_test, predLambda_VS_targetPoly_test, lstsqLambda_VS_targetPoly_test, lstsqTarget_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test'])\n",
    "\n",
    "        ax.set_title(metric)\n",
    "        sns.set(font_scale = 1.25)\n",
    "        p = sns.lineplot(data=plot_df, ax=ax)\n",
    "        p.set(ylim=(-5, 1))\n",
    "        p.set_yticklabels(p.get_yticks(), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_REAL_' + metric + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else:\n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        #inetPoly_VS_targetPoly_test = []\n",
    "        inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        inetPoly_VS_lstsqLambda_test = []\n",
    "        lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        #lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        predLambda_VS_targetPoly_test = []\n",
    "        #lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_lstsqLambda_test.append(scores[metric].loc['inetPoly_VS_lstsqLambda_test'])\n",
    "            inetPoly_VS_predLambda_test.append(scores[metric].loc['inetPoly_VS_predLambda_test'])\n",
    "            lstsqLambda_VS_predLambda_test.append(scores[metric].loc['lstsqLambda_VS_predLambda_test'])     \n",
    "            predLambda_VS_targetPoly_test.append(scores[metric].loc['predLambda_VS_targetPoly_test'])     \n",
    "            \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_predLambda_test, inetPoly_VS_lstsqLambda_test, lstsqLambda_VS_predLambda_test, predLambda_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test'])\n",
    "\n",
    "        ax.set_title(metric)\n",
    "        sns.set(font_scale = 1.25)\n",
    "        p = sns.lineplot(data=plot_df, ax=ax)\n",
    "        p.set(ylim=(-5, 1))\n",
    "        p.set_yticklabels(p.get_yticks(), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_MODEL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else: \n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.486Z"
    }
   },
   "outputs": [],
   "source": [
    "rand_index = 42\n",
    "\n",
    "lambda_model_preds = function_values_test_list[-1][0][rand_index].ravel()\n",
    "real_poly_fvs = function_values_test_list[-1][1][rand_index]\n",
    "lstsq_lambda_preds_poly = function_values_test_list[-1][2][rand_index]\n",
    "lstsq_target_poly = function_values_test_list[-1][3][rand_index]\n",
    "inet_poly_fvs = function_values_test_list[-1][4][rand_index]\n",
    "\n",
    "    \n",
    "x_vars = ['x' + str(i) for i in range(1, n+1)]\n",
    "\n",
    "columns = x_vars.copy()\n",
    "columns.append('FVs')\n",
    "\n",
    "columns_single = x_vars.copy()\n",
    "\n",
    "eval_size_plot = inet_poly_fvs.shape[0]\n",
    "vars_plot = lambda_net_test_dataset_list[-1].test_data_list[rand_index]\n",
    "\n",
    "    \n",
    "if evaluate_with_real_function:\n",
    "    columns_single.extend(['Lambda Model Preds', 'Target Poly FVs', 'LSTSQ Target Poly FVs', 'I-Net Poly FVs'])\n",
    "    plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, lambda_model_preds, real_poly_fvs, lstsq_target_poly, inet_poly_fvs]), columns=columns_single)\n",
    "    preds_plot_all = np.vstack([lambda_model_preds, real_poly_fvs, lstsq_target_poly, inet_poly_fvs]).ravel()\n",
    "    vars_plot_all_preds = np.vstack([vars_plot for i in range(len(columns_single[n:]))])\n",
    "    \n",
    "    lambda_model_preds_str = np.array(['Lambda Model Preds' for i in range(eval_size_plot)])\n",
    "    real_poly_fvs_str = np.array(['Target Poly FVs' for i in range(eval_size_plot)])\n",
    "    lstsq_target_poly_str = np.array(['LSTSQ Target Poly FVs' for i in range(eval_size_plot)])\n",
    "    inet_poly_fvs_str = np.array(['I-Net Poly FVs' for i in range(eval_size_plot)])\n",
    "    \n",
    "    identifier = np.concatenate([lambda_model_preds_str, real_poly_fvs_str, lstsq_target_poly_str, inet_poly_fvs_str])\n",
    "else:\n",
    "    columns_single.extend(['Lambda Model Preds', 'Target Poly FVs', 'LSTSQ Lambda Poly FVs', 'I-Net Poly FVs'])\n",
    "    plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, lambda_model_preds, real_poly_fvs, lstsq_lambda_preds_poly, inet_poly_fvs]), columns=columns_single)\n",
    "    preds_plot_all = np.vstack([lambda_model_preds, real_poly_fvs, lstsq_lambda_preds_poly, inet_poly_fvs]).ravel()\n",
    "    vars_plot_all_preds = np.vstack([vars_plot for i in range(len(columns_single[n:]))])\n",
    "    \n",
    "    lambda_model_preds_str = np.array(['Lambda Model Preds' for i in range(eval_size_plot)])\n",
    "    real_poly_fvs_str = np.array(['Target Poly FVs' for i in range(eval_size_plot)])\n",
    "    lstsq_lambda_preds_poly_str = np.array(['LSTSQ Lambda Poly FVs' for i in range(eval_size_plot)])\n",
    "    inet_poly_fvs_str = np.array(['I-Net Poly FVs' for i in range(eval_size_plot)])\n",
    "    \n",
    "    identifier = np.concatenate([lambda_model_preds_str, real_poly_fvs_str, lstsq_lambda_preds_poly_str, inet_poly_fvs_str])\n",
    "\n",
    "plot_data = pd.DataFrame(data=np.column_stack([vars_plot_all_preds, preds_plot_all]), columns=columns)\n",
    "plot_data['Identifier'] = identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.488Z"
    }
   },
   "outputs": [],
   "source": [
    "pp1 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  y_vars=['FVs'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.490Z"
    }
   },
   "outputs": [],
   "source": [
    "pp2 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  #y_vars=['FVs'],\n",
    "                  #x_vars=x_vars\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.497Z"
    }
   },
   "outputs": [],
   "source": [
    "pp3 = sns.pairplot(data=plot_data_single,\n",
    "                  #kind='reg',\n",
    "                  y_vars=columns_single[n:],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.500Z"
    }
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_REAL_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')\n",
    "else:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_PRED_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.503Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = []\n",
    "for i in range(len(lambda_net_test_dataset_list[-1])):\n",
    "    random_polynomial = list(random_product([i*a_step for i in range(int(a_min*10**int(-np.log10(a_step))), int(a_max*10**int(-np.log10(a_step))))], repeat=nCr(n+d, d)))\n",
    "    list_of_random_polynomials.append(random_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.505Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].test_data_list)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.507Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.509Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.511Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].test_data_list)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.513Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-19T08:20:49.514Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
