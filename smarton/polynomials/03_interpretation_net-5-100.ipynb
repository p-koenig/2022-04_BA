{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 2, #degree\n",
    "        'n': 5, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)  \n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 5,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 100,\n",
    "        'a_min': -100,\n",
    "        'lambda_nets_total': 10000,\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'border_min': 0.2, #needs to be between 0 and (x_max-x_min)/2\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5,\n",
    "        'a_zero_prob': 0.25,\n",
    "        'a_random_prob': 0.1,      \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0.25,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': 5, #(None, int) #CONSTANT IS NOT INCLUDED\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'normalize_inet_data': False,\n",
    "        'inet_training_without_noise': False, #dataset size without noise hardcoded to 50k in generate_paths\n",
    "        'sparse_poly_representation_version': 1, #(1, 2); 1=old, 2=new\n",
    "\n",
    "        'evaluate_with_real_function': False,\n",
    "        'consider_labels_training': False,\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "        \n",
    "        'symbolic_metamodeling_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_evaluation': False,\n",
    "        'symbolic_metamodeling_function_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_function_evaluation': False,\n",
    "        \n",
    "        'symbolic_regression_evaluation': True,\n",
    "        'per_network_evaluation': False,\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 243\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 2], [0, 0, 0, 2, 0], [0, 0, 0, 2, 1], [0, 0, 0, 2, 2], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 0, 2], [0, 0, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 0], [0, 0, 1, 2, 1], [0, 0, 1, 2, 2], [0, 0, 2, 0, 0], [0, 0, 2, 0, 1], [0, 0, 2, 0, 2], [0, 0, 2, 1, 0], [0, 0, 2, 1, 1], [0, 0, 2, 1, 2], [0, 0, 2, 2, 0], [0, 0, 2, 2, 1], [0, 0, 2, 2, 2], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 0, 2], [0, 1, 0, 1, 0], [0, 1, 0, 1, 1], [0, 1, 0, 1, 2], [0, 1, 0, 2, 0], [0, 1, 0, 2, 1], [0, 1, 0, 2, 2], [0, 1, 1, 0, 0], [0, 1, 1, 0, 1], [0, 1, 1, 0, 2], [0, 1, 1, 1, 0], [0, 1, 1, 1, 1], [0, 1, 1, 1, 2], [0, 1, 1, 2, 0], [0, 1, 1, 2, 1], [0, 1, 1, 2, 2], [0, 1, 2, 0, 0], [0, 1, 2, 0, 1], [0, 1, 2, 0, 2], [0, 1, 2, 1, 0], [0, 1, 2, 1, 1], [0, 1, 2, 1, 2], [0, 1, 2, 2, 0], [0, 1, 2, 2, 1], [0, 1, 2, 2, 2], [0, 2, 0, 0, 0], [0, 2, 0, 0, 1], [0, 2, 0, 0, 2], [0, 2, 0, 1, 0], [0, 2, 0, 1, 1], [0, 2, 0, 1, 2], [0, 2, 0, 2, 0], [0, 2, 0, 2, 1], [0, 2, 0, 2, 2], [0, 2, 1, 0, 0], [0, 2, 1, 0, 1], [0, 2, 1, 0, 2], [0, 2, 1, 1, 0], [0, 2, 1, 1, 1], [0, 2, 1, 1, 2], [0, 2, 1, 2, 0], [0, 2, 1, 2, 1], [0, 2, 1, 2, 2], [0, 2, 2, 0, 0], [0, 2, 2, 0, 1], [0, 2, 2, 0, 2], [0, 2, 2, 1, 0], [0, 2, 2, 1, 1], [0, 2, 2, 1, 2], [0, 2, 2, 2, 0], [0, 2, 2, 2, 1], [0, 2, 2, 2, 2], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 0, 2], [1, 0, 0, 1, 0], [1, 0, 0, 1, 1], [1, 0, 0, 1, 2], [1, 0, 0, 2, 0], [1, 0, 0, 2, 1], [1, 0, 0, 2, 2], [1, 0, 1, 0, 0], [1, 0, 1, 0, 1], [1, 0, 1, 0, 2], [1, 0, 1, 1, 0], [1, 0, 1, 1, 1], [1, 0, 1, 1, 2], [1, 0, 1, 2, 0], [1, 0, 1, 2, 1], [1, 0, 1, 2, 2], [1, 0, 2, 0, 0], [1, 0, 2, 0, 1], [1, 0, 2, 0, 2], [1, 0, 2, 1, 0], [1, 0, 2, 1, 1], [1, 0, 2, 1, 2], [1, 0, 2, 2, 0], [1, 0, 2, 2, 1], [1, 0, 2, 2, 2], [1, 1, 0, 0, 0], [1, 1, 0, 0, 1], [1, 1, 0, 0, 2], [1, 1, 0, 1, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 2], [1, 1, 0, 2, 0], [1, 1, 0, 2, 1], [1, 1, 0, 2, 2], [1, 1, 1, 0, 0], [1, 1, 1, 0, 1], [1, 1, 1, 0, 2], [1, 1, 1, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 2], [1, 1, 1, 2, 0], [1, 1, 1, 2, 1], [1, 1, 1, 2, 2], [1, 1, 2, 0, 0], [1, 1, 2, 0, 1], [1, 1, 2, 0, 2], [1, 1, 2, 1, 0], [1, 1, 2, 1, 1], [1, 1, 2, 1, 2], [1, 1, 2, 2, 0], [1, 1, 2, 2, 1], [1, 1, 2, 2, 2], [1, 2, 0, 0, 0], [1, 2, 0, 0, 1], [1, 2, 0, 0, 2], [1, 2, 0, 1, 0], [1, 2, 0, 1, 1], [1, 2, 0, 1, 2], [1, 2, 0, 2, 0], [1, 2, 0, 2, 1], [1, 2, 0, 2, 2], [1, 2, 1, 0, 0], [1, 2, 1, 0, 1], [1, 2, 1, 0, 2], [1, 2, 1, 1, 0], [1, 2, 1, 1, 1], [1, 2, 1, 1, 2], [1, 2, 1, 2, 0], [1, 2, 1, 2, 1], [1, 2, 1, 2, 2], [1, 2, 2, 0, 0], [1, 2, 2, 0, 1], [1, 2, 2, 0, 2], [1, 2, 2, 1, 0], [1, 2, 2, 1, 1], [1, 2, 2, 1, 2], [1, 2, 2, 2, 0], [1, 2, 2, 2, 1], [1, 2, 2, 2, 2], [2, 0, 0, 0, 0], [2, 0, 0, 0, 1], [2, 0, 0, 0, 2], [2, 0, 0, 1, 0], [2, 0, 0, 1, 1], [2, 0, 0, 1, 2], [2, 0, 0, 2, 0], [2, 0, 0, 2, 1], [2, 0, 0, 2, 2], [2, 0, 1, 0, 0], [2, 0, 1, 0, 1], [2, 0, 1, 0, 2], [2, 0, 1, 1, 0], [2, 0, 1, 1, 1], [2, 0, 1, 1, 2], [2, 0, 1, 2, 0], [2, 0, 1, 2, 1], [2, 0, 1, 2, 2], [2, 0, 2, 0, 0], [2, 0, 2, 0, 1], [2, 0, 2, 0, 2], [2, 0, 2, 1, 0], [2, 0, 2, 1, 1], [2, 0, 2, 1, 2], [2, 0, 2, 2, 0], [2, 0, 2, 2, 1], [2, 0, 2, 2, 2], [2, 1, 0, 0, 0], [2, 1, 0, 0, 1], [2, 1, 0, 0, 2], [2, 1, 0, 1, 0], [2, 1, 0, 1, 1], [2, 1, 0, 1, 2], [2, 1, 0, 2, 0], [2, 1, 0, 2, 1], [2, 1, 0, 2, 2], [2, 1, 1, 0, 0], [2, 1, 1, 0, 1], [2, 1, 1, 0, 2], [2, 1, 1, 1, 0], [2, 1, 1, 1, 1], [2, 1, 1, 1, 2], [2, 1, 1, 2, 0], [2, 1, 1, 2, 1], [2, 1, 1, 2, 2], [2, 1, 2, 0, 0], [2, 1, 2, 0, 1], [2, 1, 2, 0, 2], [2, 1, 2, 1, 0], [2, 1, 2, 1, 1], [2, 1, 2, 1, 2], [2, 1, 2, 2, 0], [2, 1, 2, 2, 1], [2, 1, 2, 2, 2], [2, 2, 0, 0, 0], [2, 2, 0, 0, 1], [2, 2, 0, 0, 2], [2, 2, 0, 1, 0], [2, 2, 0, 1, 1], [2, 2, 0, 1, 2], [2, 2, 0, 2, 0], [2, 2, 0, 2, 1], [2, 2, 0, 2, 2], [2, 2, 1, 0, 0], [2, 2, 1, 0, 1], [2, 2, 1, 0, 2], [2, 2, 1, 1, 0], [2, 2, 1, 1, 1], [2, 2, 1, 1, 2], [2, 2, 1, 2, 0], [2, 2, 1, 2, 1], [2, 2, 1, 2, 2], [2, 2, 2, 0, 0], [2, 2, 2, 0, 1], [2, 2, 2, 0, 2], [2, 2, 2, 1, 0], [2, 2, 2, 1, 1], [2, 2, 2, 1, 2], [2, 2, 2, 2, 0], [2, 2, 2, 2, 1], [2, 2, 2, 2, 2]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe58720987ec4931a49391f63606e975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 21\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 2, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 1, 0], [0, 0, 2, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 1, 0], [0, 1, 1, 0, 0], [0, 2, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 1, 0], [1, 0, 1, 0, 0], [1, 1, 0, 0, 0], [2, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)        \n",
    "else:\n",
    "    variable_sets = [[_d for _d in range(d+1)] for _ in range(n)]  \n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)\n",
    "\n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity: ' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)    \n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    if np.sum(monomial_identifier) <= d:\n",
    "        if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "            list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']\n",
    "\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense512-1024-output_110_drop0.25e500b256_custom/lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n",
      "lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index, no_noise=False):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path_identifier_lambda_net_data_loading = None \n",
    "                \n",
    "    if no_noise==True:\n",
    "        path_identifier_lambda_net_data_loading = generate_paths(path_type='interpretation_net_no_noise')['path_identifier_lambda_net_data']\n",
    "    else:\n",
    "        path_identifier_lambda_net_data_loading = path_identifier_lambda_net_data \n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data_loading + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == False:\n",
    "        lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend MultiprocessingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:   36.7s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if inet_training_without_noise:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list_without_noise = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1, no_noise=True) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.763</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>70.476</td>\n",
       "      <td>83.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-41.185</td>\n",
       "      <td>8.102</td>\n",
       "      <td>-2.361</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>70.312</td>\n",
       "      <td>4.155</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-7.685</td>\n",
       "      <td>45.664</td>\n",
       "      <td>1.059</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-22.699</td>\n",
       "      <td>-22.228</td>\n",
       "      <td>67.163</td>\n",
       "      <td>-4.917</td>\n",
       "      <td>0.747</td>\n",
       "      <td>31.916</td>\n",
       "      <td>57.046</td>\n",
       "      <td>-32.837</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.763</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>70.476</td>\n",
       "      <td>83.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.535</td>\n",
       "      <td>-1.360</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>2.249</td>\n",
       "      <td>2.175</td>\n",
       "      <td>2.428</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.545</td>\n",
       "      <td>2.848</td>\n",
       "      <td>-2.743</td>\n",
       "      <td>-2.918</td>\n",
       "      <td>2.354</td>\n",
       "      <td>2.211</td>\n",
       "      <td>2.566</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.412</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.388</td>\n",
       "      <td>2.234</td>\n",
       "      <td>-1.104</td>\n",
       "      <td>2.259</td>\n",
       "      <td>-3.106</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>1.359</td>\n",
       "      <td>1.806</td>\n",
       "      <td>2.103</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.176</td>\n",
       "      <td>1.731</td>\n",
       "      <td>-2.936</td>\n",
       "      <td>1.512</td>\n",
       "      <td>1.893</td>\n",
       "      <td>-2.204</td>\n",
       "      <td>1.344</td>\n",
       "      <td>-2.573</td>\n",
       "      <td>1.774</td>\n",
       "      <td>2.078</td>\n",
       "      <td>1.368</td>\n",
       "      <td>2.008</td>\n",
       "      <td>1.869</td>\n",
       "      <td>0.072</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.320</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-1.358</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-1.710</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>1.801</td>\n",
       "      <td>2.547</td>\n",
       "      <td>2.753</td>\n",
       "      <td>-1.258</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>2.124</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>2.089</td>\n",
       "      <td>1.331</td>\n",
       "      <td>-1.423</td>\n",
       "      <td>2.456</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>1.775</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.174</td>\n",
       "      <td>1.968</td>\n",
       "      <td>2.669</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>2.343</td>\n",
       "      <td>1.604</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>1.134</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.769</td>\n",
       "      <td>1.116</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>1.296</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.133</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.647</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.561</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.650</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>1.140</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.971</td>\n",
       "      <td>1.152</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.122</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>1.171</td>\n",
       "      <td>2.409</td>\n",
       "      <td>1.767</td>\n",
       "      <td>2.446</td>\n",
       "      <td>2.111</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>2.201</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2.063</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>2.312</td>\n",
       "      <td>-3.131</td>\n",
       "      <td>-2.432</td>\n",
       "      <td>-3.276</td>\n",
       "      <td>-2.749</td>\n",
       "      <td>1.911</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-2.923</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>1.832</td>\n",
       "      <td>-4.610</td>\n",
       "      <td>-4.665</td>\n",
       "      <td>-5.314</td>\n",
       "      <td>-4.788</td>\n",
       "      <td>1.856</td>\n",
       "      <td>-4.437</td>\n",
       "      <td>2.250</td>\n",
       "      <td>1.728</td>\n",
       "      <td>2.036</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1.835</td>\n",
       "      <td>-2.751</td>\n",
       "      <td>1.862</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-2.982</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>21.501</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.649</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.613</td>\n",
       "      <td>-31.331</td>\n",
       "      <td>-86.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.778</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.989</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-1.449</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>27.798</td>\n",
       "      <td>-1.441</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-42.118</td>\n",
       "      <td>-30.896</td>\n",
       "      <td>-86.983</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.137</td>\n",
       "      <td>21.501</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>27.649</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.613</td>\n",
       "      <td>-31.331</td>\n",
       "      <td>-86.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.457</td>\n",
       "      <td>1.951</td>\n",
       "      <td>2.862</td>\n",
       "      <td>2.649</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.054</td>\n",
       "      <td>2.546</td>\n",
       "      <td>3.061</td>\n",
       "      <td>2.993</td>\n",
       "      <td>2.102</td>\n",
       "      <td>1.684</td>\n",
       "      <td>2.832</td>\n",
       "      <td>-2.667</td>\n",
       "      <td>2.894</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>2.774</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.689</td>\n",
       "      <td>2.505</td>\n",
       "      <td>2.417</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>3.252</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.086</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.639</td>\n",
       "      <td>1.217</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.157</td>\n",
       "      <td>1.766</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>2.227</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.787</td>\n",
       "      <td>1.583</td>\n",
       "      <td>4.509</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>3.694</td>\n",
       "      <td>0.779</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>2.800</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-4.630</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.226</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.825</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.889</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-2.702</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>2.423</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>1.777</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-1.596</td>\n",
       "      <td>-2.369</td>\n",
       "      <td>-2.116</td>\n",
       "      <td>0.107</td>\n",
       "      <td>5.442</td>\n",
       "      <td>-7.049</td>\n",
       "      <td>-1.882</td>\n",
       "      <td>-1.946</td>\n",
       "      <td>-1.859</td>\n",
       "      <td>-2.118</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>-1.574</td>\n",
       "      <td>-1.978</td>\n",
       "      <td>3.529</td>\n",
       "      <td>-2.011</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.208</td>\n",
       "      <td>5.516</td>\n",
       "      <td>-1.897</td>\n",
       "      <td>-2.041</td>\n",
       "      <td>-1.495</td>\n",
       "      <td>-6.235</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.972</td>\n",
       "      <td>47.932</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-35.026</td>\n",
       "      <td>79.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-64.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6.966</td>\n",
       "      <td>46.836</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>49.512</td>\n",
       "      <td>-1.387</td>\n",
       "      <td>-3.075</td>\n",
       "      <td>4.746</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-15.663</td>\n",
       "      <td>0.802</td>\n",
       "      <td>4.168</td>\n",
       "      <td>1.668</td>\n",
       "      <td>-34.058</td>\n",
       "      <td>73.139</td>\n",
       "      <td>3.937</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>2.379</td>\n",
       "      <td>-44.606</td>\n",
       "      <td>3.116</td>\n",
       "      <td>-16.918</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.972</td>\n",
       "      <td>47.932</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-35.026</td>\n",
       "      <td>79.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-64.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.361</td>\n",
       "      <td>2.719</td>\n",
       "      <td>1.734</td>\n",
       "      <td>3.569</td>\n",
       "      <td>2.739</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.748</td>\n",
       "      <td>1.138</td>\n",
       "      <td>7.690</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.219</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>1.630</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-2.118</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>2.537</td>\n",
       "      <td>1.898</td>\n",
       "      <td>3.675</td>\n",
       "      <td>2.422</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.247</td>\n",
       "      <td>2.441</td>\n",
       "      <td>1.840</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>2.478</td>\n",
       "      <td>-1.954</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-2.081</td>\n",
       "      <td>2.237</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.592</td>\n",
       "      <td>1.866</td>\n",
       "      <td>2.011</td>\n",
       "      <td>1.955</td>\n",
       "      <td>1.728</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>1.866</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>2.378</td>\n",
       "      <td>2.066</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>2.059</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>2.487</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>2.299</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.523</td>\n",
       "      <td>0.239</td>\n",
       "      <td>2.602</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.544</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.747</td>\n",
       "      <td>-3.771</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>1.887</td>\n",
       "      <td>1.649</td>\n",
       "      <td>10.813</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>1.604</td>\n",
       "      <td>-2.985</td>\n",
       "      <td>-2.603</td>\n",
       "      <td>-9.552</td>\n",
       "      <td>-2.988</td>\n",
       "      <td>1.709</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>2.085</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.830</td>\n",
       "      <td>1.650</td>\n",
       "      <td>1.536</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>1.634</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>0.000</td>\n",
       "      <td>90.525</td>\n",
       "      <td>-36.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-17.307</td>\n",
       "      <td>-11.759</td>\n",
       "      <td>43.727</td>\n",
       "      <td>-21.109</td>\n",
       "      <td>25.278</td>\n",
       "      <td>69.439</td>\n",
       "      <td>-50.581</td>\n",
       "      <td>9.683</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>1.343</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-18.028</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>90.525</td>\n",
       "      <td>-36.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>8.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-17.307</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>1.222</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.087</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.325</td>\n",
       "      <td>1.375</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.107</td>\n",
       "      <td>1.182</td>\n",
       "      <td>0.107</td>\n",
       "      <td>1.177</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.671</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.383</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.391</td>\n",
       "      <td>1.133</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.489</td>\n",
       "      <td>1.159</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.797</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>1.830</td>\n",
       "      <td>3.048</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>2.560</td>\n",
       "      <td>1.977</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.403</td>\n",
       "      <td>2.740</td>\n",
       "      <td>-4.148</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1.777</td>\n",
       "      <td>0.814</td>\n",
       "      <td>1.147</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.565</td>\n",
       "      <td>1.461</td>\n",
       "      <td>1.235</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>1.323</td>\n",
       "      <td>2.914</td>\n",
       "      <td>-4.159</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.984</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>-1.841</td>\n",
       "      <td>1.921</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-1.283</td>\n",
       "      <td>-2.064</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>2.827</td>\n",
       "      <td>-1.584</td>\n",
       "      <td>2.039</td>\n",
       "      <td>-1.704</td>\n",
       "      <td>1.858</td>\n",
       "      <td>2.246</td>\n",
       "      <td>1.766</td>\n",
       "      <td>1.869</td>\n",
       "      <td>2.071</td>\n",
       "      <td>0.264</td>\n",
       "      <td>2.167</td>\n",
       "      <td>-2.012</td>\n",
       "      <td>2.306</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.279</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.534</td>\n",
       "      <td>1.151</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-2.380</td>\n",
       "      <td>-5.257</td>\n",
       "      <td>-2.280</td>\n",
       "      <td>-2.972</td>\n",
       "      <td>1.784</td>\n",
       "      <td>1.516</td>\n",
       "      <td>-1.550</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-2.016</td>\n",
       "      <td>-1.814</td>\n",
       "      <td>-2.032</td>\n",
       "      <td>-3.822</td>\n",
       "      <td>-5.174</td>\n",
       "      <td>-2.085</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>1.995</td>\n",
       "      <td>1.346</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.536</td>\n",
       "      <td>1.436</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>1.571</td>\n",
       "      <td>-4.513</td>\n",
       "      <td>-5.060</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-52.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.785</td>\n",
       "      <td>5.567</td>\n",
       "      <td>-6.165</td>\n",
       "      <td>1.923</td>\n",
       "      <td>-10.618</td>\n",
       "      <td>-89.536</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.762</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-31.265</td>\n",
       "      <td>1.290</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-6.202</td>\n",
       "      <td>2.913</td>\n",
       "      <td>-48.055</td>\n",
       "      <td>-2.042</td>\n",
       "      <td>2.466</td>\n",
       "      <td>-35.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.089</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-52.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.785</td>\n",
       "      <td>1.367</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.547</td>\n",
       "      <td>-1.377</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>1.640</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.883</td>\n",
       "      <td>1.843</td>\n",
       "      <td>1.228</td>\n",
       "      <td>1.239</td>\n",
       "      <td>1.694</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.744</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>2.466</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>1.460</td>\n",
       "      <td>4.547</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>1.507</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.047</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>1.524</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.061</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>1.380</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.103</td>\n",
       "      <td>1.747</td>\n",
       "      <td>1.561</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.355</td>\n",
       "      <td>1.784</td>\n",
       "      <td>1.832</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.933</td>\n",
       "      <td>4.398</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>1.729</td>\n",
       "      <td>2.252</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.996</td>\n",
       "      <td>1.443</td>\n",
       "      <td>1.764</td>\n",
       "      <td>1.023</td>\n",
       "      <td>1.446</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.697</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>3.340</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-4.390</td>\n",
       "      <td>0.982</td>\n",
       "      <td>2.697</td>\n",
       "      <td>1.266</td>\n",
       "      <td>1.857</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.296</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-3.556</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.087</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.144</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.073</td>\n",
       "      <td>2.234</td>\n",
       "      <td>0.037</td>\n",
       "      <td>3.779</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.944</td>\n",
       "      <td>1.409</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.584</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>2.015</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-1.541</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>2.421</td>\n",
       "      <td>-4.040</td>\n",
       "      <td>1.454</td>\n",
       "      <td>-1.494</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>-2.611</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-1.609</td>\n",
       "      <td>-1.587</td>\n",
       "      <td>-1.689</td>\n",
       "      <td>-1.651</td>\n",
       "      <td>6.844</td>\n",
       "      <td>2.094</td>\n",
       "      <td>-1.265</td>\n",
       "      <td>-2.889</td>\n",
       "      <td>-1.571</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.459</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>-1.551</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-1.524</td>\n",
       "      <td>2.835</td>\n",
       "      <td>-7.792</td>\n",
       "      <td>1.709</td>\n",
       "      <td>6.428</td>\n",
       "      <td>-7.317</td>\n",
       "      <td>-6.289</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>-1.675</td>\n",
       "      <td>-0.256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "6252  1373158606         0.000         0.000         0.000         0.000   \n",
       "4684  1373158606        21.501         0.000         0.000         0.000   \n",
       "1731  1373158606         0.000         0.000        47.972        47.932   \n",
       "4742  1373158606         0.000         0.000        13.349         0.000   \n",
       "4521  1373158606         0.000         0.261         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "6252         0.000         0.000         0.000        13.490         0.000   \n",
       "4684         0.000         0.000         0.000         0.000         0.000   \n",
       "1731         0.000         0.000         0.000         0.000         0.000   \n",
       "4742        90.525       -36.009         0.000         0.000         0.000   \n",
       "4521       -94.870         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "6252        27.763         0.000        -1.570         0.000         0.000   \n",
       "4684        27.649         0.000         0.000         0.000       -44.613   \n",
       "1731         0.000         0.000         0.000         0.000       -35.026   \n",
       "4742         8.893         0.000         0.000         0.000         0.000   \n",
       "4521         0.000       -28.089         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "6252         0.000         0.000         0.000         0.000        70.476   \n",
       "4684       -31.331       -86.712         0.000         0.000         0.000   \n",
       "1731        79.336         0.000         0.000         0.000       -64.234   \n",
       "4742         0.000         0.000         0.000         0.000         0.000   \n",
       "4521         0.000         0.000         0.000       -52.851         0.000   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "6252        83.810         0.000             -41.185               8.102   \n",
       "4684         0.000         0.000              21.778               0.372   \n",
       "1731         0.000         0.000              -6.966              46.836   \n",
       "4742         0.000       -17.307             -11.759              43.727   \n",
       "4521         0.000       -37.785               5.567              -6.165   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "6252              -2.361              -0.098               0.420   \n",
       "4684               0.327               0.989              -0.265   \n",
       "1731              -0.257              49.512              -1.387   \n",
       "4742             -21.109              25.278              69.439   \n",
       "4521               1.923             -10.618             -89.536   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "6252              -0.787              70.312               4.155   \n",
       "4684              -0.262              -1.449              -0.276   \n",
       "1731              -3.075               4.746               0.526   \n",
       "4742             -50.581               9.683               1.056   \n",
       "4521               5.157               1.762              -0.711   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "6252               0.154              -7.685              45.664   \n",
       "4684              -0.300              27.798              -1.441   \n",
       "1731              -0.141             -15.663               0.802   \n",
       "4742              -1.405              -0.290              -0.017   \n",
       "4521              -0.171              -0.511             -31.265   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "6252               1.059              -0.689             -22.699   \n",
       "4684              -0.186              -0.502             -42.118   \n",
       "1731               4.168               1.668             -34.058   \n",
       "4742               1.343              -0.966              -0.418   \n",
       "4521               1.290               1.220              -0.147   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "6252             -22.228              67.163              -4.917   \n",
       "4684             -30.896             -86.983              -0.677   \n",
       "1731              73.139               3.937              -0.805   \n",
       "4742              -0.220             -18.028              -0.205   \n",
       "4521               0.458              -6.202               2.913   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "6252               0.747              31.916              57.046   \n",
       "4684               0.339              -0.071               0.637   \n",
       "1731               2.379             -44.606               3.116   \n",
       "4742               0.507              -0.218              -0.296   \n",
       "4521             -48.055              -2.042               2.466   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "6252             -32.837              -0.000               0.000   \n",
       "4684               0.137              21.501              -0.000   \n",
       "1731             -16.918              -0.000               0.000   \n",
       "4742              -0.136              -0.000              -0.000   \n",
       "4521             -35.436               0.000               0.261   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "6252               0.000              -0.000              -0.000   \n",
       "4684               0.000              -0.000               0.000   \n",
       "1731              47.972              47.932              -0.000   \n",
       "4742              13.349              -0.000              90.525   \n",
       "4521              -0.000               0.000             -94.870   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "6252               0.000               0.000              13.490   \n",
       "4684               0.000              -0.000               0.000   \n",
       "1731              -0.000               0.000               0.000   \n",
       "4742             -36.009               0.000               0.000   \n",
       "4521               0.000              -0.000              -0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "6252               0.000              27.763               0.000   \n",
       "4684              -0.000              27.649              -0.000   \n",
       "1731              -0.000              -0.000               0.000   \n",
       "4742              -0.000               8.893               0.000   \n",
       "4521              -0.000               0.000             -28.089   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "6252              -1.570              -0.000               0.000   \n",
       "4684              -0.000               0.000             -44.613   \n",
       "1731              -0.000               0.000             -35.026   \n",
       "4742               0.000              -0.000               0.000   \n",
       "4521              -0.000              -0.000               0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "6252              -0.000              -0.000               0.000   \n",
       "4684             -31.331             -86.712               0.000   \n",
       "1731              79.336               0.000              -0.000   \n",
       "4742              -0.000               0.000              -0.000   \n",
       "4521               0.000              -0.000               0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "6252              -0.000              70.476              83.810   \n",
       "4684               0.000               0.000              -0.000   \n",
       "1731              -0.000             -64.234               0.000   \n",
       "4742               0.000              -0.000              -0.000   \n",
       "4521             -52.851               0.000               0.000   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1   wb_2   wb_3   wb_4   wb_5  wb_6  \\\n",
       "6252               0.000 -1.535 -1.360 -0.996 -1.032  2.249  2.175 2.428   \n",
       "4684               0.000  2.457  1.951  2.862  2.649  0.011  0.006 0.002   \n",
       "1731              -0.000 -0.070 -0.342  0.204  0.040  0.226  0.179 0.208   \n",
       "4742             -17.307  0.747 -0.579  1.222  0.697  0.185  0.121 1.087   \n",
       "4521             -37.785  1.367  1.032  1.693  1.547 -1.377 -0.567 1.640   \n",
       "\n",
       "       wb_7  wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  \\\n",
       "6252 -1.586 0.007 2.545  2.848 -2.743 -2.918  2.354  2.211  2.566  2.000   \n",
       "4684  2.054 2.546 3.061  2.993  2.102  1.684  2.832 -2.667  2.894 -0.039   \n",
       "1731 -0.413 0.003 0.361  2.719  1.734  3.569  2.739  0.169  0.200  0.092   \n",
       "4742 -0.424 0.967 1.325  1.375  0.251  0.107  1.182  0.107  1.177  0.006   \n",
       "4521  1.299 1.434 1.883  1.843  1.228  1.239  1.694  0.003  1.744 -0.701   \n",
       "\n",
       "      wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  \\\n",
       "6252  2.412  2.316  2.388  2.234 -1.104  2.259 -3.106 -1.624  0.023  0.119   \n",
       "4684  2.774  0.173  0.012  2.689  2.505  2.417 -0.027 -0.429  0.744  0.764   \n",
       "1731  0.289  0.332  0.292  0.104 -0.062 -0.083 -0.227 -0.497 -0.146 -0.045   \n",
       "4742  0.228  0.271  0.250  0.019 -0.066 -0.274 -0.052 -0.041  0.278 -0.158   \n",
       "4521  2.466 -0.008  1.460  4.547 -0.582  1.507  1.255  0.958  1.176  1.283   \n",
       "\n",
       "      wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  \\\n",
       "6252 -0.229 -0.179  1.359  1.806  2.103  0.326  0.176  1.731 -2.936  1.512   \n",
       "4684  0.661  0.658 -0.350 -1.593  3.252  0.912  1.086  0.985  0.680  0.639   \n",
       "1731 -0.317 -0.301  0.748  1.138  7.690  0.043  0.170  1.122  0.041  0.321   \n",
       "4742  0.657  0.097  0.261  0.684  0.732  0.043  0.697  0.545  0.617 -0.247   \n",
       "4521  1.024  1.047 -0.086  0.075  1.649 -0.024  1.524  1.304  0.981  0.932   \n",
       "\n",
       "      wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  \\\n",
       "6252  1.893 -2.204  1.344 -2.573  1.774  2.078  1.368  2.008  1.869  0.072   \n",
       "4684  1.217  0.709 -0.574  0.999  0.157  1.766 -0.274  2.227  1.570  0.787   \n",
       "1731  0.084  0.089  0.692 -0.020  1.144  1.429  0.710  1.392  1.219 -0.108   \n",
       "4742  0.081  0.660  0.205  0.547  0.717  0.890  0.263  0.850  0.671 -0.116   \n",
       "4521  1.182  1.061 -0.380  1.380  0.349 -0.086 -0.633 -0.046 -0.002  0.103   \n",
       "\n",
       "      wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  \\\n",
       "6252  2.216  2.320  0.008 -1.358 -0.793 -1.710 -1.290  1.801  2.547  2.753   \n",
       "4684  1.583  4.509 -0.492 -0.153 -0.039 -0.357 -0.180 -0.442  3.694  0.779   \n",
       "1731  1.630  0.091 -0.497 -0.106 -0.009 -0.433 -0.208 -0.421  0.359 -2.118   \n",
       "4742  0.887  0.035 -0.204 -0.103 -0.035  0.321 -0.284  0.383  1.082  0.471   \n",
       "4521  1.747  1.561  0.883  0.210  0.365 -0.095  0.124 -0.100  1.424  0.643   \n",
       "\n",
       "      wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  \\\n",
       "6252 -1.258 -0.147  2.124 -0.736  2.089  1.331 -1.423  2.456 -0.833  1.775   \n",
       "4684 -0.385 -0.143 -0.285 -0.027  0.229  0.176 -0.072  2.800 -0.213 -0.371   \n",
       "1731 -0.530 -0.149 -0.286  2.537  1.898  3.675  2.422  0.215 -0.231 -0.316   \n",
       "4742 -0.530  0.343  0.102  0.410 -0.099 -0.137  0.380  0.991  0.140  0.391   \n",
       "4521 -0.245  0.141  0.023  0.439  0.629  0.557  0.382  0.110  0.003  1.268   \n",
       "\n",
       "      wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  \\\n",
       "6252  2.644  2.174  1.968  2.669 -1.414  2.343  1.604 -1.120  1.134  0.769   \n",
       "4684  0.358 -0.051 -4.630  0.380 -0.407  0.143  1.226 -0.086  0.375  0.427   \n",
       "1731  0.307 -0.001 -0.339  0.324 -0.539 -0.256  0.125 -0.053  0.161  0.207   \n",
       "4742  1.133  0.733  0.489  1.159 -0.536  0.797 -0.082 -0.225  1.830  3.048   \n",
       "4521  0.015  0.255 -0.100  0.059  0.017  0.254  0.617  0.355  1.784  1.832   \n",
       "\n",
       "      wb_77  wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  \\\n",
       "6252  0.940  0.952  1.325  0.768  0.769  1.116 -0.138  1.296 -0.089  0.016   \n",
       "4684  0.105  0.485  0.222  0.010 -0.004  0.416  0.181  0.419  0.116  0.307   \n",
       "1731 -0.550  0.247  2.441  1.840  0.063  0.033 -0.139  2.478 -1.954 -0.921   \n",
       "4742 -0.237  2.560  1.977  1.384  0.427  0.021  0.001  1.017  0.403  2.740   \n",
       "4521  1.200  1.933  4.398 -1.300  1.729  2.252  1.409  1.996  1.443  1.764   \n",
       "\n",
       "      wb_87  wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  \\\n",
       "6252 -0.286 -0.285  1.133 -0.136  0.532  0.734  0.934  0.817  0.589  0.911   \n",
       "4684 -0.204  0.134  1.825  0.350 -0.462 -0.032 -0.070  0.040 -0.087  0.371   \n",
       "1731  0.110 -2.081  2.237  0.007  1.592  1.866  2.011  1.955  1.728 -0.027   \n",
       "4742 -4.148  0.111  1.777  0.814  1.147  1.380  1.565  1.461  1.235 -0.041   \n",
       "4521  1.023  1.446  0.034  1.697 -1.865  3.340 -0.758 -4.390  0.982  2.697   \n",
       "\n",
       "      wb_97  wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  \\\n",
       "6252  0.598  0.058  0.850   0.919   0.647   1.081   0.561   1.018   0.749   \n",
       "4684 -0.127  0.032 -0.338   0.102   0.226   0.391   0.071  -0.053  -0.004   \n",
       "1731  1.866  0.122 -0.359  -0.383  -0.051  -0.057  -0.534   2.378   2.066   \n",
       "4742  1.323  2.914 -4.159  -1.798  -1.984  -1.352  -1.841   1.921   1.602   \n",
       "4521  1.266  1.857  1.260   0.885   1.190   1.296   0.757  -3.556  -0.196   \n",
       "\n",
       "      wb_106  wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  \\\n",
       "6252   0.422   1.180  -0.121   0.650  -0.302  -1.219  -0.264  -0.129   1.140   \n",
       "4684  -0.031   0.539   0.255   0.101   0.024  -0.119   0.177   0.231   1.889   \n",
       "1731   0.166   0.161  -0.121   2.059  -0.717  -1.365   0.050  -0.356   2.487   \n",
       "4742  -1.593   0.178  -1.283  -2.064  -1.916  -2.114   2.827  -1.584   2.039   \n",
       "4521   0.944   1.578   1.087   0.951   0.841   0.740   1.250   1.144   0.037   \n",
       "\n",
       "      wb_115  wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  \\\n",
       "6252  -0.347   0.988   1.340   0.892   0.971   1.152   1.040   1.122  -0.394   \n",
       "4684   0.128   0.001   0.339  -0.097  -0.001   0.208   0.593   0.232   0.013   \n",
       "1731  -0.397   2.299   2.707   2.216   2.346   2.523   0.239   2.602  -0.346   \n",
       "4742  -1.704   1.858   2.246   1.766   1.869   2.071   0.264   2.167  -2.012   \n",
       "4521   0.856   0.073   2.234   0.037   3.779   0.123   2.944   1.409   0.955   \n",
       "\n",
       "      wb_124  wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  \\\n",
       "6252   1.171   2.409   1.767   2.446   2.111   0.472   0.433  -0.051   2.201   \n",
       "4684   0.279  -0.096  -0.232   0.004  -0.073  -0.174  -0.144  -2.702  -0.081   \n",
       "1731   0.288  -0.086  -0.168  -0.069  -0.152   0.368   0.331  -2.834  -0.193   \n",
       "4742   2.306   0.673  -1.044   1.190   0.159   0.639   0.592   0.955  -0.196   \n",
       "4521   1.584  -0.004  -0.033   0.052  -0.053  -0.599   2.015  -0.051  -1.541   \n",
       "\n",
       "      wb_133  wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  \\\n",
       "6252  -0.128   0.566   0.741   0.281  -1.015  -0.183   0.453   0.338   0.514   \n",
       "4684  -0.135  -0.074  -0.087  -0.205  -0.359  -0.130   2.423  -0.116  -0.076   \n",
       "1731  -0.136   0.544   1.062   1.747  -3.771   0.522   0.364  -0.164   0.368   \n",
       "4742   0.958   1.136   1.279  -0.258   0.534   1.151   0.617   0.944   0.643   \n",
       "4521  -0.055  -0.047   0.033   0.029   0.044  -0.022  -0.166  -0.156   2.421   \n",
       "\n",
       "      wb_142  wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  \\\n",
       "6252   0.471   0.444   0.475   0.497   2.063   0.436  -0.347   2.312  -3.131   \n",
       "4684  -0.490  -0.139   1.777  -0.374  -0.078  -0.259  -2.300  -0.148  -2.000   \n",
       "1731   0.416   0.336   0.406   0.450  -0.170   0.483  -0.242  -0.132  -0.224   \n",
       "4742   0.649   0.604   0.646   0.673  -0.174   0.649  -0.898   0.156  -2.380   \n",
       "4521  -4.040   1.454  -1.494  -2.834  -2.611  -0.007  -0.038   0.107  -1.609   \n",
       "\n",
       "      wb_151  wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  \\\n",
       "6252  -2.432  -3.276  -2.749   1.911   2.000   1.990  -2.923  -0.265   1.832   \n",
       "4684  -1.596  -2.369  -2.116   0.107   5.442  -7.049  -1.882  -1.946  -1.859   \n",
       "1731  -0.295  -0.307  -0.213   1.887   1.649  10.813  -0.319  -0.263   1.604   \n",
       "4742  -5.257  -2.280  -2.972   1.784   1.516  -1.550  -0.324  -2.016  -1.814   \n",
       "4521  -1.587  -1.689  -1.651   6.844   2.094  -1.265  -2.889  -1.571  -1.370   \n",
       "\n",
       "      wb_160  wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  \\\n",
       "6252  -4.610  -4.665  -5.314  -4.788   1.856  -4.437   2.250   1.728   2.036   \n",
       "4684  -2.118  -1.840  -1.574  -1.978   3.529  -2.011   0.400  -1.595   0.208   \n",
       "1731  -2.985  -2.603  -9.552  -2.988   1.709  -0.148   2.085   1.450   1.830   \n",
       "4742  -2.032  -3.822  -5.174  -2.085   1.602  -2.028   1.995   1.346   1.723   \n",
       "4521  -1.459  -1.621  -1.551  -1.421   0.085  -1.524   2.835  -7.792   1.709   \n",
       "\n",
       "      wb_169  wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "6252   1.875   1.835  -2.751   1.862  -4.165  -2.982   0.453  \n",
       "4684   5.516  -1.897  -2.041  -1.495  -6.235  -0.319   0.627  \n",
       "1731   1.650   1.536  -0.275   1.634  -0.129  -0.338   0.079  \n",
       "4742   1.536   1.436  -0.277   1.571  -4.513  -5.060   0.472  \n",
       "4521   6.428  -7.317  -6.289  -1.280  -1.407  -1.675  -0.256  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-4.569</td>\n",
       "      <td>5.873</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>5.054</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-4.454</td>\n",
       "      <td>5.521</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-4.879</td>\n",
       "      <td>5.760</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-5.212</td>\n",
       "      <td>5.537</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-5.045</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.660</td>\n",
       "      <td>1.010</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.580</td>\n",
       "      <td>1.426</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.657</td>\n",
       "      <td>1.085</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.897</td>\n",
       "      <td>1.165</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.721</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.597</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.237</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.365</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.238</td>\n",
       "      <td>1.463</td>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>1.353</td>\n",
       "      <td>1.252</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.707</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.635</td>\n",
       "      <td>1.105</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.052</td>\n",
       "      <td>1.217</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>1.460</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.976</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.484</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.375</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.813</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.777</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.534</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.939</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1.233</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.405</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.156</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>1.192</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.639</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-2.153</td>\n",
       "      <td>-2.743</td>\n",
       "      <td>-2.056</td>\n",
       "      <td>-2.603</td>\n",
       "      <td>2.996</td>\n",
       "      <td>2.918</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-2.912</td>\n",
       "      <td>-2.557</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-2.026</td>\n",
       "      <td>-2.608</td>\n",
       "      <td>-2.669</td>\n",
       "      <td>-2.218</td>\n",
       "      <td>2.537</td>\n",
       "      <td>-2.593</td>\n",
       "      <td>2.581</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>2.878</td>\n",
       "      <td>2.242</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-2.897</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-2.508</td>\n",
       "      <td>-2.562</td>\n",
       "      <td>0.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>27.624</td>\n",
       "      <td>28.250</td>\n",
       "      <td>28.421</td>\n",
       "      <td>28.151</td>\n",
       "      <td>28.125</td>\n",
       "      <td>27.360</td>\n",
       "      <td>27.919</td>\n",
       "      <td>28.099</td>\n",
       "      <td>27.657</td>\n",
       "      <td>29.141</td>\n",
       "      <td>27.608</td>\n",
       "      <td>28.560</td>\n",
       "      <td>27.877</td>\n",
       "      <td>28.028</td>\n",
       "      <td>28.426</td>\n",
       "      <td>28.539</td>\n",
       "      <td>27.815</td>\n",
       "      <td>28.991</td>\n",
       "      <td>28.325</td>\n",
       "      <td>28.276</td>\n",
       "      <td>27.948</td>\n",
       "      <td>28.864</td>\n",
       "      <td>38.672</td>\n",
       "      <td>22.765</td>\n",
       "      <td>39.263</td>\n",
       "      <td>22.560</td>\n",
       "      <td>21.677</td>\n",
       "      <td>38.323</td>\n",
       "      <td>22.524</td>\n",
       "      <td>22.292</td>\n",
       "      <td>23.853</td>\n",
       "      <td>37.254</td>\n",
       "      <td>23.033</td>\n",
       "      <td>22.553</td>\n",
       "      <td>22.826</td>\n",
       "      <td>23.592</td>\n",
       "      <td>36.164</td>\n",
       "      <td>22.916</td>\n",
       "      <td>23.417</td>\n",
       "      <td>23.188</td>\n",
       "      <td>22.946</td>\n",
       "      <td>24.269</td>\n",
       "      <td>27.624</td>\n",
       "      <td>28.250</td>\n",
       "      <td>28.421</td>\n",
       "      <td>28.151</td>\n",
       "      <td>28.125</td>\n",
       "      <td>27.360</td>\n",
       "      <td>27.919</td>\n",
       "      <td>28.099</td>\n",
       "      <td>27.657</td>\n",
       "      <td>29.141</td>\n",
       "      <td>27.608</td>\n",
       "      <td>28.560</td>\n",
       "      <td>27.877</td>\n",
       "      <td>28.028</td>\n",
       "      <td>28.426</td>\n",
       "      <td>28.539</td>\n",
       "      <td>27.815</td>\n",
       "      <td>28.991</td>\n",
       "      <td>28.325</td>\n",
       "      <td>28.276</td>\n",
       "      <td>27.948</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.015</td>\n",
       "      <td>1.076</td>\n",
       "      <td>1.300</td>\n",
       "      <td>1.385</td>\n",
       "      <td>1.313</td>\n",
       "      <td>1.237</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.223</td>\n",
       "      <td>1.142</td>\n",
       "      <td>1.149</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.332</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.253</td>\n",
       "      <td>1.277</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.326</td>\n",
       "      <td>1.282</td>\n",
       "      <td>1.155</td>\n",
       "      <td>1.131</td>\n",
       "      <td>1.238</td>\n",
       "      <td>1.132</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.175</td>\n",
       "      <td>1.050</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.551</td>\n",
       "      <td>1.297</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.121</td>\n",
       "      <td>1.059</td>\n",
       "      <td>1.141</td>\n",
       "      <td>1.313</td>\n",
       "      <td>1.183</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.516</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.336</td>\n",
       "      <td>1.431</td>\n",
       "      <td>1.347</td>\n",
       "      <td>1.329</td>\n",
       "      <td>1.189</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.138</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.108</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.175</td>\n",
       "      <td>1.600</td>\n",
       "      <td>1.277</td>\n",
       "      <td>1.325</td>\n",
       "      <td>1.257</td>\n",
       "      <td>1.123</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.255</td>\n",
       "      <td>1.215</td>\n",
       "      <td>1.267</td>\n",
       "      <td>1.314</td>\n",
       "      <td>1.351</td>\n",
       "      <td>1.681</td>\n",
       "      <td>1.343</td>\n",
       "      <td>1.272</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.081</td>\n",
       "      <td>1.129</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.028</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.561</td>\n",
       "      <td>1.351</td>\n",
       "      <td>1.134</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.119</td>\n",
       "      <td>1.325</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.194</td>\n",
       "      <td>1.167</td>\n",
       "      <td>1.279</td>\n",
       "      <td>1.091</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.329</td>\n",
       "      <td>1.413</td>\n",
       "      <td>1.694</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.166</td>\n",
       "      <td>1.277</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.109</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1.152</td>\n",
       "      <td>1.505</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.121</td>\n",
       "      <td>1.104</td>\n",
       "      <td>1.229</td>\n",
       "      <td>1.308</td>\n",
       "      <td>1.140</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.118</td>\n",
       "      <td>1.302</td>\n",
       "      <td>1.237</td>\n",
       "      <td>1.241</td>\n",
       "      <td>1.323</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.616</td>\n",
       "      <td>1.332</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.129</td>\n",
       "      <td>1.242</td>\n",
       "      <td>1.092</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.131</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.177</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.260</td>\n",
       "      <td>1.472</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.118</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.264</td>\n",
       "      <td>1.134</td>\n",
       "      <td>1.152</td>\n",
       "      <td>1.104</td>\n",
       "      <td>1.311</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.124</td>\n",
       "      <td>1.815</td>\n",
       "      <td>1.218</td>\n",
       "      <td>1.371</td>\n",
       "      <td>1.668</td>\n",
       "      <td>1.219</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.174</td>\n",
       "      <td>1.129</td>\n",
       "      <td>1.529</td>\n",
       "      <td>2.086</td>\n",
       "      <td>1.686</td>\n",
       "      <td>1.942</td>\n",
       "      <td>2.157</td>\n",
       "      <td>2.133</td>\n",
       "      <td>3.402</td>\n",
       "      <td>2.195</td>\n",
       "      <td>1.836</td>\n",
       "      <td>2.285</td>\n",
       "      <td>2.906</td>\n",
       "      <td>1.972</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1.648</td>\n",
       "      <td>2.091</td>\n",
       "      <td>1.939</td>\n",
       "      <td>1.918</td>\n",
       "      <td>3.557</td>\n",
       "      <td>2.006</td>\n",
       "      <td>3.281</td>\n",
       "      <td>3.528</td>\n",
       "      <td>2.089</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.196</td>\n",
       "      <td>1.882</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-99.990</td>\n",
       "      <td>-99.917</td>\n",
       "      <td>-99.960</td>\n",
       "      <td>-99.940</td>\n",
       "      <td>-99.913</td>\n",
       "      <td>-99.932</td>\n",
       "      <td>-99.937</td>\n",
       "      <td>-99.773</td>\n",
       "      <td>-99.939</td>\n",
       "      <td>-99.915</td>\n",
       "      <td>-99.978</td>\n",
       "      <td>-99.759</td>\n",
       "      <td>-99.995</td>\n",
       "      <td>-99.691</td>\n",
       "      <td>-99.994</td>\n",
       "      <td>-99.938</td>\n",
       "      <td>-99.792</td>\n",
       "      <td>-99.992</td>\n",
       "      <td>-99.953</td>\n",
       "      <td>-99.951</td>\n",
       "      <td>-99.999</td>\n",
       "      <td>-115.210</td>\n",
       "      <td>-249.907</td>\n",
       "      <td>-101.704</td>\n",
       "      <td>-188.259</td>\n",
       "      <td>-97.695</td>\n",
       "      <td>-100.432</td>\n",
       "      <td>-200.007</td>\n",
       "      <td>-96.910</td>\n",
       "      <td>-98.448</td>\n",
       "      <td>-101.943</td>\n",
       "      <td>-190.762</td>\n",
       "      <td>-97.929</td>\n",
       "      <td>-96.384</td>\n",
       "      <td>-97.691</td>\n",
       "      <td>-103.220</td>\n",
       "      <td>-164.662</td>\n",
       "      <td>-99.797</td>\n",
       "      <td>-97.278</td>\n",
       "      <td>-97.507</td>\n",
       "      <td>-96.521</td>\n",
       "      <td>-99.013</td>\n",
       "      <td>-99.990</td>\n",
       "      <td>-99.917</td>\n",
       "      <td>-99.960</td>\n",
       "      <td>-99.940</td>\n",
       "      <td>-99.913</td>\n",
       "      <td>-99.932</td>\n",
       "      <td>-99.937</td>\n",
       "      <td>-99.773</td>\n",
       "      <td>-99.939</td>\n",
       "      <td>-99.915</td>\n",
       "      <td>-99.978</td>\n",
       "      <td>-99.759</td>\n",
       "      <td>-99.995</td>\n",
       "      <td>-99.691</td>\n",
       "      <td>-99.994</td>\n",
       "      <td>-99.938</td>\n",
       "      <td>-99.792</td>\n",
       "      <td>-99.992</td>\n",
       "      <td>-99.953</td>\n",
       "      <td>-99.951</td>\n",
       "      <td>-99.999</td>\n",
       "      <td>-5.788</td>\n",
       "      <td>-8.141</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>-5.186</td>\n",
       "      <td>-4.866</td>\n",
       "      <td>-6.934</td>\n",
       "      <td>-5.730</td>\n",
       "      <td>-7.206</td>\n",
       "      <td>-5.454</td>\n",
       "      <td>-2.355</td>\n",
       "      <td>-3.553</td>\n",
       "      <td>-6.756</td>\n",
       "      <td>-7.128</td>\n",
       "      <td>-2.445</td>\n",
       "      <td>-6.582</td>\n",
       "      <td>-2.216</td>\n",
       "      <td>-6.384</td>\n",
       "      <td>-4.908</td>\n",
       "      <td>-3.295</td>\n",
       "      <td>-5.653</td>\n",
       "      <td>-5.721</td>\n",
       "      <td>-5.362</td>\n",
       "      <td>-7.792</td>\n",
       "      <td>-7.991</td>\n",
       "      <td>-8.100</td>\n",
       "      <td>-6.161</td>\n",
       "      <td>-7.256</td>\n",
       "      <td>-7.584</td>\n",
       "      <td>-7.848</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-5.184</td>\n",
       "      <td>-6.947</td>\n",
       "      <td>-4.995</td>\n",
       "      <td>-2.656</td>\n",
       "      <td>-4.144</td>\n",
       "      <td>-9.570</td>\n",
       "      <td>-7.475</td>\n",
       "      <td>-6.757</td>\n",
       "      <td>-7.855</td>\n",
       "      <td>-8.951</td>\n",
       "      <td>-4.880</td>\n",
       "      <td>-2.805</td>\n",
       "      <td>-6.159</td>\n",
       "      <td>-8.799</td>\n",
       "      <td>-3.486</td>\n",
       "      <td>-6.886</td>\n",
       "      <td>-7.012</td>\n",
       "      <td>-2.964</td>\n",
       "      <td>-3.434</td>\n",
       "      <td>-7.481</td>\n",
       "      <td>-6.892</td>\n",
       "      <td>-5.020</td>\n",
       "      <td>-7.141</td>\n",
       "      <td>-7.649</td>\n",
       "      <td>-9.350</td>\n",
       "      <td>-4.900</td>\n",
       "      <td>-6.229</td>\n",
       "      <td>-7.351</td>\n",
       "      <td>-7.367</td>\n",
       "      <td>-6.229</td>\n",
       "      <td>-7.010</td>\n",
       "      <td>-3.467</td>\n",
       "      <td>-2.206</td>\n",
       "      <td>-4.563</td>\n",
       "      <td>-4.035</td>\n",
       "      <td>-7.044</td>\n",
       "      <td>-7.787</td>\n",
       "      <td>-4.851</td>\n",
       "      <td>-7.377</td>\n",
       "      <td>-9.271</td>\n",
       "      <td>-4.962</td>\n",
       "      <td>-8.531</td>\n",
       "      <td>-5.223</td>\n",
       "      <td>-4.944</td>\n",
       "      <td>-5.572</td>\n",
       "      <td>-2.066</td>\n",
       "      <td>-4.030</td>\n",
       "      <td>-6.563</td>\n",
       "      <td>-2.081</td>\n",
       "      <td>-3.876</td>\n",
       "      <td>-8.212</td>\n",
       "      <td>-5.787</td>\n",
       "      <td>-5.505</td>\n",
       "      <td>-7.305</td>\n",
       "      <td>-2.192</td>\n",
       "      <td>-8.931</td>\n",
       "      <td>-3.572</td>\n",
       "      <td>-8.269</td>\n",
       "      <td>-7.613</td>\n",
       "      <td>-5.446</td>\n",
       "      <td>-5.358</td>\n",
       "      <td>-7.793</td>\n",
       "      <td>-7.050</td>\n",
       "      <td>-7.868</td>\n",
       "      <td>-10.001</td>\n",
       "      <td>-7.657</td>\n",
       "      <td>-5.927</td>\n",
       "      <td>-5.534</td>\n",
       "      <td>-4.123</td>\n",
       "      <td>-6.366</td>\n",
       "      <td>-6.368</td>\n",
       "      <td>-5.602</td>\n",
       "      <td>-4.875</td>\n",
       "      <td>-8.039</td>\n",
       "      <td>-7.619</td>\n",
       "      <td>-9.254</td>\n",
       "      <td>-8.466</td>\n",
       "      <td>-3.277</td>\n",
       "      <td>-7.663</td>\n",
       "      <td>-8.007</td>\n",
       "      <td>-9.362</td>\n",
       "      <td>-7.239</td>\n",
       "      <td>-5.340</td>\n",
       "      <td>-6.399</td>\n",
       "      <td>-5.068</td>\n",
       "      <td>-8.037</td>\n",
       "      <td>-8.667</td>\n",
       "      <td>-5.843</td>\n",
       "      <td>-7.817</td>\n",
       "      <td>-8.911</td>\n",
       "      <td>-8.530</td>\n",
       "      <td>-2.682</td>\n",
       "      <td>-4.030</td>\n",
       "      <td>-7.363</td>\n",
       "      <td>-1.641</td>\n",
       "      <td>-4.057</td>\n",
       "      <td>-5.455</td>\n",
       "      <td>-4.459</td>\n",
       "      <td>-5.512</td>\n",
       "      <td>-4.769</td>\n",
       "      <td>-5.696</td>\n",
       "      <td>-5.722</td>\n",
       "      <td>-5.491</td>\n",
       "      <td>-4.336</td>\n",
       "      <td>-4.742</td>\n",
       "      <td>-5.880</td>\n",
       "      <td>-4.861</td>\n",
       "      <td>-4.771</td>\n",
       "      <td>-4.160</td>\n",
       "      <td>-5.656</td>\n",
       "      <td>-4.678</td>\n",
       "      <td>-4.586</td>\n",
       "      <td>-5.631</td>\n",
       "      <td>-4.327</td>\n",
       "      <td>-5.116</td>\n",
       "      <td>-6.224</td>\n",
       "      <td>-5.120</td>\n",
       "      <td>-4.424</td>\n",
       "      <td>-5.624</td>\n",
       "      <td>-5.248</td>\n",
       "      <td>-11.567</td>\n",
       "      <td>-13.441</td>\n",
       "      <td>-12.502</td>\n",
       "      <td>-12.682</td>\n",
       "      <td>-8.375</td>\n",
       "      <td>-3.007</td>\n",
       "      <td>-11.956</td>\n",
       "      <td>-13.422</td>\n",
       "      <td>-11.929</td>\n",
       "      <td>-10.121</td>\n",
       "      <td>-12.531</td>\n",
       "      <td>-12.851</td>\n",
       "      <td>-13.990</td>\n",
       "      <td>-11.576</td>\n",
       "      <td>-6.036</td>\n",
       "      <td>-12.173</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-12.950</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-14.495</td>\n",
       "      <td>-13.248</td>\n",
       "      <td>-13.183</td>\n",
       "      <td>-10.104</td>\n",
       "      <td>-12.481</td>\n",
       "      <td>-13.800</td>\n",
       "      <td>-2.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-15.367</td>\n",
       "      <td>-2.542</td>\n",
       "      <td>-8.066</td>\n",
       "      <td>-2.880</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-6.525</td>\n",
       "      <td>-2.733</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>-7.129</td>\n",
       "      <td>-2.824</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>-7.760</td>\n",
       "      <td>-2.826</td>\n",
       "      <td>-0.865</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-8.588</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.576</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.246</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>0.498</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.731</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.299</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-1.004</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-2.768</td>\n",
       "      <td>-3.552</td>\n",
       "      <td>-2.830</td>\n",
       "      <td>-3.217</td>\n",
       "      <td>1.873</td>\n",
       "      <td>1.796</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>-3.888</td>\n",
       "      <td>-3.072</td>\n",
       "      <td>-1.769</td>\n",
       "      <td>-3.084</td>\n",
       "      <td>-3.267</td>\n",
       "      <td>-3.314</td>\n",
       "      <td>-2.793</td>\n",
       "      <td>1.593</td>\n",
       "      <td>-3.287</td>\n",
       "      <td>1.715</td>\n",
       "      <td>-1.987</td>\n",
       "      <td>1.892</td>\n",
       "      <td>1.725</td>\n",
       "      <td>-1.943</td>\n",
       "      <td>-3.834</td>\n",
       "      <td>-1.828</td>\n",
       "      <td>-3.450</td>\n",
       "      <td>-3.309</td>\n",
       "      <td>-0.266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>0.569</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.655</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.577</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.869</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.393</td>\n",
       "      <td>1.407</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.335</td>\n",
       "      <td>1.079</td>\n",
       "      <td>1.138</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.836</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.838</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.732</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.893</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.120</td>\n",
       "      <td>1.240</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.399</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.240</td>\n",
       "      <td>1.131</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>1.212</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.112</td>\n",
       "      <td>1.401</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.700</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.379</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>1.330</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.528</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.591</td>\n",
       "      <td>1.218</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.542</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.696</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-2.040</td>\n",
       "      <td>-2.060</td>\n",
       "      <td>-2.022</td>\n",
       "      <td>-2.189</td>\n",
       "      <td>2.318</td>\n",
       "      <td>2.231</td>\n",
       "      <td>-1.196</td>\n",
       "      <td>-2.290</td>\n",
       "      <td>-2.095</td>\n",
       "      <td>-1.079</td>\n",
       "      <td>-2.102</td>\n",
       "      <td>-2.200</td>\n",
       "      <td>-2.217</td>\n",
       "      <td>-1.974</td>\n",
       "      <td>2.061</td>\n",
       "      <td>-2.050</td>\n",
       "      <td>2.425</td>\n",
       "      <td>1.360</td>\n",
       "      <td>2.318</td>\n",
       "      <td>2.147</td>\n",
       "      <td>1.599</td>\n",
       "      <td>-2.266</td>\n",
       "      <td>-1.218</td>\n",
       "      <td>-1.782</td>\n",
       "      <td>-2.290</td>\n",
       "      <td>0.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.636</td>\n",
       "      <td>14.795</td>\n",
       "      <td>0.282</td>\n",
       "      <td>14.201</td>\n",
       "      <td>1.095</td>\n",
       "      <td>0.321</td>\n",
       "      <td>13.034</td>\n",
       "      <td>1.088</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.303</td>\n",
       "      <td>12.544</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.300</td>\n",
       "      <td>13.985</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.023</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.123</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1.561</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.729</td>\n",
       "      <td>1.523</td>\n",
       "      <td>1.794</td>\n",
       "      <td>0.408</td>\n",
       "      <td>1.199</td>\n",
       "      <td>2.152</td>\n",
       "      <td>2.003</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.610</td>\n",
       "      <td>1.521</td>\n",
       "      <td>1.774</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.923</td>\n",
       "      <td>1.969</td>\n",
       "      <td>1.998</td>\n",
       "      <td>1.719</td>\n",
       "      <td>1.108</td>\n",
       "      <td>1.616</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.877</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.977</td>\n",
       "      <td>1.743</td>\n",
       "      <td>2.035</td>\n",
       "      <td>1.439</td>\n",
       "      <td>1.647</td>\n",
       "      <td>1.534</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.855</td>\n",
       "      <td>1.258</td>\n",
       "      <td>1.799</td>\n",
       "      <td>2.126</td>\n",
       "      <td>0.933</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.882</td>\n",
       "      <td>0.924</td>\n",
       "      <td>2.264</td>\n",
       "      <td>1.509</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.894</td>\n",
       "      <td>1.124</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.918</td>\n",
       "      <td>2.185</td>\n",
       "      <td>2.110</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.296</td>\n",
       "      <td>1.455</td>\n",
       "      <td>1.721</td>\n",
       "      <td>1.665</td>\n",
       "      <td>1.326</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.848</td>\n",
       "      <td>2.129</td>\n",
       "      <td>1.505</td>\n",
       "      <td>1.025</td>\n",
       "      <td>2.341</td>\n",
       "      <td>0.219</td>\n",
       "      <td>1.619</td>\n",
       "      <td>1.549</td>\n",
       "      <td>1.198</td>\n",
       "      <td>1.413</td>\n",
       "      <td>1.659</td>\n",
       "      <td>0.356</td>\n",
       "      <td>1.739</td>\n",
       "      <td>2.378</td>\n",
       "      <td>1.325</td>\n",
       "      <td>1.768</td>\n",
       "      <td>1.331</td>\n",
       "      <td>0.691</td>\n",
       "      <td>2.193</td>\n",
       "      <td>0.703</td>\n",
       "      <td>1.444</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1.939</td>\n",
       "      <td>1.198</td>\n",
       "      <td>0.888</td>\n",
       "      <td>1.509</td>\n",
       "      <td>1.587</td>\n",
       "      <td>1.386</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.095</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.427</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.328</td>\n",
       "      <td>1.599</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.961</td>\n",
       "      <td>1.633</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.337</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.840</td>\n",
       "      <td>1.794</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1.544</td>\n",
       "      <td>2.121</td>\n",
       "      <td>1.334</td>\n",
       "      <td>1.453</td>\n",
       "      <td>1.924</td>\n",
       "      <td>1.821</td>\n",
       "      <td>1.796</td>\n",
       "      <td>0.454</td>\n",
       "      <td>1.851</td>\n",
       "      <td>1.251</td>\n",
       "      <td>0.757</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1.156</td>\n",
       "      <td>1.447</td>\n",
       "      <td>1.338</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.883</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.471</td>\n",
       "      <td>1.376</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.273</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.386</td>\n",
       "      <td>1.298</td>\n",
       "      <td>1.442</td>\n",
       "      <td>1.516</td>\n",
       "      <td>1.378</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.337</td>\n",
       "      <td>0.559</td>\n",
       "      <td>1.218</td>\n",
       "      <td>-1.419</td>\n",
       "      <td>-1.557</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-1.569</td>\n",
       "      <td>3.547</td>\n",
       "      <td>3.272</td>\n",
       "      <td>2.009</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-1.567</td>\n",
       "      <td>1.978</td>\n",
       "      <td>-1.472</td>\n",
       "      <td>-1.567</td>\n",
       "      <td>-1.628</td>\n",
       "      <td>-1.423</td>\n",
       "      <td>2.812</td>\n",
       "      <td>-1.503</td>\n",
       "      <td>3.019</td>\n",
       "      <td>1.865</td>\n",
       "      <td>3.119</td>\n",
       "      <td>3.030</td>\n",
       "      <td>2.072</td>\n",
       "      <td>-1.655</td>\n",
       "      <td>1.917</td>\n",
       "      <td>-1.301</td>\n",
       "      <td>-1.550</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>99.925</td>\n",
       "      <td>99.812</td>\n",
       "      <td>99.873</td>\n",
       "      <td>99.984</td>\n",
       "      <td>99.957</td>\n",
       "      <td>99.996</td>\n",
       "      <td>99.915</td>\n",
       "      <td>99.982</td>\n",
       "      <td>99.976</td>\n",
       "      <td>99.939</td>\n",
       "      <td>99.952</td>\n",
       "      <td>99.833</td>\n",
       "      <td>99.966</td>\n",
       "      <td>99.919</td>\n",
       "      <td>99.973</td>\n",
       "      <td>99.969</td>\n",
       "      <td>99.959</td>\n",
       "      <td>99.998</td>\n",
       "      <td>99.954</td>\n",
       "      <td>99.976</td>\n",
       "      <td>99.968</td>\n",
       "      <td>109.030</td>\n",
       "      <td>218.296</td>\n",
       "      <td>99.537</td>\n",
       "      <td>220.494</td>\n",
       "      <td>96.983</td>\n",
       "      <td>100.964</td>\n",
       "      <td>197.450</td>\n",
       "      <td>98.467</td>\n",
       "      <td>96.732</td>\n",
       "      <td>101.082</td>\n",
       "      <td>224.960</td>\n",
       "      <td>95.322</td>\n",
       "      <td>97.006</td>\n",
       "      <td>98.955</td>\n",
       "      <td>98.950</td>\n",
       "      <td>203.581</td>\n",
       "      <td>95.318</td>\n",
       "      <td>97.111</td>\n",
       "      <td>96.114</td>\n",
       "      <td>97.142</td>\n",
       "      <td>98.530</td>\n",
       "      <td>99.925</td>\n",
       "      <td>99.812</td>\n",
       "      <td>99.873</td>\n",
       "      <td>99.984</td>\n",
       "      <td>99.957</td>\n",
       "      <td>99.996</td>\n",
       "      <td>99.915</td>\n",
       "      <td>99.982</td>\n",
       "      <td>99.976</td>\n",
       "      <td>99.939</td>\n",
       "      <td>99.952</td>\n",
       "      <td>99.833</td>\n",
       "      <td>99.966</td>\n",
       "      <td>99.919</td>\n",
       "      <td>99.973</td>\n",
       "      <td>99.969</td>\n",
       "      <td>99.959</td>\n",
       "      <td>99.998</td>\n",
       "      <td>99.954</td>\n",
       "      <td>99.976</td>\n",
       "      <td>99.968</td>\n",
       "      <td>3.915</td>\n",
       "      <td>4.245</td>\n",
       "      <td>6.232</td>\n",
       "      <td>6.977</td>\n",
       "      <td>7.847</td>\n",
       "      <td>6.710</td>\n",
       "      <td>8.528</td>\n",
       "      <td>4.238</td>\n",
       "      <td>5.422</td>\n",
       "      <td>6.311</td>\n",
       "      <td>7.964</td>\n",
       "      <td>4.623</td>\n",
       "      <td>6.149</td>\n",
       "      <td>5.329</td>\n",
       "      <td>6.830</td>\n",
       "      <td>6.516</td>\n",
       "      <td>7.047</td>\n",
       "      <td>7.526</td>\n",
       "      <td>7.159</td>\n",
       "      <td>7.846</td>\n",
       "      <td>7.497</td>\n",
       "      <td>5.942</td>\n",
       "      <td>6.351</td>\n",
       "      <td>5.821</td>\n",
       "      <td>4.175</td>\n",
       "      <td>5.940</td>\n",
       "      <td>5.664</td>\n",
       "      <td>5.804</td>\n",
       "      <td>4.675</td>\n",
       "      <td>6.376</td>\n",
       "      <td>6.397</td>\n",
       "      <td>8.694</td>\n",
       "      <td>7.141</td>\n",
       "      <td>6.168</td>\n",
       "      <td>6.735</td>\n",
       "      <td>5.844</td>\n",
       "      <td>4.159</td>\n",
       "      <td>6.039</td>\n",
       "      <td>4.600</td>\n",
       "      <td>5.643</td>\n",
       "      <td>7.102</td>\n",
       "      <td>6.964</td>\n",
       "      <td>7.052</td>\n",
       "      <td>6.530</td>\n",
       "      <td>8.451</td>\n",
       "      <td>6.758</td>\n",
       "      <td>5.740</td>\n",
       "      <td>7.902</td>\n",
       "      <td>6.669</td>\n",
       "      <td>3.942</td>\n",
       "      <td>6.270</td>\n",
       "      <td>6.098</td>\n",
       "      <td>3.780</td>\n",
       "      <td>5.384</td>\n",
       "      <td>7.399</td>\n",
       "      <td>8.012</td>\n",
       "      <td>8.558</td>\n",
       "      <td>3.231</td>\n",
       "      <td>4.057</td>\n",
       "      <td>4.861</td>\n",
       "      <td>7.331</td>\n",
       "      <td>6.775</td>\n",
       "      <td>6.525</td>\n",
       "      <td>5.405</td>\n",
       "      <td>6.883</td>\n",
       "      <td>4.807</td>\n",
       "      <td>6.771</td>\n",
       "      <td>6.963</td>\n",
       "      <td>7.401</td>\n",
       "      <td>6.703</td>\n",
       "      <td>7.720</td>\n",
       "      <td>3.320</td>\n",
       "      <td>7.221</td>\n",
       "      <td>6.983</td>\n",
       "      <td>6.947</td>\n",
       "      <td>6.203</td>\n",
       "      <td>6.153</td>\n",
       "      <td>6.313</td>\n",
       "      <td>6.765</td>\n",
       "      <td>7.767</td>\n",
       "      <td>6.430</td>\n",
       "      <td>8.635</td>\n",
       "      <td>7.949</td>\n",
       "      <td>4.708</td>\n",
       "      <td>7.353</td>\n",
       "      <td>7.288</td>\n",
       "      <td>8.025</td>\n",
       "      <td>2.817</td>\n",
       "      <td>3.796</td>\n",
       "      <td>7.662</td>\n",
       "      <td>6.540</td>\n",
       "      <td>7.036</td>\n",
       "      <td>7.788</td>\n",
       "      <td>6.630</td>\n",
       "      <td>6.527</td>\n",
       "      <td>7.086</td>\n",
       "      <td>6.072</td>\n",
       "      <td>6.657</td>\n",
       "      <td>6.935</td>\n",
       "      <td>7.832</td>\n",
       "      <td>3.683</td>\n",
       "      <td>5.641</td>\n",
       "      <td>5.678</td>\n",
       "      <td>3.663</td>\n",
       "      <td>7.281</td>\n",
       "      <td>5.951</td>\n",
       "      <td>8.046</td>\n",
       "      <td>7.383</td>\n",
       "      <td>4.706</td>\n",
       "      <td>3.532</td>\n",
       "      <td>4.193</td>\n",
       "      <td>3.924</td>\n",
       "      <td>7.355</td>\n",
       "      <td>4.472</td>\n",
       "      <td>6.956</td>\n",
       "      <td>3.698</td>\n",
       "      <td>7.032</td>\n",
       "      <td>7.555</td>\n",
       "      <td>6.112</td>\n",
       "      <td>7.152</td>\n",
       "      <td>7.694</td>\n",
       "      <td>7.631</td>\n",
       "      <td>6.667</td>\n",
       "      <td>5.649</td>\n",
       "      <td>7.041</td>\n",
       "      <td>4.206</td>\n",
       "      <td>4.294</td>\n",
       "      <td>3.821</td>\n",
       "      <td>4.504</td>\n",
       "      <td>6.443</td>\n",
       "      <td>5.524</td>\n",
       "      <td>4.208</td>\n",
       "      <td>3.962</td>\n",
       "      <td>3.674</td>\n",
       "      <td>3.791</td>\n",
       "      <td>5.669</td>\n",
       "      <td>4.051</td>\n",
       "      <td>3.828</td>\n",
       "      <td>4.404</td>\n",
       "      <td>5.318</td>\n",
       "      <td>4.614</td>\n",
       "      <td>4.849</td>\n",
       "      <td>3.662</td>\n",
       "      <td>5.140</td>\n",
       "      <td>5.846</td>\n",
       "      <td>3.720</td>\n",
       "      <td>3.825</td>\n",
       "      <td>3.805</td>\n",
       "      <td>3.874</td>\n",
       "      <td>4.282</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>5.041</td>\n",
       "      <td>14.210</td>\n",
       "      <td>13.382</td>\n",
       "      <td>14.142</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>11.205</td>\n",
       "      <td>14.878</td>\n",
       "      <td>5.665</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>9.987</td>\n",
       "      <td>13.553</td>\n",
       "      <td>2.281</td>\n",
       "      <td>14.141</td>\n",
       "      <td>11.819</td>\n",
       "      <td>13.697</td>\n",
       "      <td>13.436</td>\n",
       "      <td>11.861</td>\n",
       "      <td>0.509</td>\n",
       "      <td>11.145</td>\n",
       "      <td>12.505</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>3.567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "count      10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean  1373158606.000         0.412         0.085        -0.258         0.059   \n",
       "std            0.000        27.624        28.250        28.421        28.151   \n",
       "min   1373158606.000       -99.990       -99.917       -99.960       -99.940   \n",
       "25%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "50%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "75%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "max   1373158606.000        99.925        99.812        99.873        99.984   \n",
       "\n",
       "       00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         -0.358         0.302         0.110        -0.081         0.530   \n",
       "std          28.125        27.360        27.919        28.099        27.657   \n",
       "min         -99.913       -99.932       -99.937       -99.773       -99.939   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          99.957        99.996        99.915        99.982        99.976   \n",
       "\n",
       "       00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean          0.273         0.145        -0.386         0.140        -0.179   \n",
       "std          29.141        27.608        28.560        27.877        28.028   \n",
       "min         -99.915       -99.978       -99.759       -99.995       -99.691   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          99.939        99.952        99.833        99.966        99.919   \n",
       "\n",
       "       02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean          0.066         0.189         0.381        -0.238         0.174   \n",
       "std          28.426        28.539        27.815        28.991        28.325   \n",
       "min         -99.994       -99.938       -99.792       -99.992       -99.953   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          99.973        99.969        99.959        99.998        99.954   \n",
       "\n",
       "       11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "count     10000.000     10000.000           10000.000           10000.000   \n",
       "mean          0.073         0.115              -4.569               5.873   \n",
       "std          28.276        27.948              28.864              38.672   \n",
       "min         -99.951       -99.999            -115.210            -249.907   \n",
       "25%           0.000         0.000             -15.367              -2.542   \n",
       "50%           0.000         0.000              -1.378               0.569   \n",
       "75%           0.000         0.000               1.636              14.795   \n",
       "max          99.976        99.968             109.030             218.296   \n",
       "\n",
       "       00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -5.790               5.054              -0.315   \n",
       "std                22.765              39.263              22.560   \n",
       "min              -101.704            -188.259             -97.695   \n",
       "25%                -8.066              -2.880              -0.950   \n",
       "50%                -0.709               0.618               0.010   \n",
       "75%                 0.282              14.201               1.095   \n",
       "max                99.537             220.494              96.983   \n",
       "\n",
       "       00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -4.454               5.521              -0.219   \n",
       "std                21.677              38.323              22.524   \n",
       "min              -100.432            -200.007             -96.910   \n",
       "25%                -6.525              -2.733              -0.802   \n",
       "50%                -0.655               0.563               0.056   \n",
       "75%                 0.321              13.034               1.088   \n",
       "max               100.964             197.450              98.467   \n",
       "\n",
       "       00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.180              -4.879               5.760   \n",
       "std                22.292              23.853              37.254   \n",
       "min               -98.448            -101.943            -190.762   \n",
       "25%                -0.878              -7.129              -2.824   \n",
       "50%                 0.009              -0.599               0.617   \n",
       "75%                 0.993               0.303              12.544   \n",
       "max                96.732             101.082             224.960   \n",
       "\n",
       "       01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.538              -0.134              -0.200   \n",
       "std                23.033              22.553              22.826   \n",
       "min               -97.929             -96.384             -97.691   \n",
       "25%                -0.943              -0.944              -0.912   \n",
       "50%                 0.003              -0.002               0.013   \n",
       "75%                 1.018               1.039               1.036   \n",
       "max                95.322              97.006              98.955   \n",
       "\n",
       "       02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -5.212               5.537               0.138   \n",
       "std                23.592              36.164              22.916   \n",
       "min              -103.220            -164.662             -99.797   \n",
       "25%                -7.760              -2.826              -0.865   \n",
       "50%                -0.577               0.745               0.036   \n",
       "75%                 0.300              13.985               1.043   \n",
       "max                98.950             203.581              95.318   \n",
       "\n",
       "       10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.145               0.154              -0.154   \n",
       "std                23.417              23.188              22.946   \n",
       "min               -97.278             -97.507             -96.521   \n",
       "25%                -0.899              -0.948              -0.923   \n",
       "50%                 0.012              -0.020               0.049   \n",
       "75%                 1.023               0.949               1.123   \n",
       "max                97.111              96.114              97.142   \n",
       "\n",
       "       20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -5.045               0.412               0.085   \n",
       "std                24.269              27.624              28.250   \n",
       "min               -99.013             -99.990             -99.917   \n",
       "25%                -8.588              -0.000              -0.000   \n",
       "50%                -0.725               0.000              -0.000   \n",
       "75%                 0.268               0.000               0.000   \n",
       "max                98.530              99.925              99.812   \n",
       "\n",
       "       00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.258               0.059              -0.358   \n",
       "std                28.421              28.151              28.125   \n",
       "min               -99.960             -99.940             -99.913   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                 0.000               0.000               0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                99.873              99.984              99.957   \n",
       "\n",
       "       00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.302               0.110              -0.081   \n",
       "std                27.360              27.919              28.099   \n",
       "min               -99.932             -99.937             -99.773   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                -0.000               0.000              -0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                99.996              99.915              99.982   \n",
       "\n",
       "       00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.530               0.273               0.145   \n",
       "std                27.657              29.141              27.608   \n",
       "min               -99.939             -99.915             -99.978   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                 0.000               0.000               0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                99.976              99.939              99.952   \n",
       "\n",
       "       01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.386               0.140              -0.179   \n",
       "std                28.560              27.877              28.028   \n",
       "min               -99.759             -99.995             -99.691   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                -0.000              -0.000              -0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                99.833              99.966              99.919   \n",
       "\n",
       "       02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                0.066               0.189               0.381   \n",
       "std                28.426              28.539              27.815   \n",
       "min               -99.994             -99.938             -99.792   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                 0.000              -0.000               0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                99.973              99.969              99.959   \n",
       "\n",
       "       10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean               -0.238               0.174               0.073   \n",
       "std                28.991              28.325              28.276   \n",
       "min               -99.992             -99.953             -99.951   \n",
       "25%                -0.000              -0.000              -0.000   \n",
       "50%                -0.000              -0.000              -0.000   \n",
       "75%                 0.000               0.000               0.000   \n",
       "max                99.998              99.954              99.976   \n",
       "\n",
       "       20000-lstsq_target      wb_0      wb_1      wb_2      wb_3      wb_4  \\\n",
       "count           10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean                0.115     0.337    -0.225     0.961     0.725     0.901   \n",
       "std                27.948     1.037     1.204     1.015     1.076     1.300   \n",
       "min               -99.999    -5.788    -8.141    -1.621    -5.186    -4.866   \n",
       "25%                -0.000    -0.081    -0.576     0.198     0.020     0.044   \n",
       "50%                 0.000     0.134    -0.161     0.621     0.489     0.880   \n",
       "75%                 0.000     1.004     0.390     1.561     1.383     1.729   \n",
       "max                99.968     3.915     4.245     6.232     6.977     7.847   \n",
       "\n",
       "           wb_5      wb_6      wb_7      wb_8      wb_9     wb_10     wb_11  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.660     1.010    -0.257     0.580     1.426     1.304     0.116   \n",
       "std       1.385     1.313     1.237     1.008     1.041     1.223     1.142   \n",
       "min      -6.934    -5.730    -7.206    -5.454    -2.355    -3.553    -6.756   \n",
       "25%      -0.019     0.174    -0.552     0.005     0.690     0.272    -0.227   \n",
       "50%       0.689     0.869    -0.217     0.393     1.407     1.051    -0.002   \n",
       "75%       1.523     1.794     0.408     1.199     2.152     2.003     0.778   \n",
       "max       6.710     8.528     4.238     5.422     6.311     7.964     4.623   \n",
       "\n",
       "          wb_12     wb_13     wb_14     wb_15     wb_16     wb_17     wb_18  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.207     0.961     0.657     1.085     0.493     1.073     1.224   \n",
       "std       1.149     1.001     1.332     1.056     1.253     1.277     1.178   \n",
       "min      -7.128    -2.445    -6.582    -2.216    -6.384    -4.908    -3.295   \n",
       "25%      -0.130     0.165    -0.002     0.209    -0.046     0.134     0.201   \n",
       "50%       0.072     0.725     0.608     0.840     0.335     1.079     1.138   \n",
       "75%       0.875     1.610     1.521     1.774     1.301     1.923     1.969   \n",
       "max       6.149     5.329     6.830     6.516     7.047     7.526     7.159   \n",
       "\n",
       "          wb_19     wb_20     wb_21     wb_22     wb_23     wb_24     wb_25  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.180     0.870     0.427     0.840     0.018    -0.189     0.213   \n",
       "std       1.326     1.282     1.155     1.131     1.238     1.132     1.056   \n",
       "min      -5.653    -5.721    -5.362    -7.792    -7.991    -8.100    -6.161   \n",
       "25%       0.166     0.025    -0.069     0.098    -0.311    -0.505    -0.152   \n",
       "50%       1.153     0.887     0.275     0.836    -0.029    -0.248     0.041   \n",
       "75%       1.998     1.719     1.108     1.616     0.695     0.481     0.877   \n",
       "max       7.846     7.497     5.942     6.351     5.821     4.175     5.940   \n",
       "\n",
       "          wb_26     wb_27     wb_28     wb_29     wb_30     wb_31     wb_32  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.334    -0.062    -0.055     0.006     0.897     1.165     0.784   \n",
       "std       1.175     1.050     1.187     1.551     1.297     1.310     1.121   \n",
       "min      -7.256    -7.584    -7.848    -9.200    -5.184    -6.947    -4.995   \n",
       "25%      -0.076    -0.360    -0.363    -0.496     0.032     0.348     0.055   \n",
       "50%       0.196    -0.336    -0.080     0.082     0.838     1.035     0.504   \n",
       "75%       1.000     0.515     0.647     0.977     1.743     2.035     1.439   \n",
       "max       5.664     5.804     4.675     6.376     6.397     8.694     7.141   \n",
       "\n",
       "          wb_33     wb_34     wb_35     wb_36     wb_37     wb_38     wb_39  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.006     0.721    -0.214    -0.197     0.216    -0.057    -0.090   \n",
       "std       1.059     1.141     1.313     1.183     1.144     1.136     1.516   \n",
       "min      -2.656    -4.144    -9.570    -7.475    -6.757    -7.855    -8.951   \n",
       "25%       0.178     0.033    -0.652    -0.486    -0.134    -0.398    -0.523   \n",
       "50%       0.773     0.732    -0.108    -0.213     0.059    -0.096    -0.019   \n",
       "75%       1.647     1.534     0.557     0.484     0.873     0.617     0.855   \n",
       "max       6.168     6.735     5.844     4.159     6.039     4.600     5.643   \n",
       "\n",
       "          wb_40     wb_41     wb_42     wb_43     wb_44     wb_45     wb_46  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.597     1.060     1.237     0.062     1.365     0.994     0.238   \n",
       "std       1.112     1.133     1.336     1.431     1.347     1.329     1.189   \n",
       "min      -4.880    -2.805    -6.159    -8.799    -3.486    -6.886    -7.012   \n",
       "25%      -0.008     0.142     0.291    -0.406     0.345     0.090    -0.165   \n",
       "50%       0.407     0.893     1.200     0.120     1.240     0.963     0.134   \n",
       "75%       1.258     1.799     2.126     0.933     2.220     1.882     0.924   \n",
       "max       7.102     6.964     7.052     6.530     8.451     6.758     5.740   \n",
       "\n",
       "          wb_47     wb_48     wb_49     wb_50     wb_51     wb_52     wb_53  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.463     0.848    -0.205     0.234     0.478    -0.179     0.057   \n",
       "std       1.127     1.115     1.138     1.068     1.108     1.037     1.175   \n",
       "min      -2.964    -3.434    -7.481    -6.892    -5.020    -7.141    -7.649   \n",
       "25%       0.701     0.084    -0.508    -0.109    -0.016    -0.459    -0.278   \n",
       "50%       1.399     0.595    -0.269     0.043     0.285    -0.435    -0.029   \n",
       "75%       2.264     1.509     0.466     0.894     1.124     0.397     0.768   \n",
       "max       7.902     6.669     3.942     6.270     6.098     3.780     5.384   \n",
       "\n",
       "          wb_54     wb_55     wb_56     wb_57     wb_58     wb_59     wb_60  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.101     1.353     1.252    -0.420     0.096     0.458     0.707   \n",
       "std       1.600     1.277     1.325     1.257     1.123     1.207     1.205   \n",
       "min      -9.350    -4.900    -6.229    -7.351    -7.367    -6.229    -7.010   \n",
       "25%      -0.616     0.226     0.409    -0.747    -0.248    -0.205     0.009   \n",
       "50%       0.024     1.240     1.131    -0.348     0.027     0.501     0.509   \n",
       "75%       0.918     2.185     2.110     0.260     0.750     1.296     1.455   \n",
       "max       7.399     8.012     8.558     3.231     4.057     4.861     7.331   \n",
       "\n",
       "          wb_61     wb_62     wb_63     wb_64     wb_65     wb_66     wb_67  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.025     0.981     0.635     1.105    -0.077     0.052     1.217   \n",
       "std       1.085     1.071     1.056     1.255     1.215     1.267     1.314   \n",
       "min      -3.467    -2.206    -4.563    -4.035    -7.044    -7.787    -4.851   \n",
       "25%       0.198     0.173     0.018     0.110    -0.465    -0.379     0.246   \n",
       "50%       0.739     0.697     0.410     0.993    -0.062    -0.061     1.212   \n",
       "75%       1.721     1.665     1.326     1.940     0.604     0.848     2.129   \n",
       "max       6.775     6.525     5.405     6.883     4.807     6.771     6.963   \n",
       "\n",
       "          wb_68     wb_69     wb_70     wb_71     wb_72     wb_73     wb_74  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.625    -0.054     1.460    -0.473     0.771     0.871     0.483   \n",
       "std       1.351     1.681     1.343     1.272     1.187     1.081     1.129   \n",
       "min      -7.377    -9.271    -4.962    -8.531    -5.223    -4.944    -5.572   \n",
       "25%      -0.047    -0.774     0.498    -0.926     0.024     0.125    -0.075   \n",
       "50%       0.626     0.112     1.401    -0.313     0.779     0.611     0.238   \n",
       "75%       1.505     1.025     2.341     0.219     1.619     1.549     1.198   \n",
       "max       7.401     6.703     7.720     3.320     7.221     6.983     6.947   \n",
       "\n",
       "          wb_75     wb_76     wb_77     wb_78     wb_79     wb_80     wb_81  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.819     0.976    -0.204     1.069     1.484     0.290     0.902   \n",
       "std       0.946     1.064     1.028     1.097     1.299     1.561     1.351   \n",
       "min      -2.066    -4.030    -6.563    -2.081    -3.876    -8.212    -5.787   \n",
       "25%       0.159     0.205    -0.465     0.235     0.303    -0.259     0.028   \n",
       "50%       0.571     0.700    -0.442     0.784     1.370     0.345     0.758   \n",
       "75%       1.413     1.659     0.356     1.739     2.378     1.325     1.768   \n",
       "max       6.203     6.153     6.313     6.765     7.767     6.430     8.635   \n",
       "\n",
       "          wb_82     wb_83     wb_84     wb_85     wb_86     wb_87     wb_88  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.673     0.033     1.375    -0.076     0.813    -0.402    -0.023   \n",
       "std       1.134     1.158     1.119     1.325     1.074     1.194     1.167   \n",
       "min      -5.505    -7.305    -2.192    -8.931    -3.572    -8.269    -7.613   \n",
       "25%       0.034    -0.355     0.567    -0.499     0.105    -0.772    -0.380   \n",
       "50%       0.379    -0.005     1.330    -0.039     0.528    -0.373    -0.077   \n",
       "75%       1.331     0.691     2.193     0.703     1.444     0.253     0.681   \n",
       "max       7.949     4.708     7.353     7.288     8.025     2.817     3.796   \n",
       "\n",
       "          wb_89     wb_90     wb_91     wb_92     wb_93     wb_94     wb_95  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.043     0.538     0.057     0.563     0.648     0.264     0.307   \n",
       "std       1.279     1.091     1.310     1.329     1.413     1.694     1.409   \n",
       "min      -5.446    -5.358    -7.793    -7.050    -7.868   -10.001    -7.657   \n",
       "25%       0.043    -0.005    -0.462    -0.198    -0.061    -0.356    -0.451   \n",
       "50%       0.940     0.333    -0.033     0.542     0.656     0.404     0.315   \n",
       "75%       1.939     1.198     0.888     1.509     1.587     1.386     1.280   \n",
       "max       7.662     6.540     7.036     7.788     6.630     6.527     7.086   \n",
       "\n",
       "          wb_96     wb_97     wb_98     wb_99    wb_100    wb_101    wb_102  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.430     0.252     0.777    -0.069    -0.084     0.376     0.534   \n",
       "std       1.166     1.277     1.068     1.190     1.008     1.109     0.991   \n",
       "min      -5.927    -5.534    -4.123    -6.366    -6.368    -5.602    -4.875   \n",
       "25%      -0.047    -0.429     0.106    -0.359    -0.382    -0.062    -0.023   \n",
       "50%       0.265     0.286     0.513    -0.157    -0.147     0.230     0.197   \n",
       "75%       1.095     1.125     1.427     0.629     0.508     0.960     1.100   \n",
       "max       6.072     6.657     6.935     7.832     3.683     5.641     5.678   \n",
       "\n",
       "         wb_103    wb_104    wb_105    wb_106    wb_107    wb_108    wb_109  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.317     0.644     0.081     0.168     0.978     0.174     0.189   \n",
       "std       1.152     1.505     1.582     1.299     1.121     1.104     1.229   \n",
       "min      -8.039    -7.619    -9.254    -8.466    -3.277    -7.663    -8.007   \n",
       "25%      -0.598    -0.064    -0.407    -0.476     0.167    -0.172    -0.450   \n",
       "50%      -0.261     0.663     0.188     0.123     0.685     0.096     0.259   \n",
       "75%       0.328     1.599     1.090     0.961     1.633     0.788     0.998   \n",
       "max       3.663     7.281     5.951     8.046     7.383     4.706     3.532   \n",
       "\n",
       "         wb_110    wb_111    wb_112    wb_113    wb_114    wb_115    wb_116  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.321    -0.290     0.653     0.176     0.939    -0.268     0.743   \n",
       "std       1.308     1.140     1.040     1.118     1.302     1.237     1.241   \n",
       "min      -9.362    -7.239    -5.340    -6.399    -5.068    -8.037    -8.667   \n",
       "25%      -0.731    -0.547     0.021    -0.173     0.041    -0.639    -0.005   \n",
       "50%      -0.167    -0.267     0.427     0.079     0.835    -0.174     0.591   \n",
       "75%       0.443     0.337     1.250     0.840     1.794     0.390     1.544   \n",
       "max       4.193     3.924     7.355     4.472     6.956     3.698     7.032   \n",
       "\n",
       "         wb_117    wb_118    wb_119    wb_120    wb_121    wb_122    wb_123  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.233     0.435     0.405     1.046     1.156     0.997    -0.166   \n",
       "std       1.323     1.409     1.616     1.332     1.113     1.129     1.242   \n",
       "min      -5.843    -7.817    -8.911    -8.530    -2.682    -4.030    -7.363   \n",
       "25%       0.299    -0.105    -0.239     0.152     0.249     0.271    -0.506   \n",
       "50%       1.218     0.467     0.542     1.038     0.887     0.955    -0.118   \n",
       "75%       2.121     1.334     1.453     1.924     1.821     1.796     0.454   \n",
       "max       7.555     6.112     7.152     7.694     7.631     6.667     5.649   \n",
       "\n",
       "         wb_124    wb_125    wb_126    wb_127    wb_128    wb_129    wb_130  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.192     0.487     0.080     0.461     0.328     0.686     0.596   \n",
       "std       1.092     1.069     1.131     1.043     1.177     1.283     1.260   \n",
       "min      -1.641    -4.057    -5.455    -4.459    -5.512    -4.769    -5.696   \n",
       "25%       0.289    -0.089    -0.267    -0.093    -0.166    -0.154    -0.180   \n",
       "50%       0.903     0.315     0.084     0.173     0.249     0.817     0.754   \n",
       "75%       1.851     1.251     0.757     1.192     1.156     1.447     1.338   \n",
       "max       7.041     4.206     4.294     3.821     4.504     6.443     5.524   \n",
       "\n",
       "         wb_131    wb_132    wb_133    wb_134    wb_135    wb_136    wb_137  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.133     0.091     0.246     0.814     0.486     0.279     0.274   \n",
       "std       1.472     1.224     1.118     1.099     1.264     1.134     1.152   \n",
       "min      -5.722    -5.491    -4.336    -4.742    -5.880    -4.861    -4.771   \n",
       "25%      -0.306    -0.238    -0.146     0.271    -0.151    -0.184    -0.138   \n",
       "50%       0.309     0.059     0.187     0.860     0.422     0.194     0.197   \n",
       "75%       0.980     0.883     1.001     1.471     1.376     1.026     1.040   \n",
       "max       4.208     3.962     3.674     3.791     5.669     4.051     3.828   \n",
       "\n",
       "         wb_138    wb_139    wb_140    wb_141    wb_142    wb_143    wb_144  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.428     0.471     0.178     0.708     0.092     0.719     0.724   \n",
       "std       1.104     1.311     1.178     1.124     1.815     1.218     1.371   \n",
       "min      -4.160    -5.656    -4.678    -4.586    -5.631    -4.327    -5.116   \n",
       "25%      -0.119    -0.167    -0.204    -0.085    -1.004     0.032     0.184   \n",
       "50%       0.307     0.682     0.160     0.748     0.665     0.829     0.888   \n",
       "75%       1.205     1.273     0.965     1.386     1.298     1.442     1.516   \n",
       "max       4.404     5.318     4.614     4.849     3.662     5.140     5.846   \n",
       "\n",
       "         wb_145    wb_146    wb_147    wb_148    wb_149    wb_150    wb_151  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.344     0.164     0.639    -0.098     0.452    -2.153    -2.743   \n",
       "std       1.668     1.219     1.178     1.174     1.129     1.529     2.086   \n",
       "min      -6.224    -5.120    -4.424    -5.624    -5.248   -11.567   -13.441   \n",
       "25%      -0.278    -0.239     0.101    -0.421    -0.143    -2.768    -3.552   \n",
       "50%       0.773     0.170     0.696    -0.046     0.296    -2.040    -2.060   \n",
       "75%       1.378     0.987     1.337     0.559     1.218    -1.419    -1.557   \n",
       "max       3.720     3.825     3.805     3.874     4.282    -0.188    -0.148   \n",
       "\n",
       "         wb_152    wb_153    wb_154    wb_155    wb_156    wb_157    wb_158  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -2.056    -2.603     2.996     2.918     0.112    -2.912    -2.557   \n",
       "std       1.686     1.942     2.157     2.133     3.402     2.195     1.836   \n",
       "min     -12.502   -12.682    -8.375    -3.007   -11.956   -13.422   -11.929   \n",
       "25%      -2.830    -3.217     1.873     1.796    -1.719    -3.888    -3.072   \n",
       "50%      -2.022    -2.189     2.318     2.231    -1.196    -2.290    -2.095   \n",
       "75%      -0.269    -1.569     3.547     3.272     2.009    -1.624    -1.567   \n",
       "max      -0.156     5.041    14.210    13.382    14.142     1.158    -0.129   \n",
       "\n",
       "         wb_159    wb_160    wb_161    wb_162    wb_163    wb_164    wb_165  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.002    -2.026    -2.608    -2.669    -2.218     2.537    -2.593   \n",
       "std       2.285     2.906     1.972     1.961     1.648     2.091     1.939   \n",
       "min     -10.121   -12.531   -12.851   -13.990   -11.576    -6.036   -12.173   \n",
       "25%      -1.769    -3.084    -3.267    -3.314    -2.793     1.593    -3.287   \n",
       "50%      -1.079    -2.102    -2.200    -2.217    -1.974     2.061    -2.050   \n",
       "75%       1.978    -1.472    -1.567    -1.628    -1.423     2.812    -1.503   \n",
       "max      11.205    14.878     5.665    -0.262     9.987    13.553     2.281   \n",
       "\n",
       "         wb_166    wb_167    wb_168    wb_169    wb_170    wb_171    wb_172  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      2.581    -0.559     2.878     2.242    -0.187    -2.897    -0.240   \n",
       "std       1.918     3.557     2.006     3.281     3.528     2.089     2.420   \n",
       "min       0.325   -12.950     0.170   -14.495   -13.248   -13.183   -10.104   \n",
       "25%       1.715    -1.987     1.892     1.725    -1.943    -3.834    -1.828   \n",
       "50%       2.425     1.360     2.318     2.147     1.599    -2.266    -1.218   \n",
       "75%       3.019     1.865     3.119     3.030     2.072    -1.655     1.917   \n",
       "max      14.141    11.819    13.697    13.436    11.861     0.509    11.145   \n",
       "\n",
       "         wb_173    wb_174    wb_175  \n",
       "count 10000.000 10000.000 10000.000  \n",
       "mean     -2.508    -2.562     0.312  \n",
       "std       2.196     1.882     0.783  \n",
       "min     -12.481   -13.800    -2.114  \n",
       "25%      -3.450    -3.309    -0.266  \n",
       "50%      -1.782    -2.290     0.397  \n",
       "75%      -1.301    -1.550     0.832  \n",
       "max      12.505    -0.137     3.567  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61953536, 0.9741457 , 0.46680659, 0.49776036, 0.86019679],\n",
       "       [0.72910667, 0.61094799, 0.92371269, 0.08555449, 0.07980001],\n",
       "       [0.81699541, 0.46397194, 0.19362828, 0.24013508, 0.33092769],\n",
       "       [0.86053752, 0.33458561, 0.41499257, 0.28032716, 0.6508531 ],\n",
       "       [0.3949162 , 0.74394349, 0.86208017, 0.9772995 , 0.57861012],\n",
       "       [0.43701616, 0.5057832 , 0.07975839, 0.30121402, 0.94739198],\n",
       "       [0.26152861, 0.51987584, 0.75460088, 0.61431661, 0.19476404],\n",
       "       [0.35163938, 0.20872544, 0.14158339, 0.43655386, 0.40572667],\n",
       "       [0.65534944, 0.36340559, 0.8659874 , 0.14811638, 0.87008502],\n",
       "       [0.3979848 , 0.75216565, 0.32811658, 0.64674227, 0.5343556 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 81.11473083],\n",
       "       [109.40452576],\n",
       "       [ 44.58266449],\n",
       "       [ 57.38260269],\n",
       "       [ 75.30336761],\n",
       "       [ 21.4254837 ],\n",
       "       [ 42.93628311],\n",
       "       [ 10.85868359],\n",
       "       [ 90.44625092],\n",
       "       [ 39.01533508]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "\n",
    "if inet_training_without_noise:\n",
    "   \n",
    "    for lambda_net_dataset, lambda_net_dataset_without_noise in zip(lambda_net_dataset_list, lambda_net_dataset_list_without_noise):\n",
    "        if inet_holdout_seed_evaluation:\n",
    "            raise SystemExit('Holdout Evaluation not implemented with inet training without noise')\n",
    "            \n",
    "        else:\n",
    "            lambda_net_train_dataset = lambda_net_dataset_without_noise\n",
    "\n",
    "            lambda_net_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_dataset_list_without_noise\n",
    "        \n",
    "else:\n",
    "\n",
    "    for lambda_net_dataset in lambda_net_dataset_list:\n",
    "\n",
    "        if inet_holdout_seed_evaluation:\n",
    "\n",
    "            complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "\n",
    "            if isinstance(test_size, float):\n",
    "                test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "\n",
    "            test_seeds = random.sample(complete_seed_list, test_size)\n",
    "            lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "            valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "            lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "            train_seeds = complete_seed_list\n",
    "            lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset\n",
    "        else:\n",
    "\n",
    "            lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "\n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8910, 240)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 240)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 240)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>25.485</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-49.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-10.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-77.851</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>1.120</td>\n",
       "      <td>24.799</td>\n",
       "      <td>-1.205</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-47.760</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.009</td>\n",
       "      <td>4.726</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-10.282</td>\n",
       "      <td>-1.117</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-76.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>25.485</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-49.284</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>5.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-10.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-77.851</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.722</td>\n",
       "      <td>3.803</td>\n",
       "      <td>2.430</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1.188</td>\n",
       "      <td>1.817</td>\n",
       "      <td>4.665</td>\n",
       "      <td>1.069</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.993</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>2.395</td>\n",
       "      <td>-1.014</td>\n",
       "      <td>4.939</td>\n",
       "      <td>0.193</td>\n",
       "      <td>1.324</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.293</td>\n",
       "      <td>0.979</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.387</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.607</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.073</td>\n",
       "      <td>1.266</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>1.147</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.645</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.721</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.644</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.183</td>\n",
       "      <td>1.349</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>2.907</td>\n",
       "      <td>2.311</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>1.457</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>2.245</td>\n",
       "      <td>2.149</td>\n",
       "      <td>1.913</td>\n",
       "      <td>2.361</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.546</td>\n",
       "      <td>1.922</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>1.627</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>-0.663</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-4.253</td>\n",
       "      <td>-1.483</td>\n",
       "      <td>2.081</td>\n",
       "      <td>1.771</td>\n",
       "      <td>2.351</td>\n",
       "      <td>2.923</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-3.767</td>\n",
       "      <td>1.125</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>3.189</td>\n",
       "      <td>0.171</td>\n",
       "      <td>2.159</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.018</td>\n",
       "      <td>2.051</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-1.481</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-2.464</td>\n",
       "      <td>-2.731</td>\n",
       "      <td>0.654</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-2.352</td>\n",
       "      <td>-1.029</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>-1.653</td>\n",
       "      <td>-1.892</td>\n",
       "      <td>1.650</td>\n",
       "      <td>-2.052</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.548</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-1.786</td>\n",
       "      <td>-3.466</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>-8.282</td>\n",
       "      <td>-6.081</td>\n",
       "      <td>4.115</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.505</td>\n",
       "      <td>-1.868</td>\n",
       "      <td>-1.774</td>\n",
       "      <td>-3.542</td>\n",
       "      <td>-5.644</td>\n",
       "      <td>-2.405</td>\n",
       "      <td>-1.744</td>\n",
       "      <td>-2.054</td>\n",
       "      <td>6.007</td>\n",
       "      <td>-3.266</td>\n",
       "      <td>2.760</td>\n",
       "      <td>-6.296</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-4.519</td>\n",
       "      <td>1.867</td>\n",
       "      <td>-1.817</td>\n",
       "      <td>-1.401</td>\n",
       "      <td>-1.645</td>\n",
       "      <td>-6.117</td>\n",
       "      <td>-0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.638</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.248</td>\n",
       "      <td>56.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-82.556</td>\n",
       "      <td>90.677</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.810</td>\n",
       "      <td>4.306</td>\n",
       "      <td>-9.234</td>\n",
       "      <td>8.510</td>\n",
       "      <td>4.951</td>\n",
       "      <td>38.272</td>\n",
       "      <td>57.778</td>\n",
       "      <td>-3.033</td>\n",
       "      <td>2.684</td>\n",
       "      <td>-2.591</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>1.326</td>\n",
       "      <td>0.302</td>\n",
       "      <td>1.088</td>\n",
       "      <td>8.109</td>\n",
       "      <td>-76.949</td>\n",
       "      <td>82.223</td>\n",
       "      <td>1.915</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-7.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.638</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.248</td>\n",
       "      <td>56.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-82.556</td>\n",
       "      <td>90.677</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-2.116</td>\n",
       "      <td>3.616</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>3.743</td>\n",
       "      <td>2.254</td>\n",
       "      <td>3.135</td>\n",
       "      <td>0.222</td>\n",
       "      <td>3.267</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>3.059</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-3.020</td>\n",
       "      <td>2.658</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>1.403</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.131</td>\n",
       "      <td>1.194</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>1.703</td>\n",
       "      <td>2.417</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.828</td>\n",
       "      <td>-0.387</td>\n",
       "      <td>2.006</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.341</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>1.730</td>\n",
       "      <td>2.485</td>\n",
       "      <td>2.053</td>\n",
       "      <td>1.824</td>\n",
       "      <td>2.517</td>\n",
       "      <td>-1.139</td>\n",
       "      <td>2.203</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>0.169</td>\n",
       "      <td>1.776</td>\n",
       "      <td>-2.454</td>\n",
       "      <td>0.242</td>\n",
       "      <td>2.684</td>\n",
       "      <td>2.100</td>\n",
       "      <td>1.465</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>2.744</td>\n",
       "      <td>5.686</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>2.506</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.864</td>\n",
       "      <td>2.093</td>\n",
       "      <td>2.264</td>\n",
       "      <td>2.165</td>\n",
       "      <td>1.963</td>\n",
       "      <td>-1.385</td>\n",
       "      <td>2.093</td>\n",
       "      <td>1.809</td>\n",
       "      <td>-1.877</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-2.708</td>\n",
       "      <td>4.002</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-4.661</td>\n",
       "      <td>2.055</td>\n",
       "      <td>2.527</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>-1.088</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>2.795</td>\n",
       "      <td>-0.328</td>\n",
       "      <td>-2.713</td>\n",
       "      <td>2.507</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>2.951</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>1.579</td>\n",
       "      <td>1.547</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.403</td>\n",
       "      <td>1.722</td>\n",
       "      <td>-3.920</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>1.573</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>1.630</td>\n",
       "      <td>1.622</td>\n",
       "      <td>1.552</td>\n",
       "      <td>1.610</td>\n",
       "      <td>1.654</td>\n",
       "      <td>-2.029</td>\n",
       "      <td>1.633</td>\n",
       "      <td>2.272</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-7.027</td>\n",
       "      <td>-4.667</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>2.210</td>\n",
       "      <td>2.122</td>\n",
       "      <td>5.039</td>\n",
       "      <td>-2.817</td>\n",
       "      <td>-3.334</td>\n",
       "      <td>2.077</td>\n",
       "      <td>10.848</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>2.090</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>2.322</td>\n",
       "      <td>1.716</td>\n",
       "      <td>2.217</td>\n",
       "      <td>1.937</td>\n",
       "      <td>1.900</td>\n",
       "      <td>-4.787</td>\n",
       "      <td>1.965</td>\n",
       "      <td>-6.158</td>\n",
       "      <td>-3.207</td>\n",
       "      <td>1.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>21.824</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.945</td>\n",
       "      <td>-27.212</td>\n",
       "      <td>69.619</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-81.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.204</td>\n",
       "      <td>130.730</td>\n",
       "      <td>-2.329</td>\n",
       "      <td>11.054</td>\n",
       "      <td>10.198</td>\n",
       "      <td>-10.669</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>1.414</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-81.115</td>\n",
       "      <td>1.488</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-1.058</td>\n",
       "      <td>-1.279</td>\n",
       "      <td>3.391</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>21.824</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>97.945</td>\n",
       "      <td>-27.212</td>\n",
       "      <td>69.619</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-81.351</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.063</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.202</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.375</td>\n",
       "      <td>1.039</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0.241</td>\n",
       "      <td>1.063</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.163</td>\n",
       "      <td>1.412</td>\n",
       "      <td>1.070</td>\n",
       "      <td>1.263</td>\n",
       "      <td>1.587</td>\n",
       "      <td>1.526</td>\n",
       "      <td>1.582</td>\n",
       "      <td>1.410</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.872</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.919</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>2.202</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>2.566</td>\n",
       "      <td>0.025</td>\n",
       "      <td>2.305</td>\n",
       "      <td>2.500</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>2.115</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-1.056</td>\n",
       "      <td>2.396</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.402</td>\n",
       "      <td>-0.518</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.841</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.740</td>\n",
       "      <td>1.065</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.653</td>\n",
       "      <td>0.979</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.812</td>\n",
       "      <td>1.376</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.838</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.987</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>1.117</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.323</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.146</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-3.096</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>1.705</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.322</td>\n",
       "      <td>1.484</td>\n",
       "      <td>1.400</td>\n",
       "      <td>1.182</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-2.569</td>\n",
       "      <td>-1.742</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-2.653</td>\n",
       "      <td>3.149</td>\n",
       "      <td>2.854</td>\n",
       "      <td>-2.420</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-2.130</td>\n",
       "      <td>-2.809</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-2.545</td>\n",
       "      <td>1.603</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>3.290</td>\n",
       "      <td>-2.428</td>\n",
       "      <td>3.062</td>\n",
       "      <td>3.520</td>\n",
       "      <td>3.001</td>\n",
       "      <td>3.122</td>\n",
       "      <td>3.334</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-2.255</td>\n",
       "      <td>-2.194</td>\n",
       "      <td>0.281</td>\n",
       "      <td>1.388</td>\n",
       "      <td>0.812</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>1.429</td>\n",
       "      <td>1.734</td>\n",
       "      <td>1.697</td>\n",
       "      <td>1.095</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.472</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>1.366</td>\n",
       "      <td>0.385</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>1.751</td>\n",
       "      <td>1.198</td>\n",
       "      <td>1.721</td>\n",
       "      <td>1.821</td>\n",
       "      <td>1.697</td>\n",
       "      <td>1.795</td>\n",
       "      <td>1.857</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>1.022</td>\n",
       "      <td>0.956</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-3.266</td>\n",
       "      <td>-2.547</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-3.195</td>\n",
       "      <td>2.702</td>\n",
       "      <td>2.579</td>\n",
       "      <td>-2.671</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-3.169</td>\n",
       "      <td>-2.914</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-3.151</td>\n",
       "      <td>-4.385</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>2.550</td>\n",
       "      <td>-3.084</td>\n",
       "      <td>3.040</td>\n",
       "      <td>2.362</td>\n",
       "      <td>2.714</td>\n",
       "      <td>2.569</td>\n",
       "      <td>2.487</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-2.680</td>\n",
       "      <td>-2.740</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>1.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-49.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.050</td>\n",
       "      <td>-70.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-43.183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-1.140</td>\n",
       "      <td>-47.700</td>\n",
       "      <td>0.888</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>10.075</td>\n",
       "      <td>-69.514</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.122</td>\n",
       "      <td>2.179</td>\n",
       "      <td>-1.322</td>\n",
       "      <td>-42.036</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>47.217</td>\n",
       "      <td>2.036</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-49.292</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.050</td>\n",
       "      <td>-70.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-43.183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>49.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.628</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.562</td>\n",
       "      <td>1.035</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>2.737</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.555</td>\n",
       "      <td>2.999</td>\n",
       "      <td>2.665</td>\n",
       "      <td>2.867</td>\n",
       "      <td>2.197</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-1.310</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-2.320</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>0.435</td>\n",
       "      <td>2.844</td>\n",
       "      <td>-2.306</td>\n",
       "      <td>2.298</td>\n",
       "      <td>2.252</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.963</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-1.933</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-0.764</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.439</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.955</td>\n",
       "      <td>2.998</td>\n",
       "      <td>3.450</td>\n",
       "      <td>4.487</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.802</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>1.243</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>4.480</td>\n",
       "      <td>0.490</td>\n",
       "      <td>1.138</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.224</td>\n",
       "      <td>1.098</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>1.166</td>\n",
       "      <td>1.464</td>\n",
       "      <td>3.559</td>\n",
       "      <td>1.015</td>\n",
       "      <td>-1.888</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>1.228</td>\n",
       "      <td>1.738</td>\n",
       "      <td>1.352</td>\n",
       "      <td>1.248</td>\n",
       "      <td>-1.240</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.361</td>\n",
       "      <td>2.639</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.098</td>\n",
       "      <td>-2.096</td>\n",
       "      <td>2.791</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>-1.251</td>\n",
       "      <td>0.232</td>\n",
       "      <td>1.826</td>\n",
       "      <td>1.817</td>\n",
       "      <td>1.216</td>\n",
       "      <td>3.984</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-1.382</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.190</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-1.591</td>\n",
       "      <td>-1.019</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-2.800</td>\n",
       "      <td>-1.942</td>\n",
       "      <td>-2.615</td>\n",
       "      <td>-2.985</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-2.511</td>\n",
       "      <td>-1.238</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>-5.162</td>\n",
       "      <td>-1.167</td>\n",
       "      <td>1.801</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-1.511</td>\n",
       "      <td>-1.317</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-3.789</td>\n",
       "      <td>-4.053</td>\n",
       "      <td>-6.765</td>\n",
       "      <td>-4.948</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-1.103</td>\n",
       "      <td>3.517</td>\n",
       "      <td>-6.148</td>\n",
       "      <td>5.830</td>\n",
       "      <td>5.001</td>\n",
       "      <td>-7.794</td>\n",
       "      <td>-1.411</td>\n",
       "      <td>-1.270</td>\n",
       "      <td>-1.079</td>\n",
       "      <td>-7.175</td>\n",
       "      <td>-0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>38.782</td>\n",
       "      <td>0.000</td>\n",
       "      <td>57.340</td>\n",
       "      <td>0.000</td>\n",
       "      <td>67.658</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.477</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>71.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.894</td>\n",
       "      <td>23.106</td>\n",
       "      <td>45.124</td>\n",
       "      <td>14.788</td>\n",
       "      <td>59.781</td>\n",
       "      <td>-7.002</td>\n",
       "      <td>127.544</td>\n",
       "      <td>-5.514</td>\n",
       "      <td>-4.007</td>\n",
       "      <td>-24.065</td>\n",
       "      <td>2.993</td>\n",
       "      <td>-1.442</td>\n",
       "      <td>-2.615</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-1.117</td>\n",
       "      <td>35.226</td>\n",
       "      <td>-7.935</td>\n",
       "      <td>-2.493</td>\n",
       "      <td>52.867</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-21.520</td>\n",
       "      <td>38.782</td>\n",
       "      <td>0.000</td>\n",
       "      <td>57.340</td>\n",
       "      <td>0.000</td>\n",
       "      <td>67.658</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>89.477</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>71.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.204</td>\n",
       "      <td>-2.821</td>\n",
       "      <td>0.209</td>\n",
       "      <td>1.930</td>\n",
       "      <td>2.115</td>\n",
       "      <td>2.024</td>\n",
       "      <td>2.009</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>1.050</td>\n",
       "      <td>2.357</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.151</td>\n",
       "      <td>2.063</td>\n",
       "      <td>1.369</td>\n",
       "      <td>1.879</td>\n",
       "      <td>2.243</td>\n",
       "      <td>2.171</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.057</td>\n",
       "      <td>3.975</td>\n",
       "      <td>2.025</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>2.348</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>1.341</td>\n",
       "      <td>0.717</td>\n",
       "      <td>1.146</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.044</td>\n",
       "      <td>2.297</td>\n",
       "      <td>1.021</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.671</td>\n",
       "      <td>2.320</td>\n",
       "      <td>1.175</td>\n",
       "      <td>1.373</td>\n",
       "      <td>0.717</td>\n",
       "      <td>1.333</td>\n",
       "      <td>1.159</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>1.433</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-2.323</td>\n",
       "      <td>2.826</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>-2.645</td>\n",
       "      <td>2.498</td>\n",
       "      <td>3.237</td>\n",
       "      <td>3.260</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-2.148</td>\n",
       "      <td>2.782</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.017</td>\n",
       "      <td>3.165</td>\n",
       "      <td>-2.326</td>\n",
       "      <td>2.449</td>\n",
       "      <td>3.342</td>\n",
       "      <td>2.863</td>\n",
       "      <td>2.643</td>\n",
       "      <td>3.358</td>\n",
       "      <td>-3.921</td>\n",
       "      <td>2.974</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.413</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.978</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.534</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.300</td>\n",
       "      <td>2.878</td>\n",
       "      <td>-2.881</td>\n",
       "      <td>2.976</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>2.699</td>\n",
       "      <td>0.573</td>\n",
       "      <td>2.036</td>\n",
       "      <td>2.320</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.381</td>\n",
       "      <td>2.172</td>\n",
       "      <td>0.768</td>\n",
       "      <td>2.207</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-1.512</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-1.583</td>\n",
       "      <td>3.156</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2.794</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-1.283</td>\n",
       "      <td>2.825</td>\n",
       "      <td>-6.456</td>\n",
       "      <td>-2.225</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>3.309</td>\n",
       "      <td>-1.384</td>\n",
       "      <td>3.060</td>\n",
       "      <td>3.534</td>\n",
       "      <td>3.023</td>\n",
       "      <td>3.125</td>\n",
       "      <td>3.341</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>3.327</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.279</td>\n",
       "      <td>3.112</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>2.420</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.674</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>2.819</td>\n",
       "      <td>1.030</td>\n",
       "      <td>3.902</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.856</td>\n",
       "      <td>2.923</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.951</td>\n",
       "      <td>1.671</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-3.946</td>\n",
       "      <td>-6.029</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-3.446</td>\n",
       "      <td>2.648</td>\n",
       "      <td>2.614</td>\n",
       "      <td>2.343</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-3.704</td>\n",
       "      <td>2.422</td>\n",
       "      <td>10.231</td>\n",
       "      <td>-5.674</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>2.599</td>\n",
       "      <td>-3.565</td>\n",
       "      <td>2.884</td>\n",
       "      <td>2.377</td>\n",
       "      <td>2.709</td>\n",
       "      <td>2.473</td>\n",
       "      <td>2.473</td>\n",
       "      <td>-7.164</td>\n",
       "      <td>2.413</td>\n",
       "      <td>-2.177</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "5548  1373158606         0.000         0.000        25.485         0.000   \n",
       "1854  1373158606         0.000         0.000        -1.638         0.000   \n",
       "739   1373158606        21.824         0.000        97.945       -27.212   \n",
       "3588  1373158606         0.000         0.000       -49.292         0.000   \n",
       "4649  1373158606        38.782         0.000        57.340         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "5548         0.000       -49.284         0.000         0.000         5.040   \n",
       "1854         0.000        47.248        56.115         0.000         0.000   \n",
       "739         69.619         0.000         0.000         0.000         0.000   \n",
       "3588         0.000         0.000         0.000         0.000        11.050   \n",
       "4649        67.658         0.000        89.477         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "5548         0.000         0.000         0.000         0.000         0.000   \n",
       "1854         0.000         0.000         0.000         0.000         0.000   \n",
       "739          0.000         0.000         0.000         0.000         0.000   \n",
       "3588       -70.065         0.000         0.000         0.000         0.000   \n",
       "4649         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "5548         0.000         0.000       -10.153         0.000         0.000   \n",
       "1854         0.000         0.000       -82.556        90.677         0.000   \n",
       "739        -81.351         0.000         0.000         0.000         0.000   \n",
       "3588         0.000         0.000       -43.183         0.000         0.000   \n",
       "4649         0.000         0.000         0.000         0.000        71.413   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "5548         0.000       -77.851              -0.042               1.120   \n",
       "1854         0.000         0.000              -3.810               4.306   \n",
       "739          0.000         0.000               1.204             130.730   \n",
       "3588        49.208         0.000               0.393              -1.140   \n",
       "4649         0.000         0.000              15.894              23.106   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "5548              24.799              -1.205              -0.087   \n",
       "1854              -9.234               8.510               4.951   \n",
       "739               -2.329              11.054              10.198   \n",
       "3588             -47.700               0.888              -0.532   \n",
       "4649              45.124              14.788              59.781   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "5548             -47.760               0.864               0.009   \n",
       "1854              38.272              57.778              -3.033   \n",
       "739              -10.669              -0.086              -0.453   \n",
       "3588              -0.278               0.080              -0.344   \n",
       "4649              -7.002             127.544              -5.514   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "5548               4.726              -0.491              -0.234   \n",
       "1854               2.684              -2.591              -0.763   \n",
       "739                1.414              -0.100             -81.115   \n",
       "3588              10.075             -69.514              -1.378   \n",
       "4649              -4.007             -24.065               2.993   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "5548              -0.707               0.718              -0.428   \n",
       "1854              -0.750               1.326               0.302   \n",
       "739                1.488              -2.624              -0.182   \n",
       "3588              -0.497               0.870               0.122   \n",
       "4649              -1.442              -2.615              -0.030   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "5548               0.355              -1.180             -10.282   \n",
       "1854               1.088               8.109             -76.949   \n",
       "739               -0.041              -1.058              -1.279   \n",
       "3588               2.179              -1.322             -42.036   \n",
       "4649              -1.117              35.226              -7.935   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "5548              -1.117               0.325               0.149   \n",
       "1854              82.223               1.915              -1.050   \n",
       "739                3.391              -0.268              -0.185   \n",
       "3588              -0.281              -0.499              47.217   \n",
       "4649              -2.493              52.867               0.104   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "5548             -76.002               0.000               0.000   \n",
       "1854              -7.371               0.000               0.000   \n",
       "739               -0.026              21.824              -0.000   \n",
       "3588               2.036              -0.000               0.000   \n",
       "4649             -21.520              38.782               0.000   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "5548              25.485               0.000               0.000   \n",
       "1854              -1.638              -0.000               0.000   \n",
       "739               97.945             -27.212              69.619   \n",
       "3588             -49.292              -0.000              -0.000   \n",
       "4649              57.340               0.000              67.658   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "5548             -49.284              -0.000              -0.000   \n",
       "1854              47.248              56.115               0.000   \n",
       "739               -0.000              -0.000               0.000   \n",
       "3588               0.000              -0.000               0.000   \n",
       "4649              -0.000              89.477               0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "5548               5.040               0.000               0.000   \n",
       "1854              -0.000               0.000              -0.000   \n",
       "739                0.000               0.000               0.000   \n",
       "3588              11.050             -70.065               0.000   \n",
       "4649              -0.000              -0.000               0.000   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "5548               0.000              -0.000               0.000   \n",
       "1854              -0.000               0.000               0.000   \n",
       "739               -0.000              -0.000               0.000   \n",
       "3588              -0.000               0.000               0.000   \n",
       "4649              -0.000              -0.000              -0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "5548               0.000              -0.000             -10.153   \n",
       "1854               0.000              -0.000             -82.556   \n",
       "739              -81.351              -0.000               0.000   \n",
       "3588              -0.000              -0.000             -43.183   \n",
       "4649              -0.000              -0.000              -0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "5548               0.000               0.000              -0.000   \n",
       "1854              90.677              -0.000               0.000   \n",
       "739               -0.000               0.000               0.000   \n",
       "3588               0.000              -0.000              49.208   \n",
       "4649              -0.000              71.413               0.000   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1  wb_2  wb_3  wb_4   wb_5  wb_6   wb_7  \\\n",
       "5548             -77.851  0.940  0.722 3.803 2.430 0.512 -0.341 1.429  0.665   \n",
       "1854               0.000 -0.079 -2.116 3.616 0.029 0.028 -0.065 3.743  2.254   \n",
       "739                0.000  1.063  0.710 0.202 1.136 1.464  1.375 1.039 -0.419   \n",
       "3588               0.000  0.187 -0.101 1.341 0.375 1.628 -0.010 0.391 -0.140   \n",
       "4649               0.000  1.204 -2.821 0.209 1.930 2.115  2.024 2.009 -0.415   \n",
       "\n",
       "      wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "5548 1.188 1.817  4.665  1.069  0.982  1.993 -0.422  2.395 -1.014  4.939   \n",
       "1854 3.135 0.222  3.267 -0.230 -0.116  0.148 -0.044  0.190 -0.186  0.111   \n",
       "739  0.917 1.180  0.241  1.063  0.558  0.163  1.412  1.070  1.263  1.587   \n",
       "3588 0.284 0.562  1.035 -0.258 -0.746  2.737 -0.004  0.555  2.999  2.665   \n",
       "4649 1.050 2.357  0.432 -0.216 -0.115  0.151  2.063  1.369  1.879  2.243   \n",
       "\n",
       "      wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "5548  0.193  1.324 -0.759  1.120  1.293  0.979 -0.445  0.034  0.387 -0.016   \n",
       "1854  0.091  0.119 -0.073  3.059 -0.174 -3.020  2.658 -0.146  0.047 -0.060   \n",
       "739   1.526  1.582  1.410 -0.060  0.925  0.872 -0.497  2.220  1.919 -0.356   \n",
       "3588  2.867  2.197 -0.546  0.235 -0.166  0.092 -0.077  0.190  0.354 -0.515   \n",
       "4649  2.171  2.226  2.057  3.975  2.025 -0.051 -0.485  2.348 -0.225 -0.339   \n",
       "\n",
       "      wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "5548 -0.052  0.022  0.675  0.684  0.556  0.607 -0.018 -0.008 -0.138  0.303   \n",
       "1854 -0.310  0.180  0.617 -0.058  1.403  0.572  0.453 -0.031 -0.407 -0.119   \n",
       "739   2.202 -1.012 -0.532  2.566  0.025  2.305  2.500 -0.418  2.115 -0.381   \n",
       "3588  0.075  1.313  0.028  0.614  0.526  0.584  0.203 -1.310  0.166  0.421   \n",
       "4649  1.341  0.717  1.146  1.092  0.044  2.297  1.021  0.116  0.134 -0.115   \n",
       "\n",
       "      wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "5548 -0.155  0.009  0.073  1.266  0.058 -0.274 -0.078  1.147  0.304  0.771   \n",
       "1854 -0.276  0.137 -0.006  0.703  0.834  0.190  0.791  0.620  0.183  0.838   \n",
       "739  -0.306 -1.056  2.396 -0.474 -0.402 -0.974 -0.414 -0.632 -0.134  2.625   \n",
       "3588 -2.320 -0.385  0.435  2.844 -2.306  2.298  2.252  0.458  0.269  0.511   \n",
       "4649 -0.291  0.671  2.320  1.175  1.373  0.717  1.333  1.159 -0.396  1.433   \n",
       "\n",
       "      wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "5548  0.645 -0.021 -0.053  0.459  0.045 -0.063 -0.006  0.752  0.721 -0.089   \n",
       "1854  0.131  1.194 -0.103 -0.127 -0.296 -0.204  1.703  2.417  0.113 -0.828   \n",
       "739   2.402 -0.518  1.046  0.841 -0.452  1.073  1.008  1.740  1.065 -0.531   \n",
       "3588  0.608  0.696  0.963  1.051 -1.933  0.880 -0.764  0.205  1.439  0.496   \n",
       "4649  0.900 -0.494 -2.323  2.826 -0.449 -2.645  2.498  3.237  3.260 -0.530   \n",
       "\n",
       "      wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "5548  0.237 -0.185  0.086 -0.222  0.644 -0.456  0.001 -0.089  0.970  0.079   \n",
       "1854 -0.387  2.006 -0.010  0.194  0.184  0.001  2.341 -0.236  1.730  2.485   \n",
       "739   0.885  1.090  0.017  1.132  0.293  0.030  1.653  0.979  1.033  1.812   \n",
       "3588  0.851  0.955  2.998  3.450  4.487  1.170  0.114  0.802 -0.329  1.243   \n",
       "4649 -2.148  2.782  0.543  0.492  0.173  0.017  3.165 -2.326  2.449  3.342   \n",
       "\n",
       "      wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "5548 -0.047  0.183  1.349 -0.173  0.338  0.705 -0.168  2.907  2.311 -0.780   \n",
       "1854  2.053  1.824  2.517 -1.139  2.203 -0.892 -0.587  0.169  1.776 -2.454   \n",
       "739   1.376  1.161  1.838 -0.537  0.918  0.987 -0.078  1.024  0.754 -0.448   \n",
       "3588 -0.268 -0.366  4.480  0.490  1.138  1.320  0.511  0.071  0.175  0.380   \n",
       "4649  2.863  2.643  3.358 -3.921  2.974 -0.549 -0.078  0.562  0.413 -0.457   \n",
       "\n",
       "      wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "5548  1.457  0.269 -0.911  2.245  2.149  1.913  2.361 -0.540  2.500  1.546   \n",
       "1854  0.242  2.684  2.100  1.465 -1.129 -1.763  2.744  5.686  0.120 -0.480   \n",
       "739   1.117  1.884  1.323  1.027  0.041  0.867  1.146 -0.271  1.056 -3.096   \n",
       "3588  0.224  1.098 -0.252 -0.057  0.029 -0.266  0.121 -0.308 -0.091 -0.377   \n",
       "4649  0.978  2.875  2.314  2.534  0.024  0.300  2.878 -2.881  2.976 -0.482   \n",
       "\n",
       "      wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "5548  1.922 -0.228  1.627 -1.405 -0.663 -0.059 -4.253 -1.483  2.081  1.771   \n",
       "1854 -0.219  2.506  0.005  1.864  2.093  2.264  2.165  1.963 -1.385  2.093   \n",
       "739  -0.214  1.705  1.000  1.074  1.322  1.484  1.400  1.182 -0.030  0.889   \n",
       "3588 -0.181  0.036 -0.024 -0.202 -0.102 -0.007  0.012 -0.307 -0.090 -0.833   \n",
       "4649 -0.214  2.699  0.573  2.036  2.320  2.475  2.381  2.172  0.768  2.207   \n",
       "\n",
       "      wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "5548  2.351  2.923  -0.110  -0.468   0.184  -0.032  -3.767   1.125  -0.855   \n",
       "1854  1.809 -1.877  -0.372  -2.708   4.002  -0.530  -0.185  -0.440  -4.661   \n",
       "739   0.982 -0.335  -2.569  -1.742  -0.019  -2.653   3.149   2.854  -2.420   \n",
       "3588  0.168 -0.136   1.166   1.464   3.559   1.015  -1.888  -0.274   1.228   \n",
       "4649 -0.197 -0.346  -1.512  -0.545  -0.016  -1.583   3.156   2.890   2.794   \n",
       "\n",
       "      wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "5548  -0.169  -0.612  -0.130   0.243  -0.306  -0.380  -0.214   3.189   0.171   \n",
       "1854   2.055   2.527  -0.665  -1.088  -0.521   0.027  -0.155  -0.086  -0.399   \n",
       "739    0.176  -2.130  -2.809  -0.526  -2.545   1.603  -0.155   3.290  -2.428   \n",
       "3588   1.738   1.352   1.248  -1.240  -0.099   0.361   2.639   0.046   1.098   \n",
       "4649   0.159  -1.283   2.825  -6.456  -2.225   0.022  -0.161   3.309  -1.384   \n",
       "\n",
       "      wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "5548   2.159   0.126  -0.092   0.018   2.051  -0.165  -0.463  -0.747  -0.134   \n",
       "1854  -0.179   0.100  -0.291  -0.247  -0.099   2.795  -0.328  -2.713   2.507   \n",
       "739    3.062   3.520   3.001   3.122   3.334   0.256  -2.255  -2.194   0.281   \n",
       "3588  -2.096   2.791  -1.158  -1.251   0.232   1.826   1.817   1.216   3.984   \n",
       "4649   3.060   3.534   3.023   3.125   3.341  -0.368   3.327  -1.090   0.279   \n",
       "\n",
       "      wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "5548  -1.481   0.295  -2.464  -2.731   0.654   1.026   0.282   0.327   0.270   \n",
       "1854  -0.079   2.951  -0.399  -0.147   1.579   1.547  -0.147   0.749   0.403   \n",
       "739    1.388   0.812  -0.086   1.429   1.734   1.697   1.095  -0.195   1.012   \n",
       "3588   0.119   0.050  -1.382   0.118   0.940  -0.191   0.068   0.080   0.060   \n",
       "4649   3.112   0.574  -0.080   2.420   0.865   0.825   0.674  -0.190   2.819   \n",
       "\n",
       "      wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "5548  -2.352  -1.029  -0.887   0.323  -0.714  -1.653  -1.892   1.650  -2.052   \n",
       "1854   1.722  -3.920  -0.173  -0.130  -0.127   1.573  -0.171   1.630   1.622   \n",
       "739    1.472  -0.156   1.366   0.385  -0.113   1.751   1.198   1.721   1.821   \n",
       "3588   0.190  -0.625  -0.659  -1.591  -1.019  -0.170   0.087  -0.070  -2.800   \n",
       "4649   1.030   3.902  -0.548  -0.128  -0.125   0.856   2.923   0.858   0.913   \n",
       "\n",
       "      wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "5548  -0.117   1.096   1.548   0.259   0.315   0.286  -1.786  -3.466  -1.812   \n",
       "1854   1.552   1.610   1.654  -2.029   1.633   2.272   1.030  -0.227  -7.027   \n",
       "739    1.697   1.795   1.857  -0.173   1.022   0.956  -0.133  -3.266  -2.547   \n",
       "3588  -1.942  -2.615  -2.985   0.053  -0.320   0.057  -2.511  -1.238  -1.294   \n",
       "4649   0.829   0.904   0.940   0.360   0.951   1.671  -0.143  -3.946  -6.029   \n",
       "\n",
       "      wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "5548  -8.282  -6.081   4.115   1.444  -1.505  -1.868  -1.774  -3.542  -5.644   \n",
       "1854  -4.667  -0.207   2.210   2.122   5.039  -2.817  -3.334   2.077  10.848   \n",
       "739   -0.265  -3.195   2.702   2.579  -2.671  -0.322  -3.169  -2.914  -0.053   \n",
       "3588  -5.162  -1.167   1.801   0.154  -0.952  -1.511  -1.317  -0.915  -3.789   \n",
       "4649  -0.264  -3.446   2.648   2.614   2.343  -0.316  -3.704   2.422  10.231   \n",
       "\n",
       "      wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "5548  -2.405  -1.744  -2.054   6.007  -3.266   2.760  -6.296   0.214  -4.519   \n",
       "1854  -0.268  -0.318  -0.035   2.090  -0.153   2.322   1.716   2.217   1.937   \n",
       "739   -3.151  -4.385  -0.090   2.550  -3.084   3.040   2.362   2.714   2.569   \n",
       "3588  -4.053  -6.765  -4.948   0.087  -1.103   3.517  -6.148   5.830   5.001   \n",
       "4649  -5.674  -0.318  -0.076   2.599  -3.565   2.884   2.377   2.709   2.473   \n",
       "\n",
       "      wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "5548   1.867  -1.817  -1.401  -1.645  -6.117  -0.429  \n",
       "1854   1.900  -4.787   1.965  -6.158  -3.207   1.395  \n",
       "739    2.487  -0.278  -2.680  -2.740  -0.330   1.176  \n",
       "3588  -7.794  -1.411  -1.270  -1.079  -7.175  -0.153  \n",
       "4649   2.473  -7.164   2.413  -2.177  -0.305   0.544  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-43.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>95.216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>67.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-60.313</td>\n",
       "      <td>-57.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>1.229</td>\n",
       "      <td>-43.201</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>1.547</td>\n",
       "      <td>91.763</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.981</td>\n",
       "      <td>65.712</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.230</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-58.973</td>\n",
       "      <td>-57.010</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-1.275</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-43.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>95.216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>67.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-60.313</td>\n",
       "      <td>-57.505</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.602</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.735</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-1.387</td>\n",
       "      <td>1.790</td>\n",
       "      <td>0.041</td>\n",
       "      <td>1.586</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.032</td>\n",
       "      <td>1.526</td>\n",
       "      <td>1.653</td>\n",
       "      <td>1.893</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>1.937</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-1.455</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-1.629</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-5.343</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1.436</td>\n",
       "      <td>0.553</td>\n",
       "      <td>1.475</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>1.134</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.056</td>\n",
       "      <td>1.635</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.171</td>\n",
       "      <td>1.491</td>\n",
       "      <td>0.107</td>\n",
       "      <td>4.852</td>\n",
       "      <td>1.032</td>\n",
       "      <td>-1.619</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-2.108</td>\n",
       "      <td>2.477</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-2.821</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>2.421</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-3.279</td>\n",
       "      <td>2.650</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-2.463</td>\n",
       "      <td>2.553</td>\n",
       "      <td>-2.718</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.716</td>\n",
       "      <td>-2.585</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.486</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.641</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>1.083</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-1.075</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>2.942</td>\n",
       "      <td>-1.549</td>\n",
       "      <td>4.234</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-1.003</td>\n",
       "      <td>-1.216</td>\n",
       "      <td>-1.657</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.897</td>\n",
       "      <td>3.193</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>3.356</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-1.837</td>\n",
       "      <td>3.223</td>\n",
       "      <td>4.184</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-2.148</td>\n",
       "      <td>3.765</td>\n",
       "      <td>1.184</td>\n",
       "      <td>0.582</td>\n",
       "      <td>1.318</td>\n",
       "      <td>1.216</td>\n",
       "      <td>1.980</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.288</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.085</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.080</td>\n",
       "      <td>0.658</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.721</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>2.266</td>\n",
       "      <td>0.791</td>\n",
       "      <td>-1.932</td>\n",
       "      <td>-2.970</td>\n",
       "      <td>0.524</td>\n",
       "      <td>1.026</td>\n",
       "      <td>-2.344</td>\n",
       "      <td>-1.984</td>\n",
       "      <td>-5.957</td>\n",
       "      <td>-2.329</td>\n",
       "      <td>4.417</td>\n",
       "      <td>2.984</td>\n",
       "      <td>-2.029</td>\n",
       "      <td>-6.886</td>\n",
       "      <td>-1.884</td>\n",
       "      <td>-1.923</td>\n",
       "      <td>-2.280</td>\n",
       "      <td>-2.903</td>\n",
       "      <td>-2.182</td>\n",
       "      <td>-2.078</td>\n",
       "      <td>2.965</td>\n",
       "      <td>-2.037</td>\n",
       "      <td>5.577</td>\n",
       "      <td>2.675</td>\n",
       "      <td>0.207</td>\n",
       "      <td>5.149</td>\n",
       "      <td>2.950</td>\n",
       "      <td>-8.813</td>\n",
       "      <td>-8.763</td>\n",
       "      <td>-2.261</td>\n",
       "      <td>-6.745</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-65.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>93.460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>68.174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-30.648</td>\n",
       "      <td>68.090</td>\n",
       "      <td>-3.752</td>\n",
       "      <td>44.568</td>\n",
       "      <td>-12.721</td>\n",
       "      <td>-22.732</td>\n",
       "      <td>-64.730</td>\n",
       "      <td>1.806</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-1.392</td>\n",
       "      <td>60.045</td>\n",
       "      <td>9.860</td>\n",
       "      <td>58.508</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-31.175</td>\n",
       "      <td>41.671</td>\n",
       "      <td>-1.426</td>\n",
       "      <td>-22.233</td>\n",
       "      <td>0.249</td>\n",
       "      <td>37.003</td>\n",
       "      <td>-14.112</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>77.013</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-65.423</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>93.460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>68.174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>2.708</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1.909</td>\n",
       "      <td>1.799</td>\n",
       "      <td>1.853</td>\n",
       "      <td>-2.425</td>\n",
       "      <td>-1.115</td>\n",
       "      <td>2.186</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.290</td>\n",
       "      <td>1.843</td>\n",
       "      <td>0.209</td>\n",
       "      <td>1.654</td>\n",
       "      <td>2.033</td>\n",
       "      <td>1.954</td>\n",
       "      <td>2.031</td>\n",
       "      <td>1.852</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>1.887</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-1.563</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>-4.116</td>\n",
       "      <td>-1.994</td>\n",
       "      <td>1.767</td>\n",
       "      <td>2.175</td>\n",
       "      <td>2.524</td>\n",
       "      <td>1.646</td>\n",
       "      <td>3.878</td>\n",
       "      <td>2.140</td>\n",
       "      <td>-1.803</td>\n",
       "      <td>-1.696</td>\n",
       "      <td>-1.313</td>\n",
       "      <td>-1.647</td>\n",
       "      <td>1.726</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>2.170</td>\n",
       "      <td>2.465</td>\n",
       "      <td>1.751</td>\n",
       "      <td>2.419</td>\n",
       "      <td>2.252</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>2.637</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>2.293</td>\n",
       "      <td>1.622</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>2.056</td>\n",
       "      <td>-1.013</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>2.495</td>\n",
       "      <td>2.518</td>\n",
       "      <td>2.328</td>\n",
       "      <td>2.407</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>1.912</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.123</td>\n",
       "      <td>3.107</td>\n",
       "      <td>0.490</td>\n",
       "      <td>2.061</td>\n",
       "      <td>1.477</td>\n",
       "      <td>1.777</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-4.080</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.266</td>\n",
       "      <td>1.864</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.497</td>\n",
       "      <td>1.649</td>\n",
       "      <td>1.574</td>\n",
       "      <td>1.356</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>1.464</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>1.941</td>\n",
       "      <td>1.638</td>\n",
       "      <td>1.688</td>\n",
       "      <td>0.885</td>\n",
       "      <td>-1.354</td>\n",
       "      <td>1.639</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>1.856</td>\n",
       "      <td>2.287</td>\n",
       "      <td>1.788</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.243</td>\n",
       "      <td>2.177</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>0.278</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1.169</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>1.643</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>1.513</td>\n",
       "      <td>0.896</td>\n",
       "      <td>1.985</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.735</td>\n",
       "      <td>1.896</td>\n",
       "      <td>0.749</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.819</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.797</td>\n",
       "      <td>1.211</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-3.109</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-7.482</td>\n",
       "      <td>-2.870</td>\n",
       "      <td>2.145</td>\n",
       "      <td>1.984</td>\n",
       "      <td>2.140</td>\n",
       "      <td>-4.037</td>\n",
       "      <td>-7.557</td>\n",
       "      <td>1.975</td>\n",
       "      <td>-2.976</td>\n",
       "      <td>-3.124</td>\n",
       "      <td>-2.880</td>\n",
       "      <td>-2.946</td>\n",
       "      <td>1.912</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>2.466</td>\n",
       "      <td>1.766</td>\n",
       "      <td>2.116</td>\n",
       "      <td>2.045</td>\n",
       "      <td>1.851</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>2.020</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-56.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.834</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>25.618</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>76.361</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-83.319</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-55.326</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.083</td>\n",
       "      <td>16.102</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.033</td>\n",
       "      <td>2.927</td>\n",
       "      <td>22.778</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>73.243</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-1.217</td>\n",
       "      <td>1.666</td>\n",
       "      <td>-5.747</td>\n",
       "      <td>1.791</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-77.520</td>\n",
       "      <td>2.220</td>\n",
       "      <td>-56.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.834</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>25.618</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>76.361</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-83.319</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.242</td>\n",
       "      <td>1.025</td>\n",
       "      <td>2.848</td>\n",
       "      <td>-1.541</td>\n",
       "      <td>1.017</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.879</td>\n",
       "      <td>1.360</td>\n",
       "      <td>1.375</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.754</td>\n",
       "      <td>1.194</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.199</td>\n",
       "      <td>-2.761</td>\n",
       "      <td>1.358</td>\n",
       "      <td>2.715</td>\n",
       "      <td>3.266</td>\n",
       "      <td>1.345</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-1.332</td>\n",
       "      <td>-1.184</td>\n",
       "      <td>-1.437</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>-2.926</td>\n",
       "      <td>2.089</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>-1.424</td>\n",
       "      <td>-1.502</td>\n",
       "      <td>-1.425</td>\n",
       "      <td>-1.229</td>\n",
       "      <td>-1.453</td>\n",
       "      <td>2.324</td>\n",
       "      <td>-1.186</td>\n",
       "      <td>2.588</td>\n",
       "      <td>-1.393</td>\n",
       "      <td>-2.607</td>\n",
       "      <td>3.246</td>\n",
       "      <td>-1.592</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-1.512</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-2.431</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.128</td>\n",
       "      <td>0.063</td>\n",
       "      <td>2.064</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.160</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>1.892</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>0.906</td>\n",
       "      <td>-0.781</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>2.344</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>1.593</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>2.270</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.553</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.730</td>\n",
       "      <td>1.599</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>1.323</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.694</td>\n",
       "      <td>1.353</td>\n",
       "      <td>1.779</td>\n",
       "      <td>1.770</td>\n",
       "      <td>1.551</td>\n",
       "      <td>1.472</td>\n",
       "      <td>1.676</td>\n",
       "      <td>0.246</td>\n",
       "      <td>1.499</td>\n",
       "      <td>-1.816</td>\n",
       "      <td>1.885</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-3.240</td>\n",
       "      <td>2.115</td>\n",
       "      <td>1.389</td>\n",
       "      <td>1.539</td>\n",
       "      <td>1.194</td>\n",
       "      <td>1.613</td>\n",
       "      <td>-1.998</td>\n",
       "      <td>-1.632</td>\n",
       "      <td>-2.245</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>4.900</td>\n",
       "      <td>4.190</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>-4.577</td>\n",
       "      <td>-1.696</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>-2.025</td>\n",
       "      <td>-2.099</td>\n",
       "      <td>-2.006</td>\n",
       "      <td>-1.898</td>\n",
       "      <td>2.174</td>\n",
       "      <td>-1.701</td>\n",
       "      <td>5.059</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>4.777</td>\n",
       "      <td>-9.583</td>\n",
       "      <td>-2.264</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>-1.456</td>\n",
       "      <td>-1.274</td>\n",
       "      <td>-2.320</td>\n",
       "      <td>-0.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-89.786</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.551</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>74.653</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>1.121</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>1.362</td>\n",
       "      <td>1.644</td>\n",
       "      <td>-1.164</td>\n",
       "      <td>90.029</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.778</td>\n",
       "      <td>-90.077</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-49.611</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.208</td>\n",
       "      <td>73.830</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>2.716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-89.786</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.551</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>74.653</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>5.931</td>\n",
       "      <td>1.490</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>4.621</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>1.489</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.777</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.863</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.618</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-3.733</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>-3.636</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>0.898</td>\n",
       "      <td>-4.995</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>3.499</td>\n",
       "      <td>1.902</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-5.116</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.772</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.743</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.499</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.296</td>\n",
       "      <td>1.791</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.570</td>\n",
       "      <td>1.523</td>\n",
       "      <td>-1.884</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>0.578</td>\n",
       "      <td>1.706</td>\n",
       "      <td>-2.984</td>\n",
       "      <td>-2.727</td>\n",
       "      <td>-0.734</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>1.549</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.590</td>\n",
       "      <td>1.574</td>\n",
       "      <td>1.536</td>\n",
       "      <td>1.586</td>\n",
       "      <td>1.613</td>\n",
       "      <td>1.373</td>\n",
       "      <td>1.617</td>\n",
       "      <td>-2.051</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-5.332</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-4.561</td>\n",
       "      <td>1.512</td>\n",
       "      <td>1.307</td>\n",
       "      <td>8.471</td>\n",
       "      <td>-1.921</td>\n",
       "      <td>-5.180</td>\n",
       "      <td>1.199</td>\n",
       "      <td>10.658</td>\n",
       "      <td>-9.849</td>\n",
       "      <td>-2.681</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>1.221</td>\n",
       "      <td>-1.753</td>\n",
       "      <td>2.021</td>\n",
       "      <td>0.903</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.132</td>\n",
       "      <td>-5.634</td>\n",
       "      <td>1.263</td>\n",
       "      <td>-6.412</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>1.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-17.010</td>\n",
       "      <td>-95.249</td>\n",
       "      <td>65.531</td>\n",
       "      <td>-8.063</td>\n",
       "      <td>100.976</td>\n",
       "      <td>-2.376</td>\n",
       "      <td>-3.584</td>\n",
       "      <td>0.824</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>47.645</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-1.556</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>4.431</td>\n",
       "      <td>-1.358</td>\n",
       "      <td>-27.988</td>\n",
       "      <td>38.979</td>\n",
       "      <td>0.651</td>\n",
       "      <td>1.728</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>-40.644</td>\n",
       "      <td>-10.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.399</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-17.010</td>\n",
       "      <td>-95.249</td>\n",
       "      <td>65.531</td>\n",
       "      <td>1.688</td>\n",
       "      <td>-2.943</td>\n",
       "      <td>0.206</td>\n",
       "      <td>1.643</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.746</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.209</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.766</td>\n",
       "      <td>0.776</td>\n",
       "      <td>1.084</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.513</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>3.205</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>2.845</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-1.123</td>\n",
       "      <td>0.052</td>\n",
       "      <td>3.149</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>1.388</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-1.074</td>\n",
       "      <td>3.223</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.984</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>2.761</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.524</td>\n",
       "      <td>2.236</td>\n",
       "      <td>2.346</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>1.802</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.027</td>\n",
       "      <td>2.174</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.489</td>\n",
       "      <td>2.338</td>\n",
       "      <td>1.881</td>\n",
       "      <td>1.657</td>\n",
       "      <td>2.364</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>2.014</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>1.529</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>1.562</td>\n",
       "      <td>1.307</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.290</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.797</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>1.114</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.576</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>-2.182</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-2.019</td>\n",
       "      <td>2.658</td>\n",
       "      <td>2.362</td>\n",
       "      <td>2.424</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>2.316</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>2.811</td>\n",
       "      <td>-2.647</td>\n",
       "      <td>2.553</td>\n",
       "      <td>3.024</td>\n",
       "      <td>2.518</td>\n",
       "      <td>2.612</td>\n",
       "      <td>2.843</td>\n",
       "      <td>0.222</td>\n",
       "      <td>2.860</td>\n",
       "      <td>-2.149</td>\n",
       "      <td>0.285</td>\n",
       "      <td>1.314</td>\n",
       "      <td>3.328</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.163</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.701</td>\n",
       "      <td>1.623</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.930</td>\n",
       "      <td>1.915</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>1.745</td>\n",
       "      <td>0.727</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.801</td>\n",
       "      <td>1.706</td>\n",
       "      <td>1.777</td>\n",
       "      <td>1.833</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>1.862</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-3.469</td>\n",
       "      <td>-9.875</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-3.065</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.158</td>\n",
       "      <td>2.100</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-3.326</td>\n",
       "      <td>2.006</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-1.686</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>2.228</td>\n",
       "      <td>-3.205</td>\n",
       "      <td>2.558</td>\n",
       "      <td>1.946</td>\n",
       "      <td>2.343</td>\n",
       "      <td>2.062</td>\n",
       "      <td>2.105</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>2.089</td>\n",
       "      <td>-2.493</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>1.346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "4197  1373158606         0.000         0.000       -43.199         0.000   \n",
       "5318  1373158606         0.000        77.013         0.000         0.000   \n",
       "3815  1373158606       -56.269         0.000         0.000        15.834   \n",
       "9727  1373158606         0.000         0.000         0.000         0.000   \n",
       "5410  1373158606         0.000        99.132         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "4197         0.000         0.000         0.000        95.216         0.000   \n",
       "5318       -28.002         0.000       -65.423         0.000         0.000   \n",
       "3815         0.000         0.000         0.000        25.618         0.000   \n",
       "9727         2.716         0.000        89.905         0.000         0.000   \n",
       "5410         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "4197         0.000        67.360         0.000         0.000         0.000   \n",
       "5318         0.000         0.000         0.000        93.460         0.000   \n",
       "3815         0.000        76.361         0.000         0.000         0.000   \n",
       "9727       -89.786         0.000         0.000         0.000         0.000   \n",
       "5410        55.399         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "4197       -60.313       -57.505         0.000         0.000         0.000   \n",
       "5318         0.000         0.000         0.000         0.000         0.000   \n",
       "3815         0.000         0.000         0.000         0.000         0.000   \n",
       "9727         0.000       -50.551         0.000         0.000         0.000   \n",
       "5410         0.000         0.000         0.000         0.000       -17.010   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "4197         0.000         0.000              -0.917               1.229   \n",
       "5318        68.174         0.000             -30.648              68.090   \n",
       "3815       -83.319         0.000             -55.326               1.082   \n",
       "9727         0.000        74.653              -0.921               1.121   \n",
       "5410       -95.249        65.531              -8.063             100.976   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "4197             -43.201               0.970               0.286   \n",
       "5318              -3.752              44.568             -12.721   \n",
       "3815               0.083              16.102               0.263   \n",
       "9727              -0.087               1.362               1.644   \n",
       "5410              -2.376              -3.584               0.824   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "4197              -0.578               1.547              91.763   \n",
       "5318             -22.732             -64.730               1.806   \n",
       "3815               0.033               2.927              22.778   \n",
       "9727              -1.164              90.029              -0.002   \n",
       "5410              -0.702              47.645              -0.044   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "4197               0.186               0.981              65.712   \n",
       "5318               0.033              -1.392              60.045   \n",
       "3815              -0.448              -0.635              73.243   \n",
       "9727               0.778             -90.077               0.216   \n",
       "5410               1.180              -0.211              -1.556   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "4197               0.432               0.230              -0.283   \n",
       "5318               9.860              58.508               0.167   \n",
       "3815              -1.083              -0.672              -1.217   \n",
       "9727              -0.134               0.215              -0.094   \n",
       "5410              -0.435               4.431              -1.358   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "4197             -58.973             -57.010               0.478   \n",
       "5318             -31.175              41.671              -1.426   \n",
       "3815               1.666              -5.747               1.791   \n",
       "9727              -0.492             -49.611              -0.575   \n",
       "5410             -27.988              38.979               0.651   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "4197              -0.761              -1.275               0.614   \n",
       "5318             -22.233               0.249              37.003   \n",
       "3815              -0.357               0.063             -77.520   \n",
       "9727              -0.073               0.180               0.208   \n",
       "5410               1.728              -0.650             -40.644   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "4197              -0.065              -0.000               0.000   \n",
       "5318             -14.112              -0.000              77.013   \n",
       "3815               2.220             -56.269               0.000   \n",
       "9727              73.830               0.000               0.000   \n",
       "5410             -10.242               0.000              99.132   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "4197             -43.199               0.000              -0.000   \n",
       "5318              -0.000               0.000             -28.002   \n",
       "3815               0.000              15.834              -0.000   \n",
       "9727              -0.000              -0.000               2.716   \n",
       "5410               0.000              -0.000               0.000   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "4197              -0.000              -0.000              95.216   \n",
       "5318               0.000             -65.423              -0.000   \n",
       "3815               0.000              -0.000              25.618   \n",
       "9727               0.000              89.905               0.000   \n",
       "5410               0.000               0.000              -0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "4197               0.000              -0.000              67.360   \n",
       "5318               0.000               0.000               0.000   \n",
       "3815               0.000               0.000              76.361   \n",
       "9727               0.000             -89.786              -0.000   \n",
       "5410               0.000              55.399              -0.000   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "4197               0.000               0.000               0.000   \n",
       "5318              -0.000              93.460               0.000   \n",
       "3815              -0.000               0.000               0.000   \n",
       "9727               0.000              -0.000               0.000   \n",
       "5410               0.000               0.000              -0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "4197             -60.313             -57.505              -0.000   \n",
       "5318               0.000               0.000              -0.000   \n",
       "3815               0.000               0.000              -0.000   \n",
       "9727               0.000             -50.551              -0.000   \n",
       "5410              -0.000               0.000              -0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "4197              -0.000              -0.000               0.000   \n",
       "5318              -0.000              -0.000              68.174   \n",
       "3815               0.000               0.000             -83.319   \n",
       "9727               0.000              -0.000               0.000   \n",
       "5410              -0.000             -17.010             -95.249   \n",
       "\n",
       "      20000-lstsq_target  wb_0   wb_1  wb_2  wb_3   wb_4   wb_5  wb_6   wb_7  \\\n",
       "4197               0.000 1.602  0.393 0.046 1.735  0.026 -1.387 1.790  0.041   \n",
       "5318               0.000 0.208 -0.125 2.708 0.678  1.909  1.799 1.853 -2.425   \n",
       "3815              -0.000 0.836  0.422 1.242 1.025  2.848 -1.541 1.017 -0.498   \n",
       "9727              74.653 0.648 -0.333 0.188 0.604 -0.892 -1.015 5.931  1.490   \n",
       "5410              65.531 1.688 -2.943 0.206 1.643  0.980  0.889 0.746 -0.418   \n",
       "\n",
       "       wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "4197  1.586  2.074  2.032  1.526  1.653  1.893 -1.480  1.937  0.069 -1.455   \n",
       "5318 -1.115  2.186  0.445  0.329  0.079  0.290  1.843  0.209  1.654  2.033   \n",
       "3815  0.879  1.360  1.375  0.717  0.754  1.194  1.071  1.199 -2.761  1.358   \n",
       "9727  0.323 -0.552  4.621 -0.093  1.489  0.160 -0.970  0.059 -1.153 -0.788   \n",
       "5410  1.621  1.209  0.258  0.753 -0.116  0.166  0.917  1.766  0.776  1.084   \n",
       "\n",
       "      wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "4197  0.190  0.020 -1.629  0.052  0.005  0.307 -0.006  0.065  0.077 -5.343   \n",
       "5318  1.954  2.031  1.852 -0.061  1.887 -0.095 -0.501 -1.563 -0.894 -4.116   \n",
       "3815  2.715  3.266  1.345  0.836  0.970  0.611  0.526 -1.332 -1.184 -1.437   \n",
       "9727 -0.823 -0.777 -0.978  0.314 -0.863  0.515 -0.500  0.222 -0.051 -0.353   \n",
       "5410  1.040  1.082  0.902 -0.069  0.864  1.513 -0.504  3.205 -4.068 -0.358   \n",
       "\n",
       "      wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "4197  0.036  0.111  1.436  0.553  1.475  0.367  0.143 -0.089 -0.402 -0.054   \n",
       "5318 -1.994  1.767  2.175  2.524  1.646  3.878  2.140 -1.803 -1.696 -1.313   \n",
       "3815 -1.407 -2.926  2.089 -1.083 -0.754 -1.062 -1.424 -1.502 -1.425 -1.229   \n",
       "9727  0.134  0.050  0.502  0.035  0.794  0.067  0.256 -0.001  0.046  0.618   \n",
       "5410  2.845 -1.005 -0.546 -1.123  0.052  3.149 -0.758 -0.436  1.388 -0.120   \n",
       "\n",
       "      wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "4197 -0.068  1.134  0.258  0.056  1.635 -0.279  0.171  1.491  0.107  4.852   \n",
       "5318 -1.647  1.726 -0.002  2.170  2.465  1.751  2.419  2.252 -0.108  2.637   \n",
       "3815 -1.453  2.324 -1.186  2.588 -1.393 -2.607  3.246 -1.592 -1.294 -1.073   \n",
       "9727 -0.291  0.006 -0.044  0.556  0.678  0.070  0.640  0.453  0.025  0.571   \n",
       "5410 -0.288 -1.074  3.223 -0.461 -0.397 -0.984 -0.402 -0.619 -0.112 -0.427   \n",
       "\n",
       "      wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "4197  1.032 -1.619 -0.405 -1.480  0.098 -0.351 -2.108  2.477  0.205 -2.821   \n",
       "5318 -0.730 -0.491  2.293  1.622 -0.035  2.056 -1.013 -0.237 -1.595 -0.098   \n",
       "3815 -1.050 -1.512  0.115  0.167  0.039  0.108  0.574 -0.760  0.330 -2.431   \n",
       "9727  0.272 -0.490 -3.733 -0.006 -0.464 -3.636 -0.795 -0.028 -0.393  0.898   \n",
       "5410  2.761 -0.494 -0.062 -0.187 -0.458  0.009  1.524  2.236  2.346 -0.531   \n",
       "\n",
       "      wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "4197 -0.649 -0.573 -0.128  0.445 -0.334 -0.375  2.421 -0.489 -3.279  2.650   \n",
       "5318  0.034 -0.876  2.495  2.518  2.328  2.407 -0.374 -0.233 -0.931 -0.282   \n",
       "3815  0.108  0.012  0.255  0.318  0.329  0.187  1.128  0.063  2.064  0.143   \n",
       "9727 -4.995 -0.647 -0.267  3.499  1.902  0.026 -0.160 -0.118 -0.732 -0.041   \n",
       "5410 -0.025  1.802  0.012  0.350  0.170  0.027  2.174 -0.021  1.489  2.338   \n",
       "\n",
       "      wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "4197 -0.057 -2.463  2.553 -2.718 -0.098  0.716 -2.585  0.497  0.170 -0.029   \n",
       "5318 -0.599 -0.937 -0.266 -0.534 -0.843  1.912 -0.078  0.242  0.123  3.107   \n",
       "3815  1.160 -0.287  0.166 -0.131  0.114  0.295  0.217 -0.207 -0.125 -0.441   \n",
       "9727 -0.401 -0.695 -0.036 -5.116 -0.601  2.969 -0.063  0.044  0.210 -0.457   \n",
       "5410  1.881  1.657  2.364 -0.537  2.014  0.186 -0.077  1.529  0.420 -0.450   \n",
       "\n",
       "      wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "4197  0.615  0.068  0.728  0.406  0.049  0.263  0.569  0.293  0.420  0.152   \n",
       "5318  0.490  2.061  1.477  1.777  0.185 -4.080  2.100  0.328  0.354  0.040   \n",
       "3815 -0.142 -0.052  0.092 -0.367 -0.024 -0.358 -0.258 -0.373 -0.192 -0.491   \n",
       "9727  0.138  0.834  0.276  0.028  0.575 -0.030  0.772 -0.001  0.018  0.219   \n",
       "5410  1.562  1.307  0.746  0.795  0.040  1.100  1.290 -0.273  0.797 -0.480   \n",
       "\n",
       "      wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "4197  0.298  0.965  0.486 -0.015  0.641 -0.064  0.020  0.681  0.008 -0.071   \n",
       "5318  0.266  1.864 -0.009  1.205  1.497  1.649  1.574  1.356 -0.030  1.464   \n",
       "3815 -0.362  1.892 -0.270 -0.124 -0.671 -0.009  0.003 -0.702 -0.233 -0.622   \n",
       "9727 -0.208  0.632  0.988  0.065  0.216  0.447  0.311  0.068  0.019  0.041   \n",
       "5410 -0.212  1.114  0.841  0.512  0.718  0.912  0.802  0.576 -0.028  0.600   \n",
       "\n",
       "      wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "4197  0.128 -0.059  -0.989   1.083   0.229  -1.075  -0.890   2.942  -1.549   \n",
       "5318  0.038 -0.339  -0.892  -0.449   0.391  -1.024   1.941   1.638   1.688   \n",
       "3815 -0.211 -0.301  -0.220  -0.057  -0.067  -0.266   0.906  -0.781  -0.390   \n",
       "9727  0.131 -0.342   0.077  -0.063  -0.002  -0.005   0.391   0.126  -0.009   \n",
       "5410  0.443 -0.349  -2.182  -0.238  -0.012  -2.019   2.658   2.362   2.424   \n",
       "\n",
       "      wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "4197   4.234  -0.534  -1.003  -1.216  -1.657  -0.740  -0.897   3.193  -0.968   \n",
       "5318   0.885  -1.354   1.639  -0.993  -0.946  -0.751  -0.904   2.053  -0.400   \n",
       "3815   2.344  -0.164  -0.335  -0.264  -0.238  -0.083  -0.118   1.593  -0.276   \n",
       "9727   0.818   0.028  -0.056  -0.000  -0.007   0.743  -0.162   0.499  -0.986   \n",
       "5410   0.176  -1.907   2.316  -0.530  -0.925   0.018  -0.166   2.811  -2.647   \n",
       "\n",
       "      wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "4197  -1.907   3.356  -0.104  -1.837   3.223   4.184   0.108  -2.148   3.765   \n",
       "5318   1.856   2.287   1.788   1.920   2.099   0.243   2.177  -0.644   0.278   \n",
       "3815   2.270  -0.164   1.290   0.211  -0.210   0.137  -0.139  -0.259   0.021   \n",
       "9727   0.404   0.671   0.273   0.318   0.473   0.034   0.322   0.046   0.296   \n",
       "5410   2.553   3.024   2.518   2.612   2.843   0.222   2.860  -2.149   0.285   \n",
       "\n",
       "      wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "4197   1.184   0.582   1.318   1.216   1.980   0.674   0.859  -0.763   0.885   \n",
       "5318   1.875   1.169  -0.554   1.643   0.742   0.726   0.104  -0.149   1.513   \n",
       "3815   1.553   1.234   1.730   1.599  -0.884   1.323   1.370   0.694   1.353   \n",
       "9727   1.791  -0.163  -0.093   1.292   1.570   1.523  -1.884  -0.535   0.578   \n",
       "5410   1.314   3.328  -0.080   1.163   1.739   1.701   1.623  -0.185   0.930   \n",
       "\n",
       "      wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "4197   1.288   1.162   1.085   0.954   1.080   0.658   1.144   1.721   0.660   \n",
       "5318   0.896   1.985   1.910   1.735   1.896   0.749  -0.165   0.750   0.785   \n",
       "3815   1.779   1.770   1.551   1.472   1.676   0.246   1.499  -1.816   1.885   \n",
       "9727   1.706  -2.984  -2.727  -0.734  -0.121   1.549   0.125   1.590   1.574   \n",
       "5410   1.915  -0.144   0.417  -0.132  -0.112   1.745   0.727   1.723   1.801   \n",
       "\n",
       "      wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "4197  -0.136   2.266   0.791  -1.932  -2.970   0.524   1.026  -2.344  -1.984   \n",
       "5318   0.731   0.764   0.819  -0.167   0.797   1.211  -0.141  -3.109  -1.997   \n",
       "3815  -0.358  -3.240   2.115   1.389   1.539   1.194   1.613  -1.998  -1.632   \n",
       "9727   1.536   1.586   1.613   1.373   1.617  -2.051  -0.124  -5.332  -0.294   \n",
       "5410   1.706   1.777   1.833  -0.185   1.862   0.312  -0.131  -3.469  -9.875   \n",
       "\n",
       "      wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "4197  -5.957  -2.329   4.417   2.984  -2.029  -6.886  -1.884  -1.923  -2.280   \n",
       "5318  -7.482  -2.870   2.145   1.984   2.140  -4.037  -7.557   1.975  -2.976   \n",
       "3815  -2.245  -2.024   4.900   4.190  -1.250  -4.577  -1.696  -1.688  -2.025   \n",
       "9727  -0.262  -4.561   1.512   1.307   8.471  -1.921  -5.180   1.199  10.658   \n",
       "5410  -0.275  -3.065   2.294   2.158   2.100  -0.325  -3.326   2.006  -0.053   \n",
       "\n",
       "      wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "4197  -2.903  -2.182  -2.078   2.965  -2.037   5.577   2.675   0.207   5.149   \n",
       "5318  -3.124  -2.880  -2.946   1.912  -0.152   2.466   1.766   2.116   2.045   \n",
       "3815  -2.099  -2.006  -1.898   2.174  -1.701   5.059  -1.686   4.777  -9.583   \n",
       "9727  -9.849  -2.681  -0.076   1.221  -1.753   2.021   0.903   1.432   1.292   \n",
       "5410  -1.686  -0.317  -0.086   2.228  -3.205   2.558   1.946   2.343   2.062   \n",
       "\n",
       "      wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "4197   2.950  -8.813  -8.763  -2.261  -6.745   0.150  \n",
       "5318   1.851  -0.275   2.020  -1.724  -0.317   0.626  \n",
       "3815  -2.264  -1.768  -1.456  -1.274  -2.320  -0.177  \n",
       "9727   1.132  -5.634   1.263  -6.412  -0.342   1.366  \n",
       "5410   2.105  -0.273   2.089  -2.493  -0.333   1.346  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-27.497</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-79.539</td>\n",
       "      <td>33.258</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>64.030</td>\n",
       "      <td>5.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>0.754</td>\n",
       "      <td>-27.810</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.687</td>\n",
       "      <td>-76.258</td>\n",
       "      <td>29.856</td>\n",
       "      <td>1.241</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.887</td>\n",
       "      <td>1.290</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>63.833</td>\n",
       "      <td>2.520</td>\n",
       "      <td>0.654</td>\n",
       "      <td>2.026</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-27.497</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-79.539</td>\n",
       "      <td>33.258</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>64.030</td>\n",
       "      <td>5.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-1.406</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>2.627</td>\n",
       "      <td>1.090</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-1.788</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>2.932</td>\n",
       "      <td>0.249</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2.555</td>\n",
       "      <td>0.210</td>\n",
       "      <td>2.380</td>\n",
       "      <td>2.739</td>\n",
       "      <td>2.647</td>\n",
       "      <td>2.753</td>\n",
       "      <td>1.121</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>2.614</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-1.664</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>1.161</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.624</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.783</td>\n",
       "      <td>2.200</td>\n",
       "      <td>1.515</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>1.426</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>2.563</td>\n",
       "      <td>5.541</td>\n",
       "      <td>2.023</td>\n",
       "      <td>-1.790</td>\n",
       "      <td>-0.813</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>1.381</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>2.640</td>\n",
       "      <td>-0.517</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>1.266</td>\n",
       "      <td>2.052</td>\n",
       "      <td>2.349</td>\n",
       "      <td>2.854</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>2.833</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-3.024</td>\n",
       "      <td>-1.845</td>\n",
       "      <td>1.802</td>\n",
       "      <td>-3.744</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>3.071</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-1.078</td>\n",
       "      <td>-1.289</td>\n",
       "      <td>-1.115</td>\n",
       "      <td>-1.011</td>\n",
       "      <td>-3.298</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-1.513</td>\n",
       "      <td>3.029</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.701</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.733</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-2.319</td>\n",
       "      <td>1.119</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-1.193</td>\n",
       "      <td>0.723</td>\n",
       "      <td>2.300</td>\n",
       "      <td>1.278</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-1.297</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>1.074</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.107</td>\n",
       "      <td>0.934</td>\n",
       "      <td>1.197</td>\n",
       "      <td>-1.109</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>1.245</td>\n",
       "      <td>-2.978</td>\n",
       "      <td>0.768</td>\n",
       "      <td>-2.933</td>\n",
       "      <td>-2.897</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-5.051</td>\n",
       "      <td>2.264</td>\n",
       "      <td>3.435</td>\n",
       "      <td>6.527</td>\n",
       "      <td>-2.923</td>\n",
       "      <td>-5.630</td>\n",
       "      <td>2.116</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-3.802</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>1.888</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>2.455</td>\n",
       "      <td>1.604</td>\n",
       "      <td>2.168</td>\n",
       "      <td>2.046</td>\n",
       "      <td>4.237</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>1.891</td>\n",
       "      <td>-8.871</td>\n",
       "      <td>-2.701</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-54.061</td>\n",
       "      <td>-82.317</td>\n",
       "      <td>-88.199</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-27.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-33.481</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>39.969</td>\n",
       "      <td>-97.004</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-142.694</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.742</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.595</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-43.283</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-54.061</td>\n",
       "      <td>-82.317</td>\n",
       "      <td>-88.199</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-27.222</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-33.481</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.234</td>\n",
       "      <td>0.921</td>\n",
       "      <td>1.567</td>\n",
       "      <td>1.408</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.674</td>\n",
       "      <td>1.502</td>\n",
       "      <td>0.868</td>\n",
       "      <td>1.314</td>\n",
       "      <td>1.697</td>\n",
       "      <td>1.704</td>\n",
       "      <td>1.119</td>\n",
       "      <td>1.151</td>\n",
       "      <td>1.553</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.604</td>\n",
       "      <td>0.701</td>\n",
       "      <td>1.520</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.709</td>\n",
       "      <td>1.354</td>\n",
       "      <td>1.267</td>\n",
       "      <td>1.364</td>\n",
       "      <td>1.140</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>1.749</td>\n",
       "      <td>0.917</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.978</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.690</td>\n",
       "      <td>1.875</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.513</td>\n",
       "      <td>1.009</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>1.893</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.360</td>\n",
       "      <td>1.670</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.888</td>\n",
       "      <td>1.521</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.606</td>\n",
       "      <td>2.882</td>\n",
       "      <td>2.948</td>\n",
       "      <td>2.281</td>\n",
       "      <td>3.024</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-2.776</td>\n",
       "      <td>2.834</td>\n",
       "      <td>2.783</td>\n",
       "      <td>2.498</td>\n",
       "      <td>3.090</td>\n",
       "      <td>2.523</td>\n",
       "      <td>2.866</td>\n",
       "      <td>2.116</td>\n",
       "      <td>2.543</td>\n",
       "      <td>0.037</td>\n",
       "      <td>2.790</td>\n",
       "      <td>-3.166</td>\n",
       "      <td>2.552</td>\n",
       "      <td>-1.955</td>\n",
       "      <td>-2.832</td>\n",
       "      <td>2.397</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.366</td>\n",
       "      <td>2.970</td>\n",
       "      <td>2.368</td>\n",
       "      <td>1.758</td>\n",
       "      <td>2.067</td>\n",
       "      <td>2.152</td>\n",
       "      <td>1.633</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-1.310</td>\n",
       "      <td>1.834</td>\n",
       "      <td>2.342</td>\n",
       "      <td>1.951</td>\n",
       "      <td>1.853</td>\n",
       "      <td>1.708</td>\n",
       "      <td>1.608</td>\n",
       "      <td>2.108</td>\n",
       "      <td>2.026</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.744</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>2.569</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>2.371</td>\n",
       "      <td>2.429</td>\n",
       "      <td>2.301</td>\n",
       "      <td>1.837</td>\n",
       "      <td>2.456</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>2.762</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>3.045</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>2.213</td>\n",
       "      <td>2.852</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-2.228</td>\n",
       "      <td>-2.249</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>-2.285</td>\n",
       "      <td>0.116</td>\n",
       "      <td>3.401</td>\n",
       "      <td>-1.771</td>\n",
       "      <td>-2.405</td>\n",
       "      <td>-2.098</td>\n",
       "      <td>-1.930</td>\n",
       "      <td>-2.005</td>\n",
       "      <td>-2.291</td>\n",
       "      <td>-2.126</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-2.074</td>\n",
       "      <td>4.048</td>\n",
       "      <td>-1.716</td>\n",
       "      <td>2.857</td>\n",
       "      <td>3.275</td>\n",
       "      <td>-1.821</td>\n",
       "      <td>-2.313</td>\n",
       "      <td>-1.757</td>\n",
       "      <td>-1.984</td>\n",
       "      <td>-2.376</td>\n",
       "      <td>-0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-77.469</td>\n",
       "      <td>51.265</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-77.771</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-29.638</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.726</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-64.451</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21.223</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-1.485</td>\n",
       "      <td>-33.638</td>\n",
       "      <td>-44.740</td>\n",
       "      <td>0.370</td>\n",
       "      <td>33.888</td>\n",
       "      <td>-14.100</td>\n",
       "      <td>-15.769</td>\n",
       "      <td>0.399</td>\n",
       "      <td>5.583</td>\n",
       "      <td>6.214</td>\n",
       "      <td>3.949</td>\n",
       "      <td>-22.994</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>60.606</td>\n",
       "      <td>-3.993</td>\n",
       "      <td>27.828</td>\n",
       "      <td>-77.469</td>\n",
       "      <td>51.265</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-77.771</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-29.638</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.726</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.046</td>\n",
       "      <td>2.660</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>2.322</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>3.783</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>-0.556</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.369</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>1.783</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-1.275</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.552</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.752</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>4.567</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.672</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.203</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.939</td>\n",
       "      <td>3.342</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1.195</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.543</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-4.455</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>1.746</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>0.070</td>\n",
       "      <td>1.910</td>\n",
       "      <td>1.792</td>\n",
       "      <td>1.932</td>\n",
       "      <td>1.869</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-2.442</td>\n",
       "      <td>1.916</td>\n",
       "      <td>1.823</td>\n",
       "      <td>1.804</td>\n",
       "      <td>2.096</td>\n",
       "      <td>1.946</td>\n",
       "      <td>1.853</td>\n",
       "      <td>1.815</td>\n",
       "      <td>1.924</td>\n",
       "      <td>-1.202</td>\n",
       "      <td>1.830</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>2.153</td>\n",
       "      <td>-3.044</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>2.251</td>\n",
       "      <td>1.798</td>\n",
       "      <td>2.071</td>\n",
       "      <td>1.824</td>\n",
       "      <td>1.935</td>\n",
       "      <td>-1.792</td>\n",
       "      <td>-1.667</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>-1.759</td>\n",
       "      <td>0.103</td>\n",
       "      <td>9.804</td>\n",
       "      <td>-1.345</td>\n",
       "      <td>-1.707</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>-1.671</td>\n",
       "      <td>-1.958</td>\n",
       "      <td>-1.838</td>\n",
       "      <td>-1.546</td>\n",
       "      <td>2.660</td>\n",
       "      <td>-1.520</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-1.306</td>\n",
       "      <td>8.864</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-1.683</td>\n",
       "      <td>-1.511</td>\n",
       "      <td>-1.409</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>-1.966</td>\n",
       "      <td>-1.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>88.863</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.798</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-70.730</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>39.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>85.969</td>\n",
       "      <td>-12.270</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>3.801</td>\n",
       "      <td>-1.311</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>4.975</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>0.054</td>\n",
       "      <td>3.121</td>\n",
       "      <td>-1.441</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>-0.811</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>23.287</td>\n",
       "      <td>-1.316</td>\n",
       "      <td>-1.308</td>\n",
       "      <td>-0.708</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>88.863</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>3.798</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-70.730</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.099</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>39.561</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.295</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.355</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.159</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.915</td>\n",
       "      <td>1.258</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.238</td>\n",
       "      <td>1.064</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.445</td>\n",
       "      <td>2.469</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.604</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.610</td>\n",
       "      <td>1.993</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.783</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.945</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.394</td>\n",
       "      <td>2.162</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.881</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.721</td>\n",
       "      <td>3.050</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>3.148</td>\n",
       "      <td>3.105</td>\n",
       "      <td>3.240</td>\n",
       "      <td>-2.694</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>3.381</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>3.155</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>3.074</td>\n",
       "      <td>3.217</td>\n",
       "      <td>3.109</td>\n",
       "      <td>3.204</td>\n",
       "      <td>3.264</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>3.317</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>2.239</td>\n",
       "      <td>2.168</td>\n",
       "      <td>1.983</td>\n",
       "      <td>-4.765</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>2.130</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>2.057</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>2.596</td>\n",
       "      <td>1.813</td>\n",
       "      <td>2.291</td>\n",
       "      <td>2.114</td>\n",
       "      <td>1.998</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>2.030</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>2.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>39.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-95.046</td>\n",
       "      <td>0.000</td>\n",
       "      <td>92.973</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.687</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-75.524</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-8.112</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-3.759</td>\n",
       "      <td>29.866</td>\n",
       "      <td>23.264</td>\n",
       "      <td>-35.253</td>\n",
       "      <td>6.297</td>\n",
       "      <td>21.730</td>\n",
       "      <td>-69.197</td>\n",
       "      <td>-30.440</td>\n",
       "      <td>127.626</td>\n",
       "      <td>1.657</td>\n",
       "      <td>-0.563</td>\n",
       "      <td>-1.755</td>\n",
       "      <td>-8.924</td>\n",
       "      <td>3.737</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.805</td>\n",
       "      <td>1.904</td>\n",
       "      <td>-34.465</td>\n",
       "      <td>-25.454</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>39.047</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-95.046</td>\n",
       "      <td>0.000</td>\n",
       "      <td>92.973</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.687</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-75.524</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.418</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.538</td>\n",
       "      <td>1.513</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.783</td>\n",
       "      <td>1.663</td>\n",
       "      <td>1.407</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>1.550</td>\n",
       "      <td>0.570</td>\n",
       "      <td>3.303</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.209</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>-1.085</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>-1.832</td>\n",
       "      <td>2.512</td>\n",
       "      <td>2.946</td>\n",
       "      <td>3.300</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.173</td>\n",
       "      <td>2.935</td>\n",
       "      <td>-2.012</td>\n",
       "      <td>-1.819</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-1.898</td>\n",
       "      <td>2.466</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.920</td>\n",
       "      <td>3.232</td>\n",
       "      <td>2.494</td>\n",
       "      <td>3.180</td>\n",
       "      <td>3.030</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>3.411</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-1.940</td>\n",
       "      <td>2.007</td>\n",
       "      <td>1.642</td>\n",
       "      <td>1.677</td>\n",
       "      <td>2.009</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.251</td>\n",
       "      <td>0.149</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>2.953</td>\n",
       "      <td>2.003</td>\n",
       "      <td>1.685</td>\n",
       "      <td>1.416</td>\n",
       "      <td>1.365</td>\n",
       "      <td>1.787</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.619</td>\n",
       "      <td>1.722</td>\n",
       "      <td>1.824</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>1.706</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-4.470</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>3.215</td>\n",
       "      <td>1.554</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.512</td>\n",
       "      <td>1.370</td>\n",
       "      <td>1.088</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.559</td>\n",
       "      <td>1.476</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.704</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.331</td>\n",
       "      <td>1.526</td>\n",
       "      <td>1.501</td>\n",
       "      <td>1.556</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.516</td>\n",
       "      <td>1.064</td>\n",
       "      <td>0.998</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.576</td>\n",
       "      <td>1.573</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>1.868</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.994</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.604</td>\n",
       "      <td>-2.815</td>\n",
       "      <td>1.549</td>\n",
       "      <td>1.694</td>\n",
       "      <td>1.570</td>\n",
       "      <td>1.669</td>\n",
       "      <td>1.735</td>\n",
       "      <td>2.642</td>\n",
       "      <td>1.810</td>\n",
       "      <td>-3.273</td>\n",
       "      <td>1.142</td>\n",
       "      <td>-2.904</td>\n",
       "      <td>-2.177</td>\n",
       "      <td>-2.730</td>\n",
       "      <td>-2.854</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.317</td>\n",
       "      <td>2.485</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>2.235</td>\n",
       "      <td>-2.714</td>\n",
       "      <td>-2.860</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-2.737</td>\n",
       "      <td>2.063</td>\n",
       "      <td>-11.034</td>\n",
       "      <td>2.818</td>\n",
       "      <td>2.038</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.169</td>\n",
       "      <td>-9.287</td>\n",
       "      <td>2.384</td>\n",
       "      <td>-9.178</td>\n",
       "      <td>-2.781</td>\n",
       "      <td>1.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "7217  1373158606         0.000         0.000         0.000         0.000   \n",
       "8291  1373158606         0.000         0.000       -54.061       -82.317   \n",
       "4607  1373158606       -77.469        51.265         0.000         0.000   \n",
       "5114  1373158606        88.863         0.000         0.000         0.000   \n",
       "1859  1373158606         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "7217         0.000       -27.497         0.000         0.000       -79.539   \n",
       "8291       -88.199         0.000         0.000         0.000         0.000   \n",
       "4607         0.000         0.000         0.000       -77.771         0.000   \n",
       "5114         0.000         0.000         3.798         0.000         0.000   \n",
       "1859        39.047         0.000         0.000         0.000       -95.046   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "7217        33.258         0.000         0.000         0.000         0.000   \n",
       "8291         0.000         0.000         0.000         0.000         0.000   \n",
       "4607         0.000         0.000       -29.638         0.000         0.000   \n",
       "5114         0.000         0.000       -70.730         0.000         0.000   \n",
       "1859         0.000        92.973         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "7217         0.000        64.030         5.342         0.000         0.000   \n",
       "8291         0.000       -27.222         0.000       -33.481         0.000   \n",
       "4607         0.000         0.000         0.000         0.000        97.726   \n",
       "5114        35.099         0.000        39.561         0.000         0.000   \n",
       "1859        45.687         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "7217         0.000         0.000               0.476               0.326   \n",
       "8291         0.000         0.000              39.969             -97.004   \n",
       "4607         0.000         0.000             -64.451              18.715   \n",
       "5114         0.000         0.000              85.969             -12.270   \n",
       "1859       -75.524         0.000              -8.112               0.492   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "7217               0.000              -1.843               0.754   \n",
       "8291              -0.050            -142.694               0.103   \n",
       "4607              21.223               0.157               0.646   \n",
       "5114              -0.621               3.801              -1.311   \n",
       "1859              -3.759              29.866              23.264   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "7217             -27.810              -0.217               0.687   \n",
       "8291               0.233              -0.742              -0.067   \n",
       "4607              -1.485             -33.638             -44.740   \n",
       "5114              -0.641               4.975              -0.937   \n",
       "1859             -35.253               6.297              21.730   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "7217             -76.258              29.856               1.241   \n",
       "8291              -0.304               0.202               0.595   \n",
       "4607               0.370              33.888             -14.100   \n",
       "5114              -0.676               0.054               3.121   \n",
       "1859             -69.197             -30.440             127.626   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "7217              -0.099              -0.887               1.290   \n",
       "8291              -0.076              -0.244               0.238   \n",
       "4607             -15.769               0.399               5.583   \n",
       "5114              -1.441              -1.168              -0.811   \n",
       "1859               1.657              -0.563              -1.755   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "7217              -0.643              63.833               2.520   \n",
       "8291               0.098             -43.283              -0.051   \n",
       "4607               6.214               3.949             -22.994   \n",
       "5114              -0.543              23.287              -1.316   \n",
       "1859              -8.924               3.737               0.122   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "7217               0.654               2.026              -0.780   \n",
       "8291              -0.113               0.129               0.070   \n",
       "4607              -0.274              60.606              -3.993   \n",
       "5114              -1.308              -0.708              -0.987   \n",
       "1859               1.805               1.904             -34.465   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "7217               0.493               0.000               0.000   \n",
       "8291              -0.124              -0.000               0.000   \n",
       "4607              27.828             -77.469              51.265   \n",
       "5114              -1.017              88.863              -0.000   \n",
       "1859             -25.454              -0.000              -0.000   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "7217              -0.000              -0.000               0.000   \n",
       "8291             -54.061             -82.317             -88.199   \n",
       "4607              -0.000              -0.000              -0.000   \n",
       "5114               0.000              -0.000               0.000   \n",
       "1859               0.000               0.000              39.047   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "7217             -27.497              -0.000               0.000   \n",
       "8291              -0.000               0.000              -0.000   \n",
       "4607              -0.000               0.000             -77.771   \n",
       "5114              -0.000               3.798               0.000   \n",
       "1859              -0.000               0.000               0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "7217             -79.539              33.258              -0.000   \n",
       "8291              -0.000              -0.000              -0.000   \n",
       "4607               0.000              -0.000              -0.000   \n",
       "5114               0.000              -0.000              -0.000   \n",
       "1859             -95.046               0.000              92.973   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "7217              -0.000               0.000               0.000   \n",
       "8291               0.000               0.000              -0.000   \n",
       "4607             -29.638               0.000               0.000   \n",
       "5114             -70.730               0.000               0.000   \n",
       "1859              -0.000              -0.000               0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "7217               0.000              64.030               5.342   \n",
       "8291              -0.000             -27.222              -0.000   \n",
       "4607              -0.000              -0.000               0.000   \n",
       "5114              35.099              -0.000              39.561   \n",
       "1859              45.687              -0.000               0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "7217               0.000              -0.000               0.000   \n",
       "8291             -33.481              -0.000              -0.000   \n",
       "4607               0.000              97.726               0.000   \n",
       "5114              -0.000              -0.000               0.000   \n",
       "1859               0.000              -0.000             -75.524   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1   wb_2   wb_3  wb_4  wb_5   wb_6  \\\n",
       "7217              -0.000 -1.406 -1.148  0.187 -0.278 2.627 1.090 -0.859   \n",
       "8291              -0.000  1.234  0.921  1.567  1.408 0.068 0.674  1.502   \n",
       "4607               0.000 -0.483 -0.740 -0.100 -0.273 0.046 2.660 -0.274   \n",
       "5114               0.000 -0.078 -0.337  0.226  0.031 1.133 1.046  0.947   \n",
       "1859              -0.000  1.418  0.864  1.538  1.513 0.650 0.554 -0.075   \n",
       "\n",
       "       wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  \\\n",
       "7217 -1.788 -0.683  2.932  0.249 -1.083 -0.108  0.160  2.555  0.210  2.380   \n",
       "8291  0.868  1.314  1.697  1.704  1.119  1.151  1.553  0.001  1.604  0.701   \n",
       "4607 -0.803 -0.352 -0.168 -0.014 -0.543 -0.488 -0.192  2.322 -0.083 -0.044   \n",
       "5114  1.295  0.020  1.355  0.251 -0.214 -0.117  0.159  1.079  0.211  0.915   \n",
       "1859 -0.419 -0.001  0.783  1.663  1.407 -0.106  1.550  0.570  3.303  0.434   \n",
       "\n",
       "      wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  \\\n",
       "7217  2.739  2.647  2.753  1.121 -0.103  2.614 -0.284 -1.664  0.327 -0.272   \n",
       "8291  1.520  0.377  0.709  1.354  1.267  1.364  1.140  0.862  0.452  0.573   \n",
       "4607 -0.789  3.783  0.116 -0.954 -0.389 -0.516 -0.556 -0.831  0.494  0.606   \n",
       "5114  1.258  1.191  1.238  1.064 -0.059  1.012 -0.233 -0.486 -0.146 -0.048   \n",
       "1859  0.688  0.720  0.692  0.499  0.347  0.341  0.030  1.209 -1.763 -1.085   \n",
       "\n",
       "      wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  \\\n",
       "7217 -0.367 -0.031  0.064  0.080 -0.044  1.161 -0.135  0.316 -0.436 -0.062   \n",
       "8291  0.324  0.325 -0.338  1.749  0.917  0.734  0.818  0.537  0.256  0.227   \n",
       "4607  0.352  0.369 -0.355  1.783  0.980  0.765  0.847  0.628  0.302  0.257   \n",
       "5114 -0.341 -0.303  0.160  0.600  0.445  2.469  0.178  0.429 -0.444 -0.408   \n",
       "1859 -1.840 -1.832  2.512  2.946  3.300  0.057  0.173  2.935 -2.012 -1.819   \n",
       "\n",
       "      wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  \\\n",
       "7217 -0.115 -0.289 -0.011  0.010  0.487  0.669 -0.199  0.650  0.053 -0.112   \n",
       "8291  0.498  0.329 -0.380  0.664  1.978  0.715  0.690  1.875  0.546  0.513   \n",
       "4607  0.512  0.381  0.391  0.712  0.134  0.900 -1.275  0.155  0.739  0.552   \n",
       "5114 -0.115 -0.294  0.126 -0.001  0.633  0.829  0.167  0.769  0.604 -0.103   \n",
       "1859 -0.152 -1.898  2.466  2.346  2.920  3.232  2.494  3.180  3.030 -0.096   \n",
       "\n",
       "      wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  \\\n",
       "7217  0.624 -0.066  0.783  2.200  1.515 -0.454  1.426 -0.954  2.563  5.541   \n",
       "8291  1.009  0.847  0.189  0.479  0.606  0.208  0.420 -0.436  1.893  0.927   \n",
       "4607  1.079  0.890  0.215  0.796  0.931  0.528  0.752 -0.436  4.567  1.242   \n",
       "5114  0.845  0.078 -0.499 -0.099 -0.006 -0.446 -0.199  0.150  0.883  0.610   \n",
       "1859  3.411  0.034 -1.940  2.007  1.642  1.677  2.009 -0.599  0.134 -0.651   \n",
       "\n",
       "      wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  \\\n",
       "7217  2.023 -1.790 -0.813 -0.018  1.381  0.150  0.003 -0.375 -0.241 -0.591   \n",
       "8291  0.051  0.416  0.357  0.722  0.878  0.805  0.662  0.106  0.360  1.670   \n",
       "4607  0.367  0.733  0.672  1.043  1.203  1.116  0.980  0.975  0.692 -0.373   \n",
       "5114  1.993 -0.143  0.370 -0.001  0.206  0.180  0.011  0.783 -0.239  0.177   \n",
       "1859 -0.533 -0.156 -0.570  2.216  2.251  0.149  2.150  0.017  0.034 -0.581   \n",
       "\n",
       "      wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  \\\n",
       "7217 -0.252 -0.120 -0.759  2.640 -0.517 -0.234  1.266  2.052  2.349  2.854   \n",
       "8291  0.704  0.888  1.521  0.765  0.022  0.553  0.873  0.606  2.882  2.948   \n",
       "4607  0.939  3.342 -0.341  0.995  0.353  0.833  1.195  0.911  0.378  0.472   \n",
       "5114  0.937  0.517  0.271  0.945 -0.536  0.517  0.128 -0.064  0.153  0.195   \n",
       "1859  0.102 -0.209 -0.551  0.109 -4.200 -0.479  2.953  2.003  1.685  1.416   \n",
       "\n",
       "      wb_77  wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  \\\n",
       "7217 -0.437  2.833 -0.533 -3.024 -1.845  1.802 -3.744 -0.844 -0.271  3.071   \n",
       "8291  2.281  3.024  0.238 -2.776  2.834  2.783  2.498  3.090  2.523  2.866   \n",
       "4607 -0.172  0.529  0.205 -0.033  0.292  0.315  0.039  0.467  0.025  0.411   \n",
       "5114 -0.444  0.230  0.924  0.368  0.394  2.162 -0.141  0.881 -0.263  0.123   \n",
       "1859  1.365  1.787  0.709  0.146 -0.159  0.031 -0.145  0.619  1.722  1.824   \n",
       "\n",
       "      wb_87  wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  \\\n",
       "7217 -0.494 -0.215 -0.783 -0.050 -1.078 -1.289 -1.115 -1.011 -3.298 -0.014   \n",
       "8291  2.116  2.543  0.037  2.790 -3.166  2.552 -1.955 -2.832  2.397  2.707   \n",
       "4607 -0.294  0.014  1.543  0.289 -0.463 -0.313  0.000 -0.243 -0.407  0.224   \n",
       "5114 -0.481 -0.209  0.737  0.016  0.130  0.348  0.529  0.415  0.192 -0.042   \n",
       "1859 -0.469  1.706  0.503  0.082 -0.117  0.075  0.322  0.152 -0.069 -4.470   \n",
       "\n",
       "      wb_97  wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  \\\n",
       "7217 -1.513  3.029  1.323   0.147   0.343  -0.042  -0.055   0.656   0.701   \n",
       "8291  2.366  2.970  2.368   1.758   2.067   2.152   1.633  -0.073  -1.310   \n",
       "4607 -0.210  0.478 -0.080  -0.682  -0.341  -0.244  -0.802  -0.002  -4.455   \n",
       "5114  0.188  0.115 -0.336  -0.377  -0.052  -0.008  -0.538   0.047  -0.218   \n",
       "1859 -0.143  3.215  1.554   0.522   0.471   0.626   0.512   1.370   1.088   \n",
       "\n",
       "      wb_106  wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  \\\n",
       "7217  -0.356   0.984   0.494   0.263  -0.535   0.271   0.020  -0.166   0.753   \n",
       "8291   1.834   2.342   1.951   1.853   1.708   1.608   2.108   2.026   0.037   \n",
       "4607  -0.654  -0.054  -0.450  -0.704  -0.740  -0.793  -0.253  -0.444   1.746   \n",
       "5114  -0.721   3.050  -0.124  -0.377  -0.530  -0.528   0.028  -0.166   0.166   \n",
       "1859   0.919   0.161  -0.123   1.074   0.535   0.483   0.017   0.559   1.476   \n",
       "\n",
       "      wb_115  wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  \\\n",
       "7217  -0.405   0.623   0.937   0.433   0.601   0.380   0.239   0.733  -0.069   \n",
       "8291   1.744  -1.288   2.569  -0.838  -1.300   2.371   2.429   2.301   1.837   \n",
       "4607  -0.697  -0.006  -0.233   0.111  -0.118  -0.383   0.018  -0.218  -0.596   \n",
       "5114  -0.401   0.054   0.361  -0.077  -0.015   0.158   0.227   0.077  -0.345   \n",
       "1859  -0.079   1.292   1.704   1.225   1.331   1.526   1.501   1.556  -0.958   \n",
       "\n",
       "      wb_124  wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  \\\n",
       "7217   1.122   0.142  -0.435  -0.095  -2.319   1.119   0.153  -1.193   0.723   \n",
       "8291   2.456  -0.044  -0.121   0.045  -0.058  -0.163   2.762  -0.062  -0.100   \n",
       "4607   0.070   1.910   1.792   1.932   1.869  -0.185  -2.442   1.916   1.823   \n",
       "5114   0.302  -0.093  -0.172  -0.062  -0.159   3.148   3.105   3.240  -2.694   \n",
       "1859   0.748   0.960   0.516   1.064   0.998   1.621   1.576   1.573  -0.190   \n",
       "\n",
       "      wb_133  wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  \\\n",
       "7217   2.300   1.278  -0.154  -1.297  -0.139  -0.119   1.074  -0.180   1.161   \n",
       "8291  -0.089   0.015   0.018  -0.049  -0.040  -0.039  -0.168  -0.088   3.045   \n",
       "4607   1.804   2.096   1.946   1.853   1.815   1.924  -1.202   1.830  -0.095   \n",
       "5114  -0.127   3.381  -0.153  -0.169  -0.124  -0.120   3.155  -0.163   3.074   \n",
       "1859  -0.141   1.868   1.116   0.994  -0.136   1.032   1.604  -2.815   1.549   \n",
       "\n",
       "      wb_142  wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  \\\n",
       "7217   1.107   0.934   1.197  -1.109  -0.166   1.245  -2.978   0.768  -2.933   \n",
       "8291  -0.304   2.213   2.852  -0.214  -0.110   0.005  -0.102   0.017  -2.228   \n",
       "4607   2.153  -3.044  -0.169   2.251   1.798   2.071   1.824   1.935  -1.792   \n",
       "5114   3.217   3.109   3.204   3.264  -0.177   3.317  -0.245  -0.123  -0.214   \n",
       "1859   1.694   1.570   1.669   1.735   2.642   1.810  -3.273   1.142  -2.904   \n",
       "\n",
       "      wb_151  wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  \\\n",
       "7217  -2.897  -0.250  -5.051   2.264   3.435   6.527  -2.923  -5.630   2.116   \n",
       "8291  -2.249  -2.246  -2.285   0.116   3.401  -1.771  -2.405  -2.098  -1.930   \n",
       "4607  -1.667  -1.761  -1.759   0.103   9.804  -1.345  -1.707  -1.624  -1.407   \n",
       "5114  -0.293  -0.286  -0.200   2.239   2.168   1.983  -4.765  -0.265   2.130   \n",
       "1859  -2.177  -2.730  -2.854   2.273   2.317   2.485  -0.326  -0.263   2.235   \n",
       "\n",
       "      wb_160  wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  \\\n",
       "7217  -0.019  -3.802  -0.318  -0.075   1.888  -0.153   2.455   1.604   2.168   \n",
       "8291  -2.005  -2.291  -2.126  -1.995   0.085  -2.074   4.048  -1.716   2.857   \n",
       "4607  -1.671  -1.958  -1.838  -1.546   2.660  -1.520   0.393  -1.306   8.864   \n",
       "5114  -0.032  -0.273  -0.322  -0.083   2.057  -0.158   2.596   1.813   2.291   \n",
       "1859  -2.714  -2.860  -0.333  -2.737   2.063 -11.034   2.818   2.038   2.314   \n",
       "\n",
       "      wb_169  wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "7217   2.046   4.237  -0.279   1.891  -8.871  -2.701   0.736  \n",
       "8291   3.275  -1.821  -2.313  -1.757  -1.984  -2.376  -0.068  \n",
       "4607   0.008  -1.683  -1.511  -1.409  -1.506  -1.966  -1.274  \n",
       "5114   2.114   1.998  -0.272   2.030  -0.133  -0.337   2.269  \n",
       "1859   2.332   2.169  -9.287   2.384  -9.178  -2.781   1.077  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- TRAINING INTERPRETATION NET -----------------------------------------------\n",
      "[<function inet_coefficient_loss_wrapper.<locals>.inet_coefficient_loss at 0x7f40db81b940>, <function inet_lambda_fv_loss_wrapper.<locals>.inet_lambda_fv_loss at 0x7f40db81baf0>, <function inet_coefficient_loss_wrapper.<locals>.inet_coefficient_loss at 0x7f40db81bb80>, <function inet_lambda_fv_loss_wrapper.<locals>.inet_lambda_fv_loss at 0x7f40db81bc10>]\n",
      "Epoch 1/500\n",
      "35/35 [==============================] - 14s 242ms/step - loss: 45.0559 - r2_inet_coefficient_loss: -0.0115 - r2_inet_lambda_fv_loss: 2.8025 - mae_inet_coefficient_loss: 14.4928 - mae_inet_lambda_fv_loss: 45.0554 - val_loss: 41.9818 - val_r2_inet_coefficient_loss: -0.0301 - val_r2_inet_lambda_fv_loss: 2.3295 - val_mae_inet_coefficient_loss: 15.8920 - val_mae_inet_lambda_fv_loss: 42.0518\n",
      "Epoch 2/500\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 37.8774 - r2_inet_coefficient_loss: -0.0318 - r2_inet_lambda_fv_loss: 1.8054 - mae_inet_coefficient_loss: 17.8929 - mae_inet_lambda_fv_loss: 37.8761 - val_loss: 30.9010 - val_r2_inet_coefficient_loss: 0.0682 - val_r2_inet_lambda_fv_loss: 0.9295 - val_mae_inet_coefficient_loss: 23.4959 - val_mae_inet_lambda_fv_loss: 30.9251\n",
      "Epoch 3/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 29.7436 - r2_inet_coefficient_loss: 0.0991 - r2_inet_lambda_fv_loss: 0.8235 - mae_inet_coefficient_loss: 24.6433 - mae_inet_lambda_fv_loss: 29.7436 - val_loss: 28.9620 - val_r2_inet_coefficient_loss: 0.0709 - val_r2_inet_lambda_fv_loss: 0.6290 - val_mae_inet_coefficient_loss: 24.3183 - val_mae_inet_lambda_fv_loss: 28.9847\n",
      "Epoch 4/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 28.1993 - r2_inet_coefficient_loss: 0.0652 - r2_inet_lambda_fv_loss: 0.5810 - mae_inet_coefficient_loss: 24.6157 - mae_inet_lambda_fv_loss: 28.1991 - val_loss: 27.3911 - val_r2_inet_coefficient_loss: 0.0746 - val_r2_inet_lambda_fv_loss: 0.4216 - val_mae_inet_coefficient_loss: 24.4636 - val_mae_inet_lambda_fv_loss: 27.4029\n",
      "Epoch 5/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 27.0830 - r2_inet_coefficient_loss: 0.0444 - r2_inet_lambda_fv_loss: 0.4242 - mae_inet_coefficient_loss: 24.3910 - mae_inet_lambda_fv_loss: 27.0832 - val_loss: 26.1379 - val_r2_inet_coefficient_loss: 0.0542 - val_r2_inet_lambda_fv_loss: 0.2852 - val_mae_inet_coefficient_loss: 23.9524 - val_mae_inet_lambda_fv_loss: 26.1499\n",
      "Epoch 6/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 25.8071 - r2_inet_coefficient_loss: 0.0316 - r2_inet_lambda_fv_loss: 0.3470 - mae_inet_coefficient_loss: 23.7119 - mae_inet_lambda_fv_loss: 25.8073 - val_loss: 25.1995 - val_r2_inet_coefficient_loss: 0.0644 - val_r2_inet_lambda_fv_loss: 0.1945 - val_mae_inet_coefficient_loss: 24.2681 - val_mae_inet_lambda_fv_loss: 25.2124\n",
      "Epoch 7/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 24.9518 - r2_inet_coefficient_loss: 0.0638 - r2_inet_lambda_fv_loss: 0.2183 - mae_inet_coefficient_loss: 23.8209 - mae_inet_lambda_fv_loss: 24.9516 - val_loss: 24.2745 - val_r2_inet_coefficient_loss: 0.0347 - val_r2_inet_lambda_fv_loss: 0.0914 - val_mae_inet_coefficient_loss: 23.9938 - val_mae_inet_lambda_fv_loss: 24.2757\n",
      "Epoch 8/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 24.4074 - r2_inet_coefficient_loss: 0.0491 - r2_inet_lambda_fv_loss: 0.1515 - mae_inet_coefficient_loss: 23.9444 - mae_inet_lambda_fv_loss: 24.4073 - val_loss: 23.7425 - val_r2_inet_coefficient_loss: 0.0209 - val_r2_inet_lambda_fv_loss: 0.0660 - val_mae_inet_coefficient_loss: 23.8744 - val_mae_inet_lambda_fv_loss: 23.7503\n",
      "Epoch 9/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 24.1696 - r2_inet_coefficient_loss: 0.0626 - r2_inet_lambda_fv_loss: 0.1204 - mae_inet_coefficient_loss: 24.2933 - mae_inet_lambda_fv_loss: 24.1698 - val_loss: 23.2259 - val_r2_inet_coefficient_loss: 0.0388 - val_r2_inet_lambda_fv_loss: 0.0416 - val_mae_inet_coefficient_loss: 24.2806 - val_mae_inet_lambda_fv_loss: 23.2403\n",
      "Epoch 10/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 23.5318 - r2_inet_coefficient_loss: 0.0824 - r2_inet_lambda_fv_loss: 0.0863 - mae_inet_coefficient_loss: 24.2673 - mae_inet_lambda_fv_loss: 23.5317 - val_loss: 22.9988 - val_r2_inet_coefficient_loss: 0.0828 - val_r2_inet_lambda_fv_loss: 0.0245 - val_mae_inet_coefficient_loss: 24.6197 - val_mae_inet_lambda_fv_loss: 23.0150\n",
      "Epoch 11/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 23.4095 - r2_inet_coefficient_loss: 0.1360 - r2_inet_lambda_fv_loss: 0.1041 - mae_inet_coefficient_loss: 24.9806 - mae_inet_lambda_fv_loss: 23.4097 - val_loss: 22.6245 - val_r2_inet_coefficient_loss: 0.0701 - val_r2_inet_lambda_fv_loss: -0.0096 - val_mae_inet_coefficient_loss: 24.6998 - val_mae_inet_lambda_fv_loss: 22.6376\n",
      "Epoch 12/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 23.0480 - r2_inet_coefficient_loss: 0.1225 - r2_inet_lambda_fv_loss: 0.0229 - mae_inet_coefficient_loss: 24.7935 - mae_inet_lambda_fv_loss: 23.0481 - val_loss: 22.4173 - val_r2_inet_coefficient_loss: 0.1185 - val_r2_inet_lambda_fv_loss: -0.0221 - val_mae_inet_coefficient_loss: 25.2934 - val_mae_inet_lambda_fv_loss: 22.4268\n",
      "Epoch 13/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 22.8276 - r2_inet_coefficient_loss: 0.1527 - r2_inet_lambda_fv_loss: 0.0489 - mae_inet_coefficient_loss: 25.3613 - mae_inet_lambda_fv_loss: 22.8281 - val_loss: 22.2323 - val_r2_inet_coefficient_loss: 0.1036 - val_r2_inet_lambda_fv_loss: -0.0387 - val_mae_inet_coefficient_loss: 25.0188 - val_mae_inet_lambda_fv_loss: 22.2413\n",
      "Epoch 14/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 22.6568 - r2_inet_coefficient_loss: 0.1587 - r2_inet_lambda_fv_loss: 0.0227 - mae_inet_coefficient_loss: 25.2893 - mae_inet_lambda_fv_loss: 22.6566 - val_loss: 21.8558 - val_r2_inet_coefficient_loss: 0.1031 - val_r2_inet_lambda_fv_loss: -0.0635 - val_mae_inet_coefficient_loss: 25.0856 - val_mae_inet_lambda_fv_loss: 21.8635\n",
      "Epoch 15/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 22.3576 - r2_inet_coefficient_loss: 0.1902 - r2_inet_lambda_fv_loss: 0.0104 - mae_inet_coefficient_loss: 25.3262 - mae_inet_lambda_fv_loss: 22.3576 - val_loss: 21.7025 - val_r2_inet_coefficient_loss: 0.0986 - val_r2_inet_lambda_fv_loss: -0.0708 - val_mae_inet_coefficient_loss: 25.0334 - val_mae_inet_lambda_fv_loss: 21.7169\n",
      "Epoch 16/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 22.1317 - r2_inet_coefficient_loss: 0.1748 - r2_inet_lambda_fv_loss: -0.0321 - mae_inet_coefficient_loss: 25.4811 - mae_inet_lambda_fv_loss: 22.1318 - val_loss: 21.4208 - val_r2_inet_coefficient_loss: 0.0912 - val_r2_inet_lambda_fv_loss: -0.0906 - val_mae_inet_coefficient_loss: 24.9976 - val_mae_inet_lambda_fv_loss: 21.4306\n",
      "Epoch 17/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.9945 - r2_inet_coefficient_loss: 0.1772 - r2_inet_lambda_fv_loss: -0.0344 - mae_inet_coefficient_loss: 25.3516 - mae_inet_lambda_fv_loss: 21.9942 - val_loss: 21.2853 - val_r2_inet_coefficient_loss: 0.0943 - val_r2_inet_lambda_fv_loss: -0.0999 - val_mae_inet_coefficient_loss: 25.0545 - val_mae_inet_lambda_fv_loss: 21.2951\n",
      "Epoch 18/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.9071 - r2_inet_coefficient_loss: 0.1625 - r2_inet_lambda_fv_loss: -0.0028 - mae_inet_coefficient_loss: 25.3624 - mae_inet_lambda_fv_loss: 21.9068 - val_loss: 21.2318 - val_r2_inet_coefficient_loss: 0.0883 - val_r2_inet_lambda_fv_loss: -0.1025 - val_mae_inet_coefficient_loss: 25.0218 - val_mae_inet_lambda_fv_loss: 21.2387\n",
      "Epoch 19/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.7235 - r2_inet_coefficient_loss: 0.1653 - r2_inet_lambda_fv_loss: -0.0700 - mae_inet_coefficient_loss: 25.4070 - mae_inet_lambda_fv_loss: 21.7235 - val_loss: 21.0940 - val_r2_inet_coefficient_loss: 0.0707 - val_r2_inet_lambda_fv_loss: -0.1123 - val_mae_inet_coefficient_loss: 24.7920 - val_mae_inet_lambda_fv_loss: 21.1020\n",
      "Epoch 20/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.7769 - r2_inet_coefficient_loss: 0.1591 - r2_inet_lambda_fv_loss: -0.0265 - mae_inet_coefficient_loss: 25.2660 - mae_inet_lambda_fv_loss: 21.7771 - val_loss: 20.8930 - val_r2_inet_coefficient_loss: 0.0456 - val_r2_inet_lambda_fv_loss: -0.1339 - val_mae_inet_coefficient_loss: 24.5353 - val_mae_inet_lambda_fv_loss: 20.8919\n",
      "Epoch 21/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.5698 - r2_inet_coefficient_loss: 0.1758 - r2_inet_lambda_fv_loss: -0.0632 - mae_inet_coefficient_loss: 25.0712 - mae_inet_lambda_fv_loss: 21.5695 - val_loss: 20.8993 - val_r2_inet_coefficient_loss: 0.0644 - val_r2_inet_lambda_fv_loss: -0.1287 - val_mae_inet_coefficient_loss: 24.7681 - val_mae_inet_lambda_fv_loss: 20.9047\n",
      "Epoch 22/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.5796 - r2_inet_coefficient_loss: 0.1916 - r2_inet_lambda_fv_loss: -0.0561 - mae_inet_coefficient_loss: 25.2987 - mae_inet_lambda_fv_loss: 21.5800 - val_loss: 20.8954 - val_r2_inet_coefficient_loss: 0.0644 - val_r2_inet_lambda_fv_loss: -0.1338 - val_mae_inet_coefficient_loss: 24.6960 - val_mae_inet_lambda_fv_loss: 20.9006\n",
      "Epoch 23/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.5779 - r2_inet_coefficient_loss: 0.1529 - r2_inet_lambda_fv_loss: -0.0150 - mae_inet_coefficient_loss: 25.2352 - mae_inet_lambda_fv_loss: 21.5780 - val_loss: 21.0119 - val_r2_inet_coefficient_loss: 0.0748 - val_r2_inet_lambda_fv_loss: -0.1142 - val_mae_inet_coefficient_loss: 24.8205 - val_mae_inet_lambda_fv_loss: 21.0223\n",
      "Epoch 24/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.5188 - r2_inet_coefficient_loss: 0.1989 - r2_inet_lambda_fv_loss: -0.0627 - mae_inet_coefficient_loss: 25.3400 - mae_inet_lambda_fv_loss: 21.5188 - val_loss: 21.0410 - val_r2_inet_coefficient_loss: 0.0937 - val_r2_inet_lambda_fv_loss: -0.1141 - val_mae_inet_coefficient_loss: 24.9355 - val_mae_inet_lambda_fv_loss: 21.0518\n",
      "Epoch 25/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.7657 - r2_inet_coefficient_loss: 0.2034 - r2_inet_lambda_fv_loss: -0.0504 - mae_inet_coefficient_loss: 25.4622 - mae_inet_lambda_fv_loss: 21.7656 - val_loss: 21.0833 - val_r2_inet_coefficient_loss: 0.0906 - val_r2_inet_lambda_fv_loss: -0.1123 - val_mae_inet_coefficient_loss: 24.7797 - val_mae_inet_lambda_fv_loss: 21.0928\n",
      "Epoch 26/500\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 21.3012 - r2_inet_coefficient_loss: 0.1634 - r2_inet_lambda_fv_loss: -0.0791 - mae_inet_coefficient_loss: 25.2938 - mae_inet_lambda_fv_loss: 21.3011 - val_loss: 21.0334 - val_r2_inet_coefficient_loss: 0.1003 - val_r2_inet_lambda_fv_loss: -0.1094 - val_mae_inet_coefficient_loss: 25.0118 - val_mae_inet_lambda_fv_loss: 21.0429\n",
      "Epoch 27/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.4687 - r2_inet_coefficient_loss: 0.1873 - r2_inet_lambda_fv_loss: -0.0751 - mae_inet_coefficient_loss: 25.3733 - mae_inet_lambda_fv_loss: 21.4687 - val_loss: 20.9458 - val_r2_inet_coefficient_loss: 0.0835 - val_r2_inet_lambda_fv_loss: -0.1173 - val_mae_inet_coefficient_loss: 24.8612 - val_mae_inet_lambda_fv_loss: 20.9561\n",
      "Epoch 28/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.4008 - r2_inet_coefficient_loss: 0.2005 - r2_inet_lambda_fv_loss: -0.0510 - mae_inet_coefficient_loss: 25.4437 - mae_inet_lambda_fv_loss: 21.4010 - val_loss: 20.9692 - val_r2_inet_coefficient_loss: 0.0976 - val_r2_inet_lambda_fv_loss: -0.1150 - val_mae_inet_coefficient_loss: 24.8780 - val_mae_inet_lambda_fv_loss: 20.9796\n",
      "Epoch 29/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.5063 - r2_inet_coefficient_loss: 0.1750 - r2_inet_lambda_fv_loss: -0.0394 - mae_inet_coefficient_loss: 25.3523 - mae_inet_lambda_fv_loss: 21.5061 - val_loss: 20.9367 - val_r2_inet_coefficient_loss: 0.0970 - val_r2_inet_lambda_fv_loss: -0.1167 - val_mae_inet_coefficient_loss: 24.8952 - val_mae_inet_lambda_fv_loss: 20.9499\n",
      "Epoch 30/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.3639 - r2_inet_coefficient_loss: 0.1957 - r2_inet_lambda_fv_loss: -0.0428 - mae_inet_coefficient_loss: 25.4410 - mae_inet_lambda_fv_loss: 21.3640 - val_loss: 21.0138 - val_r2_inet_coefficient_loss: 0.1343 - val_r2_inet_lambda_fv_loss: -0.1168 - val_mae_inet_coefficient_loss: 24.9471 - val_mae_inet_lambda_fv_loss: 21.0253\n",
      "Epoch 31/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.2565 - r2_inet_coefficient_loss: 0.2229 - r2_inet_lambda_fv_loss: -0.0611 - mae_inet_coefficient_loss: 25.3807 - mae_inet_lambda_fv_loss: 21.2565 - val_loss: 21.0646 - val_r2_inet_coefficient_loss: 0.1566 - val_r2_inet_lambda_fv_loss: -0.1109 - val_mae_inet_coefficient_loss: 25.1373 - val_mae_inet_lambda_fv_loss: 21.0795\n",
      "Epoch 32/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.3947 - r2_inet_coefficient_loss: 0.2152 - r2_inet_lambda_fv_loss: -0.0655 - mae_inet_coefficient_loss: 25.4799 - mae_inet_lambda_fv_loss: 21.3945 - val_loss: 20.9395 - val_r2_inet_coefficient_loss: 0.1236 - val_r2_inet_lambda_fv_loss: -0.1156 - val_mae_inet_coefficient_loss: 25.0290 - val_mae_inet_lambda_fv_loss: 20.9528\n",
      "Epoch 33/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.4202 - r2_inet_coefficient_loss: 0.2132 - r2_inet_lambda_fv_loss: -0.0732 - mae_inet_coefficient_loss: 25.6852 - mae_inet_lambda_fv_loss: 21.4201 - val_loss: 20.8262 - val_r2_inet_coefficient_loss: 0.1219 - val_r2_inet_lambda_fv_loss: -0.1265 - val_mae_inet_coefficient_loss: 24.9540 - val_mae_inet_lambda_fv_loss: 20.8379\n",
      "Epoch 34/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.4325 - r2_inet_coefficient_loss: 0.2534 - r2_inet_lambda_fv_loss: -0.0742 - mae_inet_coefficient_loss: 25.6894 - mae_inet_lambda_fv_loss: 21.4325 - val_loss: 20.8882 - val_r2_inet_coefficient_loss: 0.1544 - val_r2_inet_lambda_fv_loss: -0.1163 - val_mae_inet_coefficient_loss: 25.2500 - val_mae_inet_lambda_fv_loss: 20.9029\n",
      "Epoch 35/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.3707 - r2_inet_coefficient_loss: 0.2384 - r2_inet_lambda_fv_loss: -0.0757 - mae_inet_coefficient_loss: 25.7003 - mae_inet_lambda_fv_loss: 21.3713 - val_loss: 20.7515 - val_r2_inet_coefficient_loss: 0.1311 - val_r2_inet_lambda_fv_loss: -0.1278 - val_mae_inet_coefficient_loss: 25.0518 - val_mae_inet_lambda_fv_loss: 20.7699\n",
      "Epoch 36/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.1924 - r2_inet_coefficient_loss: 0.2190 - r2_inet_lambda_fv_loss: -0.0731 - mae_inet_coefficient_loss: 25.4250 - mae_inet_lambda_fv_loss: 21.1924 - val_loss: 20.9020 - val_r2_inet_coefficient_loss: 0.1823 - val_r2_inet_lambda_fv_loss: -0.1179 - val_mae_inet_coefficient_loss: 25.3249 - val_mae_inet_lambda_fv_loss: 20.9249\n",
      "Epoch 37/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.4286 - r2_inet_coefficient_loss: 0.2972 - r2_inet_lambda_fv_loss: -0.0739 - mae_inet_coefficient_loss: 25.9252 - mae_inet_lambda_fv_loss: 21.4287 - val_loss: 20.8537 - val_r2_inet_coefficient_loss: 0.1767 - val_r2_inet_lambda_fv_loss: -0.1187 - val_mae_inet_coefficient_loss: 25.3702 - val_mae_inet_lambda_fv_loss: 20.8746\n",
      "Epoch 38/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.1258 - r2_inet_coefficient_loss: 0.2859 - r2_inet_lambda_fv_loss: -0.0634 - mae_inet_coefficient_loss: 26.0125 - mae_inet_lambda_fv_loss: 21.1257 - val_loss: 20.8890 - val_r2_inet_coefficient_loss: 0.1921 - val_r2_inet_lambda_fv_loss: -0.1198 - val_mae_inet_coefficient_loss: 25.4118 - val_mae_inet_lambda_fv_loss: 20.9124\n",
      "Epoch 39/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.4199 - r2_inet_coefficient_loss: 0.2758 - r2_inet_lambda_fv_loss: -0.0669 - mae_inet_coefficient_loss: 25.9064 - mae_inet_lambda_fv_loss: 21.4198 - val_loss: 20.8439 - val_r2_inet_coefficient_loss: 0.1854 - val_r2_inet_lambda_fv_loss: -0.1223 - val_mae_inet_coefficient_loss: 25.5102 - val_mae_inet_lambda_fv_loss: 20.8694\n",
      "Epoch 40/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.2309 - r2_inet_coefficient_loss: 0.2515 - r2_inet_lambda_fv_loss: -0.1038 - mae_inet_coefficient_loss: 25.9654 - mae_inet_lambda_fv_loss: 21.2314 - val_loss: 20.8315 - val_r2_inet_coefficient_loss: 0.2220 - val_r2_inet_lambda_fv_loss: -0.1255 - val_mae_inet_coefficient_loss: 25.6614 - val_mae_inet_lambda_fv_loss: 20.8511\n",
      "Epoch 41/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.1779 - r2_inet_coefficient_loss: 0.2784 - r2_inet_lambda_fv_loss: -0.0649 - mae_inet_coefficient_loss: 26.0966 - mae_inet_lambda_fv_loss: 21.1783 - val_loss: 20.7501 - val_r2_inet_coefficient_loss: 0.2271 - val_r2_inet_lambda_fv_loss: -0.1269 - val_mae_inet_coefficient_loss: 25.6892 - val_mae_inet_lambda_fv_loss: 20.7768\n",
      "Epoch 42/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.2075 - r2_inet_coefficient_loss: 0.2812 - r2_inet_lambda_fv_loss: -0.0598 - mae_inet_coefficient_loss: 26.1644 - mae_inet_lambda_fv_loss: 21.2078 - val_loss: 20.6465 - val_r2_inet_coefficient_loss: 0.2283 - val_r2_inet_lambda_fv_loss: -0.1326 - val_mae_inet_coefficient_loss: 25.7827 - val_mae_inet_lambda_fv_loss: 20.6727\n",
      "Epoch 43/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.1613 - r2_inet_coefficient_loss: 0.2781 - r2_inet_lambda_fv_loss: -0.0616 - mae_inet_coefficient_loss: 26.1529 - mae_inet_lambda_fv_loss: 21.1614 - val_loss: 20.6393 - val_r2_inet_coefficient_loss: 0.2547 - val_r2_inet_lambda_fv_loss: -0.1361 - val_mae_inet_coefficient_loss: 26.0097 - val_mae_inet_lambda_fv_loss: 20.6663\n",
      "Epoch 44/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.2503 - r2_inet_coefficient_loss: 0.3233 - r2_inet_lambda_fv_loss: -0.0839 - mae_inet_coefficient_loss: 26.2682 - mae_inet_lambda_fv_loss: 21.2504 - val_loss: 20.6924 - val_r2_inet_coefficient_loss: 0.2637 - val_r2_inet_lambda_fv_loss: -0.1351 - val_mae_inet_coefficient_loss: 26.0468 - val_mae_inet_lambda_fv_loss: 20.7143\n",
      "Epoch 45/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.0011 - r2_inet_coefficient_loss: 0.3101 - r2_inet_lambda_fv_loss: -0.0837 - mae_inet_coefficient_loss: 26.0438 - mae_inet_lambda_fv_loss: 21.0010 - val_loss: 20.6789 - val_r2_inet_coefficient_loss: 0.2899 - val_r2_inet_lambda_fv_loss: -0.1322 - val_mae_inet_coefficient_loss: 26.2639 - val_mae_inet_lambda_fv_loss: 20.7003\n",
      "Epoch 46/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.8859 - r2_inet_coefficient_loss: 0.2861 - r2_inet_lambda_fv_loss: -0.1130 - mae_inet_coefficient_loss: 26.0936 - mae_inet_lambda_fv_loss: 20.8862 - val_loss: 20.7065 - val_r2_inet_coefficient_loss: 0.2979 - val_r2_inet_lambda_fv_loss: -0.1305 - val_mae_inet_coefficient_loss: 26.4129 - val_mae_inet_lambda_fv_loss: 20.7276\n",
      "Epoch 47/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.2020 - r2_inet_coefficient_loss: 0.3863 - r2_inet_lambda_fv_loss: -0.0718 - mae_inet_coefficient_loss: 26.6996 - mae_inet_lambda_fv_loss: 21.2019 - val_loss: 20.6601 - val_r2_inet_coefficient_loss: 0.3281 - val_r2_inet_lambda_fv_loss: -0.1373 - val_mae_inet_coefficient_loss: 26.5263 - val_mae_inet_lambda_fv_loss: 20.6817\n",
      "Epoch 48/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.9709 - r2_inet_coefficient_loss: 0.3696 - r2_inet_lambda_fv_loss: -0.0844 - mae_inet_coefficient_loss: 26.5965 - mae_inet_lambda_fv_loss: 20.9714 - val_loss: 20.6967 - val_r2_inet_coefficient_loss: 0.3444 - val_r2_inet_lambda_fv_loss: -0.1356 - val_mae_inet_coefficient_loss: 26.5878 - val_mae_inet_lambda_fv_loss: 20.7238\n",
      "Epoch 49/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.0046 - r2_inet_coefficient_loss: 0.4104 - r2_inet_lambda_fv_loss: -0.0755 - mae_inet_coefficient_loss: 26.8274 - mae_inet_lambda_fv_loss: 21.0045 - val_loss: 20.6401 - val_r2_inet_coefficient_loss: 0.3648 - val_r2_inet_lambda_fv_loss: -0.1337 - val_mae_inet_coefficient_loss: 26.9138 - val_mae_inet_lambda_fv_loss: 20.6637\n",
      "Epoch 50/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.9981 - r2_inet_coefficient_loss: 0.3964 - r2_inet_lambda_fv_loss: -0.1101 - mae_inet_coefficient_loss: 27.0260 - mae_inet_lambda_fv_loss: 20.9980 - val_loss: 20.6272 - val_r2_inet_coefficient_loss: 0.3681 - val_r2_inet_lambda_fv_loss: -0.1286 - val_mae_inet_coefficient_loss: 26.9146 - val_mae_inet_lambda_fv_loss: 20.6488\n",
      "Epoch 51/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.0822 - r2_inet_coefficient_loss: 0.4013 - r2_inet_lambda_fv_loss: -0.0825 - mae_inet_coefficient_loss: 27.1817 - mae_inet_lambda_fv_loss: 21.0823 - val_loss: 20.5721 - val_r2_inet_coefficient_loss: 0.3796 - val_r2_inet_lambda_fv_loss: -0.1320 - val_mae_inet_coefficient_loss: 27.2086 - val_mae_inet_lambda_fv_loss: 20.5916\n",
      "Epoch 52/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.9751 - r2_inet_coefficient_loss: 0.4060 - r2_inet_lambda_fv_loss: -0.0792 - mae_inet_coefficient_loss: 27.2260 - mae_inet_lambda_fv_loss: 20.9752 - val_loss: 20.5132 - val_r2_inet_coefficient_loss: 0.4022 - val_r2_inet_lambda_fv_loss: -0.1379 - val_mae_inet_coefficient_loss: 27.3331 - val_mae_inet_lambda_fv_loss: 20.5305\n",
      "Epoch 53/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.0200 - r2_inet_coefficient_loss: 0.4244 - r2_inet_lambda_fv_loss: -0.0688 - mae_inet_coefficient_loss: 27.4611 - mae_inet_lambda_fv_loss: 21.0201 - val_loss: 20.4024 - val_r2_inet_coefficient_loss: 0.3986 - val_r2_inet_lambda_fv_loss: -0.1490 - val_mae_inet_coefficient_loss: 27.3272 - val_mae_inet_lambda_fv_loss: 20.4196\n",
      "Epoch 54/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.9906 - r2_inet_coefficient_loss: 0.4657 - r2_inet_lambda_fv_loss: -0.1018 - mae_inet_coefficient_loss: 27.6864 - mae_inet_lambda_fv_loss: 20.9906 - val_loss: 20.3952 - val_r2_inet_coefficient_loss: 0.4359 - val_r2_inet_lambda_fv_loss: -0.1497 - val_mae_inet_coefficient_loss: 27.6856 - val_mae_inet_lambda_fv_loss: 20.4154\n",
      "Epoch 55/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.9184 - r2_inet_coefficient_loss: 0.5015 - r2_inet_lambda_fv_loss: -0.0726 - mae_inet_coefficient_loss: 27.9311 - mae_inet_lambda_fv_loss: 20.9188 - val_loss: 20.3144 - val_r2_inet_coefficient_loss: 0.4219 - val_r2_inet_lambda_fv_loss: -0.1572 - val_mae_inet_coefficient_loss: 27.6052 - val_mae_inet_lambda_fv_loss: 20.3309\n",
      "Epoch 56/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.9763 - r2_inet_coefficient_loss: 0.4900 - r2_inet_lambda_fv_loss: -0.0741 - mae_inet_coefficient_loss: 28.0822 - mae_inet_lambda_fv_loss: 20.9764 - val_loss: 20.4132 - val_r2_inet_coefficient_loss: 0.4548 - val_r2_inet_lambda_fv_loss: -0.1499 - val_mae_inet_coefficient_loss: 28.0296 - val_mae_inet_lambda_fv_loss: 20.4239\n",
      "Epoch 57/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.9566 - r2_inet_coefficient_loss: 0.4927 - r2_inet_lambda_fv_loss: -0.0976 - mae_inet_coefficient_loss: 28.1501 - mae_inet_lambda_fv_loss: 20.9565 - val_loss: 20.2895 - val_r2_inet_coefficient_loss: 0.4312 - val_r2_inet_lambda_fv_loss: -0.1613 - val_mae_inet_coefficient_loss: 27.9433 - val_mae_inet_lambda_fv_loss: 20.2995\n",
      "Epoch 58/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.0078 - r2_inet_coefficient_loss: 0.5446 - r2_inet_lambda_fv_loss: -0.0607 - mae_inet_coefficient_loss: 28.2258 - mae_inet_lambda_fv_loss: 21.0076 - val_loss: 20.2782 - val_r2_inet_coefficient_loss: 0.4461 - val_r2_inet_lambda_fv_loss: -0.1572 - val_mae_inet_coefficient_loss: 28.1152 - val_mae_inet_lambda_fv_loss: 20.2892\n",
      "Epoch 59/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6244 - r2_inet_coefficient_loss: 0.4725 - r2_inet_lambda_fv_loss: -0.0999 - mae_inet_coefficient_loss: 28.3139 - mae_inet_lambda_fv_loss: 20.6246 - val_loss: 20.3144 - val_r2_inet_coefficient_loss: 0.4767 - val_r2_inet_lambda_fv_loss: -0.1525 - val_mae_inet_coefficient_loss: 28.1870 - val_mae_inet_lambda_fv_loss: 20.3272\n",
      "Epoch 60/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8972 - r2_inet_coefficient_loss: 0.5579 - r2_inet_lambda_fv_loss: -0.0987 - mae_inet_coefficient_loss: 28.4540 - mae_inet_lambda_fv_loss: 20.8973 - val_loss: 20.2043 - val_r2_inet_coefficient_loss: 0.4890 - val_r2_inet_lambda_fv_loss: -0.1552 - val_mae_inet_coefficient_loss: 28.5000 - val_mae_inet_lambda_fv_loss: 20.2183\n",
      "Epoch 61/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8056 - r2_inet_coefficient_loss: 0.5567 - r2_inet_lambda_fv_loss: -0.0716 - mae_inet_coefficient_loss: 28.6530 - mae_inet_lambda_fv_loss: 20.8055 - val_loss: 20.1673 - val_r2_inet_coefficient_loss: 0.5116 - val_r2_inet_lambda_fv_loss: -0.1557 - val_mae_inet_coefficient_loss: 28.6658 - val_mae_inet_lambda_fv_loss: 20.1764\n",
      "Epoch 62/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7546 - r2_inet_coefficient_loss: 0.6184 - r2_inet_lambda_fv_loss: -0.0958 - mae_inet_coefficient_loss: 28.7482 - mae_inet_lambda_fv_loss: 20.7545 - val_loss: 20.2765 - val_r2_inet_coefficient_loss: 0.5378 - val_r2_inet_lambda_fv_loss: -0.1485 - val_mae_inet_coefficient_loss: 28.7643 - val_mae_inet_lambda_fv_loss: 20.2908\n",
      "Epoch 63/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8488 - r2_inet_coefficient_loss: 0.6480 - r2_inet_lambda_fv_loss: -0.1150 - mae_inet_coefficient_loss: 29.1287 - mae_inet_lambda_fv_loss: 20.8486 - val_loss: 20.2609 - val_r2_inet_coefficient_loss: 0.5865 - val_r2_inet_lambda_fv_loss: -0.1538 - val_mae_inet_coefficient_loss: 29.0018 - val_mae_inet_lambda_fv_loss: 20.2699\n",
      "Epoch 64/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6768 - r2_inet_coefficient_loss: 0.6630 - r2_inet_lambda_fv_loss: -0.1208 - mae_inet_coefficient_loss: 28.8104 - mae_inet_lambda_fv_loss: 20.6769 - val_loss: 20.3726 - val_r2_inet_coefficient_loss: 0.6375 - val_r2_inet_lambda_fv_loss: -0.1371 - val_mae_inet_coefficient_loss: 29.1976 - val_mae_inet_lambda_fv_loss: 20.3787\n",
      "Epoch 65/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6385 - r2_inet_coefficient_loss: 0.6430 - r2_inet_lambda_fv_loss: -0.1076 - mae_inet_coefficient_loss: 28.8790 - mae_inet_lambda_fv_loss: 20.6387 - val_loss: 20.2587 - val_r2_inet_coefficient_loss: 0.5986 - val_r2_inet_lambda_fv_loss: -0.1504 - val_mae_inet_coefficient_loss: 29.0547 - val_mae_inet_lambda_fv_loss: 20.2682\n",
      "Epoch 66/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8373 - r2_inet_coefficient_loss: 0.6783 - r2_inet_lambda_fv_loss: -0.1054 - mae_inet_coefficient_loss: 29.1900 - mae_inet_lambda_fv_loss: 20.8372 - val_loss: 20.3065 - val_r2_inet_coefficient_loss: 0.6114 - val_r2_inet_lambda_fv_loss: -0.1446 - val_mae_inet_coefficient_loss: 29.1067 - val_mae_inet_lambda_fv_loss: 20.3145\n",
      "Epoch 67/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8372 - r2_inet_coefficient_loss: 0.6893 - r2_inet_lambda_fv_loss: -0.0903 - mae_inet_coefficient_loss: 29.2833 - mae_inet_lambda_fv_loss: 20.8374 - val_loss: 20.3097 - val_r2_inet_coefficient_loss: 0.6377 - val_r2_inet_lambda_fv_loss: -0.1439 - val_mae_inet_coefficient_loss: 29.4118 - val_mae_inet_lambda_fv_loss: 20.3162\n",
      "Epoch 68/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6034 - r2_inet_coefficient_loss: 0.6746 - r2_inet_lambda_fv_loss: -0.1249 - mae_inet_coefficient_loss: 29.3405 - mae_inet_lambda_fv_loss: 20.6036 - val_loss: 20.2195 - val_r2_inet_coefficient_loss: 0.6432 - val_r2_inet_lambda_fv_loss: -0.1500 - val_mae_inet_coefficient_loss: 29.4940 - val_mae_inet_lambda_fv_loss: 20.2274\n",
      "Epoch 69/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.6605 - r2_inet_coefficient_loss: 0.7225 - r2_inet_lambda_fv_loss: -0.0880 - mae_inet_coefficient_loss: 29.4541 - mae_inet_lambda_fv_loss: 20.6607 - val_loss: 20.2104 - val_r2_inet_coefficient_loss: 0.6535 - val_r2_inet_lambda_fv_loss: -0.1517 - val_mae_inet_coefficient_loss: 29.4500 - val_mae_inet_lambda_fv_loss: 20.2252\n",
      "Epoch 70/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6755 - r2_inet_coefficient_loss: 0.7056 - r2_inet_lambda_fv_loss: -0.1088 - mae_inet_coefficient_loss: 29.3184 - mae_inet_lambda_fv_loss: 20.6753 - val_loss: 20.2935 - val_r2_inet_coefficient_loss: 0.6522 - val_r2_inet_lambda_fv_loss: -0.1449 - val_mae_inet_coefficient_loss: 29.4011 - val_mae_inet_lambda_fv_loss: 20.3067\n",
      "Epoch 71/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6587 - r2_inet_coefficient_loss: 0.7671 - r2_inet_lambda_fv_loss: -0.1175 - mae_inet_coefficient_loss: 29.6308 - mae_inet_lambda_fv_loss: 20.6589 - val_loss: 20.3174 - val_r2_inet_coefficient_loss: 0.6023 - val_r2_inet_lambda_fv_loss: -0.1484 - val_mae_inet_coefficient_loss: 28.9868 - val_mae_inet_lambda_fv_loss: 20.3231\n",
      "Epoch 72/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6260 - r2_inet_coefficient_loss: 0.6399 - r2_inet_lambda_fv_loss: -0.0946 - mae_inet_coefficient_loss: 28.9397 - mae_inet_lambda_fv_loss: 20.6260 - val_loss: 20.3967 - val_r2_inet_coefficient_loss: 0.6170 - val_r2_inet_lambda_fv_loss: -0.1474 - val_mae_inet_coefficient_loss: 29.1333 - val_mae_inet_lambda_fv_loss: 20.4070\n",
      "Epoch 73/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7608 - r2_inet_coefficient_loss: 0.7041 - r2_inet_lambda_fv_loss: -0.1072 - mae_inet_coefficient_loss: 28.9610 - mae_inet_lambda_fv_loss: 20.7603 - val_loss: 20.2691 - val_r2_inet_coefficient_loss: 0.6007 - val_r2_inet_lambda_fv_loss: -0.1550 - val_mae_inet_coefficient_loss: 29.0983 - val_mae_inet_lambda_fv_loss: 20.2759\n",
      "Epoch 74/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8631 - r2_inet_coefficient_loss: 0.6727 - r2_inet_lambda_fv_loss: -0.0933 - mae_inet_coefficient_loss: 28.9198 - mae_inet_lambda_fv_loss: 20.8629 - val_loss: 20.2476 - val_r2_inet_coefficient_loss: 0.6051 - val_r2_inet_lambda_fv_loss: -0.1528 - val_mae_inet_coefficient_loss: 29.1617 - val_mae_inet_lambda_fv_loss: 20.2575\n",
      "Epoch 75/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7480 - r2_inet_coefficient_loss: 0.7607 - r2_inet_lambda_fv_loss: -0.1091 - mae_inet_coefficient_loss: 29.1700 - mae_inet_lambda_fv_loss: 20.7480 - val_loss: 20.2293 - val_r2_inet_coefficient_loss: 0.6178 - val_r2_inet_lambda_fv_loss: -0.1529 - val_mae_inet_coefficient_loss: 29.2655 - val_mae_inet_lambda_fv_loss: 20.2401\n",
      "Epoch 76/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7000 - r2_inet_coefficient_loss: 0.7167 - r2_inet_lambda_fv_loss: -0.0924 - mae_inet_coefficient_loss: 29.1711 - mae_inet_lambda_fv_loss: 20.7000 - val_loss: 20.2446 - val_r2_inet_coefficient_loss: 0.6148 - val_r2_inet_lambda_fv_loss: -0.1536 - val_mae_inet_coefficient_loss: 29.2982 - val_mae_inet_lambda_fv_loss: 20.2512\n",
      "Epoch 77/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7771 - r2_inet_coefficient_loss: 0.6993 - r2_inet_lambda_fv_loss: -0.0896 - mae_inet_coefficient_loss: 29.3396 - mae_inet_lambda_fv_loss: 20.7773 - val_loss: 20.1868 - val_r2_inet_coefficient_loss: 0.6015 - val_r2_inet_lambda_fv_loss: -0.1586 - val_mae_inet_coefficient_loss: 29.2897 - val_mae_inet_lambda_fv_loss: 20.1911\n",
      "Epoch 78/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.5548 - r2_inet_coefficient_loss: 0.6860 - r2_inet_lambda_fv_loss: -0.1167 - mae_inet_coefficient_loss: 28.9638 - mae_inet_lambda_fv_loss: 20.5545 - val_loss: 20.1694 - val_r2_inet_coefficient_loss: 0.5962 - val_r2_inet_lambda_fv_loss: -0.1597 - val_mae_inet_coefficient_loss: 29.1725 - val_mae_inet_lambda_fv_loss: 20.1718\n",
      "Epoch 79/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7733 - r2_inet_coefficient_loss: 0.7224 - r2_inet_lambda_fv_loss: -0.0988 - mae_inet_coefficient_loss: 29.3543 - mae_inet_lambda_fv_loss: 20.7730 - val_loss: 20.1978 - val_r2_inet_coefficient_loss: 0.5878 - val_r2_inet_lambda_fv_loss: -0.1558 - val_mae_inet_coefficient_loss: 29.1578 - val_mae_inet_lambda_fv_loss: 20.1968\n",
      "Epoch 80/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6411 - r2_inet_coefficient_loss: 0.6891 - r2_inet_lambda_fv_loss: -0.0932 - mae_inet_coefficient_loss: 28.9653 - mae_inet_lambda_fv_loss: 20.6411 - val_loss: 20.2175 - val_r2_inet_coefficient_loss: 0.6129 - val_r2_inet_lambda_fv_loss: -0.1495 - val_mae_inet_coefficient_loss: 29.3837 - val_mae_inet_lambda_fv_loss: 20.2159\n",
      "Epoch 81/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6865 - r2_inet_coefficient_loss: 0.6668 - r2_inet_lambda_fv_loss: -0.1061 - mae_inet_coefficient_loss: 29.1491 - mae_inet_lambda_fv_loss: 20.6865 - val_loss: 20.2024 - val_r2_inet_coefficient_loss: 0.6246 - val_r2_inet_lambda_fv_loss: -0.1516 - val_mae_inet_coefficient_loss: 29.3279 - val_mae_inet_lambda_fv_loss: 20.2030\n",
      "Epoch 82/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.6288 - r2_inet_coefficient_loss: 0.6785 - r2_inet_lambda_fv_loss: -0.0950 - mae_inet_coefficient_loss: 29.0781 - mae_inet_lambda_fv_loss: 20.6293 - val_loss: 20.2048 - val_r2_inet_coefficient_loss: 0.6109 - val_r2_inet_lambda_fv_loss: -0.1492 - val_mae_inet_coefficient_loss: 29.3199 - val_mae_inet_lambda_fv_loss: 20.2099\n",
      "Epoch 83/500\n",
      "35/35 [==============================] - 5s 155ms/step - loss: 20.6831 - r2_inet_coefficient_loss: 0.6746 - r2_inet_lambda_fv_loss: -0.0941 - mae_inet_coefficient_loss: 29.0732 - mae_inet_lambda_fv_loss: 20.6830 - val_loss: 20.1807 - val_r2_inet_coefficient_loss: 0.5742 - val_r2_inet_lambda_fv_loss: -0.1573 - val_mae_inet_coefficient_loss: 29.0107 - val_mae_inet_lambda_fv_loss: 20.1845\n",
      "Epoch 84/500\n",
      "35/35 [==============================] - 5s 154ms/step - loss: 20.5811 - r2_inet_coefficient_loss: 0.6571 - r2_inet_lambda_fv_loss: -0.1104 - mae_inet_coefficient_loss: 28.8082 - mae_inet_lambda_fv_loss: 20.5814 - val_loss: 20.2385 - val_r2_inet_coefficient_loss: 0.6264 - val_r2_inet_lambda_fv_loss: -0.1501 - val_mae_inet_coefficient_loss: 29.4117 - val_mae_inet_lambda_fv_loss: 20.2449\n",
      "Epoch 85/500\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 20.8466 - r2_inet_coefficient_loss: 0.7135 - r2_inet_lambda_fv_loss: -0.0746 - mae_inet_coefficient_loss: 29.2728 - mae_inet_lambda_fv_loss: 20.8466 - val_loss: 20.3589 - val_r2_inet_coefficient_loss: 0.5906 - val_r2_inet_lambda_fv_loss: -0.1404 - val_mae_inet_coefficient_loss: 29.2397 - val_mae_inet_lambda_fv_loss: 20.3646\n",
      "Epoch 86/500\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 20.8017 - r2_inet_coefficient_loss: 0.6858 - r2_inet_lambda_fv_loss: -0.0988 - mae_inet_coefficient_loss: 29.1175 - mae_inet_lambda_fv_loss: 20.8016 - val_loss: 20.3601 - val_r2_inet_coefficient_loss: 0.6169 - val_r2_inet_lambda_fv_loss: -0.1426 - val_mae_inet_coefficient_loss: 29.2803 - val_mae_inet_lambda_fv_loss: 20.3654\n",
      "Epoch 87/500\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 20.7282 - r2_inet_coefficient_loss: 0.6989 - r2_inet_lambda_fv_loss: -0.1001 - mae_inet_coefficient_loss: 29.0064 - mae_inet_lambda_fv_loss: 20.7280 - val_loss: 20.3995 - val_r2_inet_coefficient_loss: 0.6103 - val_r2_inet_lambda_fv_loss: -0.1413 - val_mae_inet_coefficient_loss: 29.2021 - val_mae_inet_lambda_fv_loss: 20.4007\n",
      "Epoch 88/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.9605 - r2_inet_coefficient_loss: 0.7173 - r2_inet_lambda_fv_loss: -0.1029 - mae_inet_coefficient_loss: 29.2949 - mae_inet_lambda_fv_loss: 20.9604 - val_loss: 20.3802 - val_r2_inet_coefficient_loss: 0.6161 - val_r2_inet_lambda_fv_loss: -0.1460 - val_mae_inet_coefficient_loss: 29.0227 - val_mae_inet_lambda_fv_loss: 20.3797\n",
      "Epoch 89/500\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 20.7107 - r2_inet_coefficient_loss: 0.7131 - r2_inet_lambda_fv_loss: -0.1150 - mae_inet_coefficient_loss: 29.1175 - mae_inet_lambda_fv_loss: 20.7110 - val_loss: 20.4049 - val_r2_inet_coefficient_loss: 0.6571 - val_r2_inet_lambda_fv_loss: -0.1425 - val_mae_inet_coefficient_loss: 29.2165 - val_mae_inet_lambda_fv_loss: 20.4111\n",
      "Epoch 90/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.0809 - r2_inet_coefficient_loss: 0.8020 - r2_inet_lambda_fv_loss: -0.0711 - mae_inet_coefficient_loss: 29.2575 - mae_inet_lambda_fv_loss: 21.0809 - val_loss: 20.3261 - val_r2_inet_coefficient_loss: 0.6348 - val_r2_inet_lambda_fv_loss: -0.1491 - val_mae_inet_coefficient_loss: 29.2541 - val_mae_inet_lambda_fv_loss: 20.3293\n",
      "Epoch 91/500\n",
      "35/35 [==============================] - 5s 152ms/step - loss: 20.8452 - r2_inet_coefficient_loss: 0.6962 - r2_inet_lambda_fv_loss: -0.0858 - mae_inet_coefficient_loss: 29.2191 - mae_inet_lambda_fv_loss: 20.8450 - val_loss: 20.2906 - val_r2_inet_coefficient_loss: 0.6048 - val_r2_inet_lambda_fv_loss: -0.1525 - val_mae_inet_coefficient_loss: 29.0733 - val_mae_inet_lambda_fv_loss: 20.2921\n",
      "Epoch 92/500\n",
      "35/35 [==============================] - 5s 152ms/step - loss: 20.6248 - r2_inet_coefficient_loss: 0.7467 - r2_inet_lambda_fv_loss: -0.1210 - mae_inet_coefficient_loss: 29.0861 - mae_inet_lambda_fv_loss: 20.6243 - val_loss: 20.2958 - val_r2_inet_coefficient_loss: 0.6281 - val_r2_inet_lambda_fv_loss: -0.1466 - val_mae_inet_coefficient_loss: 29.2064 - val_mae_inet_lambda_fv_loss: 20.3004\n",
      "Epoch 93/500\n",
      "35/35 [==============================] - 5s 153ms/step - loss: 20.8438 - r2_inet_coefficient_loss: 0.7413 - r2_inet_lambda_fv_loss: -0.0618 - mae_inet_coefficient_loss: 29.2250 - mae_inet_lambda_fv_loss: 20.8438 - val_loss: 20.3753 - val_r2_inet_coefficient_loss: 0.6407 - val_r2_inet_lambda_fv_loss: -0.1423 - val_mae_inet_coefficient_loss: 29.2178 - val_mae_inet_lambda_fv_loss: 20.3772\n",
      "Epoch 94/500\n",
      "35/35 [==============================] - 5s 152ms/step - loss: 20.8100 - r2_inet_coefficient_loss: 0.7667 - r2_inet_lambda_fv_loss: -0.0923 - mae_inet_coefficient_loss: 29.2498 - mae_inet_lambda_fv_loss: 20.8103 - val_loss: 20.3611 - val_r2_inet_coefficient_loss: 0.6644 - val_r2_inet_lambda_fv_loss: -0.1366 - val_mae_inet_coefficient_loss: 29.4211 - val_mae_inet_lambda_fv_loss: 20.3628\n",
      "Epoch 95/500\n",
      "35/35 [==============================] - 5s 151ms/step - loss: 20.8710 - r2_inet_coefficient_loss: 0.7607 - r2_inet_lambda_fv_loss: -0.1028 - mae_inet_coefficient_loss: 29.3200 - mae_inet_lambda_fv_loss: 20.8709 - val_loss: 20.3802 - val_r2_inet_coefficient_loss: 0.6351 - val_r2_inet_lambda_fv_loss: -0.1384 - val_mae_inet_coefficient_loss: 29.1420 - val_mae_inet_lambda_fv_loss: 20.3803\n",
      "Epoch 96/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.9570 - r2_inet_coefficient_loss: 0.7513 - r2_inet_lambda_fv_loss: -0.0862 - mae_inet_coefficient_loss: 29.2813 - mae_inet_lambda_fv_loss: 20.9573 - val_loss: 20.3127 - val_r2_inet_coefficient_loss: 0.6126 - val_r2_inet_lambda_fv_loss: -0.1472 - val_mae_inet_coefficient_loss: 29.0329 - val_mae_inet_lambda_fv_loss: 20.3123\n",
      "Epoch 97/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7465 - r2_inet_coefficient_loss: 0.7052 - r2_inet_lambda_fv_loss: -0.1099 - mae_inet_coefficient_loss: 28.8498 - mae_inet_lambda_fv_loss: 20.7468 - val_loss: 20.4164 - val_r2_inet_coefficient_loss: 0.6430 - val_r2_inet_lambda_fv_loss: -0.1345 - val_mae_inet_coefficient_loss: 29.2399 - val_mae_inet_lambda_fv_loss: 20.4148\n",
      "Epoch 98/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6749 - r2_inet_coefficient_loss: 0.7227 - r2_inet_lambda_fv_loss: -0.0963 - mae_inet_coefficient_loss: 29.0922 - mae_inet_lambda_fv_loss: 20.6751 - val_loss: 20.4334 - val_r2_inet_coefficient_loss: 0.6551 - val_r2_inet_lambda_fv_loss: -0.1341 - val_mae_inet_coefficient_loss: 29.2966 - val_mae_inet_lambda_fv_loss: 20.4340\n",
      "Epoch 99/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 20.9061 - r2_inet_coefficient_loss: 0.7477 - r2_inet_lambda_fv_loss: -0.0870 - mae_inet_coefficient_loss: 29.4032 - mae_inet_lambda_fv_loss: 20.9065 - val_loss: 20.4753 - val_r2_inet_coefficient_loss: 0.6845 - val_r2_inet_lambda_fv_loss: -0.1287 - val_mae_inet_coefficient_loss: 29.4275 - val_mae_inet_lambda_fv_loss: 20.4803\n",
      "Epoch 100/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8133 - r2_inet_coefficient_loss: 0.7648 - r2_inet_lambda_fv_loss: -0.0815 - mae_inet_coefficient_loss: 29.2755 - mae_inet_lambda_fv_loss: 20.8133 - val_loss: 20.5537 - val_r2_inet_coefficient_loss: 0.6652 - val_r2_inet_lambda_fv_loss: -0.1291 - val_mae_inet_coefficient_loss: 29.2062 - val_mae_inet_lambda_fv_loss: 20.5591\n",
      "Epoch 101/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8568 - r2_inet_coefficient_loss: 0.7574 - r2_inet_lambda_fv_loss: -0.1032 - mae_inet_coefficient_loss: 29.3432 - mae_inet_lambda_fv_loss: 20.8567 - val_loss: 20.6705 - val_r2_inet_coefficient_loss: 0.7187 - val_r2_inet_lambda_fv_loss: -0.1124 - val_mae_inet_coefficient_loss: 29.5328 - val_mae_inet_lambda_fv_loss: 20.6849\n",
      "Epoch 102/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8171 - r2_inet_coefficient_loss: 0.7638 - r2_inet_lambda_fv_loss: -0.0946 - mae_inet_coefficient_loss: 29.3652 - mae_inet_lambda_fv_loss: 20.8172 - val_loss: 20.5938 - val_r2_inet_coefficient_loss: 0.6980 - val_r2_inet_lambda_fv_loss: -0.1189 - val_mae_inet_coefficient_loss: 29.5218 - val_mae_inet_lambda_fv_loss: 20.6011\n",
      "Epoch 103/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.9427 - r2_inet_coefficient_loss: 0.8535 - r2_inet_lambda_fv_loss: -0.0927 - mae_inet_coefficient_loss: 29.7768 - mae_inet_lambda_fv_loss: 20.9424 - val_loss: 20.6143 - val_r2_inet_coefficient_loss: 0.7505 - val_r2_inet_lambda_fv_loss: -0.1176 - val_mae_inet_coefficient_loss: 29.6608 - val_mae_inet_lambda_fv_loss: 20.6225\n",
      "Epoch 104/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.7813 - r2_inet_coefficient_loss: 0.8398 - r2_inet_lambda_fv_loss: -0.1039 - mae_inet_coefficient_loss: 29.4249 - mae_inet_lambda_fv_loss: 20.7818 - val_loss: 20.6780 - val_r2_inet_coefficient_loss: 0.7579 - val_r2_inet_lambda_fv_loss: -0.1165 - val_mae_inet_coefficient_loss: 29.6585 - val_mae_inet_lambda_fv_loss: 20.6845\n",
      "Epoch 105/500\n",
      "35/35 [==============================] - 5s 150ms/step - loss: 21.0365 - r2_inet_coefficient_loss: 0.8556 - r2_inet_lambda_fv_loss: -0.0751 - mae_inet_coefficient_loss: 29.7696 - mae_inet_lambda_fv_loss: 21.0368 - val_loss: 20.6506 - val_r2_inet_coefficient_loss: 0.7469 - val_r2_inet_lambda_fv_loss: -0.1233 - val_mae_inet_coefficient_loss: 29.5511 - val_mae_inet_lambda_fv_loss: 20.6558\n",
      "Epoch 106/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.8546 - r2_inet_coefficient_loss: 0.8177 - r2_inet_lambda_fv_loss: -0.0794 - mae_inet_coefficient_loss: 29.5915 - mae_inet_lambda_fv_loss: 20.8543 - val_loss: 20.6107 - val_r2_inet_coefficient_loss: 0.7206 - val_r2_inet_lambda_fv_loss: -0.1281 - val_mae_inet_coefficient_loss: 29.3560 - val_mae_inet_lambda_fv_loss: 20.6163\n",
      "Epoch 107/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.9031 - r2_inet_coefficient_loss: 0.8399 - r2_inet_lambda_fv_loss: -0.1170 - mae_inet_coefficient_loss: 29.5305 - mae_inet_lambda_fv_loss: 20.9035 - val_loss: 20.5947 - val_r2_inet_coefficient_loss: 0.7728 - val_r2_inet_lambda_fv_loss: -0.1293 - val_mae_inet_coefficient_loss: 29.5991 - val_mae_inet_lambda_fv_loss: 20.6009\n",
      "Epoch 108/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6246 - r2_inet_coefficient_loss: 0.7964 - r2_inet_lambda_fv_loss: -0.1005 - mae_inet_coefficient_loss: 29.3817 - mae_inet_lambda_fv_loss: 20.6247 - val_loss: 20.5390 - val_r2_inet_coefficient_loss: 0.7820 - val_r2_inet_lambda_fv_loss: -0.1329 - val_mae_inet_coefficient_loss: 29.6869 - val_mae_inet_lambda_fv_loss: 20.5413\n",
      "Epoch 109/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 20.6642 - r2_inet_coefficient_loss: 0.8874 - r2_inet_lambda_fv_loss: -0.0939 - mae_inet_coefficient_loss: 29.7264 - mae_inet_lambda_fv_loss: 20.6643 - val_loss: 20.5464 - val_r2_inet_coefficient_loss: 0.8411 - val_r2_inet_lambda_fv_loss: -0.1230 - val_mae_inet_coefficient_loss: 29.9366 - val_mae_inet_lambda_fv_loss: 20.5493\n",
      "Epoch 110/500\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 20.9896 - r2_inet_coefficient_loss: 0.9399 - r2_inet_lambda_fv_loss: -0.0916 - mae_inet_coefficient_loss: 30.1633 - mae_inet_lambda_fv_loss: 20.9895 - val_loss: 20.6891 - val_r2_inet_coefficient_loss: 0.8928 - val_r2_inet_lambda_fv_loss: -0.1146 - val_mae_inet_coefficient_loss: 30.1482 - val_mae_inet_lambda_fv_loss: 20.6946\n",
      "Epoch 111/500\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 21.1027 - r2_inet_coefficient_loss: 0.9918 - r2_inet_lambda_fv_loss: -0.0652 - mae_inet_coefficient_loss: 29.9784 - mae_inet_lambda_fv_loss: 21.1024 - val_loss: 20.6702 - val_r2_inet_coefficient_loss: 0.8962 - val_r2_inet_lambda_fv_loss: -0.1188 - val_mae_inet_coefficient_loss: 29.9493 - val_mae_inet_lambda_fv_loss: 20.6781\n",
      "Training Time: 0:09:51\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------- PREDICT INET ------------------------------------------------------\n",
      "Predict Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=10)]: Done   2 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=10)]: Done   3 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=10)]: Done   4 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=10)]: Done   6 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=10)]: Done   7 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=10)]: Done   8 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=10)]: Done   9 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=10)]: Done  10 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=10)]: Done  11 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=10)]: Done  13 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=10)]: Done  14 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=10)]: Done  15 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=10)]: Done  16 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=10)]: Done  17 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=10)]: Done  18 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=10)]: Done  19 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=10)]: Done  20 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=10)]: Done  22 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=10)]: Done  23 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=10)]: Done  24 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=10)]: Done  25 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=10)]: Done  26 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=10)]: Done  27 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=10)]: Done  28 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=10)]: Done  29 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=10)]: Done  31 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=10)]: Done  32 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=10)]: Done  33 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=10)]: Done  34 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=10)]: Done  35 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=10)]: Done  36 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=10)]: Done  37 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=10)]: Done  38 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=10)]: Done  39 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=10)]: Done  40 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=10)]: Done  42 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=10)]: Done  43 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=10)]: Done  44 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=10)]: Done  45 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=10)]: Done  46 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=10)]: Done  47 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=10)]: Done  48 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=10)]: Done  49 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=10)]: Done  50 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=10)]: Done  51 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=10)]: Done  53 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=10)]: Done  54 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=10)]: Done  55 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=10)]: Done  56 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=10)]: Done  57 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=10)]: Done  58 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=10)]: Done  59 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=10)]: Done  60 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=10)]: Done  61 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=10)]: Done  62 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=10)]: Done  63 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=10)]: Done  64 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=10)]: Done  66 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=10)]: Done  67 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=10)]: Done  68 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=10)]: Done  69 tasks      | elapsed: 20.5min\n",
      "[Parallel(n_jobs=10)]: Done  70 tasks      | elapsed: 20.7min\n",
      "[Parallel(n_jobs=10)]: Done  71 tasks      | elapsed: 20.7min\n",
      "[Parallel(n_jobs=10)]: Done  72 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=10)]: Done  73 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=10)]: Done  74 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=10)]: Done  75 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=10)]: Done  76 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=10)]: Done  77 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed: 23.3min\n",
      "[Parallel(n_jobs=10)]: Done  79 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=10)]: Done  80 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=10)]: Done  81 tasks      | elapsed: 24.2min\n",
      "[Parallel(n_jobs=10)]: Done  91 out of 100 | elapsed: 26.9min remaining:  2.7min\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed: 29.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Regression Optimization Time: 0:29:23\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: tf__flip() takes from 0 to 2 positional arguments but 5 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c5d89a0a4fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdistrib_dict_test_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_interpretation_net_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_train_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                    \u001b[0mlambda_net_valid_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                    lambda_net_test_dataset_list)\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mcalculate_interpretation_net_results\u001b[0;34m(lambda_net_train_dataset_list, lambda_net_valid_dataset_list, lambda_net_test_dataset_list)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mfunction_values_test_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlambda_net_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_test_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfunction_values_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_all_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mfunction_values_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_values_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mcalculate_all_function_values\u001b[0;34m(lambda_net_dataset, polynomial_dict)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     function_value_dict = {\n\u001b[1;32m   1130\u001b[0m         \u001b[0;34m'lambda_preds'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0;34m'target_polynomials'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_target_poly_fvs_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         \u001b[0;34m'lstsq_lambda_pred_polynomials'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_lstsq_lambda_pred_polynomial_fvs_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;34m'lstsq_target_polynomials'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_lstsq_target_polynomial_fvs_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/LambdaNet.py\u001b[0m in \u001b[0;36mreturn_target_poly_fvs_on_test_data\u001b[0;34m(self, n_jobs_parallel_fv, backend)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutility_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparallel_fv_calculation_from_polynomial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtarget_poly_fvs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_fv_calculation_from_polynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_polynomial_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test_data_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_complete_poly_representation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs_parallel_fv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_poly_fvs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/utility_functions.py\u001b[0m in \u001b[0;36mparallel_fv_calculation_from_polynomial\u001b[0;34m(polynomial_list, lambda_input_list, force_complete_poly_representation, n_jobs_parallel_fv, backend)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'interpretation_net_output_monomials'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m     \u001b[0mpolynomial_true_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_poly_fv_tf_wrapper_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_float_tensor_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_monomial_identifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_float_tensor_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolynomial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_float_tensor_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_complete_poly_representation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_complete_poly_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpolynomial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_inputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolynomial_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_input_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;31m# We capture the KeyboardInterrupt and reraise it as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    628\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m           \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: tf__flip() takes from 0 to 2 positional arguments but 5 were given\n"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "(history_list, \n",
    "\n",
    "#scores_valid_list,\n",
    "scores_test_list, \n",
    "\n",
    "#function_values_valid_list, \n",
    "function_values_test_list, \n",
    "\n",
    "#polynomial_dict_valid_list,\n",
    "polynomial_dict_test_list,\n",
    "\n",
    "#distrib_dict_valid_list,\n",
    "distrib_dict_test_list,\n",
    "\n",
    "model_list) = calculate_interpretation_net_results(lambda_net_train_dataset_list, \n",
    "                                                   lambda_net_valid_dataset_list, \n",
    "                                                   lambda_net_test_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_net_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interpretation_net_output_monomials+1)*sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list[-1]['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list[-1]['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_dict_test_list[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize = tf.constant([float(i) for i in range(interpretation_net_output_shape)])\n",
    "\n",
    "if interpretation_net_output_monomials != None:\n",
    "    poly_optimize_coeffs = poly_optimize[:interpretation_net_output_monomials]\n",
    "\n",
    "    poly_optimize_identifiers_list = []\n",
    "    for i in range(interpretation_net_output_monomials):\n",
    "        poly_optimize_identifiers = tf.math.softmax(poly_optimize[sparsity*i+interpretation_net_output_monomials:sparsity*(i+1)+interpretation_net_output_monomials])\n",
    "        poly_optimize_identifiers_list.append(poly_optimize_identifiers)\n",
    "    poly_optimize_identifiers_list = tf.keras.backend.flatten(poly_optimize_identifiers_list)\n",
    "    poly_optimize = tf.concat([poly_optimize_coeffs, poly_optimize_identifiers_list], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
