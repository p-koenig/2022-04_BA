{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 5, #degree\n",
    "        'n': 1, #number of variables\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': None,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 100,\n",
    "        'a_min': -100,\n",
    "        'lambda_nets_total': 10000,\n",
    "        'noise': 0.1,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0.25,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 500,\n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': None, #(None, int)\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number\n",
    "        \n",
    "        'evaluate_with_real_function': False,\n",
    "        'consider_labels_training': False,\n",
    "                      \n",
    "        'data_reshape_version': 2, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': True,\n",
    "        'nas_type': 'CNN', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 5,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '2',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from IPython.display import display, Math, Latex, clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "\n",
    "#from symbolic_metamodeling.pysymbolic.algorithms.symbolic_metamodeling import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d'])\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']\n",
    "\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sparsity')*config['data']['sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense512-1024-output_6_drop0.25e500b256_custom/lnets_10000_30-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_5_sparsity_6_amin_-100_amax_100_xdist_uniform_noise_normal_0.1\n",
      "lnets_10000_30-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_5_sparsity_6_amin_-100_amax_100_xmin_0_xmax_1_xdist_uniform_noise_normal_0.1\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.789212Z",
     "start_time": "2021-01-05T08:33:49.725485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1801dd0ed24e4c08bce8f36994e9d071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 6\n",
      "Number of monomials in a polynomial with 1 variables and degree 5: 6\n",
      "Sparsity: 6\n",
      "['0', '1', '2', '3', '4', '5']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a02e17364743518e1dd0db7f419d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 6\n",
      "Number of monomials in a polynomial with 1 variables and degree 5: 6\n",
      "Sparsity: 6\n",
      "['0', '1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n",
    "\n",
    "\n",
    "layers_with_input_output = list(flatten([[n], lambda_network_layers, [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend MultiprocessingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   1 out of   1 | elapsed:   36.0s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "\n",
    "parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>19.441</td>\n",
       "      <td>64.331</td>\n",
       "      <td>9.054</td>\n",
       "      <td>86.972</td>\n",
       "      <td>79.463</td>\n",
       "      <td>5.935</td>\n",
       "      <td>15.294</td>\n",
       "      <td>121.910</td>\n",
       "      <td>-262.623</td>\n",
       "      <td>257.461</td>\n",
       "      <td>616.384</td>\n",
       "      <td>-523.246</td>\n",
       "      <td>19.451</td>\n",
       "      <td>64.248</td>\n",
       "      <td>9.070</td>\n",
       "      <td>87.143</td>\n",
       "      <td>79.482</td>\n",
       "      <td>5.796</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.082</td>\n",
       "      <td>5.706</td>\n",
       "      <td>6.366</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>4.648</td>\n",
       "      <td>7.439</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.130</td>\n",
       "      <td>4.331</td>\n",
       "      <td>0.179</td>\n",
       "      <td>6.276</td>\n",
       "      <td>4.517</td>\n",
       "      <td>6.461</td>\n",
       "      <td>4.542</td>\n",
       "      <td>4.301</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-1.843</td>\n",
       "      <td>-2.770</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-4.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.584</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-3.438</td>\n",
       "      <td>0.650</td>\n",
       "      <td>-3.460</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.706</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>5.771</td>\n",
       "      <td>8.075</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>3.265</td>\n",
       "      <td>10.767</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>3.541</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>9.544</td>\n",
       "      <td>3.395</td>\n",
       "      <td>8.852</td>\n",
       "      <td>3.464</td>\n",
       "      <td>3.431</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>5.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>1.590</td>\n",
       "      <td>-44.282</td>\n",
       "      <td>-11.825</td>\n",
       "      <td>-83.220</td>\n",
       "      <td>-1.186</td>\n",
       "      <td>39.111</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-44.165</td>\n",
       "      <td>125.741</td>\n",
       "      <td>-625.284</td>\n",
       "      <td>696.732</td>\n",
       "      <td>-250.025</td>\n",
       "      <td>1.586</td>\n",
       "      <td>-44.076</td>\n",
       "      <td>-13.515</td>\n",
       "      <td>-78.249</td>\n",
       "      <td>-7.254</td>\n",
       "      <td>41.711</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>3.352</td>\n",
       "      <td>3.258</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>3.396</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>3.272</td>\n",
       "      <td>3.592</td>\n",
       "      <td>3.553</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>3.421</td>\n",
       "      <td>0.079</td>\n",
       "      <td>3.548</td>\n",
       "      <td>0.008</td>\n",
       "      <td>3.636</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.033</td>\n",
       "      <td>3.167</td>\n",
       "      <td>3.286</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>3.138</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.383</td>\n",
       "      <td>-1.271</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.659</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-1.767</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.297</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-3.529</td>\n",
       "      <td>-3.492</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-2.748</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-3.380</td>\n",
       "      <td>-2.553</td>\n",
       "      <td>-2.753</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.749</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-3.027</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-3.728</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-3.572</td>\n",
       "      <td>-2.768</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-3.699</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-19.084</td>\n",
       "      <td>-21.097</td>\n",
       "      <td>93.004</td>\n",
       "      <td>45.811</td>\n",
       "      <td>3.748</td>\n",
       "      <td>43.701</td>\n",
       "      <td>-18.767</td>\n",
       "      <td>-35.482</td>\n",
       "      <td>429.612</td>\n",
       "      <td>-1371.706</td>\n",
       "      <td>2065.681</td>\n",
       "      <td>-938.295</td>\n",
       "      <td>-19.088</td>\n",
       "      <td>-21.132</td>\n",
       "      <td>93.113</td>\n",
       "      <td>46.175</td>\n",
       "      <td>2.656</td>\n",
       "      <td>44.383</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.049</td>\n",
       "      <td>4.247</td>\n",
       "      <td>4.145</td>\n",
       "      <td>4.402</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.045</td>\n",
       "      <td>4.586</td>\n",
       "      <td>4.562</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>4.323</td>\n",
       "      <td>4.204</td>\n",
       "      <td>0.191</td>\n",
       "      <td>3.982</td>\n",
       "      <td>4.458</td>\n",
       "      <td>4.361</td>\n",
       "      <td>4.450</td>\n",
       "      <td>4.195</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-2.879</td>\n",
       "      <td>-2.901</td>\n",
       "      <td>-2.532</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-1.437</td>\n",
       "      <td>-2.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.071</td>\n",
       "      <td>-2.981</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-2.785</td>\n",
       "      <td>-2.793</td>\n",
       "      <td>-1.676</td>\n",
       "      <td>-2.284</td>\n",
       "      <td>-1.177</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>3.636</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>7.084</td>\n",
       "      <td>7.238</td>\n",
       "      <td>6.182</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>4.437</td>\n",
       "      <td>5.163</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>7.019</td>\n",
       "      <td>7.272</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>7.726</td>\n",
       "      <td>6.598</td>\n",
       "      <td>5.052</td>\n",
       "      <td>5.753</td>\n",
       "      <td>4.609</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-4.479</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-4.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-2.763</td>\n",
       "      <td>80.962</td>\n",
       "      <td>-48.518</td>\n",
       "      <td>-66.867</td>\n",
       "      <td>-71.702</td>\n",
       "      <td>-69.024</td>\n",
       "      <td>6.019</td>\n",
       "      <td>74.395</td>\n",
       "      <td>-606.442</td>\n",
       "      <td>2191.199</td>\n",
       "      <td>-3137.256</td>\n",
       "      <td>1310.095</td>\n",
       "      <td>-2.768</td>\n",
       "      <td>81.211</td>\n",
       "      <td>-50.215</td>\n",
       "      <td>-62.322</td>\n",
       "      <td>-76.914</td>\n",
       "      <td>-66.894</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>4.700</td>\n",
       "      <td>4.648</td>\n",
       "      <td>1.451</td>\n",
       "      <td>1.403</td>\n",
       "      <td>4.730</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>4.483</td>\n",
       "      <td>4.916</td>\n",
       "      <td>4.731</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>4.826</td>\n",
       "      <td>1.403</td>\n",
       "      <td>4.697</td>\n",
       "      <td>1.339</td>\n",
       "      <td>4.768</td>\n",
       "      <td>1.605</td>\n",
       "      <td>4.821</td>\n",
       "      <td>1.292</td>\n",
       "      <td>4.422</td>\n",
       "      <td>4.611</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>4.461</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.335</td>\n",
       "      <td>-2.739</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.613</td>\n",
       "      <td>-2.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.445</td>\n",
       "      <td>-2.518</td>\n",
       "      <td>-3.732</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.061</td>\n",
       "      <td>0.706</td>\n",
       "      <td>-3.641</td>\n",
       "      <td>0.426</td>\n",
       "      <td>-3.589</td>\n",
       "      <td>0.549</td>\n",
       "      <td>-2.211</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-3.366</td>\n",
       "      <td>-2.209</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-9.917</td>\n",
       "      <td>-8.202</td>\n",
       "      <td>1.295</td>\n",
       "      <td>1.355</td>\n",
       "      <td>-6.621</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-10.648</td>\n",
       "      <td>-6.575</td>\n",
       "      <td>-10.339</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-8.521</td>\n",
       "      <td>1.278</td>\n",
       "      <td>-10.373</td>\n",
       "      <td>1.640</td>\n",
       "      <td>-9.943</td>\n",
       "      <td>1.347</td>\n",
       "      <td>-6.420</td>\n",
       "      <td>1.121</td>\n",
       "      <td>-10.671</td>\n",
       "      <td>-6.831</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-9.398</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>2.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>93.063</td>\n",
       "      <td>-23.940</td>\n",
       "      <td>-91.272</td>\n",
       "      <td>26.621</td>\n",
       "      <td>-90.579</td>\n",
       "      <td>-61.679</td>\n",
       "      <td>97.680</td>\n",
       "      <td>-10.869</td>\n",
       "      <td>-818.271</td>\n",
       "      <td>2848.944</td>\n",
       "      <td>-3879.414</td>\n",
       "      <td>1631.414</td>\n",
       "      <td>93.056</td>\n",
       "      <td>-23.777</td>\n",
       "      <td>-92.449</td>\n",
       "      <td>29.989</td>\n",
       "      <td>-94.540</td>\n",
       "      <td>-60.070</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>4.457</td>\n",
       "      <td>4.229</td>\n",
       "      <td>-2.990</td>\n",
       "      <td>-2.972</td>\n",
       "      <td>4.532</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>4.220</td>\n",
       "      <td>4.728</td>\n",
       "      <td>4.619</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>4.536</td>\n",
       "      <td>-3.008</td>\n",
       "      <td>4.448</td>\n",
       "      <td>-2.878</td>\n",
       "      <td>-3.057</td>\n",
       "      <td>-2.935</td>\n",
       "      <td>-3.026</td>\n",
       "      <td>-3.091</td>\n",
       "      <td>4.173</td>\n",
       "      <td>4.399</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>4.198</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.803</td>\n",
       "      <td>-3.248</td>\n",
       "      <td>2.919</td>\n",
       "      <td>2.911</td>\n",
       "      <td>-2.899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.236</td>\n",
       "      <td>-2.579</td>\n",
       "      <td>-2.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.689</td>\n",
       "      <td>2.948</td>\n",
       "      <td>-3.203</td>\n",
       "      <td>2.823</td>\n",
       "      <td>3.010</td>\n",
       "      <td>2.877</td>\n",
       "      <td>2.962</td>\n",
       "      <td>3.020</td>\n",
       "      <td>-3.180</td>\n",
       "      <td>-2.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.239</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-8.204</td>\n",
       "      <td>-9.489</td>\n",
       "      <td>4.021</td>\n",
       "      <td>4.071</td>\n",
       "      <td>-8.156</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-9.735</td>\n",
       "      <td>-7.027</td>\n",
       "      <td>-7.042</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-7.510</td>\n",
       "      <td>4.014</td>\n",
       "      <td>-8.784</td>\n",
       "      <td>4.246</td>\n",
       "      <td>3.884</td>\n",
       "      <td>4.019</td>\n",
       "      <td>3.916</td>\n",
       "      <td>3.894</td>\n",
       "      <td>-9.771</td>\n",
       "      <td>-7.342</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-9.907</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>5.726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "6252  1373158606    19.441    64.331     9.054    86.972    79.463     5.935   \n",
       "4684  1373158606     1.590   -44.282   -11.825   -83.220    -1.186    39.111   \n",
       "1731  1373158606   -19.084   -21.097    93.004    45.811     3.748    43.701   \n",
       "4742  1373158606    -2.763    80.962   -48.518   -66.867   -71.702   -69.024   \n",
       "4521  1373158606    93.063   -23.940   -91.272    26.621   -90.579   -61.679   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "6252          15.294         121.910        -262.623         257.461   \n",
       "4684           0.440         -44.165         125.741        -625.284   \n",
       "1731         -18.767         -35.482         429.612       -1371.706   \n",
       "4742           6.019          74.395        -606.442        2191.199   \n",
       "4521          97.680         -10.869        -818.271        2848.944   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "6252         616.384        -523.246          19.451          64.248   \n",
       "4684         696.732        -250.025           1.586         -44.076   \n",
       "1731        2065.681        -938.295         -19.088         -21.132   \n",
       "4742       -3137.256        1310.095          -2.768          81.211   \n",
       "4521       -3879.414        1631.414          93.056         -23.777   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "6252           9.070          87.143          79.482           5.796 -0.009   \n",
       "4684         -13.515         -78.249          -7.254          41.711 -0.009   \n",
       "1731          93.113          46.175           2.656          44.383 -0.009   \n",
       "4742         -50.215         -62.322         -76.914         -66.894 -0.009   \n",
       "4521         -92.449          29.989         -94.540         -60.070 -0.009   \n",
       "\n",
       "       wb_1  wb_2  wb_3   wb_4   wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "6252 -0.227 0.131 0.082  5.706  6.366 0.110 -0.297 0.067 4.648  7.439 -0.088   \n",
       "4684 -0.227 3.352 3.258  0.109  0.080 3.396 -0.297 3.272 3.592  3.553 -0.088   \n",
       "1731 -0.227 0.118 0.049  4.247  4.145 4.402 -0.297 0.045 4.586  4.562 -0.088   \n",
       "4742 -0.227 4.700 4.648  1.451  1.403 4.730 -0.297 4.483 4.916  4.731 -0.088   \n",
       "4521 -0.227 4.457 4.229 -2.990 -2.972 4.532 -0.297 4.220 4.728  4.619 -0.088   \n",
       "\n",
       "      wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "6252 -0.019  0.130  4.331  0.179  6.276  4.517  6.461  4.542  4.301  0.041   \n",
       "4684 -0.019  3.421  0.079  3.548  0.008  3.636  0.148  0.137  0.033  3.167   \n",
       "1731 -0.019  4.323  4.204  0.191  3.982  4.458  4.361  4.450  4.195  0.018   \n",
       "4742 -0.019  4.826  1.403  4.697  1.339  4.768  1.605  4.821  1.292  4.422   \n",
       "4521 -0.019  4.536 -3.008  4.448 -2.878 -3.057 -2.935 -3.026 -3.091  4.173   \n",
       "\n",
       "      wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "6252  0.009 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "4684  3.286 -0.042 -0.404 -0.090  3.138 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "1731 -4.000 -0.042 -0.404 -0.090  0.029 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "4742  4.611 -0.042 -0.404 -0.090  4.461 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "4521  4.399 -0.042 -0.404 -0.090  4.198 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "\n",
       "      wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "6252 -0.131 -0.083 -1.843 -2.770 -0.110  0.000 -0.069  0.730 -4.506  0.000   \n",
       "4684 -1.383 -1.271 -0.109 -0.081 -0.188  0.000 -0.919 -0.179 -0.272  0.000   \n",
       "1731 -0.122 -0.064 -2.879 -2.901 -2.532  0.000 -0.061 -1.437 -2.049  0.000   \n",
       "4742 -3.335 -2.739  0.647  0.613 -2.292  0.000 -3.445 -2.518 -3.732  0.000   \n",
       "4521 -2.803 -3.248  2.919  2.911 -2.899  0.000 -3.236 -2.579 -2.507  0.000   \n",
       "\n",
       "      wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "6252  0.000 -0.130  0.584 -0.179 -3.438  0.650 -3.460  0.525  0.706 -0.047   \n",
       "4684  0.000 -0.184 -0.080 -0.659 -0.023 -1.767 -0.148 -0.136 -0.040 -1.150   \n",
       "1731  0.000 -3.071 -2.981 -0.191 -2.785 -2.793 -1.676 -2.284 -1.177 -0.039   \n",
       "4742  0.000 -3.061  0.706 -3.641  0.426 -3.589  0.549 -2.211  0.830 -3.366   \n",
       "4521  0.000 -2.689  2.948 -3.203  2.823  3.010  2.877  2.962  3.020 -3.180   \n",
       "\n",
       "      wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "6252 -0.022  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254 -0.359   \n",
       "4684 -0.163  0.000  0.000  0.000 -1.297  0.000  0.000  0.000 -0.254 -0.359   \n",
       "1731  3.636  0.000  0.000  0.000 -0.048  0.000  0.000  0.000 -0.254 -0.359   \n",
       "4742 -2.209  0.000  0.000  0.000 -3.000  0.000  0.000  0.000 -0.254 -0.359   \n",
       "4521 -2.372  0.000  0.000  0.000 -3.239  0.000  0.000  0.000 -0.254 -0.359   \n",
       "\n",
       "      wb_62  wb_63  wb_64  wb_65  wb_66  wb_67   wb_68  wb_69   wb_70  wb_71  \\\n",
       "6252 -0.200 -0.225  5.771  8.075 -0.042 -0.408  -0.257  3.265  10.767 -0.352   \n",
       "4684 -3.529 -3.492  0.139  0.187 -2.748 -0.408  -3.380 -2.553  -2.753 -0.352   \n",
       "1731 -0.159 -0.181  7.084  7.238  6.182 -0.408  -0.230  4.437   5.163 -0.352   \n",
       "4742 -9.917 -8.202  1.295  1.355 -6.621 -0.408 -10.648 -6.575 -10.339 -0.352   \n",
       "4521 -8.204 -9.489  4.021  4.071 -8.156 -0.408  -9.735 -7.027  -7.042 -0.352   \n",
       "\n",
       "      wb_72  wb_73  wb_74   wb_75  wb_76  wb_77  wb_78  wb_79  wb_80   wb_81  \\\n",
       "6252 -0.364 -0.042  3.541  -0.098  9.544  3.395  8.852  3.464  3.431  -0.310   \n",
       "4684 -0.364 -2.749  0.122  -3.027  0.393 -3.728  0.151  0.010  0.049  -3.572   \n",
       "1731 -0.364  7.019  7.272  -0.056  7.726  6.598  5.052  5.753  4.609  -0.288   \n",
       "4742 -0.364 -8.521  1.278 -10.373  1.640 -9.943  1.347 -6.420  1.121 -10.671   \n",
       "4521 -0.364 -7.510  4.014  -8.784  4.246  3.884  4.019  3.916  3.894  -9.771   \n",
       "\n",
       "      wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  \n",
       "6252 -0.044 -0.261 -0.383 -0.059 -0.334  0.144 -0.258 -0.315  5.911  \n",
       "4684 -2.768 -0.261 -0.383 -0.059 -3.699  0.144 -0.258 -0.315 -0.929  \n",
       "1731 -4.479 -0.261 -0.383 -0.059 -0.315  0.144 -0.258 -0.315 -4.023  \n",
       "4742 -6.831 -0.261 -0.383 -0.059 -9.398  0.144 -0.258 -0.315  2.658  \n",
       "4521 -7.342 -0.261 -0.383 -0.059 -9.907  0.144 -0.258 -0.315  5.726  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>0.513</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.701</td>\n",
       "      <td>2.472</td>\n",
       "      <td>-6.378</td>\n",
       "      <td>3.158</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.189</td>\n",
       "      <td>1.077</td>\n",
       "      <td>1.293</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.298</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>1.039</td>\n",
       "      <td>2.090</td>\n",
       "      <td>2.311</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>1.359</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.419</td>\n",
       "      <td>0.907</td>\n",
       "      <td>2.134</td>\n",
       "      <td>1.404</td>\n",
       "      <td>2.050</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.868</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.972</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.224</td>\n",
       "      <td>1.185</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.406</td>\n",
       "      <td>-2.450</td>\n",
       "      <td>2.485</td>\n",
       "      <td>2.596</td>\n",
       "      <td>-1.977</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.504</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-1.991</td>\n",
       "      <td>2.464</td>\n",
       "      <td>-2.159</td>\n",
       "      <td>2.624</td>\n",
       "      <td>0.508</td>\n",
       "      <td>2.515</td>\n",
       "      <td>1.995</td>\n",
       "      <td>2.221</td>\n",
       "      <td>-2.532</td>\n",
       "      <td>-1.862</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.612</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>57.302</td>\n",
       "      <td>57.204</td>\n",
       "      <td>57.898</td>\n",
       "      <td>57.427</td>\n",
       "      <td>57.970</td>\n",
       "      <td>57.724</td>\n",
       "      <td>53.038</td>\n",
       "      <td>77.525</td>\n",
       "      <td>372.447</td>\n",
       "      <td>1177.199</td>\n",
       "      <td>1583.079</td>\n",
       "      <td>678.519</td>\n",
       "      <td>57.302</td>\n",
       "      <td>57.209</td>\n",
       "      <td>57.918</td>\n",
       "      <td>57.541</td>\n",
       "      <td>58.138</td>\n",
       "      <td>57.721</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.982</td>\n",
       "      <td>1.942</td>\n",
       "      <td>2.158</td>\n",
       "      <td>2.132</td>\n",
       "      <td>1.976</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.218</td>\n",
       "      <td>2.455</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.988</td>\n",
       "      <td>2.131</td>\n",
       "      <td>2.008</td>\n",
       "      <td>2.032</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.544</td>\n",
       "      <td>2.119</td>\n",
       "      <td>1.903</td>\n",
       "      <td>1.918</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.576</td>\n",
       "      <td>1.570</td>\n",
       "      <td>1.711</td>\n",
       "      <td>1.715</td>\n",
       "      <td>1.534</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.571</td>\n",
       "      <td>1.546</td>\n",
       "      <td>2.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.533</td>\n",
       "      <td>1.690</td>\n",
       "      <td>1.557</td>\n",
       "      <td>1.611</td>\n",
       "      <td>1.931</td>\n",
       "      <td>1.722</td>\n",
       "      <td>2.052</td>\n",
       "      <td>1.622</td>\n",
       "      <td>1.543</td>\n",
       "      <td>1.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.569</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.485</td>\n",
       "      <td>2.566</td>\n",
       "      <td>2.682</td>\n",
       "      <td>2.743</td>\n",
       "      <td>2.557</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.617</td>\n",
       "      <td>2.869</td>\n",
       "      <td>4.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.594</td>\n",
       "      <td>2.655</td>\n",
       "      <td>2.578</td>\n",
       "      <td>2.745</td>\n",
       "      <td>3.801</td>\n",
       "      <td>2.723</td>\n",
       "      <td>4.636</td>\n",
       "      <td>2.546</td>\n",
       "      <td>2.643</td>\n",
       "      <td>2.128</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.700</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-99.998</td>\n",
       "      <td>-99.979</td>\n",
       "      <td>-99.984</td>\n",
       "      <td>-99.998</td>\n",
       "      <td>-99.978</td>\n",
       "      <td>-99.999</td>\n",
       "      <td>-117.886</td>\n",
       "      <td>-333.987</td>\n",
       "      <td>-1716.536</td>\n",
       "      <td>-4497.236</td>\n",
       "      <td>-6998.846</td>\n",
       "      <td>-2654.429</td>\n",
       "      <td>-100.004</td>\n",
       "      <td>-100.103</td>\n",
       "      <td>-102.569</td>\n",
       "      <td>-109.313</td>\n",
       "      <td>-112.078</td>\n",
       "      <td>-102.793</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-6.374</td>\n",
       "      <td>-5.841</td>\n",
       "      <td>-4.298</td>\n",
       "      <td>-5.613</td>\n",
       "      <td>-3.767</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-5.712</td>\n",
       "      <td>-4.127</td>\n",
       "      <td>-8.676</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-3.814</td>\n",
       "      <td>-4.458</td>\n",
       "      <td>-3.246</td>\n",
       "      <td>-6.276</td>\n",
       "      <td>-7.805</td>\n",
       "      <td>-3.692</td>\n",
       "      <td>-4.463</td>\n",
       "      <td>-5.246</td>\n",
       "      <td>-6.360</td>\n",
       "      <td>-5.768</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-5.282</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.607</td>\n",
       "      <td>-4.780</td>\n",
       "      <td>-4.814</td>\n",
       "      <td>-4.711</td>\n",
       "      <td>-4.962</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-4.877</td>\n",
       "      <td>-4.274</td>\n",
       "      <td>-8.347</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.347</td>\n",
       "      <td>-4.725</td>\n",
       "      <td>-6.573</td>\n",
       "      <td>-5.042</td>\n",
       "      <td>-6.762</td>\n",
       "      <td>-5.033</td>\n",
       "      <td>-9.311</td>\n",
       "      <td>-5.151</td>\n",
       "      <td>-4.751</td>\n",
       "      <td>-4.576</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-16.634</td>\n",
       "      <td>-13.860</td>\n",
       "      <td>-11.631</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-14.291</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-14.278</td>\n",
       "      <td>-11.083</td>\n",
       "      <td>-12.481</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-18.143</td>\n",
       "      <td>-11.503</td>\n",
       "      <td>-21.425</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-25.240</td>\n",
       "      <td>-10.927</td>\n",
       "      <td>-28.026</td>\n",
       "      <td>-16.903</td>\n",
       "      <td>-15.380</td>\n",
       "      <td>-13.857</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-15.604</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-39.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-48.870</td>\n",
       "      <td>-49.740</td>\n",
       "      <td>-50.010</td>\n",
       "      <td>-49.281</td>\n",
       "      <td>-50.204</td>\n",
       "      <td>-51.609</td>\n",
       "      <td>-42.872</td>\n",
       "      <td>-49.610</td>\n",
       "      <td>-49.189</td>\n",
       "      <td>-214.238</td>\n",
       "      <td>-154.201</td>\n",
       "      <td>-119.215</td>\n",
       "      <td>-48.857</td>\n",
       "      <td>-49.788</td>\n",
       "      <td>-50.199</td>\n",
       "      <td>-49.782</td>\n",
       "      <td>-49.752</td>\n",
       "      <td>-51.359</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.116</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-3.033</td>\n",
       "      <td>-3.063</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-2.815</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-3.106</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-2.637</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.824</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-2.927</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-2.284</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-3.153</td>\n",
       "      <td>-2.699</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-3.197</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-2.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>1.324</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>1.580</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.068</td>\n",
       "      <td>2.485</td>\n",
       "      <td>2.661</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2.572</td>\n",
       "      <td>0.165</td>\n",
       "      <td>2.047</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>1.559</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>1.389</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.150</td>\n",
       "      <td>-2.167</td>\n",
       "      <td>2.263</td>\n",
       "      <td>2.328</td>\n",
       "      <td>-1.965</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.200</td>\n",
       "      <td>-1.011</td>\n",
       "      <td>-1.779</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-1.980</td>\n",
       "      <td>2.251</td>\n",
       "      <td>-2.047</td>\n",
       "      <td>2.330</td>\n",
       "      <td>1.832</td>\n",
       "      <td>2.291</td>\n",
       "      <td>2.366</td>\n",
       "      <td>2.109</td>\n",
       "      <td>-2.227</td>\n",
       "      <td>-1.814</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.262</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>49.511</td>\n",
       "      <td>47.261</td>\n",
       "      <td>50.398</td>\n",
       "      <td>48.598</td>\n",
       "      <td>50.568</td>\n",
       "      <td>49.653</td>\n",
       "      <td>41.596</td>\n",
       "      <td>50.770</td>\n",
       "      <td>74.696</td>\n",
       "      <td>135.255</td>\n",
       "      <td>277.008</td>\n",
       "      <td>61.705</td>\n",
       "      <td>49.503</td>\n",
       "      <td>47.324</td>\n",
       "      <td>50.610</td>\n",
       "      <td>48.552</td>\n",
       "      <td>50.608</td>\n",
       "      <td>49.484</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>2.821</td>\n",
       "      <td>2.690</td>\n",
       "      <td>3.092</td>\n",
       "      <td>2.973</td>\n",
       "      <td>3.004</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>2.628</td>\n",
       "      <td>3.833</td>\n",
       "      <td>4.017</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>3.062</td>\n",
       "      <td>3.005</td>\n",
       "      <td>3.093</td>\n",
       "      <td>2.447</td>\n",
       "      <td>3.936</td>\n",
       "      <td>3.214</td>\n",
       "      <td>3.859</td>\n",
       "      <td>2.877</td>\n",
       "      <td>2.495</td>\n",
       "      <td>2.411</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>2.513</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.564</td>\n",
       "      <td>1.577</td>\n",
       "      <td>1.757</td>\n",
       "      <td>1.752</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.563</td>\n",
       "      <td>2.456</td>\n",
       "      <td>2.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.794</td>\n",
       "      <td>1.528</td>\n",
       "      <td>1.665</td>\n",
       "      <td>2.365</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1.552</td>\n",
       "      <td>1.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>3.212</td>\n",
       "      <td>3.279</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>2.428</td>\n",
       "      <td>2.505</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>3.186</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>3.394</td>\n",
       "      <td>2.818</td>\n",
       "      <td>3.259</td>\n",
       "      <td>3.343</td>\n",
       "      <td>3.040</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>2.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>99.984</td>\n",
       "      <td>99.957</td>\n",
       "      <td>99.996</td>\n",
       "      <td>99.977</td>\n",
       "      <td>99.963</td>\n",
       "      <td>99.998</td>\n",
       "      <td>120.706</td>\n",
       "      <td>298.189</td>\n",
       "      <td>1490.708</td>\n",
       "      <td>5413.530</td>\n",
       "      <td>6072.430</td>\n",
       "      <td>3122.619</td>\n",
       "      <td>100.010</td>\n",
       "      <td>100.224</td>\n",
       "      <td>102.911</td>\n",
       "      <td>107.173</td>\n",
       "      <td>110.965</td>\n",
       "      <td>101.610</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>8.510</td>\n",
       "      <td>8.430</td>\n",
       "      <td>7.956</td>\n",
       "      <td>7.956</td>\n",
       "      <td>6.992</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>8.101</td>\n",
       "      <td>7.540</td>\n",
       "      <td>13.975</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>7.395</td>\n",
       "      <td>7.791</td>\n",
       "      <td>9.918</td>\n",
       "      <td>8.262</td>\n",
       "      <td>10.182</td>\n",
       "      <td>8.230</td>\n",
       "      <td>15.647</td>\n",
       "      <td>7.234</td>\n",
       "      <td>6.880</td>\n",
       "      <td>6.892</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>7.607</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.625</td>\n",
       "      <td>4.149</td>\n",
       "      <td>3.584</td>\n",
       "      <td>5.313</td>\n",
       "      <td>3.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.037</td>\n",
       "      <td>3.814</td>\n",
       "      <td>4.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.166</td>\n",
       "      <td>3.812</td>\n",
       "      <td>3.110</td>\n",
       "      <td>5.045</td>\n",
       "      <td>3.676</td>\n",
       "      <td>3.573</td>\n",
       "      <td>3.664</td>\n",
       "      <td>3.690</td>\n",
       "      <td>3.460</td>\n",
       "      <td>5.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.853</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>9.653</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>14.668</td>\n",
       "      <td>15.480</td>\n",
       "      <td>18.307</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>10.913</td>\n",
       "      <td>25.866</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>24.799</td>\n",
       "      <td>15.149</td>\n",
       "      <td>13.424</td>\n",
       "      <td>15.418</td>\n",
       "      <td>14.494</td>\n",
       "      <td>15.690</td>\n",
       "      <td>22.505</td>\n",
       "      <td>19.422</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>10.291</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>39.198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  0-target  1-target  2-target  3-target  4-target  \\\n",
       "count      10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean  1373158606.000     0.266    -0.496     0.306    -0.455     0.513   \n",
       "std            0.000    57.302    57.204    57.898    57.427    57.970   \n",
       "min   1373158606.000   -99.998   -99.979   -99.984   -99.998   -99.978   \n",
       "25%   1373158606.000   -48.870   -49.740   -50.010   -49.281   -50.204   \n",
       "50%   1373158606.000     0.508     0.426     0.118    -0.586     1.324   \n",
       "75%   1373158606.000    49.511    47.261    50.398    48.598    50.568   \n",
       "max   1373158606.000    99.984    99.957    99.996    99.977    99.963   \n",
       "\n",
       "       5-target  0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  \\\n",
       "count 10000.000       10000.000       10000.000       10000.000   \n",
       "mean     -0.389          -0.157           0.701           2.472   \n",
       "std      57.724          53.038          77.525         372.447   \n",
       "min     -99.999        -117.886        -333.987       -1716.536   \n",
       "25%     -51.609         -42.872         -49.610         -49.189   \n",
       "50%       0.343           0.289           0.441           0.000   \n",
       "75%      49.653          41.596          50.770          74.696   \n",
       "max      99.998         120.706         298.189        1490.708   \n",
       "\n",
       "       3-lstsq_lambda  4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean           -6.378           3.158          -0.782           0.266   \n",
       "std          1177.199        1583.079         678.519          57.302   \n",
       "min         -4497.236       -6998.846       -2654.429        -100.004   \n",
       "25%          -214.238        -154.201        -119.215         -48.857   \n",
       "50%            -0.000           0.000          -0.000           0.510   \n",
       "75%           135.255         277.008          61.705          49.503   \n",
       "max          5413.530        6072.430        3122.619         100.010   \n",
       "\n",
       "       1-lstsq_target  2-lstsq_target  3-lstsq_target  4-lstsq_target  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean           -0.495           0.302          -0.450           0.515   \n",
       "std            57.209          57.918          57.541          58.138   \n",
       "min          -100.103        -102.569        -109.313        -112.078   \n",
       "25%           -49.788         -50.199         -49.782         -49.752   \n",
       "50%             0.446           0.311          -0.597           1.580   \n",
       "75%            47.324          50.610          48.552          50.608   \n",
       "max           100.224         102.911         107.173         110.965   \n",
       "\n",
       "       5-lstsq_target      wb_0      wb_1      wb_2      wb_3      wb_4  \\\n",
       "count       10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean           -0.392    -0.009    -0.227     1.189     1.077     1.293   \n",
       "std            57.721     0.000     0.000     1.982     1.942     2.158   \n",
       "min          -102.793    -0.009    -0.227    -6.374    -5.841    -4.298   \n",
       "25%           -51.359    -0.009    -0.227     0.131     0.082     0.109   \n",
       "50%             0.038    -0.009    -0.227     0.138     0.084     0.118   \n",
       "75%            49.484    -0.009    -0.227     2.821     2.690     3.092   \n",
       "max           101.610    -0.009    -0.227     8.510     8.430     7.956   \n",
       "\n",
       "           wb_5      wb_6      wb_7      wb_8      wb_9     wb_10     wb_11  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      1.204     1.298    -0.297     1.039     2.090     2.311    -0.088   \n",
       "std       2.132     1.976     0.000     1.928     2.218     2.455     0.000   \n",
       "min      -5.613    -3.767    -0.297    -5.712    -4.127    -8.676    -0.088   \n",
       "25%       0.080     0.109    -0.297     0.067     0.244     0.537    -0.088   \n",
       "50%       0.084     0.125    -0.297     0.068     2.485     2.661    -0.088   \n",
       "75%       2.973     3.004    -0.297     2.628     3.833     4.017    -0.088   \n",
       "max       7.956     6.992    -0.297     8.101     7.540    13.975    -0.088   \n",
       "\n",
       "          wb_12     wb_13     wb_14     wb_15     wb_16     wb_17     wb_18  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.019     1.359     1.208     1.419     0.907     2.134     1.404   \n",
       "std       0.000     1.988     2.131     2.008     2.032     2.334     2.179   \n",
       "min      -0.019    -3.814    -4.458    -3.246    -6.276    -7.805    -3.692   \n",
       "25%      -0.019     0.130     0.078     0.179     0.008     0.334     0.148   \n",
       "50%      -0.019     0.153     0.083     0.211     0.008     2.572     0.165   \n",
       "75%      -0.019     3.062     3.005     3.093     2.447     3.936     3.214   \n",
       "max      -0.019     7.395     7.791     9.918     8.262    10.182     8.230   \n",
       "\n",
       "          wb_19     wb_20     wb_21     wb_22     wb_23     wb_24     wb_25  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      2.050     1.096     0.952     0.868    -0.042    -0.404    -0.090   \n",
       "std       2.544     2.119     1.903     1.918     0.000     0.000     0.000   \n",
       "min      -4.463    -5.246    -6.360    -5.768    -0.042    -0.404    -0.090   \n",
       "25%       0.137     0.033     0.041     0.008    -0.042    -0.404    -0.090   \n",
       "50%       2.047     0.034     0.041     0.008    -0.042    -0.404    -0.090   \n",
       "75%       3.859     2.877     2.495     2.411    -0.042    -0.404    -0.090   \n",
       "max      15.647     7.234     6.880     6.892    -0.042    -0.404    -0.090   \n",
       "\n",
       "          wb_26     wb_27     wb_28     wb_29     wb_30     wb_31     wb_32  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.972    -0.291    -0.209    -0.233     0.000     0.000     0.196   \n",
       "std       1.905     0.000     0.000     0.000     0.000     0.000     1.576   \n",
       "min      -5.282    -0.291    -0.209    -0.233     0.000     0.000    -5.607   \n",
       "25%       0.047    -0.291    -0.209    -0.233     0.000     0.000    -0.142   \n",
       "50%       0.048    -0.291    -0.209    -0.233     0.000     0.000    -0.131   \n",
       "75%       2.513    -0.291    -0.209    -0.233     0.000     0.000     1.564   \n",
       "max       7.607    -0.291    -0.209    -0.233     0.000     0.000     5.625   \n",
       "\n",
       "          wb_33     wb_34     wb_35     wb_36     wb_37     wb_38     wb_39  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.227     0.275     0.287     0.262     0.000     0.224     1.185   \n",
       "std       1.570     1.711     1.715     1.534     0.000     1.571     1.546   \n",
       "min      -4.780    -4.814    -4.711    -4.962     0.000    -4.877    -4.274   \n",
       "25%      -0.088    -0.119    -0.087    -0.124     0.000    -0.073     0.082   \n",
       "50%      -0.083    -0.109    -0.081    -0.109     0.000    -0.069     1.559   \n",
       "75%       1.577     1.757     1.752     1.599     0.000     1.563     2.456   \n",
       "max       4.149     3.584     5.313     3.188     0.000     4.037     3.814   \n",
       "\n",
       "          wb_40     wb_41     wb_42     wb_43     wb_44     wb_45     wb_46  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.572     0.000     0.000     0.240     0.337     0.157     0.342   \n",
       "std       2.067     0.000     0.000     1.533     1.690     1.557     1.611   \n",
       "min      -8.347     0.000     0.000    -5.347    -4.725    -6.573    -5.042   \n",
       "25%      -0.788     0.000     0.000    -0.151    -0.084    -0.209    -0.023   \n",
       "50%       1.196     0.000     0.000    -0.130    -0.079    -0.179    -0.023   \n",
       "75%       2.237     0.000     0.000     1.578     1.794     1.528     1.665   \n",
       "max       4.436     0.000     0.000     3.166     3.812     3.110     5.045   \n",
       "\n",
       "          wb_47     wb_48     wb_49     wb_50     wb_51     wb_52     wb_53  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.805     0.222     0.044     0.480     0.247     0.504     0.000   \n",
       "std       1.931     1.722     2.052     1.622     1.543     1.406     0.000   \n",
       "min      -6.762    -5.033    -9.311    -5.151    -4.751    -4.576     0.000   \n",
       "25%      -0.386    -0.165    -1.116    -0.040    -0.049    -0.023     0.000   \n",
       "50%       1.389    -0.148    -0.136    -0.039    -0.047    -0.022     0.000   \n",
       "75%       2.365     1.723     1.810     1.882     1.552     1.729     0.000   \n",
       "max       3.676     3.573     3.664     3.690     3.460     5.187     0.000   \n",
       "\n",
       "          wb_54     wb_55     wb_56     wb_57     wb_58     wb_59     wb_60  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.000     0.000     0.211     0.000     0.000     0.000    -0.254   \n",
       "std       0.000     0.000     1.569     0.000     0.000     0.000     0.000   \n",
       "min       0.000     0.000    -5.372     0.000     0.000     0.000    -0.254   \n",
       "25%       0.000     0.000    -0.054     0.000     0.000     0.000    -0.254   \n",
       "50%       0.000     0.000    -0.052     0.000     0.000     0.000    -0.254   \n",
       "75%       0.000     0.000     1.531     0.000     0.000     0.000    -0.254   \n",
       "max       0.000     0.000     3.853     0.000     0.000     0.000    -0.254   \n",
       "\n",
       "          wb_61     wb_62     wb_63     wb_64     wb_65     wb_66     wb_67  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.359    -2.406    -2.450     2.485     2.596    -1.977    -0.408   \n",
       "std       0.000     2.485     2.566     2.682     2.743     2.557     0.000   \n",
       "min      -0.359   -16.634   -13.860   -11.631     0.099   -14.291    -0.408   \n",
       "25%      -0.359    -3.033    -3.063     0.139     0.187    -2.815    -0.408   \n",
       "50%      -0.359    -2.150    -2.167     2.263     2.328    -1.965    -0.408   \n",
       "75%      -0.359    -0.200    -0.226     3.212     3.279    -0.042    -0.408   \n",
       "max      -0.359     9.653    -0.138    14.668    15.480    18.307    -0.408   \n",
       "\n",
       "          wb_68     wb_69     wb_70     wb_71     wb_72     wb_73     wb_74  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -2.504    -0.043    -0.174    -0.352    -0.364    -1.991     2.464   \n",
       "std       2.617     2.869     4.308     0.000     0.000     2.594     2.655   \n",
       "min     -14.278   -11.083   -12.481    -0.352    -0.364   -18.143   -11.503   \n",
       "25%      -3.106    -2.300    -2.637    -0.352    -0.364    -2.824     0.122   \n",
       "50%      -2.200    -1.011    -1.779    -0.352    -0.364    -1.980     2.251   \n",
       "75%      -0.258     2.428     2.505    -0.352    -0.364    -0.043     3.186   \n",
       "max      -0.181    10.913    25.866    -0.352    -0.364    24.799    15.149   \n",
       "\n",
       "          wb_75     wb_76     wb_77     wb_78     wb_79     wb_80     wb_81  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -2.159     2.624     0.508     2.515     1.995     2.221    -2.532   \n",
       "std       2.578     2.745     3.801     2.723     4.636     2.546     2.643   \n",
       "min     -21.425     0.376   -25.240   -10.927   -28.026   -16.903   -15.380   \n",
       "25%      -2.927     0.393    -2.284     0.150     0.023     0.049    -3.153   \n",
       "50%      -2.047     2.330     1.832     2.291     2.366     2.109    -2.227   \n",
       "75%      -0.099     3.394     2.818     3.259     3.343     3.040    -0.310   \n",
       "max      13.424    15.418    14.494    15.690    22.505    19.422    -0.048   \n",
       "\n",
       "          wb_82     wb_83     wb_84     wb_85     wb_86     wb_87     wb_88  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -1.862    -0.261    -0.383    -0.059    -2.612     0.144    -0.258   \n",
       "std       2.128     0.000     0.000     0.000     2.700     0.000     0.000   \n",
       "min     -13.857    -0.261    -0.383    -0.059   -15.604     0.144    -0.258   \n",
       "25%      -2.699    -0.261    -0.383    -0.059    -3.197     0.144    -0.258   \n",
       "50%      -1.814    -0.261    -0.383    -0.059    -2.262     0.144    -0.258   \n",
       "75%      -0.045    -0.261    -0.383    -0.059    -0.335     0.144    -0.258   \n",
       "max      10.291    -0.261    -0.383    -0.059    -0.282     0.144    -0.258   \n",
       "\n",
       "          wb_89     wb_90  \n",
       "count 10000.000 10000.000  \n",
       "mean     -0.315     0.063  \n",
       "std       0.000     5.492  \n",
       "min      -0.315   -39.572  \n",
       "25%      -0.315    -2.381  \n",
       "50%      -0.315     0.345  \n",
       "75%      -0.315     2.574  \n",
       "max      -0.315    39.198  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34115018],\n",
       "       [0.71824612],\n",
       "       [0.90024718],\n",
       "       [0.6772428 ],\n",
       "       [0.05859863],\n",
       "       [0.13896124],\n",
       "       [0.61895861],\n",
       "       [0.21873753],\n",
       "       [0.26212325],\n",
       "       [0.10843148]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 47.00390202],\n",
       "       [124.69107317],\n",
       "       [203.88128157],\n",
       "       [111.68638698],\n",
       "       [ 23.20323121],\n",
       "       [ 28.77382938],\n",
       "       [ 95.58188424],\n",
       "       [ 35.14243119],\n",
       "       [ 38.71071122],\n",
       "       [ 26.63158243]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "for lambda_net_dataset in lambda_net_dataset_list:\n",
    "    \n",
    "    \n",
    "    if inet_holdout_seed_evaluation:\n",
    "        complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "        random.seed(RANDOM_SEED)\n",
    "        \n",
    "        if isinstance(test_size, float):\n",
    "            test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "        \n",
    "        test_seeds = random.sample(complete_seed_list, test_size)\n",
    "        lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "        \n",
    "        random.seed(RANDOM_SEED)\n",
    "        valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "        lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "        train_seeds = complete_seed_list\n",
    "        lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "        \n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "        \n",
    "        del lambda_net_dataset\n",
    "    else:\n",
    "        lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "        lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "        \n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "    \n",
    "        del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "        \n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8910, 110)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 110)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 110)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>94.239</td>\n",
       "      <td>-75.912</td>\n",
       "      <td>-41.685</td>\n",
       "      <td>37.081</td>\n",
       "      <td>71.046</td>\n",
       "      <td>50.261</td>\n",
       "      <td>76.364</td>\n",
       "      <td>-14.398</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>94.228</td>\n",
       "      <td>-75.639</td>\n",
       "      <td>-43.488</td>\n",
       "      <td>41.846</td>\n",
       "      <td>65.653</td>\n",
       "      <td>52.449</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>2.782</td>\n",
       "      <td>2.768</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>3.032</td>\n",
       "      <td>2.878</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>2.806</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>2.675</td>\n",
       "      <td>2.934</td>\n",
       "      <td>2.773</td>\n",
       "      <td>2.889</td>\n",
       "      <td>2.915</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>2.564</td>\n",
       "      <td>2.667</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>2.278</td>\n",
       "      <td>2.451</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>2.577</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>3.020</td>\n",
       "      <td>2.423</td>\n",
       "      <td>2.544</td>\n",
       "      <td>2.452</td>\n",
       "      <td>2.570</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>3.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-99.843</td>\n",
       "      <td>-20.508</td>\n",
       "      <td>33.484</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-97.480</td>\n",
       "      <td>-88.433</td>\n",
       "      <td>-81.883</td>\n",
       "      <td>-87.508</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-99.860</td>\n",
       "      <td>-20.229</td>\n",
       "      <td>31.543</td>\n",
       "      <td>5.378</td>\n",
       "      <td>-102.974</td>\n",
       "      <td>-86.339</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>2.648</td>\n",
       "      <td>2.547</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.081</td>\n",
       "      <td>2.720</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>2.499</td>\n",
       "      <td>2.914</td>\n",
       "      <td>2.865</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>2.752</td>\n",
       "      <td>0.079</td>\n",
       "      <td>2.787</td>\n",
       "      <td>0.009</td>\n",
       "      <td>2.758</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.033</td>\n",
       "      <td>2.426</td>\n",
       "      <td>2.572</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>2.425</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.367</td>\n",
       "      <td>2.371</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>2.481</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.614</td>\n",
       "      <td>2.458</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.467</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>2.397</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>2.486</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.542</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.818</td>\n",
       "      <td>-2.854</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-2.671</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.889</td>\n",
       "      <td>-2.528</td>\n",
       "      <td>-2.643</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.675</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-2.720</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-2.578</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-2.953</td>\n",
       "      <td>-2.693</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.975</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-1.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-20.835</td>\n",
       "      <td>-25.978</td>\n",
       "      <td>-59.778</td>\n",
       "      <td>-14.422</td>\n",
       "      <td>-39.218</td>\n",
       "      <td>15.534</td>\n",
       "      <td>-4.968</td>\n",
       "      <td>-110.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-20.841</td>\n",
       "      <td>-25.736</td>\n",
       "      <td>-61.096</td>\n",
       "      <td>-11.525</td>\n",
       "      <td>-42.065</td>\n",
       "      <td>16.566</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>3.450</td>\n",
       "      <td>3.359</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>3.515</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>3.305</td>\n",
       "      <td>3.744</td>\n",
       "      <td>3.673</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>3.587</td>\n",
       "      <td>0.079</td>\n",
       "      <td>3.605</td>\n",
       "      <td>0.008</td>\n",
       "      <td>3.647</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.033</td>\n",
       "      <td>3.215</td>\n",
       "      <td>3.384</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>3.230</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.740</td>\n",
       "      <td>-2.761</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-2.536</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.789</td>\n",
       "      <td>-2.395</td>\n",
       "      <td>-2.564</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.596</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-2.668</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-2.578</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-2.828</td>\n",
       "      <td>-2.509</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.873</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-78.752</td>\n",
       "      <td>17.057</td>\n",
       "      <td>24.938</td>\n",
       "      <td>32.825</td>\n",
       "      <td>-89.422</td>\n",
       "      <td>69.631</td>\n",
       "      <td>-82.078</td>\n",
       "      <td>38.935</td>\n",
       "      <td>33.461</td>\n",
       "      <td>-148.684</td>\n",
       "      <td>237.656</td>\n",
       "      <td>-113.366</td>\n",
       "      <td>-78.759</td>\n",
       "      <td>17.264</td>\n",
       "      <td>23.430</td>\n",
       "      <td>36.586</td>\n",
       "      <td>-93.227</td>\n",
       "      <td>70.967</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-1.229</td>\n",
       "      <td>-1.336</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-1.249</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>-1.117</td>\n",
       "      <td>-1.070</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-1.185</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-1.121</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-1.395</td>\n",
       "      <td>0.148</td>\n",
       "      <td>3.745</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>-1.431</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.408</td>\n",
       "      <td>2.392</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>2.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.369</td>\n",
       "      <td>2.677</td>\n",
       "      <td>2.532</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.532</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>2.457</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>2.575</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-2.187</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>2.340</td>\n",
       "      <td>2.547</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.318</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.721</td>\n",
       "      <td>-2.826</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-2.577</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.879</td>\n",
       "      <td>-2.322</td>\n",
       "      <td>-2.434</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.547</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-2.535</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-2.592</td>\n",
       "      <td>0.150</td>\n",
       "      <td>4.656</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-2.998</td>\n",
       "      <td>-2.686</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-3.004</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-2.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-69.515</td>\n",
       "      <td>-52.541</td>\n",
       "      <td>30.787</td>\n",
       "      <td>13.316</td>\n",
       "      <td>-14.228</td>\n",
       "      <td>-5.614</td>\n",
       "      <td>-71.988</td>\n",
       "      <td>-25.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-69.507</td>\n",
       "      <td>-52.872</td>\n",
       "      <td>33.241</td>\n",
       "      <td>6.810</td>\n",
       "      <td>-7.070</td>\n",
       "      <td>-8.405</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.095</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.910</td>\n",
       "      <td>1.267</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.192</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.091</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-1.132</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.938</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.298</td>\n",
       "      <td>2.310</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>2.409</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.295</td>\n",
       "      <td>2.559</td>\n",
       "      <td>2.398</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.412</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>2.346</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>2.448</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>2.390</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.483</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.268</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.292</td>\n",
       "      <td>-2.355</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-2.152</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.398</td>\n",
       "      <td>-2.017</td>\n",
       "      <td>-2.109</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.164</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-2.195</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-2.081</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-2.565</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-2.466</td>\n",
       "      <td>-2.223</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.497</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-2.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "5548  1373158606    94.239   -75.912   -41.685    37.081    71.046    50.261   \n",
       "1854  1373158606   -99.843   -20.508    33.484     0.301   -97.480   -88.433   \n",
       "739   1373158606   -20.835   -25.978   -59.778   -14.422   -39.218    15.534   \n",
       "3588  1373158606   -78.752    17.057    24.938    32.825   -89.422    69.631   \n",
       "4649  1373158606   -69.515   -52.541    30.787    13.316   -14.228    -5.614   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "5548          76.364         -14.398          -0.000           0.000   \n",
       "1854         -81.883         -87.508          -0.000           0.000   \n",
       "739           -4.968        -110.441           0.000          -0.000   \n",
       "3588         -82.078          38.935          33.461        -148.684   \n",
       "4649         -71.988         -25.095           0.000          -0.000   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "5548          -0.000           0.000          94.228         -75.639   \n",
       "1854          -0.001           0.000         -99.860         -20.229   \n",
       "739           -0.000           0.000         -20.841         -25.736   \n",
       "3588         237.656        -113.366         -78.759          17.264   \n",
       "4649           0.000          -0.000         -69.507         -52.872   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "5548         -43.488          41.846          65.653          52.449 -0.009   \n",
       "1854          31.543           5.378        -102.974         -86.339 -0.009   \n",
       "739          -61.096         -11.525         -42.065          16.566 -0.009   \n",
       "3588          23.430          36.586         -93.227          70.967 -0.009   \n",
       "4649          33.241           6.810          -7.070          -8.405 -0.009   \n",
       "\n",
       "       wb_1   wb_2   wb_3   wb_4   wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  \\\n",
       "5548 -0.227  0.130  0.082 -0.579 -0.616  0.109 -0.297  0.067 -0.344 -0.639   \n",
       "1854 -0.227  2.648  2.547  0.109  0.081  2.720 -0.297  2.499  2.914  2.865   \n",
       "739  -0.227  3.450  3.359  0.109  0.080  3.515 -0.297  3.305  3.744  3.673   \n",
       "3588 -0.227 -1.229 -1.336  0.109  0.081 -1.249 -0.297 -1.352 -1.117 -1.070   \n",
       "4649 -0.227  1.046  0.952  0.109  0.080  1.095 -0.297  0.910  1.267  1.249   \n",
       "\n",
       "      wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  \\\n",
       "5548 -0.088 -0.019  0.130 -0.595  0.179 -0.832 -0.409 -0.462 -0.433 -0.642   \n",
       "1854 -0.088 -0.019  2.752  0.079  2.787  0.009  2.758  0.148  0.136  0.033   \n",
       "739  -0.088 -0.019  3.587  0.079  3.605  0.008  3.647  0.148  0.136  0.033   \n",
       "3588 -0.088 -0.019 -1.185  0.079 -1.121  0.009 -1.395  0.148  3.745  0.034   \n",
       "4649 -0.088 -0.019  1.132  0.078  1.192  0.008  1.091  0.148 -1.132  0.033   \n",
       "\n",
       "      wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  \\\n",
       "5548  0.041  0.008 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000   \n",
       "1854  2.426  2.572 -0.042 -0.404 -0.090  2.425 -0.291 -0.209 -0.233  0.000   \n",
       "739   3.215  3.384 -0.042 -0.404 -0.090  3.230 -0.291 -0.209 -0.233  0.000   \n",
       "3588 -1.420 -1.431 -0.042 -0.404 -0.090 -1.407 -0.291 -0.209 -0.233  0.000   \n",
       "4649  0.847  0.938 -0.042 -0.404 -0.090  0.850 -0.291 -0.209 -0.233  0.000   \n",
       "\n",
       "      wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  \\\n",
       "5548  0.000 -0.130 -0.083  2.782  2.768 -0.109  0.000 -0.069  3.032  2.878   \n",
       "1854  0.000  2.367  2.371 -0.109 -0.081  2.481  0.000  2.355  2.614  2.458   \n",
       "739   0.000  0.084  0.125 -0.109 -0.081  0.147  0.000  0.124  0.217  0.096   \n",
       "3588  0.000  2.408  2.392 -0.109 -0.081  2.537  0.000  2.369  2.677  2.532   \n",
       "4649  0.000  2.298  2.310 -0.109 -0.081  2.409  0.000  2.295  2.559  2.398   \n",
       "\n",
       "      wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  \\\n",
       "5548  0.000  0.000 -0.130  2.806 -0.179  2.675  2.934  2.773  2.889  2.915   \n",
       "1854  0.000  0.000  2.467 -0.079  2.397 -0.023  2.486 -0.148 -0.136 -0.040   \n",
       "739   0.000  0.000  0.146 -0.079  0.095 -0.023  0.042 -0.148 -0.136 -0.040   \n",
       "3588  0.000  0.000  2.532 -0.079  2.457 -0.023  2.575 -0.148 -2.187 -0.039   \n",
       "4649  0.000  0.000  2.412 -0.079  2.346 -0.023  2.448 -0.148  2.390 -0.039   \n",
       "\n",
       "      wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  \\\n",
       "5548 -0.047 -0.022  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254   \n",
       "1854  2.337  2.542  0.000  0.000  0.000  2.323  0.000  0.000  0.000 -0.254   \n",
       "739   0.130  0.235  0.000  0.000  0.000  0.129  0.000  0.000  0.000 -0.254   \n",
       "3588  2.340  2.547  0.000  0.000  0.000  2.318  0.000  0.000  0.000 -0.254   \n",
       "4649  2.270  2.483  0.000  0.000  0.000  2.268  0.000  0.000  0.000 -0.254   \n",
       "\n",
       "      wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  \\\n",
       "5548 -0.359 -0.200 -0.225  2.564  2.667 -0.041 -0.408 -0.257  2.278  2.451   \n",
       "1854 -0.359 -2.818 -2.854  0.138  0.187 -2.671 -0.408 -2.889 -2.528 -2.643   \n",
       "739  -0.359 -2.740 -2.761  0.138  0.187 -2.536 -0.408 -2.789 -2.395 -2.564   \n",
       "3588 -0.359 -2.721 -2.826  0.139  0.188 -2.577 -0.408 -2.879 -2.322 -2.434   \n",
       "4649 -0.359 -2.292 -2.355  0.138  0.187 -2.152 -0.408 -2.398 -2.017 -2.109   \n",
       "\n",
       "      wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  \\\n",
       "5548 -0.352 -0.364 -0.042  2.577 -0.099  3.020  2.423  2.544  2.452  2.570   \n",
       "1854 -0.352 -0.364 -2.675  0.121 -2.720  0.393 -2.578  0.150  0.004  0.049   \n",
       "739  -0.352 -0.364 -2.596  0.121 -2.668  0.393 -2.578  0.150  0.005  0.049   \n",
       "3588 -0.352 -0.364 -2.547  0.122 -2.535  0.394 -2.592  0.150  4.656  0.049   \n",
       "4649 -0.352 -0.364 -2.164  0.121 -2.195  0.393 -2.081  0.149 -2.565  0.048   \n",
       "\n",
       "      wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  \n",
       "5548 -0.310 -0.044 -0.261 -0.383 -0.059 -0.334  0.144 -0.258 -0.315  3.834  \n",
       "1854 -2.953 -2.693 -0.261 -0.383 -0.059 -2.975  0.144 -0.258 -0.315 -1.726  \n",
       "739  -2.828 -2.509 -0.261 -0.383 -0.059 -2.873  0.144 -0.258 -0.315 -0.837  \n",
       "3588 -2.998 -2.686 -0.261 -0.383 -0.059 -3.004  0.144 -0.258 -0.315 -2.883  \n",
       "4649 -2.466 -2.223 -0.261 -0.383 -0.059 -2.497  0.144 -0.258 -0.315 -2.005  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>87.132</td>\n",
       "      <td>39.582</td>\n",
       "      <td>-30.321</td>\n",
       "      <td>54.070</td>\n",
       "      <td>5.890</td>\n",
       "      <td>67.110</td>\n",
       "      <td>73.923</td>\n",
       "      <td>89.386</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>87.153</td>\n",
       "      <td>39.290</td>\n",
       "      <td>-28.670</td>\n",
       "      <td>50.287</td>\n",
       "      <td>9.550</td>\n",
       "      <td>65.865</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.056</td>\n",
       "      <td>2.993</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>3.356</td>\n",
       "      <td>3.164</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.131</td>\n",
       "      <td>3.030</td>\n",
       "      <td>0.179</td>\n",
       "      <td>2.736</td>\n",
       "      <td>3.214</td>\n",
       "      <td>3.119</td>\n",
       "      <td>3.198</td>\n",
       "      <td>3.023</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>2.449</td>\n",
       "      <td>2.442</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>2.664</td>\n",
       "      <td>2.376</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>2.479</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.553</td>\n",
       "      <td>2.418</td>\n",
       "      <td>2.517</td>\n",
       "      <td>2.569</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>2.940</td>\n",
       "      <td>3.011</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>2.712</td>\n",
       "      <td>2.703</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>2.935</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>3.232</td>\n",
       "      <td>2.806</td>\n",
       "      <td>2.949</td>\n",
       "      <td>2.846</td>\n",
       "      <td>2.870</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>2.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-45.636</td>\n",
       "      <td>-4.965</td>\n",
       "      <td>-7.004</td>\n",
       "      <td>14.531</td>\n",
       "      <td>-96.484</td>\n",
       "      <td>13.345</td>\n",
       "      <td>-34.877</td>\n",
       "      <td>-52.423</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-45.632</td>\n",
       "      <td>-4.971</td>\n",
       "      <td>-7.063</td>\n",
       "      <td>14.943</td>\n",
       "      <td>-97.284</td>\n",
       "      <td>13.804</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>2.234</td>\n",
       "      <td>2.126</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>2.274</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>2.063</td>\n",
       "      <td>2.472</td>\n",
       "      <td>2.430</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>2.328</td>\n",
       "      <td>0.079</td>\n",
       "      <td>2.373</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2.344</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.033</td>\n",
       "      <td>1.990</td>\n",
       "      <td>2.150</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>2.001</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.376</td>\n",
       "      <td>1.381</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>1.461</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.356</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.465</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>1.408</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>1.457</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>1.339</td>\n",
       "      <td>1.535</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.335</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.043</td>\n",
       "      <td>-2.063</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-1.880</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.079</td>\n",
       "      <td>-1.755</td>\n",
       "      <td>-1.879</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-1.965</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-1.809</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-2.132</td>\n",
       "      <td>-1.902</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.164</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-1.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>5.364</td>\n",
       "      <td>10.410</td>\n",
       "      <td>12.929</td>\n",
       "      <td>-61.212</td>\n",
       "      <td>62.185</td>\n",
       "      <td>38.451</td>\n",
       "      <td>7.025</td>\n",
       "      <td>-30.714</td>\n",
       "      <td>300.499</td>\n",
       "      <td>-871.241</td>\n",
       "      <td>1042.793</td>\n",
       "      <td>-388.216</td>\n",
       "      <td>5.377</td>\n",
       "      <td>10.168</td>\n",
       "      <td>14.153</td>\n",
       "      <td>-63.949</td>\n",
       "      <td>64.893</td>\n",
       "      <td>37.489</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.622</td>\n",
       "      <td>1.561</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.934</td>\n",
       "      <td>7.242</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.131</td>\n",
       "      <td>1.616</td>\n",
       "      <td>0.180</td>\n",
       "      <td>2.036</td>\n",
       "      <td>2.354</td>\n",
       "      <td>2.527</td>\n",
       "      <td>2.066</td>\n",
       "      <td>1.662</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-5.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-1.101</td>\n",
       "      <td>-1.327</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.285</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>1.036</td>\n",
       "      <td>15.645</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>1.233</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>3.420</td>\n",
       "      <td>2.706</td>\n",
       "      <td>3.681</td>\n",
       "      <td>2.013</td>\n",
       "      <td>1.197</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>6.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>53.997</td>\n",
       "      <td>-47.549</td>\n",
       "      <td>40.123</td>\n",
       "      <td>83.030</td>\n",
       "      <td>18.048</td>\n",
       "      <td>72.248</td>\n",
       "      <td>19.520</td>\n",
       "      <td>106.478</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>54.009</td>\n",
       "      <td>-47.844</td>\n",
       "      <td>41.745</td>\n",
       "      <td>79.237</td>\n",
       "      <td>22.045</td>\n",
       "      <td>70.695</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.083</td>\n",
       "      <td>3.898</td>\n",
       "      <td>3.824</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>4.171</td>\n",
       "      <td>4.125</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.131</td>\n",
       "      <td>3.884</td>\n",
       "      <td>0.180</td>\n",
       "      <td>3.549</td>\n",
       "      <td>4.045</td>\n",
       "      <td>3.933</td>\n",
       "      <td>4.032</td>\n",
       "      <td>3.894</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.624</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.697</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.785</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.800</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>2.466</td>\n",
       "      <td>2.692</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>2.746</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>2.997</td>\n",
       "      <td>2.594</td>\n",
       "      <td>2.762</td>\n",
       "      <td>2.647</td>\n",
       "      <td>2.652</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>1.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>83.626</td>\n",
       "      <td>20.678</td>\n",
       "      <td>-98.940</td>\n",
       "      <td>4.724</td>\n",
       "      <td>-90.525</td>\n",
       "      <td>45.653</td>\n",
       "      <td>89.089</td>\n",
       "      <td>-91.126</td>\n",
       "      <td>320.877</td>\n",
       "      <td>-527.133</td>\n",
       "      <td>14.663</td>\n",
       "      <td>175.587</td>\n",
       "      <td>83.618</td>\n",
       "      <td>20.801</td>\n",
       "      <td>-99.586</td>\n",
       "      <td>6.406</td>\n",
       "      <td>-92.600</td>\n",
       "      <td>46.592</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>3.313</td>\n",
       "      <td>3.165</td>\n",
       "      <td>-1.353</td>\n",
       "      <td>-1.422</td>\n",
       "      <td>3.356</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>3.134</td>\n",
       "      <td>-1.179</td>\n",
       "      <td>3.515</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>3.421</td>\n",
       "      <td>-1.395</td>\n",
       "      <td>3.416</td>\n",
       "      <td>-1.557</td>\n",
       "      <td>-1.253</td>\n",
       "      <td>-1.265</td>\n",
       "      <td>-1.282</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>3.113</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>3.080</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.582</td>\n",
       "      <td>-1.711</td>\n",
       "      <td>2.887</td>\n",
       "      <td>2.860</td>\n",
       "      <td>-1.769</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>3.113</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.696</td>\n",
       "      <td>2.904</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>2.746</td>\n",
       "      <td>2.986</td>\n",
       "      <td>2.867</td>\n",
       "      <td>2.964</td>\n",
       "      <td>3.005</td>\n",
       "      <td>-1.636</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.665</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-4.710</td>\n",
       "      <td>-5.005</td>\n",
       "      <td>3.251</td>\n",
       "      <td>3.339</td>\n",
       "      <td>-4.657</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-5.148</td>\n",
       "      <td>2.913</td>\n",
       "      <td>-4.296</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-4.572</td>\n",
       "      <td>3.245</td>\n",
       "      <td>-4.432</td>\n",
       "      <td>3.652</td>\n",
       "      <td>3.044</td>\n",
       "      <td>3.235</td>\n",
       "      <td>3.113</td>\n",
       "      <td>3.188</td>\n",
       "      <td>-5.345</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-5.347</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>2.776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "4197  1373158606    87.132    39.582   -30.321    54.070     5.890    67.110   \n",
       "5318  1373158606   -45.636    -4.965    -7.004    14.531   -96.484    13.345   \n",
       "3815  1373158606     5.364    10.410    12.929   -61.212    62.185    38.451   \n",
       "9727  1373158606    53.997   -47.549    40.123    83.030    18.048    72.248   \n",
       "5410  1373158606    83.626    20.678   -98.940     4.724   -90.525    45.653   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "4197          73.923          89.386           0.000          -0.001   \n",
       "5318         -34.877         -52.423          -0.000           0.000   \n",
       "3815           7.025         -30.714         300.499        -871.241   \n",
       "9727          19.520         106.478           0.000          -0.000   \n",
       "5410          89.089         -91.126         320.877        -527.133   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "4197           0.001          -0.000          87.153          39.290   \n",
       "5318          -0.000           0.000         -45.632          -4.971   \n",
       "3815        1042.793        -388.216           5.377          10.168   \n",
       "9727           0.000          -0.000          54.009         -47.844   \n",
       "5410          14.663         175.587          83.618          20.801   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "4197         -28.670          50.287           9.550          65.865 -0.009   \n",
       "5318          -7.063          14.943         -97.284          13.804 -0.009   \n",
       "3815          14.153         -63.949          64.893          37.489 -0.009   \n",
       "9727          41.745          79.237          22.045          70.695 -0.009   \n",
       "5410         -99.586           6.406         -92.600          46.592 -0.009   \n",
       "\n",
       "       wb_1  wb_2  wb_3   wb_4   wb_5  wb_6   wb_7  wb_8   wb_9  wb_10  wb_11  \\\n",
       "4197 -0.227 0.131 0.083  3.056  2.993 0.110 -0.297 0.067  3.356  3.164 -0.088   \n",
       "5318 -0.227 2.234 2.126  0.109  0.080 2.274 -0.297 2.063  2.472  2.430 -0.088   \n",
       "3815 -0.227 0.131 0.083  1.622  1.561 0.110 -0.297 0.067  1.934  7.242 -0.088   \n",
       "9727 -0.227 0.131 0.083  3.898  3.824 0.110 -0.297 0.067  4.171  4.125 -0.088   \n",
       "5410 -0.227 3.313 3.165 -1.353 -1.422 3.356 -0.297 3.134 -1.179  3.515 -0.088   \n",
       "\n",
       "      wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "4197 -0.019  0.131  3.030  0.179  2.736  3.214  3.119  3.198  3.023  0.041   \n",
       "5318 -0.019  2.328  0.079  2.373  0.008  2.344  0.148  0.137  0.033  1.990   \n",
       "3815 -0.019  0.131  1.616  0.180  2.036  2.354  2.527  2.066  1.662  0.041   \n",
       "9727 -0.019  0.131  3.884  0.180  3.549  4.045  3.933  4.032  3.894  0.041   \n",
       "5410 -0.019  3.421 -1.395  3.416 -1.557 -1.253 -1.265 -1.282 -1.421  3.113   \n",
       "\n",
       "      wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "4197  0.008 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "5318  2.150 -0.042 -0.404 -0.090  2.001 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "3815  0.008 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "9727  0.008 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "5410  0.014 -0.042 -0.404 -0.090  3.080 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "\n",
       "      wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "4197 -0.131 -0.083  2.449  2.442 -0.110  0.000 -0.069  2.664  2.376  0.000   \n",
       "5318  1.376  1.381 -0.109 -0.081  1.461  0.000  1.356  1.578  1.447  0.000   \n",
       "3815 -0.131 -0.084 -0.125 -0.117 -0.110  0.000 -0.069 -0.154 -5.378  0.000   \n",
       "9727 -0.131 -0.084  0.639  0.624 -0.110  0.000 -0.069  0.771  0.496  0.000   \n",
       "5410 -1.582 -1.711  2.887  2.860 -1.769  0.000 -1.719  3.113 -1.681  0.000   \n",
       "\n",
       "      wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "4197  0.000 -0.131  2.479 -0.179  2.337  2.553  2.418  2.517  2.569 -0.047   \n",
       "5318  0.000  1.465 -0.079  1.408 -0.023  1.457 -0.148 -0.137 -0.040  1.339   \n",
       "3815  0.000 -0.131 -0.120 -0.180 -0.990 -1.101 -1.327 -0.876 -0.124 -0.048   \n",
       "9727  0.000 -0.131  0.697 -0.179  0.565  0.698  0.584  0.670  0.785 -0.047   \n",
       "5410  0.000 -1.696  2.904 -1.620  2.746  2.986  2.867  2.964  3.005 -1.636   \n",
       "\n",
       "      wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "4197 -0.023  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254 -0.359   \n",
       "5318  1.535  0.000  0.000  0.000  1.335  0.000  0.000  0.000 -0.254 -0.359   \n",
       "3815 -0.022  0.000  0.000  0.000 -0.053  0.000  0.000  0.000 -0.254 -0.359   \n",
       "9727 -0.022  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254 -0.359   \n",
       "5410 -0.019  0.000  0.000  0.000 -1.665  0.000  0.000  0.000 -0.254 -0.359   \n",
       "\n",
       "      wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "4197 -0.201 -0.226  2.940  3.011 -0.043 -0.408 -0.258  2.712  2.703 -0.352   \n",
       "5318 -2.043 -2.063  0.138  0.187 -1.880 -0.408 -2.079 -1.755 -1.879 -0.352   \n",
       "3815 -0.201 -0.226  1.224  1.285 -0.044 -0.408 -0.258  1.036 15.645 -0.352   \n",
       "9727 -0.200 -0.226  2.750  2.800 -0.043 -0.408 -0.258  2.466  2.692 -0.352   \n",
       "5410 -4.710 -5.005  3.251  3.339 -4.657 -0.408 -5.148  2.913 -4.296 -0.352   \n",
       "\n",
       "      wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "4197 -0.364 -0.044  2.935 -0.100  3.232  2.806  2.949  2.846  2.870 -0.310   \n",
       "5318 -0.364 -1.907  0.121 -1.965  0.393 -1.809  0.149  0.007  0.048 -2.132   \n",
       "3815 -0.364 -0.043  1.233 -0.099  3.420  2.706  3.681  2.013  1.197 -0.311   \n",
       "9727 -0.364 -0.042  2.746 -0.099  2.997  2.594  2.762  2.647  2.652 -0.310   \n",
       "5410 -0.364 -4.572  3.245 -4.432  3.652  3.044  3.235  3.113  3.188 -5.345   \n",
       "\n",
       "      wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  \n",
       "4197 -0.045 -0.261 -0.383 -0.059 -0.335  0.144 -0.258 -0.315  2.061  \n",
       "5318 -1.902 -0.261 -0.383 -0.059 -2.164  0.144 -0.258 -0.315 -1.303  \n",
       "3815 -0.045 -0.261 -0.383 -0.059 -0.335  0.144 -0.258 -0.315  6.242  \n",
       "9727 -0.045 -0.261 -0.383 -0.059 -0.335  0.144 -0.258 -0.315  1.892  \n",
       "5410 -0.048 -0.261 -0.383 -0.059 -5.347  0.144 -0.258 -0.315  2.776  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-23.089</td>\n",
       "      <td>-14.209</td>\n",
       "      <td>-18.050</td>\n",
       "      <td>-21.425</td>\n",
       "      <td>-4.825</td>\n",
       "      <td>-69.243</td>\n",
       "      <td>-8.596</td>\n",
       "      <td>-82.228</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-23.092</td>\n",
       "      <td>-14.125</td>\n",
       "      <td>-18.688</td>\n",
       "      <td>-19.917</td>\n",
       "      <td>-6.200</td>\n",
       "      <td>-68.822</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>3.012</td>\n",
       "      <td>2.919</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>3.125</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>2.899</td>\n",
       "      <td>3.301</td>\n",
       "      <td>3.259</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>3.166</td>\n",
       "      <td>0.079</td>\n",
       "      <td>3.186</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3.227</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.033</td>\n",
       "      <td>2.794</td>\n",
       "      <td>2.970</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>2.786</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.321</td>\n",
       "      <td>-2.333</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-2.172</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.388</td>\n",
       "      <td>-1.998</td>\n",
       "      <td>-2.178</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.200</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-2.271</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-2.170</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-2.411</td>\n",
       "      <td>-2.125</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.432</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>49.102</td>\n",
       "      <td>77.846</td>\n",
       "      <td>-11.962</td>\n",
       "      <td>-31.496</td>\n",
       "      <td>-79.536</td>\n",
       "      <td>-32.633</td>\n",
       "      <td>53.415</td>\n",
       "      <td>139.776</td>\n",
       "      <td>-1007.759</td>\n",
       "      <td>3272.360</td>\n",
       "      <td>-4205.442</td>\n",
       "      <td>1736.409</td>\n",
       "      <td>49.119</td>\n",
       "      <td>77.590</td>\n",
       "      <td>-10.812</td>\n",
       "      <td>-33.521</td>\n",
       "      <td>-78.164</td>\n",
       "      <td>-32.878</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>4.210</td>\n",
       "      <td>3.999</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.134</td>\n",
       "      <td>4.201</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>3.939</td>\n",
       "      <td>1.388</td>\n",
       "      <td>1.079</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>4.351</td>\n",
       "      <td>1.155</td>\n",
       "      <td>4.359</td>\n",
       "      <td>0.916</td>\n",
       "      <td>1.301</td>\n",
       "      <td>1.243</td>\n",
       "      <td>1.296</td>\n",
       "      <td>1.152</td>\n",
       "      <td>3.980</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>3.896</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.507</td>\n",
       "      <td>-2.893</td>\n",
       "      <td>2.178</td>\n",
       "      <td>2.150</td>\n",
       "      <td>-2.964</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.839</td>\n",
       "      <td>2.387</td>\n",
       "      <td>2.293</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.664</td>\n",
       "      <td>2.196</td>\n",
       "      <td>-2.561</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.271</td>\n",
       "      <td>2.138</td>\n",
       "      <td>2.259</td>\n",
       "      <td>2.268</td>\n",
       "      <td>-2.686</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.857</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-8.278</td>\n",
       "      <td>-9.151</td>\n",
       "      <td>2.345</td>\n",
       "      <td>2.402</td>\n",
       "      <td>-8.666</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-9.308</td>\n",
       "      <td>2.093</td>\n",
       "      <td>2.278</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-8.145</td>\n",
       "      <td>2.334</td>\n",
       "      <td>-7.977</td>\n",
       "      <td>2.612</td>\n",
       "      <td>2.193</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.251</td>\n",
       "      <td>2.267</td>\n",
       "      <td>-9.371</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-9.604</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>5.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-5.699</td>\n",
       "      <td>69.181</td>\n",
       "      <td>-84.006</td>\n",
       "      <td>-11.522</td>\n",
       "      <td>-89.491</td>\n",
       "      <td>74.393</td>\n",
       "      <td>4.855</td>\n",
       "      <td>-7.759</td>\n",
       "      <td>0.472</td>\n",
       "      <td>235.220</td>\n",
       "      <td>-621.509</td>\n",
       "      <td>344.649</td>\n",
       "      <td>-5.708</td>\n",
       "      <td>69.293</td>\n",
       "      <td>-84.610</td>\n",
       "      <td>-10.186</td>\n",
       "      <td>-90.832</td>\n",
       "      <td>74.892</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>2.444</td>\n",
       "      <td>2.335</td>\n",
       "      <td>2.279</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>2.477</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>2.247</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.641</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>2.550</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>2.575</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>2.627</td>\n",
       "      <td>2.311</td>\n",
       "      <td>2.561</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>2.171</td>\n",
       "      <td>2.390</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>2.183</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.141</td>\n",
       "      <td>-1.489</td>\n",
       "      <td>-1.309</td>\n",
       "      <td>1.104</td>\n",
       "      <td>-1.569</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.518</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>-1.235</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>1.134</td>\n",
       "      <td>-1.199</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-1.235</td>\n",
       "      <td>-1.315</td>\n",
       "      <td>-1.324</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-1.553</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.896</td>\n",
       "      <td>-3.608</td>\n",
       "      <td>-3.234</td>\n",
       "      <td>1.248</td>\n",
       "      <td>-3.342</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-3.654</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-2.656</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.853</td>\n",
       "      <td>1.129</td>\n",
       "      <td>-2.758</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-2.687</td>\n",
       "      <td>-3.201</td>\n",
       "      <td>-2.865</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-3.865</td>\n",
       "      <td>-3.283</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-3.840</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>1.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>41.026</td>\n",
       "      <td>-13.291</td>\n",
       "      <td>-5.702</td>\n",
       "      <td>-24.553</td>\n",
       "      <td>-87.656</td>\n",
       "      <td>-24.134</td>\n",
       "      <td>43.019</td>\n",
       "      <td>16.991</td>\n",
       "      <td>-565.762</td>\n",
       "      <td>2002.238</td>\n",
       "      <td>-2782.870</td>\n",
       "      <td>1186.528</td>\n",
       "      <td>41.032</td>\n",
       "      <td>-13.268</td>\n",
       "      <td>-6.205</td>\n",
       "      <td>-22.845</td>\n",
       "      <td>-89.781</td>\n",
       "      <td>-23.251</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>3.877</td>\n",
       "      <td>3.817</td>\n",
       "      <td>-2.037</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>4.110</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>3.776</td>\n",
       "      <td>4.294</td>\n",
       "      <td>4.238</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>4.086</td>\n",
       "      <td>-2.050</td>\n",
       "      <td>4.129</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>-2.135</td>\n",
       "      <td>-2.014</td>\n",
       "      <td>-2.103</td>\n",
       "      <td>-2.128</td>\n",
       "      <td>3.709</td>\n",
       "      <td>3.947</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>3.699</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.909</td>\n",
       "      <td>-2.750</td>\n",
       "      <td>2.014</td>\n",
       "      <td>1.971</td>\n",
       "      <td>-2.229</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.819</td>\n",
       "      <td>-2.159</td>\n",
       "      <td>-2.148</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.613</td>\n",
       "      <td>2.025</td>\n",
       "      <td>-2.418</td>\n",
       "      <td>1.882</td>\n",
       "      <td>2.111</td>\n",
       "      <td>1.996</td>\n",
       "      <td>2.077</td>\n",
       "      <td>2.101</td>\n",
       "      <td>-2.784</td>\n",
       "      <td>-2.567</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.794</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-7.519</td>\n",
       "      <td>-7.349</td>\n",
       "      <td>2.561</td>\n",
       "      <td>2.555</td>\n",
       "      <td>-6.076</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-7.722</td>\n",
       "      <td>-5.671</td>\n",
       "      <td>-5.733</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-6.793</td>\n",
       "      <td>2.520</td>\n",
       "      <td>-6.304</td>\n",
       "      <td>2.781</td>\n",
       "      <td>2.373</td>\n",
       "      <td>2.562</td>\n",
       "      <td>2.440</td>\n",
       "      <td>2.439</td>\n",
       "      <td>-7.872</td>\n",
       "      <td>-7.156</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-7.879</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>3.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-85.035</td>\n",
       "      <td>31.816</td>\n",
       "      <td>82.658</td>\n",
       "      <td>-99.845</td>\n",
       "      <td>-56.815</td>\n",
       "      <td>-33.999</td>\n",
       "      <td>-82.064</td>\n",
       "      <td>119.538</td>\n",
       "      <td>-887.038</td>\n",
       "      <td>2976.131</td>\n",
       "      <td>-3940.529</td>\n",
       "      <td>1684.502</td>\n",
       "      <td>-85.034</td>\n",
       "      <td>31.601</td>\n",
       "      <td>84.499</td>\n",
       "      <td>-104.904</td>\n",
       "      <td>-51.196</td>\n",
       "      <td>-36.185</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.752</td>\n",
       "      <td>0.148</td>\n",
       "      <td>9.448</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>-1.023</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-1.046</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.438</td>\n",
       "      <td>2.399</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>2.565</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.402</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.553</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.565</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>2.498</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>2.598</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-6.059</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>2.362</td>\n",
       "      <td>2.555</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-2.380</td>\n",
       "      <td>-2.436</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-2.243</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-2.519</td>\n",
       "      <td>-2.042</td>\n",
       "      <td>-2.125</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-2.228</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-2.243</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-2.147</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-22.680</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-2.607</td>\n",
       "      <td>-2.306</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-2.619</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-10.214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "7217  1373158606   -23.089   -14.209   -18.050   -21.425    -4.825   -69.243   \n",
       "8291  1373158606    49.102    77.846   -11.962   -31.496   -79.536   -32.633   \n",
       "4607  1373158606    -5.699    69.181   -84.006   -11.522   -89.491    74.393   \n",
       "5114  1373158606    41.026   -13.291    -5.702   -24.553   -87.656   -24.134   \n",
       "1859  1373158606   -85.035    31.816    82.658   -99.845   -56.815   -33.999   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "7217          -8.596         -82.228           0.000          -0.000   \n",
       "8291          53.415         139.776       -1007.759        3272.360   \n",
       "4607           4.855          -7.759           0.472         235.220   \n",
       "5114          43.019          16.991        -565.762        2002.238   \n",
       "1859         -82.064         119.538        -887.038        2976.131   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "7217           0.001          -0.000         -23.092         -14.125   \n",
       "8291       -4205.442        1736.409          49.119          77.590   \n",
       "4607        -621.509         344.649          -5.708          69.293   \n",
       "5114       -2782.870        1186.528          41.032         -13.268   \n",
       "1859       -3940.529        1684.502         -85.034          31.601   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "7217         -18.688         -19.917          -6.200         -68.822 -0.009   \n",
       "8291         -10.812         -33.521         -78.164         -32.878 -0.009   \n",
       "4607         -84.610         -10.186         -90.832          74.892 -0.009   \n",
       "5114          -6.205         -22.845         -89.781         -23.251 -0.009   \n",
       "1859          84.499        -104.904         -51.196         -36.185 -0.009   \n",
       "\n",
       "       wb_1   wb_2   wb_3   wb_4   wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  \\\n",
       "7217 -0.227  3.012  2.919  0.109  0.080  3.125 -0.297  2.899  3.301  3.259   \n",
       "8291 -0.227  4.210  3.999  1.199  1.134  4.201 -0.297  3.939  1.388  1.079   \n",
       "4607 -0.227  2.444  2.335  2.279 -0.281  2.477 -0.297  2.247  2.720  2.641   \n",
       "5114 -0.227  3.877  3.817 -2.037 -1.995  4.110 -0.297  3.776  4.294  4.238   \n",
       "1859 -0.227 -0.842 -0.957  0.109  0.080 -0.832 -0.297 -0.989 -0.686 -0.635   \n",
       "\n",
       "      wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  \\\n",
       "7217 -0.088 -0.019  3.166  0.079  3.186  0.009  3.227  0.148  0.137  0.033   \n",
       "8291 -0.088 -0.019  4.351  1.155  4.359  0.916  1.301  1.243  1.296  1.152   \n",
       "4607 -0.088 -0.019  2.550 -0.280  2.575 -0.004  2.627  2.311  2.561 -0.002   \n",
       "5114 -0.088 -0.019  4.086 -2.050  4.129 -1.907 -2.135 -2.014 -2.103 -2.128   \n",
       "1859 -0.088 -0.019 -0.749  0.078 -0.752  0.008 -0.752  0.148  9.448  0.033   \n",
       "\n",
       "      wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  \\\n",
       "7217  2.794  2.970 -0.042 -0.404 -0.090  2.786 -0.291 -0.209 -0.233  0.000   \n",
       "8291  3.980  0.009 -0.042 -0.404 -0.090  3.896 -0.291 -0.209 -0.233  0.000   \n",
       "4607  2.171  2.390 -0.042 -0.404 -0.090  2.183 -0.291 -0.209 -0.233  0.000   \n",
       "5114  3.709  3.947 -0.042 -0.404 -0.090  3.699 -0.291 -0.209 -0.233  0.000   \n",
       "1859 -1.050 -1.023 -0.042 -0.404 -0.090 -1.046 -0.291 -0.209 -0.233  0.000   \n",
       "\n",
       "      wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  \\\n",
       "7217  0.000  0.252  0.271 -0.109 -0.081  0.331  0.000  0.293  0.387  0.264   \n",
       "8291  0.000 -2.507 -2.893  2.178  2.150 -2.964  0.000 -2.839  2.387  2.293   \n",
       "4607  0.000 -1.141 -1.489 -1.309  1.104 -1.569  0.000 -1.518 -1.257 -1.235   \n",
       "5114  0.000 -2.909 -2.750  2.014  1.971 -2.229  0.000 -2.819 -2.159 -2.148   \n",
       "1859  0.000  2.438  2.399 -0.109 -0.081  2.565  0.000  2.402  2.707  2.553   \n",
       "\n",
       "      wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  \\\n",
       "7217  0.000  0.000  0.328 -0.080  0.261 -0.023  0.201 -0.148 -0.137 -0.040   \n",
       "8291  0.000  0.000 -2.664  2.196 -2.561  2.013  2.271  2.138  2.259  2.268   \n",
       "4607  0.000  0.000 -1.269  1.134 -1.199 -0.016 -1.235 -1.315 -1.324 -0.001   \n",
       "5114  0.000  0.000 -2.613  2.025 -2.418  1.882  2.111  1.996  2.077  2.101   \n",
       "1859  0.000  0.000  2.565 -0.079  2.498 -0.024  2.598 -0.148 -6.059 -0.040   \n",
       "\n",
       "      wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  \\\n",
       "7217  0.277  0.412  0.000  0.000  0.000  0.267  0.000  0.000  0.000 -0.254   \n",
       "8291 -2.686 -0.022  0.000  0.000  0.000 -2.857  0.000  0.000  0.000 -0.254   \n",
       "4607 -1.553 -1.380  0.000  0.000  0.000 -1.506  0.000  0.000  0.000 -0.254   \n",
       "5114 -2.784 -2.567  0.000  0.000  0.000 -2.794  0.000  0.000  0.000 -0.254   \n",
       "1859  2.362  2.555  0.000  0.000  0.000  2.344  0.000  0.000  0.000 -0.254   \n",
       "\n",
       "      wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  \\\n",
       "7217 -0.359 -2.321 -2.333  0.138  0.187 -2.172 -0.408 -2.388 -1.998 -2.178   \n",
       "8291 -0.359 -8.278 -9.151  2.345  2.402 -8.666 -0.408 -9.308  2.093  2.278   \n",
       "4607 -0.359 -2.896 -3.608 -3.234  1.248 -3.342 -0.408 -3.654 -2.600 -2.656   \n",
       "5114 -0.359 -7.519 -7.349  2.561  2.555 -6.076 -0.408 -7.722 -5.671 -5.733   \n",
       "1859 -0.359 -2.380 -2.436  0.138  0.187 -2.243 -0.408 -2.519 -2.042 -2.125   \n",
       "\n",
       "      wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78   wb_79  wb_80  \\\n",
       "7217 -0.352 -0.364 -2.200  0.121 -2.271  0.394 -2.170  0.149   0.003  0.049   \n",
       "8291 -0.352 -0.364 -8.145  2.334 -7.977  2.612  2.193  2.333   2.251  2.267   \n",
       "4607 -0.352 -0.364 -2.853  1.129 -2.758  0.383 -2.687 -3.201  -2.865  0.010   \n",
       "5114 -0.352 -0.364 -6.793  2.520 -6.304  2.781  2.373  2.562   2.440  2.439   \n",
       "1859 -0.352 -0.364 -2.228  0.121 -2.243  0.393 -2.147  0.149 -22.680  0.048   \n",
       "\n",
       "      wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89   wb_90  \n",
       "7217 -2.411 -2.125 -0.261 -0.383 -0.059 -2.432  0.144 -0.258 -0.315  -0.666  \n",
       "8291 -9.371 -0.045 -0.261 -0.383 -0.059 -9.604  0.144 -0.258 -0.315   5.247  \n",
       "4607 -3.865 -3.283 -0.261 -0.383 -0.059 -3.840  0.144 -0.258 -0.315   1.866  \n",
       "5114 -7.872 -7.156 -0.261 -0.383 -0.059 -7.879  0.144 -0.258 -0.315   3.579  \n",
       "1859 -2.607 -2.306 -0.261 -0.383 -0.059 -2.619  0.144 -0.258 -0.315 -10.214  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 Complete [01h 52m 36s]\n",
      "val_loss: 1.898943305015564\n",
      "\n",
      "Best val_loss So Far: 1.7979326248168945\n",
      "Total elapsed time: 09h 23m 52s\n",
      "\n",
      "Search: Running Trial #17\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "conv_block_1/ke...|3                 |3                 \n",
      "conv_block_1/se...|False             |False             \n",
      "conv_block_1/ma...|False             |False             \n",
      "conv_block_1/dr...|0                 |0                 \n",
      "conv_block_1/nu...|2                 |2                 \n",
      "conv_block_1/nu...|1                 |1                 \n",
      "conv_block_1/fi...|32                |32                \n",
      "conv_block_1/fi...|32                |32                \n",
      "conv_block_1/fi...|16                |32                \n",
      "conv_block_1/fi...|32                |32                \n",
      "dense_block_1/u...|False             |False             \n",
      "dense_block_1/n...|1                 |1                 \n",
      "dense_block_1/u...|16                |16                \n",
      "dense_block_1/d...|0                 |0                 \n",
      "dense_block_1/u...|32                |32                \n",
      "regression_head...|0                 |0                 \n",
      "optimizer         |adam              |adam              \n",
      "learning_rate     |0.001             |0.001             \n",
      "conv_block_1/fi...|256               |256               \n",
      "conv_block_1/fi...|16                |16                \n",
      "\n",
      "Epoch 1/500\n",
      "35/35 [==============================] - 90s 2s/step - loss: 58.9475 - r2_inet_coefficient_loss: -3.2997e-04 - r2_inet_lambda_fv_loss: 2079.7030 - mae_inet_coefficient_loss: 391.6335 - mae_inet_lambda_fv_loss: 58.9450 - val_loss: 52.2948 - val_r2_inet_coefficient_loss: -0.0010 - val_r2_inet_lambda_fv_loss: 276.6577 - val_mae_inet_coefficient_loss: 389.8811 - val_mae_inet_lambda_fv_loss: 52.3095\n",
      "Epoch 2/500\n",
      "35/35 [==============================] - 60s 2s/step - loss: 44.2529 - r2_inet_coefficient_loss: -8.6856e-04 - r2_inet_lambda_fv_loss: 642.2528 - mae_inet_coefficient_loss: 391.0848 - mae_inet_lambda_fv_loss: 44.2498 - val_loss: 26.3395 - val_r2_inet_coefficient_loss: 0.0015 - val_r2_inet_lambda_fv_loss: 48.7876 - val_mae_inet_coefficient_loss: 391.2918 - val_mae_inet_lambda_fv_loss: 26.3375\n",
      "Epoch 3/500\n",
      "35/35 [==============================] - 68s 2s/step - loss: 22.5857 - r2_inet_coefficient_loss: 9.8465e-04 - r2_inet_lambda_fv_loss: 183.3855 - mae_inet_coefficient_loss: 391.7859 - mae_inet_lambda_fv_loss: 22.5845 - val_loss: 16.9272 - val_r2_inet_coefficient_loss: -2.3565e-04 - val_r2_inet_lambda_fv_loss: 14.5930 - val_mae_inet_coefficient_loss: 389.3658 - val_mae_inet_lambda_fv_loss: 16.9545\n",
      "Epoch 4/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 15.8502 - r2_inet_coefficient_loss: 4.8704e-07 - r2_inet_lambda_fv_loss: 127.5449 - mae_inet_coefficient_loss: 390.3397 - mae_inet_lambda_fv_loss: 15.8496 - val_loss: 13.5859 - val_r2_inet_coefficient_loss: 5.0271e-04 - val_r2_inet_lambda_fv_loss: 9.7611 - val_mae_inet_coefficient_loss: 388.8317 - val_mae_inet_lambda_fv_loss: 13.5947\n",
      "Epoch 5/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 13.0124 - r2_inet_coefficient_loss: 7.7541e-04 - r2_inet_lambda_fv_loss: 128.4151 - mae_inet_coefficient_loss: 389.9747 - mae_inet_lambda_fv_loss: 13.0121 - val_loss: 11.8179 - val_r2_inet_coefficient_loss: 8.4424e-04 - val_r2_inet_lambda_fv_loss: 7.8557 - val_mae_inet_coefficient_loss: 388.5627 - val_mae_inet_lambda_fv_loss: 11.8176\n",
      "Epoch 6/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 11.5522 - r2_inet_coefficient_loss: 0.0010 - r2_inet_lambda_fv_loss: 119.8242 - mae_inet_coefficient_loss: 389.7415 - mae_inet_lambda_fv_loss: 11.5521 - val_loss: 11.1619 - val_r2_inet_coefficient_loss: 9.1535e-04 - val_r2_inet_lambda_fv_loss: 5.4842 - val_mae_inet_coefficient_loss: 388.4767 - val_mae_inet_lambda_fv_loss: 11.1627\n",
      "Epoch 7/500\n",
      "35/35 [==============================] - 66s 2s/step - loss: 11.0403 - r2_inet_coefficient_loss: 9.9910e-04 - r2_inet_lambda_fv_loss: 118.1771 - mae_inet_coefficient_loss: 389.6113 - mae_inet_lambda_fv_loss: 11.0402 - val_loss: 10.8441 - val_r2_inet_coefficient_loss: 8.5981e-04 - val_r2_inet_lambda_fv_loss: 4.8768 - val_mae_inet_coefficient_loss: 388.3763 - val_mae_inet_lambda_fv_loss: 10.8463\n",
      "Epoch 8/500\n",
      "35/35 [==============================] - 63s 2s/step - loss: 10.7665 - r2_inet_coefficient_loss: 9.2673e-04 - r2_inet_lambda_fv_loss: 115.3945 - mae_inet_coefficient_loss: 389.4991 - mae_inet_lambda_fv_loss: 10.7664 - val_loss: 10.6178 - val_r2_inet_coefficient_loss: 8.0426e-04 - val_r2_inet_lambda_fv_loss: 4.5987 - val_mae_inet_coefficient_loss: 388.2728 - val_mae_inet_lambda_fv_loss: 10.6208\n",
      "Epoch 9/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 10.5633 - r2_inet_coefficient_loss: 8.6757e-04 - r2_inet_lambda_fv_loss: 111.5235 - mae_inet_coefficient_loss: 389.3882 - mae_inet_lambda_fv_loss: 10.5633 - val_loss: 10.4271 - val_r2_inet_coefficient_loss: 7.6367e-04 - val_r2_inet_lambda_fv_loss: 4.3042 - val_mae_inet_coefficient_loss: 388.1775 - val_mae_inet_lambda_fv_loss: 10.4302\n",
      "Epoch 10/500\n",
      "35/35 [==============================] - 60s 2s/step - loss: 10.3844 - r2_inet_coefficient_loss: 8.3505e-04 - r2_inet_lambda_fv_loss: 107.1426 - mae_inet_coefficient_loss: 389.2969 - mae_inet_lambda_fv_loss: 10.3844 - val_loss: 10.2432 - val_r2_inet_coefficient_loss: 7.4036e-04 - val_r2_inet_lambda_fv_loss: 4.0169 - val_mae_inet_coefficient_loss: 388.0963 - val_mae_inet_lambda_fv_loss: 10.2463\n",
      "Epoch 11/500\n",
      "35/35 [==============================] - 61s 2s/step - loss: 10.1986 - r2_inet_coefficient_loss: 8.0461e-04 - r2_inet_lambda_fv_loss: 101.7645 - mae_inet_coefficient_loss: 389.2079 - mae_inet_lambda_fv_loss: 10.1986 - val_loss: 10.0743 - val_r2_inet_coefficient_loss: 7.0986e-04 - val_r2_inet_lambda_fv_loss: 3.7537 - val_mae_inet_coefficient_loss: 388.0162 - val_mae_inet_lambda_fv_loss: 10.0773\n",
      "Epoch 12/500\n",
      "35/35 [==============================] - 65s 2s/step - loss: 10.0162 - r2_inet_coefficient_loss: 7.7648e-04 - r2_inet_lambda_fv_loss: 94.5566 - mae_inet_coefficient_loss: 389.1237 - mae_inet_lambda_fv_loss: 10.0162 - val_loss: 9.9009 - val_r2_inet_coefficient_loss: 6.7860e-04 - val_r2_inet_lambda_fv_loss: 3.5627 - val_mae_inet_coefficient_loss: 387.9381 - val_mae_inet_lambda_fv_loss: 9.9036\n",
      "Epoch 13/500\n",
      "35/35 [==============================] - 62s 2s/step - loss: 9.8366 - r2_inet_coefficient_loss: 7.3989e-04 - r2_inet_lambda_fv_loss: 86.2361 - mae_inet_coefficient_loss: 389.0426 - mae_inet_lambda_fv_loss: 9.8366 - val_loss: 9.7052 - val_r2_inet_coefficient_loss: 6.6562e-04 - val_r2_inet_lambda_fv_loss: 3.4113 - val_mae_inet_coefficient_loss: 387.8705 - val_mae_inet_lambda_fv_loss: 9.7075\n",
      "Epoch 14/500\n",
      "35/35 [==============================] - 65s 2s/step - loss: 9.6515 - r2_inet_coefficient_loss: 7.2461e-04 - r2_inet_lambda_fv_loss: 77.4721 - mae_inet_coefficient_loss: 388.9721 - mae_inet_lambda_fv_loss: 9.6514 - val_loss: 9.5039 - val_r2_inet_coefficient_loss: 6.5492e-04 - val_r2_inet_lambda_fv_loss: 3.2570 - val_mae_inet_coefficient_loss: 387.8092 - val_mae_inet_lambda_fv_loss: 9.5058\n",
      "Epoch 15/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 9.4612 - r2_inet_coefficient_loss: 7.0805e-04 - r2_inet_lambda_fv_loss: 68.5912 - mae_inet_coefficient_loss: 388.9049 - mae_inet_lambda_fv_loss: 9.4611 - val_loss: 9.2956 - val_r2_inet_coefficient_loss: 6.4281e-04 - val_r2_inet_lambda_fv_loss: 3.0778 - val_mae_inet_coefficient_loss: 387.7558 - val_mae_inet_lambda_fv_loss: 9.2968\n",
      "Epoch 16/500\n",
      "35/35 [==============================] - 61s 2s/step - loss: 9.2636 - r2_inet_coefficient_loss: 6.8281e-04 - r2_inet_lambda_fv_loss: 58.6588 - mae_inet_coefficient_loss: 388.8390 - mae_inet_lambda_fv_loss: 9.2635 - val_loss: 9.1001 - val_r2_inet_coefficient_loss: 6.2490e-04 - val_r2_inet_lambda_fv_loss: 2.9322 - val_mae_inet_coefficient_loss: 387.7029 - val_mae_inet_lambda_fv_loss: 9.1006\n",
      "Epoch 17/500\n",
      "35/35 [==============================] - 63s 2s/step - loss: 9.0585 - r2_inet_coefficient_loss: 6.4876e-04 - r2_inet_lambda_fv_loss: 48.6637 - mae_inet_coefficient_loss: 388.7763 - mae_inet_lambda_fv_loss: 9.0584 - val_loss: 8.8820 - val_r2_inet_coefficient_loss: 6.1457e-04 - val_r2_inet_lambda_fv_loss: 2.7582 - val_mae_inet_coefficient_loss: 387.6588 - val_mae_inet_lambda_fv_loss: 8.8815\n",
      "Epoch 18/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 8.8202 - r2_inet_coefficient_loss: 6.1646e-04 - r2_inet_lambda_fv_loss: 38.4902 - mae_inet_coefficient_loss: 388.7198 - mae_inet_lambda_fv_loss: 8.8201 - val_loss: 8.6615 - val_r2_inet_coefficient_loss: 6.0748e-04 - val_r2_inet_lambda_fv_loss: 2.5905 - val_mae_inet_coefficient_loss: 387.6205 - val_mae_inet_lambda_fv_loss: 8.6604\n",
      "Epoch 19/500\n",
      "35/35 [==============================] - 63s 2s/step - loss: 8.5612 - r2_inet_coefficient_loss: 5.9183e-04 - r2_inet_lambda_fv_loss: 28.3609 - mae_inet_coefficient_loss: 388.6659 - mae_inet_lambda_fv_loss: 8.5611 - val_loss: 8.4063 - val_r2_inet_coefficient_loss: 6.0500e-04 - val_r2_inet_lambda_fv_loss: 2.4009 - val_mae_inet_coefficient_loss: 387.5872 - val_mae_inet_lambda_fv_loss: 8.4040\n",
      "Epoch 20/500\n",
      "35/35 [==============================] - 69s 2s/step - loss: 8.2771 - r2_inet_coefficient_loss: 5.6067e-04 - r2_inet_lambda_fv_loss: 20.6106 - mae_inet_coefficient_loss: 388.6137 - mae_inet_lambda_fv_loss: 8.2770 - val_loss: 8.1477 - val_r2_inet_coefficient_loss: 5.9949e-04 - val_r2_inet_lambda_fv_loss: 2.2267 - val_mae_inet_coefficient_loss: 387.5579 - val_mae_inet_lambda_fv_loss: 8.1445\n",
      "Epoch 21/500\n",
      "35/35 [==============================] - 66s 2s/step - loss: 7.9981 - r2_inet_coefficient_loss: 5.3397e-04 - r2_inet_lambda_fv_loss: 14.9998 - mae_inet_coefficient_loss: 388.5641 - mae_inet_lambda_fv_loss: 7.9979 - val_loss: 7.8580 - val_r2_inet_coefficient_loss: 6.1171e-04 - val_r2_inet_lambda_fv_loss: 1.9632 - val_mae_inet_coefficient_loss: 387.5303 - val_mae_inet_lambda_fv_loss: 7.8543\n",
      "Epoch 22/500\n",
      "35/35 [==============================] - 65s 2s/step - loss: 7.7056 - r2_inet_coefficient_loss: 5.2408e-04 - r2_inet_lambda_fv_loss: 11.7456 - mae_inet_coefficient_loss: 388.5244 - mae_inet_lambda_fv_loss: 7.7055 - val_loss: 7.5516 - val_r2_inet_coefficient_loss: 6.0828e-04 - val_r2_inet_lambda_fv_loss: 1.6965 - val_mae_inet_coefficient_loss: 387.4891 - val_mae_inet_lambda_fv_loss: 7.5487\n",
      "Epoch 23/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 7.3948 - r2_inet_coefficient_loss: 5.0557e-04 - r2_inet_lambda_fv_loss: 10.4666 - mae_inet_coefficient_loss: 388.4823 - mae_inet_lambda_fv_loss: 7.3946 - val_loss: 7.3533 - val_r2_inet_coefficient_loss: 5.9745e-04 - val_r2_inet_lambda_fv_loss: 1.6062 - val_mae_inet_coefficient_loss: 387.4371 - val_mae_inet_lambda_fv_loss: 7.3521\n",
      "Epoch 24/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 7.1190 - r2_inet_coefficient_loss: 4.9311e-04 - r2_inet_lambda_fv_loss: 9.9279 - mae_inet_coefficient_loss: 388.4359 - mae_inet_lambda_fv_loss: 7.1188 - val_loss: 7.0091 - val_r2_inet_coefficient_loss: 5.9107e-04 - val_r2_inet_lambda_fv_loss: 1.3034 - val_mae_inet_coefficient_loss: 387.3947 - val_mae_inet_lambda_fv_loss: 7.0073\n",
      "Epoch 25/500\n",
      "35/35 [==============================] - 66s 2s/step - loss: 6.8034 - r2_inet_coefficient_loss: 4.8077e-04 - r2_inet_lambda_fv_loss: 9.5641 - mae_inet_coefficient_loss: 388.3940 - mae_inet_lambda_fv_loss: 6.8032 - val_loss: 6.8054 - val_r2_inet_coefficient_loss: 6.1688e-04 - val_r2_inet_lambda_fv_loss: 1.2078 - val_mae_inet_coefficient_loss: 387.3853 - val_mae_inet_lambda_fv_loss: 6.8042\n",
      "Epoch 26/500\n",
      "35/35 [==============================] - 65s 2s/step - loss: 6.5781 - r2_inet_coefficient_loss: 4.9671e-04 - r2_inet_lambda_fv_loss: 9.3588 - mae_inet_coefficient_loss: 388.3750 - mae_inet_lambda_fv_loss: 6.5780 - val_loss: 6.5957 - val_r2_inet_coefficient_loss: 6.4455e-04 - val_r2_inet_lambda_fv_loss: 1.0233 - val_mae_inet_coefficient_loss: 387.3725 - val_mae_inet_lambda_fv_loss: 6.5952\n",
      "Epoch 27/500\n",
      "35/35 [==============================] - 68s 2s/step - loss: 6.3611 - r2_inet_coefficient_loss: 5.1212e-04 - r2_inet_lambda_fv_loss: 9.2224 - mae_inet_coefficient_loss: 388.3556 - mae_inet_lambda_fv_loss: 6.3610 - val_loss: 6.4006 - val_r2_inet_coefficient_loss: 6.6765e-04 - val_r2_inet_lambda_fv_loss: 0.8736 - val_mae_inet_coefficient_loss: 387.3632 - val_mae_inet_lambda_fv_loss: 6.4004\n",
      "Epoch 28/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 6.1732 - r2_inet_coefficient_loss: 5.2961e-04 - r2_inet_lambda_fv_loss: 8.8568 - mae_inet_coefficient_loss: 388.3365 - mae_inet_lambda_fv_loss: 6.1732 - val_loss: 6.1948 - val_r2_inet_coefficient_loss: 6.8519e-04 - val_r2_inet_lambda_fv_loss: 0.7437 - val_mae_inet_coefficient_loss: 387.3535 - val_mae_inet_lambda_fv_loss: 6.1946\n",
      "Epoch 29/500\n",
      "35/35 [==============================] - 67s 2s/step - loss: 5.9920 - r2_inet_coefficient_loss: 5.4762e-04 - r2_inet_lambda_fv_loss: 7.9867 - mae_inet_coefficient_loss: 388.3226 - mae_inet_lambda_fv_loss: 5.9920 - val_loss: 5.9596 - val_r2_inet_coefficient_loss: 7.1368e-04 - val_r2_inet_lambda_fv_loss: 0.5088 - val_mae_inet_coefficient_loss: 387.3434 - val_mae_inet_lambda_fv_loss: 5.9598\n",
      "Epoch 30/500\n",
      "35/35 [==============================] - 67s 2s/step - loss: 5.8008 - r2_inet_coefficient_loss: 5.7197e-04 - r2_inet_lambda_fv_loss: 6.1898 - mae_inet_coefficient_loss: 388.3113 - mae_inet_lambda_fv_loss: 5.8008 - val_loss: 5.7359 - val_r2_inet_coefficient_loss: 7.4533e-04 - val_r2_inet_lambda_fv_loss: 0.2795 - val_mae_inet_coefficient_loss: 387.3328 - val_mae_inet_lambda_fv_loss: 5.7363\n",
      "Epoch 31/500\n",
      "35/35 [==============================] - 65s 2s/step - loss: 5.6376 - r2_inet_coefficient_loss: 5.9972e-04 - r2_inet_lambda_fv_loss: 4.7402 - mae_inet_coefficient_loss: 388.3068 - mae_inet_lambda_fv_loss: 5.6375 - val_loss: 5.6031 - val_r2_inet_coefficient_loss: 7.7251e-04 - val_r2_inet_lambda_fv_loss: 0.1975 - val_mae_inet_coefficient_loss: 387.3088 - val_mae_inet_lambda_fv_loss: 5.6037\n",
      "Epoch 32/500\n",
      "35/35 [==============================] - 63s 2s/step - loss: 5.5002 - r2_inet_coefficient_loss: 6.2987e-04 - r2_inet_lambda_fv_loss: 4.0995 - mae_inet_coefficient_loss: 388.2978 - mae_inet_lambda_fv_loss: 5.5002 - val_loss: 5.4791 - val_r2_inet_coefficient_loss: 7.9836e-04 - val_r2_inet_lambda_fv_loss: 0.0934 - val_mae_inet_coefficient_loss: 387.2914 - val_mae_inet_lambda_fv_loss: 5.4797\n",
      "Epoch 33/500\n",
      "35/35 [==============================] - 63s 2s/step - loss: 5.3672 - r2_inet_coefficient_loss: 6.6474e-04 - r2_inet_lambda_fv_loss: 3.6383 - mae_inet_coefficient_loss: 388.2918 - mae_inet_lambda_fv_loss: 5.3672 - val_loss: 5.2540 - val_r2_inet_coefficient_loss: 8.2582e-04 - val_r2_inet_lambda_fv_loss: -0.1236 - val_mae_inet_coefficient_loss: 387.2869 - val_mae_inet_lambda_fv_loss: 5.2544\n",
      "Epoch 34/500\n",
      "35/35 [==============================] - 67s 2s/step - loss: 5.2156 - r2_inet_coefficient_loss: 7.0281e-04 - r2_inet_lambda_fv_loss: 3.8639 - mae_inet_coefficient_loss: 388.2931 - mae_inet_lambda_fv_loss: 5.2155 - val_loss: 5.0815 - val_r2_inet_coefficient_loss: 8.8301e-04 - val_r2_inet_lambda_fv_loss: -0.2451 - val_mae_inet_coefficient_loss: 387.2987 - val_mae_inet_lambda_fv_loss: 5.0820\n",
      "Epoch 35/500\n",
      "35/35 [==============================] - 66s 2s/step - loss: 5.0713 - r2_inet_coefficient_loss: 7.4924e-04 - r2_inet_lambda_fv_loss: 3.4836 - mae_inet_coefficient_loss: 388.2988 - mae_inet_lambda_fv_loss: 5.0713 - val_loss: 4.9694 - val_r2_inet_coefficient_loss: 9.3102e-04 - val_r2_inet_lambda_fv_loss: -0.2535 - val_mae_inet_coefficient_loss: 387.2911 - val_mae_inet_lambda_fv_loss: 4.9696\n",
      "Epoch 36/500\n",
      "35/35 [==============================] - 63s 2s/step - loss: 4.9741 - r2_inet_coefficient_loss: 7.9150e-04 - r2_inet_lambda_fv_loss: 3.5427 - mae_inet_coefficient_loss: 388.2965 - mae_inet_lambda_fv_loss: 4.9740 - val_loss: 4.8581 - val_r2_inet_coefficient_loss: 9.7786e-04 - val_r2_inet_lambda_fv_loss: -0.3029 - val_mae_inet_coefficient_loss: 387.2804 - val_mae_inet_lambda_fv_loss: 4.8583\n",
      "Epoch 37/500\n",
      "35/35 [==============================] - 62s 2s/step - loss: 4.8593 - r2_inet_coefficient_loss: 8.1974e-04 - r2_inet_lambda_fv_loss: 3.5640 - mae_inet_coefficient_loss: 388.2881 - mae_inet_lambda_fv_loss: 4.8592 - val_loss: 4.7415 - val_r2_inet_coefficient_loss: 0.0010 - val_r2_inet_lambda_fv_loss: -0.3556 - val_mae_inet_coefficient_loss: 387.2623 - val_mae_inet_lambda_fv_loss: 4.7417\n",
      "Epoch 38/500\n",
      "35/35 [==============================] - 67s 2s/step - loss: 4.7462 - r2_inet_coefficient_loss: 8.3720e-04 - r2_inet_lambda_fv_loss: 3.3315 - mae_inet_coefficient_loss: 388.2795 - mae_inet_lambda_fv_loss: 4.7461 - val_loss: 4.6581 - val_r2_inet_coefficient_loss: 0.0010 - val_r2_inet_lambda_fv_loss: -0.3582 - val_mae_inet_coefficient_loss: 387.2472 - val_mae_inet_lambda_fv_loss: 4.6582\n",
      "Epoch 39/500\n",
      "35/35 [==============================] - 66s 2s/step - loss: 4.6473 - r2_inet_coefficient_loss: 8.4839e-04 - r2_inet_lambda_fv_loss: 3.3105 - mae_inet_coefficient_loss: 388.2697 - mae_inet_lambda_fv_loss: 4.6473 - val_loss: 4.5739 - val_r2_inet_coefficient_loss: 0.0010 - val_r2_inet_lambda_fv_loss: -0.3596 - val_mae_inet_coefficient_loss: 387.2231 - val_mae_inet_lambda_fv_loss: 4.5737\n",
      "Epoch 40/500\n",
      "35/35 [==============================] - 66s 2s/step - loss: 4.5459 - r2_inet_coefficient_loss: 8.4857e-04 - r2_inet_lambda_fv_loss: 3.0776 - mae_inet_coefficient_loss: 388.2550 - mae_inet_lambda_fv_loss: 4.5458 - val_loss: 4.5104 - val_r2_inet_coefficient_loss: 0.0010 - val_r2_inet_lambda_fv_loss: -0.3461 - val_mae_inet_coefficient_loss: 387.2067 - val_mae_inet_lambda_fv_loss: 4.5103\n",
      "Epoch 41/500\n",
      "35/35 [==============================] - 68s 2s/step - loss: 4.4518 - r2_inet_coefficient_loss: 8.5601e-04 - r2_inet_lambda_fv_loss: 2.9745 - mae_inet_coefficient_loss: 388.2464 - mae_inet_lambda_fv_loss: 4.4517 - val_loss: 4.4118 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.3846 - val_mae_inet_coefficient_loss: 387.1951 - val_mae_inet_lambda_fv_loss: 4.4117\n",
      "Epoch 42/500\n",
      "35/35 [==============================] - 66s 2s/step - loss: 4.3435 - r2_inet_coefficient_loss: 8.6081e-04 - r2_inet_lambda_fv_loss: 2.5860 - mae_inet_coefficient_loss: 388.2391 - mae_inet_lambda_fv_loss: 4.3434 - val_loss: 4.3130 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.4431 - val_mae_inet_coefficient_loss: 387.1785 - val_mae_inet_lambda_fv_loss: 4.3125\n",
      "Epoch 43/500\n",
      "35/35 [==============================] - 69s 2s/step - loss: 4.2282 - r2_inet_coefficient_loss: 8.5971e-04 - r2_inet_lambda_fv_loss: 1.9712 - mae_inet_coefficient_loss: 388.2293 - mae_inet_lambda_fv_loss: 4.2282 - val_loss: 4.1531 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.5399 - val_mae_inet_coefficient_loss: 387.1740 - val_mae_inet_lambda_fv_loss: 4.1520\n",
      "Epoch 44/500\n",
      "35/35 [==============================] - 67s 2s/step - loss: 4.1062 - r2_inet_coefficient_loss: 8.6311e-04 - r2_inet_lambda_fv_loss: 1.2862 - mae_inet_coefficient_loss: 388.2243 - mae_inet_lambda_fv_loss: 4.1062 - val_loss: 4.0195 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6093 - val_mae_inet_coefficient_loss: 387.1614 - val_mae_inet_lambda_fv_loss: 4.0181\n",
      "Epoch 45/500\n",
      "35/35 [==============================] - 64s 2s/step - loss: 3.9952 - r2_inet_coefficient_loss: 8.6313e-04 - r2_inet_lambda_fv_loss: 0.9090 - mae_inet_coefficient_loss: 388.2211 - mae_inet_lambda_fv_loss: 3.9952 - val_loss: 3.9087 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6794 - val_mae_inet_coefficient_loss: 387.1586 - val_mae_inet_lambda_fv_loss: 3.9069\n",
      "Epoch 46/500\n",
      "35/35 [==============================] - 42s 1s/step - loss: 3.8691 - r2_inet_coefficient_loss: 8.5756e-04 - r2_inet_lambda_fv_loss: 0.7932 - mae_inet_coefficient_loss: 388.2179 - mae_inet_lambda_fv_loss: 3.8691 - val_loss: 3.8381 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6910 - val_mae_inet_coefficient_loss: 387.1492 - val_mae_inet_lambda_fv_loss: 3.8366\n",
      "Epoch 47/500\n",
      "35/35 [==============================] - 18s 531ms/step - loss: 3.8133 - r2_inet_coefficient_loss: 8.5018e-04 - r2_inet_lambda_fv_loss: 0.8403 - mae_inet_coefficient_loss: 388.2141 - mae_inet_lambda_fv_loss: 3.8133 - val_loss: 3.7595 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.7210 - val_mae_inet_coefficient_loss: 387.1555 - val_mae_inet_lambda_fv_loss: 3.7576\n",
      "Epoch 48/500\n",
      "35/35 [==============================] - 19s 537ms/step - loss: 3.7222 - r2_inet_coefficient_loss: 8.3735e-04 - r2_inet_lambda_fv_loss: 0.8918 - mae_inet_coefficient_loss: 388.2145 - mae_inet_lambda_fv_loss: 3.7222 - val_loss: 3.7294 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.7027 - val_mae_inet_coefficient_loss: 387.1544 - val_mae_inet_lambda_fv_loss: 3.7280\n",
      "Epoch 49/500\n",
      "35/35 [==============================] - 17s 484ms/step - loss: 3.6802 - r2_inet_coefficient_loss: 8.2927e-04 - r2_inet_lambda_fv_loss: 1.0302 - mae_inet_coefficient_loss: 388.2161 - mae_inet_lambda_fv_loss: 3.6802 - val_loss: 3.6858 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.7010 - val_mae_inet_coefficient_loss: 387.1606 - val_mae_inet_lambda_fv_loss: 3.6845\n",
      "Epoch 50/500\n",
      "35/35 [==============================] - 18s 508ms/step - loss: 3.6369 - r2_inet_coefficient_loss: 8.1898e-04 - r2_inet_lambda_fv_loss: 1.0920 - mae_inet_coefficient_loss: 388.2179 - mae_inet_lambda_fv_loss: 3.6369 - val_loss: 3.6512 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.7009 - val_mae_inet_coefficient_loss: 387.1618 - val_mae_inet_lambda_fv_loss: 3.6498\n",
      "Epoch 51/500\n",
      "35/35 [==============================] - 17s 501ms/step - loss: 3.5913 - r2_inet_coefficient_loss: 8.0658e-04 - r2_inet_lambda_fv_loss: 1.1752 - mae_inet_coefficient_loss: 388.2163 - mae_inet_lambda_fv_loss: 3.5913 - val_loss: 3.6133 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6977 - val_mae_inet_coefficient_loss: 387.1582 - val_mae_inet_lambda_fv_loss: 3.6121\n",
      "Epoch 52/500\n",
      "35/35 [==============================] - 18s 520ms/step - loss: 3.5481 - r2_inet_coefficient_loss: 7.9441e-04 - r2_inet_lambda_fv_loss: 1.3298 - mae_inet_coefficient_loss: 388.2130 - mae_inet_lambda_fv_loss: 3.5481 - val_loss: 3.5774 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6984 - val_mae_inet_coefficient_loss: 387.1533 - val_mae_inet_lambda_fv_loss: 3.5761\n",
      "Epoch 53/500\n",
      "35/35 [==============================] - 18s 520ms/step - loss: 3.5032 - r2_inet_coefficient_loss: 7.8560e-04 - r2_inet_lambda_fv_loss: 1.3521 - mae_inet_coefficient_loss: 388.2085 - mae_inet_lambda_fv_loss: 3.5033 - val_loss: 3.5482 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6931 - val_mae_inet_coefficient_loss: 387.1530 - val_mae_inet_lambda_fv_loss: 3.5468\n",
      "Epoch 54/500\n",
      "35/35 [==============================] - 18s 525ms/step - loss: 3.4684 - r2_inet_coefficient_loss: 7.8535e-04 - r2_inet_lambda_fv_loss: 1.4740 - mae_inet_coefficient_loss: 388.2078 - mae_inet_lambda_fv_loss: 3.4684 - val_loss: 3.5246 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6825 - val_mae_inet_coefficient_loss: 387.1480 - val_mae_inet_lambda_fv_loss: 3.5232\n",
      "Epoch 55/500\n",
      "35/35 [==============================] - 18s 523ms/step - loss: 3.4281 - r2_inet_coefficient_loss: 7.8305e-04 - r2_inet_lambda_fv_loss: 1.3932 - mae_inet_coefficient_loss: 388.2070 - mae_inet_lambda_fv_loss: 3.4282 - val_loss: 3.4982 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6829 - val_mae_inet_coefficient_loss: 387.1436 - val_mae_inet_lambda_fv_loss: 3.4967\n",
      "Epoch 56/500\n",
      "35/35 [==============================] - 18s 503ms/step - loss: 3.3948 - r2_inet_coefficient_loss: 7.8169e-04 - r2_inet_lambda_fv_loss: 1.4761 - mae_inet_coefficient_loss: 388.2028 - mae_inet_lambda_fv_loss: 3.3949 - val_loss: 3.4839 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6774 - val_mae_inet_coefficient_loss: 387.1397 - val_mae_inet_lambda_fv_loss: 3.4825\n",
      "Epoch 57/500\n",
      "35/35 [==============================] - 14s 385ms/step - loss: 3.3672 - r2_inet_coefficient_loss: 7.7410e-04 - r2_inet_lambda_fv_loss: 1.2639 - mae_inet_coefficient_loss: 388.1974 - mae_inet_lambda_fv_loss: 3.3672 - val_loss: 3.4360 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.6940 - val_mae_inet_coefficient_loss: 387.1338 - val_mae_inet_lambda_fv_loss: 3.4344\n",
      "Epoch 58/500\n",
      "35/35 [==============================] - 16s 464ms/step - loss: 3.3394 - r2_inet_coefficient_loss: 7.5793e-04 - r2_inet_lambda_fv_loss: 1.4324 - mae_inet_coefficient_loss: 388.1863 - mae_inet_lambda_fv_loss: 3.3395 - val_loss: 3.4039 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.7012 - val_mae_inet_coefficient_loss: 387.1284 - val_mae_inet_lambda_fv_loss: 3.4022\n",
      "Epoch 59/500\n",
      "35/35 [==============================] - 17s 485ms/step - loss: 3.3107 - r2_inet_coefficient_loss: 7.4812e-04 - r2_inet_lambda_fv_loss: 1.1003 - mae_inet_coefficient_loss: 388.1788 - mae_inet_lambda_fv_loss: 3.3107 - val_loss: 3.3652 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.7124 - val_mae_inet_coefficient_loss: 387.1119 - val_mae_inet_lambda_fv_loss: 3.3636\n",
      "Epoch 60/500\n",
      "35/35 [==============================] - 18s 505ms/step - loss: 3.2924 - r2_inet_coefficient_loss: 7.2487e-04 - r2_inet_lambda_fv_loss: 1.3237 - mae_inet_coefficient_loss: 388.1615 - mae_inet_lambda_fv_loss: 3.2925 - val_loss: 3.3248 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.7283 - val_mae_inet_coefficient_loss: 387.1045 - val_mae_inet_lambda_fv_loss: 3.3228\n",
      "Epoch 61/500\n",
      "35/35 [==============================] - 17s 498ms/step - loss: 3.2561 - r2_inet_coefficient_loss: 7.1658e-04 - r2_inet_lambda_fv_loss: 1.0489 - mae_inet_coefficient_loss: 388.1507 - mae_inet_lambda_fv_loss: 3.2562 - val_loss: 3.3208 - val_r2_inet_coefficient_loss: 0.0010 - val_r2_inet_lambda_fv_loss: -0.7143 - val_mae_inet_coefficient_loss: 387.0858 - val_mae_inet_lambda_fv_loss: 3.3189\n",
      "Epoch 62/500\n",
      "35/35 [==============================] - 18s 501ms/step - loss: 3.2309 - r2_inet_coefficient_loss: 6.9552e-04 - r2_inet_lambda_fv_loss: 1.0313 - mae_inet_coefficient_loss: 388.1361 - mae_inet_lambda_fv_loss: 3.2309 - val_loss: 3.2869 - val_r2_inet_coefficient_loss: 0.0010 - val_r2_inet_lambda_fv_loss: -0.7220 - val_mae_inet_coefficient_loss: 387.0755 - val_mae_inet_lambda_fv_loss: 3.2850\n",
      "Epoch 63/500\n",
      "35/35 [==============================] - 17s 491ms/step - loss: 3.2128 - r2_inet_coefficient_loss: 6.7929e-04 - r2_inet_lambda_fv_loss: 0.9974 - mae_inet_coefficient_loss: 388.1224 - mae_inet_lambda_fv_loss: 3.2129 - val_loss: 3.2593 - val_r2_inet_coefficient_loss: 0.0010 - val_r2_inet_lambda_fv_loss: -0.7287 - val_mae_inet_coefficient_loss: 387.0616 - val_mae_inet_lambda_fv_loss: 3.2573\n",
      "Epoch 64/500\n",
      "35/35 [==============================] - 17s 487ms/step - loss: 3.1928 - r2_inet_coefficient_loss: 6.6055e-04 - r2_inet_lambda_fv_loss: 0.9973 - mae_inet_coefficient_loss: 388.1059 - mae_inet_lambda_fv_loss: 3.1929 - val_loss: 3.2446 - val_r2_inet_coefficient_loss: 9.9938e-04 - val_r2_inet_lambda_fv_loss: -0.7282 - val_mae_inet_coefficient_loss: 387.0465 - val_mae_inet_lambda_fv_loss: 3.2425\n",
      "Epoch 65/500\n",
      "35/35 [==============================] - 17s 491ms/step - loss: 3.1740 - r2_inet_coefficient_loss: 6.4516e-04 - r2_inet_lambda_fv_loss: 0.9593 - mae_inet_coefficient_loss: 388.0917 - mae_inet_lambda_fv_loss: 3.1740 - val_loss: 3.2262 - val_r2_inet_coefficient_loss: 9.9002e-04 - val_r2_inet_lambda_fv_loss: -0.7283 - val_mae_inet_coefficient_loss: 387.0344 - val_mae_inet_lambda_fv_loss: 3.2240\n",
      "Epoch 66/500\n",
      "35/35 [==============================] - 17s 489ms/step - loss: 3.1574 - r2_inet_coefficient_loss: 6.3428e-04 - r2_inet_lambda_fv_loss: 0.9368 - mae_inet_coefficient_loss: 388.0789 - mae_inet_lambda_fv_loss: 3.1575 - val_loss: 3.2081 - val_r2_inet_coefficient_loss: 9.7822e-04 - val_r2_inet_lambda_fv_loss: -0.7281 - val_mae_inet_coefficient_loss: 387.0198 - val_mae_inet_lambda_fv_loss: 3.2058\n",
      "Epoch 67/500\n",
      "35/35 [==============================] - 17s 487ms/step - loss: 3.1415 - r2_inet_coefficient_loss: 6.1754e-04 - r2_inet_lambda_fv_loss: 0.9511 - mae_inet_coefficient_loss: 388.0626 - mae_inet_lambda_fv_loss: 3.1415 - val_loss: 3.1874 - val_r2_inet_coefficient_loss: 9.6571e-04 - val_r2_inet_lambda_fv_loss: -0.7283 - val_mae_inet_coefficient_loss: 387.0049 - val_mae_inet_lambda_fv_loss: 3.1852\n",
      "Epoch 68/500\n",
      "35/35 [==============================] - 16s 462ms/step - loss: 3.1142 - r2_inet_coefficient_loss: 6.0204e-04 - r2_inet_lambda_fv_loss: 0.9661 - mae_inet_coefficient_loss: 388.0479 - mae_inet_lambda_fv_loss: 3.1143 - val_loss: 3.1684 - val_r2_inet_coefficient_loss: 9.5467e-04 - val_r2_inet_lambda_fv_loss: -0.7297 - val_mae_inet_coefficient_loss: 386.9898 - val_mae_inet_lambda_fv_loss: 3.1662\n",
      "Epoch 69/500\n",
      "35/35 [==============================] - 17s 485ms/step - loss: 3.0953 - r2_inet_coefficient_loss: 5.8582e-04 - r2_inet_lambda_fv_loss: 0.9644 - mae_inet_coefficient_loss: 388.0321 - mae_inet_lambda_fv_loss: 3.0953 - val_loss: 3.1404 - val_r2_inet_coefficient_loss: 9.4841e-04 - val_r2_inet_lambda_fv_loss: -0.7322 - val_mae_inet_coefficient_loss: 386.9774 - val_mae_inet_lambda_fv_loss: 3.1380\n",
      "Epoch 70/500\n",
      "35/35 [==============================] - 17s 487ms/step - loss: 3.0722 - r2_inet_coefficient_loss: 5.7611e-04 - r2_inet_lambda_fv_loss: 0.8980 - mae_inet_coefficient_loss: 388.0187 - mae_inet_lambda_fv_loss: 3.0723 - val_loss: 3.1190 - val_r2_inet_coefficient_loss: 9.4248e-04 - val_r2_inet_lambda_fv_loss: -0.7326 - val_mae_inet_coefficient_loss: 386.9644 - val_mae_inet_lambda_fv_loss: 3.1166\n",
      "Epoch 71/500\n",
      "35/35 [==============================] - 18s 501ms/step - loss: 3.0560 - r2_inet_coefficient_loss: 5.7025e-04 - r2_inet_lambda_fv_loss: 0.8605 - mae_inet_coefficient_loss: 388.0057 - mae_inet_lambda_fv_loss: 3.0560 - val_loss: 3.1029 - val_r2_inet_coefficient_loss: 9.3727e-04 - val_r2_inet_lambda_fv_loss: -0.7292 - val_mae_inet_coefficient_loss: 386.9503 - val_mae_inet_lambda_fv_loss: 3.1006\n",
      "Epoch 72/500\n",
      "35/35 [==============================] - 18s 507ms/step - loss: 3.0448 - r2_inet_coefficient_loss: 5.6309e-04 - r2_inet_lambda_fv_loss: 0.8519 - mae_inet_coefficient_loss: 387.9915 - mae_inet_lambda_fv_loss: 3.0448 - val_loss: 3.1039 - val_r2_inet_coefficient_loss: 9.3500e-04 - val_r2_inet_lambda_fv_loss: -0.7212 - val_mae_inet_coefficient_loss: 386.9374 - val_mae_inet_lambda_fv_loss: 3.1017\n",
      "Epoch 73/500\n",
      "35/35 [==============================] - 17s 501ms/step - loss: 3.0433 - r2_inet_coefficient_loss: 5.5711e-04 - r2_inet_lambda_fv_loss: 0.8642 - mae_inet_coefficient_loss: 387.9771 - mae_inet_lambda_fv_loss: 3.0434 - val_loss: 3.1087 - val_r2_inet_coefficient_loss: 9.3138e-04 - val_r2_inet_lambda_fv_loss: -0.7235 - val_mae_inet_coefficient_loss: 386.9224 - val_mae_inet_lambda_fv_loss: 3.1067\n",
      "Epoch 74/500\n",
      "35/35 [==============================] - 17s 489ms/step - loss: 3.0303 - r2_inet_coefficient_loss: 5.4711e-04 - r2_inet_lambda_fv_loss: 0.9885 - mae_inet_coefficient_loss: 387.9609 - mae_inet_lambda_fv_loss: 3.0303 - val_loss: 3.0430 - val_r2_inet_coefficient_loss: 9.3144e-04 - val_r2_inet_lambda_fv_loss: -0.7440 - val_mae_inet_coefficient_loss: 386.9102 - val_mae_inet_lambda_fv_loss: 3.0408\n",
      "Epoch 75/500\n",
      "35/35 [==============================] - 31s 892ms/step - loss: 15.1225 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: 132.4590 - mae_inet_coefficient_loss: 391.5081 - mae_inet_lambda_fv_loss: 15.1223 - val_loss: 15.1399 - val_r2_inet_coefficient_loss: 0.0071 - val_r2_inet_lambda_fv_loss: 12.1633 - val_mae_inet_coefficient_loss: 390.3830 - val_mae_inet_lambda_fv_loss: 15.1529\n",
      "Epoch 135/500\n",
      "35/35 [==============================] - 30s 861ms/step - loss: 15.0306 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: 132.0430 - mae_inet_coefficient_loss: 391.5070 - mae_inet_lambda_fv_loss: 15.0304 - val_loss: 15.0471 - val_r2_inet_coefficient_loss: 0.0071 - val_r2_inet_lambda_fv_loss: 12.0061 - val_mae_inet_coefficient_loss: 390.3825 - val_mae_inet_lambda_fv_loss: 15.0600\n",
      "Epoch 136/500\n",
      "35/35 [==============================] - 30s 859ms/step - loss: 14.9385 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: 131.7076 - mae_inet_coefficient_loss: 391.5061 - mae_inet_lambda_fv_loss: 14.9383 - val_loss: 14.9532 - val_r2_inet_coefficient_loss: 0.0071 - val_r2_inet_lambda_fv_loss: 11.8536 - val_mae_inet_coefficient_loss: 390.3822 - val_mae_inet_lambda_fv_loss: 14.9660\n",
      "Epoch 137/500\n",
      "35/35 [==============================] - 33s 971ms/step - loss: 14.8460 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: 131.3998 - mae_inet_coefficient_loss: 391.5054 - mae_inet_lambda_fv_loss: 14.8458 - val_loss: 14.8585 - val_r2_inet_coefficient_loss: 0.0072 - val_r2_inet_lambda_fv_loss: 11.7096 - val_mae_inet_coefficient_loss: 390.3822 - val_mae_inet_lambda_fv_loss: 14.8712\n",
      "Epoch 138/500\n",
      "35/35 [==============================] - 30s 852ms/step - loss: 14.7530 - r2_inet_coefficient_loss: 0.0075 - r2_inet_lambda_fv_loss: 131.0450 - mae_inet_coefficient_loss: 391.5047 - mae_inet_lambda_fv_loss: 14.7528 - val_loss: 14.7632 - val_r2_inet_coefficient_loss: 0.0072 - val_r2_inet_lambda_fv_loss: 11.5677 - val_mae_inet_coefficient_loss: 390.3823 - val_mae_inet_lambda_fv_loss: 14.7758\n",
      "Epoch 139/500\n",
      "35/35 [==============================] - 29s 843ms/step - loss: 14.6602 - r2_inet_coefficient_loss: 0.0075 - r2_inet_lambda_fv_loss: 130.7459 - mae_inet_coefficient_loss: 391.5042 - mae_inet_lambda_fv_loss: 14.6600 - val_loss: 14.6680 - val_r2_inet_coefficient_loss: 0.0073 - val_r2_inet_lambda_fv_loss: 11.4235 - val_mae_inet_coefficient_loss: 390.3826 - val_mae_inet_lambda_fv_loss: 14.6807\n",
      "Epoch 140/500\n",
      "35/35 [==============================] - 19s 542ms/step - loss: 14.5675 - r2_inet_coefficient_loss: 0.0075 - r2_inet_lambda_fv_loss: 130.5555 - mae_inet_coefficient_loss: 391.5037 - mae_inet_lambda_fv_loss: 14.5673 - val_loss: 14.5728 - val_r2_inet_coefficient_loss: 0.0073 - val_r2_inet_lambda_fv_loss: 11.2817 - val_mae_inet_coefficient_loss: 390.3831 - val_mae_inet_lambda_fv_loss: 14.5855\n",
      "Epoch 141/500\n",
      "35/35 [==============================] - 22s 642ms/step - loss: 14.4751 - r2_inet_coefficient_loss: 0.0076 - r2_inet_lambda_fv_loss: 130.3309 - mae_inet_coefficient_loss: 391.5034 - mae_inet_lambda_fv_loss: 14.4749 - val_loss: 14.4786 - val_r2_inet_coefficient_loss: 0.0074 - val_r2_inet_lambda_fv_loss: 11.1445 - val_mae_inet_coefficient_loss: 390.3838 - val_mae_inet_lambda_fv_loss: 14.4913\n",
      "Epoch 142/500\n",
      "35/35 [==============================] - 27s 784ms/step - loss: 14.3831 - r2_inet_coefficient_loss: 0.0076 - r2_inet_lambda_fv_loss: 130.1867 - mae_inet_coefficient_loss: 391.5028 - mae_inet_lambda_fv_loss: 14.3829 - val_loss: 14.3850 - val_r2_inet_coefficient_loss: 0.0074 - val_r2_inet_lambda_fv_loss: 11.0109 - val_mae_inet_coefficient_loss: 390.3840 - val_mae_inet_lambda_fv_loss: 14.3977\n",
      "Epoch 143/500\n",
      "35/35 [==============================] - 29s 816ms/step - loss: 14.2918 - r2_inet_coefficient_loss: 0.0077 - r2_inet_lambda_fv_loss: 129.9958 - mae_inet_coefficient_loss: 391.5025 - mae_inet_lambda_fv_loss: 14.2916 - val_loss: 14.2925 - val_r2_inet_coefficient_loss: 0.0074 - val_r2_inet_lambda_fv_loss: 10.8807 - val_mae_inet_coefficient_loss: 390.3845 - val_mae_inet_lambda_fv_loss: 14.3052\n",
      "Epoch 144/500\n",
      "35/35 [==============================] - 27s 772ms/step - loss: 14.2016 - r2_inet_coefficient_loss: 0.0077 - r2_inet_lambda_fv_loss: 129.8131 - mae_inet_coefficient_loss: 391.5022 - mae_inet_lambda_fv_loss: 14.2014 - val_loss: 14.2009 - val_r2_inet_coefficient_loss: 0.0075 - val_r2_inet_lambda_fv_loss: 10.7523 - val_mae_inet_coefficient_loss: 390.3847 - val_mae_inet_lambda_fv_loss: 14.2135\n",
      "Epoch 145/500\n",
      "35/35 [==============================] - 25s 696ms/step - loss: 14.1130 - r2_inet_coefficient_loss: 0.0078 - r2_inet_lambda_fv_loss: 129.6779 - mae_inet_coefficient_loss: 391.5016 - mae_inet_lambda_fv_loss: 14.1128 - val_loss: 14.1111 - val_r2_inet_coefficient_loss: 0.0075 - val_r2_inet_lambda_fv_loss: 10.6228 - val_mae_inet_coefficient_loss: 390.3836 - val_mae_inet_lambda_fv_loss: 14.1236\n",
      "Epoch 146/500\n",
      "35/35 [==============================] - 24s 686ms/step - loss: 14.0260 - r2_inet_coefficient_loss: 0.0078 - r2_inet_lambda_fv_loss: 129.6373 - mae_inet_coefficient_loss: 391.5001 - mae_inet_lambda_fv_loss: 14.0258 - val_loss: 14.0233 - val_r2_inet_coefficient_loss: 0.0076 - val_r2_inet_lambda_fv_loss: 10.4958 - val_mae_inet_coefficient_loss: 390.3823 - val_mae_inet_lambda_fv_loss: 14.0357\n",
      "Epoch 147/500\n",
      "35/35 [==============================] - 26s 736ms/step - loss: 13.9404 - r2_inet_coefficient_loss: 0.0079 - r2_inet_lambda_fv_loss: 129.5806 - mae_inet_coefficient_loss: 391.4985 - mae_inet_lambda_fv_loss: 13.9402 - val_loss: 13.9374 - val_r2_inet_coefficient_loss: 0.0076 - val_r2_inet_lambda_fv_loss: 10.3679 - val_mae_inet_coefficient_loss: 390.3810 - val_mae_inet_lambda_fv_loss: 13.9495\n",
      "Epoch 148/500\n",
      "35/35 [==============================] - 32s 941ms/step - loss: 13.8565 - r2_inet_coefficient_loss: 0.0079 - r2_inet_lambda_fv_loss: 129.5136 - mae_inet_coefficient_loss: 391.4968 - mae_inet_lambda_fv_loss: 13.8563 - val_loss: 13.8535 - val_r2_inet_coefficient_loss: 0.0077 - val_r2_inet_lambda_fv_loss: 10.2416 - val_mae_inet_coefficient_loss: 390.3799 - val_mae_inet_lambda_fv_loss: 13.8653\n",
      "Epoch 149/500\n",
      "35/35 [==============================] - 32s 920ms/step - loss: 13.7742 - r2_inet_coefficient_loss: 0.0080 - r2_inet_lambda_fv_loss: 129.4695 - mae_inet_coefficient_loss: 391.4952 - mae_inet_lambda_fv_loss: 13.7740 - val_loss: 13.7711 - val_r2_inet_coefficient_loss: 0.0077 - val_r2_inet_lambda_fv_loss: 10.1195 - val_mae_inet_coefficient_loss: 390.3794 - val_mae_inet_lambda_fv_loss: 13.7827\n",
      "Epoch 150/500\n",
      "35/35 [==============================] - 29s 807ms/step - loss: 13.6933 - r2_inet_coefficient_loss: 0.0080 - r2_inet_lambda_fv_loss: 129.4352 - mae_inet_coefficient_loss: 391.4937 - mae_inet_lambda_fv_loss: 13.6931 - val_loss: 13.6902 - val_r2_inet_coefficient_loss: 0.0078 - val_r2_inet_lambda_fv_loss: 9.9986 - val_mae_inet_coefficient_loss: 390.3790 - val_mae_inet_lambda_fv_loss: 13.7014\n",
      "Epoch 151/500\n",
      "35/35 [==============================] - 30s 863ms/step - loss: 13.6132 - r2_inet_coefficient_loss: 0.0081 - r2_inet_lambda_fv_loss: 129.4826 - mae_inet_coefficient_loss: 391.4925 - mae_inet_lambda_fv_loss: 13.6130 - val_loss: 13.6100 - val_r2_inet_coefficient_loss: 0.0079 - val_r2_inet_lambda_fv_loss: 9.8874 - val_mae_inet_coefficient_loss: 390.3799 - val_mae_inet_lambda_fv_loss: 13.6209\n",
      "Epoch 152/500\n",
      "35/35 [==============================] - 21s 611ms/step - loss: 13.5335 - r2_inet_coefficient_loss: 0.0081 - r2_inet_lambda_fv_loss: 129.7000 - mae_inet_coefficient_loss: 391.4928 - mae_inet_lambda_fv_loss: 13.5333 - val_loss: 13.5298 - val_r2_inet_coefficient_loss: 0.0079 - val_r2_inet_lambda_fv_loss: 9.7838 - val_mae_inet_coefficient_loss: 390.3816 - val_mae_inet_lambda_fv_loss: 13.5403\n",
      "Epoch 153/500\n",
      "35/35 [==============================] - 21s 596ms/step - loss: 13.4536 - r2_inet_coefficient_loss: 0.0082 - r2_inet_lambda_fv_loss: 129.9208 - mae_inet_coefficient_loss: 391.4935 - mae_inet_lambda_fv_loss: 13.4534 - val_loss: 13.4491 - val_r2_inet_coefficient_loss: 0.0080 - val_r2_inet_lambda_fv_loss: 9.6929 - val_mae_inet_coefficient_loss: 390.3847 - val_mae_inet_lambda_fv_loss: 13.4591\n",
      "Epoch 154/500\n",
      "35/35 [==============================] - 27s 779ms/step - loss: 13.3727 - r2_inet_coefficient_loss: 0.0083 - r2_inet_lambda_fv_loss: 130.2915 - mae_inet_coefficient_loss: 391.4950 - mae_inet_lambda_fv_loss: 13.3725 - val_loss: 13.3694 - val_r2_inet_coefficient_loss: 0.0080 - val_r2_inet_lambda_fv_loss: 9.6040 - val_mae_inet_coefficient_loss: 390.3878 - val_mae_inet_lambda_fv_loss: 13.3789\n",
      "Epoch 155/500\n",
      "35/35 [==============================] - 32s 912ms/step - loss: 13.2930 - r2_inet_coefficient_loss: 0.0083 - r2_inet_lambda_fv_loss: 130.5613 - mae_inet_coefficient_loss: 391.4965 - mae_inet_lambda_fv_loss: 13.2928 - val_loss: 13.2926 - val_r2_inet_coefficient_loss: 0.0081 - val_r2_inet_lambda_fv_loss: 9.5072 - val_mae_inet_coefficient_loss: 390.3913 - val_mae_inet_lambda_fv_loss: 13.3016\n",
      "Epoch 156/500\n",
      "35/35 [==============================] - 26s 747ms/step - loss: 13.2160 - r2_inet_coefficient_loss: 0.0084 - r2_inet_lambda_fv_loss: 130.6476 - mae_inet_coefficient_loss: 391.4984 - mae_inet_lambda_fv_loss: 13.2157 - val_loss: 13.2182 - val_r2_inet_coefficient_loss: 0.0081 - val_r2_inet_lambda_fv_loss: 9.4101 - val_mae_inet_coefficient_loss: 390.3956 - val_mae_inet_lambda_fv_loss: 13.2268\n",
      "Epoch 157/500\n",
      "35/35 [==============================] - 40s 1s/step - loss: 13.1417 - r2_inet_coefficient_loss: 0.0084 - r2_inet_lambda_fv_loss: 130.7008 - mae_inet_coefficient_loss: 391.5008 - mae_inet_lambda_fv_loss: 13.1415 - val_loss: 13.1470 - val_r2_inet_coefficient_loss: 0.0082 - val_r2_inet_lambda_fv_loss: 9.3080 - val_mae_inet_coefficient_loss: 390.4000 - val_mae_inet_lambda_fv_loss: 13.1552\n",
      "Epoch 158/500\n",
      "35/35 [==============================] - 46s 1s/step - loss: 13.0701 - r2_inet_coefficient_loss: 0.0084 - r2_inet_lambda_fv_loss: 130.6755 - mae_inet_coefficient_loss: 391.5029 - mae_inet_lambda_fv_loss: 13.0699 - val_loss: 13.0781 - val_r2_inet_coefficient_loss: 0.0082 - val_r2_inet_lambda_fv_loss: 9.2033 - val_mae_inet_coefficient_loss: 390.4051 - val_mae_inet_lambda_fv_loss: 13.0858\n",
      "Epoch 159/500\n",
      "35/35 [==============================] - 48s 1s/step - loss: 13.0008 - r2_inet_coefficient_loss: 0.0085 - r2_inet_lambda_fv_loss: 130.6246 - mae_inet_coefficient_loss: 391.5053 - mae_inet_lambda_fv_loss: 13.0006 - val_loss: 13.0112 - val_r2_inet_coefficient_loss: 0.0082 - val_r2_inet_lambda_fv_loss: 9.0927 - val_mae_inet_coefficient_loss: 390.4102 - val_mae_inet_lambda_fv_loss: 13.0187\n",
      "Epoch 160/500\n",
      "35/35 [==============================] - 40s 1s/step - loss: 12.9332 - r2_inet_coefficient_loss: 0.0085 - r2_inet_lambda_fv_loss: 130.5798 - mae_inet_coefficient_loss: 391.5076 - mae_inet_lambda_fv_loss: 12.9330 - val_loss: 12.9467 - val_r2_inet_coefficient_loss: 0.0083 - val_r2_inet_lambda_fv_loss: 8.9790 - val_mae_inet_coefficient_loss: 390.4154 - val_mae_inet_lambda_fv_loss: 12.9539\n",
      "Epoch 161/500\n",
      "35/35 [==============================] - 47s 1s/step - loss: 12.8673 - r2_inet_coefficient_loss: 0.0085 - r2_inet_lambda_fv_loss: 130.5029 - mae_inet_coefficient_loss: 391.5096 - mae_inet_lambda_fv_loss: 12.8671 - val_loss: 12.8846 - val_r2_inet_coefficient_loss: 0.0083 - val_r2_inet_lambda_fv_loss: 8.8683 - val_mae_inet_coefficient_loss: 390.4201 - val_mae_inet_lambda_fv_loss: 12.8915\n",
      "Epoch 162/500\n",
      "35/35 [==============================] - 43s 1s/step - loss: 12.8030 - r2_inet_coefficient_loss: 0.0086 - r2_inet_lambda_fv_loss: 130.4193 - mae_inet_coefficient_loss: 391.5117 - mae_inet_lambda_fv_loss: 12.8028 - val_loss: 12.8236 - val_r2_inet_coefficient_loss: 0.0083 - val_r2_inet_lambda_fv_loss: 8.7588 - val_mae_inet_coefficient_loss: 390.4245 - val_mae_inet_lambda_fv_loss: 12.8303\n",
      "Epoch 163/500\n",
      "35/35 [==============================] - 42s 1s/step - loss: 12.7402 - r2_inet_coefficient_loss: 0.0086 - r2_inet_lambda_fv_loss: 130.3497 - mae_inet_coefficient_loss: 391.5137 - mae_inet_lambda_fv_loss: 12.7399 - val_loss: 12.7649 - val_r2_inet_coefficient_loss: 0.0084 - val_r2_inet_lambda_fv_loss: 8.6493 - val_mae_inet_coefficient_loss: 390.4289 - val_mae_inet_lambda_fv_loss: 12.7713\n",
      "Epoch 164/500\n",
      "35/35 [==============================] - 45s 1s/step - loss: 12.6789 - r2_inet_coefficient_loss: 0.0086 - r2_inet_lambda_fv_loss: 130.3401 - mae_inet_coefficient_loss: 391.5159 - mae_inet_lambda_fv_loss: 12.6786 - val_loss: 12.7074 - val_r2_inet_coefficient_loss: 0.0084 - val_r2_inet_lambda_fv_loss: 8.5423 - val_mae_inet_coefficient_loss: 390.4335 - val_mae_inet_lambda_fv_loss: 12.7135\n",
      "Epoch 165/500\n",
      "35/35 [==============================] - 45s 1s/step - loss: 12.6192 - r2_inet_coefficient_loss: 0.0087 - r2_inet_lambda_fv_loss: 130.4085 - mae_inet_coefficient_loss: 391.5182 - mae_inet_lambda_fv_loss: 12.6190 - val_loss: 12.6517 - val_r2_inet_coefficient_loss: 0.0084 - val_r2_inet_lambda_fv_loss: 8.4353 - val_mae_inet_coefficient_loss: 390.4388 - val_mae_inet_lambda_fv_loss: 12.6577\n",
      "Epoch 166/500\n",
      "35/35 [==============================] - 44s 1s/step - loss: 12.5614 - r2_inet_coefficient_loss: 0.0087 - r2_inet_lambda_fv_loss: 130.4591 - mae_inet_coefficient_loss: 391.5203 - mae_inet_lambda_fv_loss: 12.5612 - val_loss: 12.5985 - val_r2_inet_coefficient_loss: 0.0085 - val_r2_inet_lambda_fv_loss: 8.3298 - val_mae_inet_coefficient_loss: 390.4444 - val_mae_inet_lambda_fv_loss: 12.6042\n",
      "Epoch 167/500\n",
      "35/35 [==============================] - 45s 1s/step - loss: 12.5058 - r2_inet_coefficient_loss: 0.0087 - r2_inet_lambda_fv_loss: 130.4527 - mae_inet_coefficient_loss: 391.5226 - mae_inet_lambda_fv_loss: 12.5055 - val_loss: 12.5466 - val_r2_inet_coefficient_loss: 0.0085 - val_r2_inet_lambda_fv_loss: 8.2247 - val_mae_inet_coefficient_loss: 390.4498 - val_mae_inet_lambda_fv_loss: 12.5521\n",
      "Epoch 168/500\n",
      "35/35 [==============================] - 45s 1s/step - loss: 12.4518 - r2_inet_coefficient_loss: 0.0087 - r2_inet_lambda_fv_loss: 130.4097 - mae_inet_coefficient_loss: 391.5248 - mae_inet_lambda_fv_loss: 12.4516 - val_loss: 12.4965 - val_r2_inet_coefficient_loss: 0.0085 - val_r2_inet_lambda_fv_loss: 8.1227 - val_mae_inet_coefficient_loss: 390.4559 - val_mae_inet_lambda_fv_loss: 12.5019\n",
      "Epoch 169/500\n",
      "35/35 [==============================] - 48s 1s/step - loss: 12.3997 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 130.3153 - mae_inet_coefficient_loss: 391.5275 - mae_inet_lambda_fv_loss: 12.3995 - val_loss: 12.4485 - val_r2_inet_coefficient_loss: 0.0085 - val_r2_inet_lambda_fv_loss: 8.0212 - val_mae_inet_coefficient_loss: 390.4623 - val_mae_inet_lambda_fv_loss: 12.4537\n",
      "Epoch 170/500\n",
      "35/35 [==============================] - 47s 1s/step - loss: 12.3497 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 130.2100 - mae_inet_coefficient_loss: 391.5304 - mae_inet_lambda_fv_loss: 12.3495 - val_loss: 12.4026 - val_r2_inet_coefficient_loss: 0.0085 - val_r2_inet_lambda_fv_loss: 7.9206 - val_mae_inet_coefficient_loss: 390.4685 - val_mae_inet_lambda_fv_loss: 12.4077\n",
      "Epoch 171/500\n",
      "35/35 [==============================] - 45s 1s/step - loss: 12.3018 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 130.0915 - mae_inet_coefficient_loss: 391.5335 - mae_inet_lambda_fv_loss: 12.3016 - val_loss: 12.3588 - val_r2_inet_coefficient_loss: 0.0086 - val_r2_inet_lambda_fv_loss: 7.8210 - val_mae_inet_coefficient_loss: 390.4748 - val_mae_inet_lambda_fv_loss: 12.3639\n",
      "Epoch 172/500\n",
      "35/35 [==============================] - 49s 1s/step - loss: 12.2559 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 129.9727 - mae_inet_coefficient_loss: 391.5370 - mae_inet_lambda_fv_loss: 12.2557 - val_loss: 12.3169 - val_r2_inet_coefficient_loss: 0.0086 - val_r2_inet_lambda_fv_loss: 7.7228 - val_mae_inet_coefficient_loss: 390.4815 - val_mae_inet_lambda_fv_loss: 12.3219\n",
      "Epoch 173/500\n",
      "35/35 [==============================] - 43s 1s/step - loss: 12.2121 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 129.8455 - mae_inet_coefficient_loss: 391.5409 - mae_inet_lambda_fv_loss: 12.2119 - val_loss: 12.2768 - val_r2_inet_coefficient_loss: 0.0086 - val_r2_inet_lambda_fv_loss: 7.6239 - val_mae_inet_coefficient_loss: 390.4882 - val_mae_inet_lambda_fv_loss: 12.2817\n",
      "Epoch 174/500\n",
      "35/35 [==============================] - 50s 1s/step - loss: 12.1703 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 129.7685 - mae_inet_coefficient_loss: 391.5449 - mae_inet_lambda_fv_loss: 12.1701 - val_loss: 12.2384 - val_r2_inet_coefficient_loss: 0.0086 - val_r2_inet_lambda_fv_loss: 7.5271 - val_mae_inet_coefficient_loss: 390.4950 - val_mae_inet_lambda_fv_loss: 12.2434\n",
      "Epoch 175/500\n",
      "35/35 [==============================] - 45s 1s/step - loss: 12.1305 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 129.6301 - mae_inet_coefficient_loss: 391.5491 - mae_inet_lambda_fv_loss: 12.1303 - val_loss: 12.2015 - val_r2_inet_coefficient_loss: 0.0086 - val_r2_inet_lambda_fv_loss: 7.4322 - val_mae_inet_coefficient_loss: 390.5018 - val_mae_inet_lambda_fv_loss: 12.2065\n",
      "Epoch 176/500\n",
      "35/35 [==============================] - 42s 1s/step - loss: 12.0922 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 129.3611 - mae_inet_coefficient_loss: 391.5532 - mae_inet_lambda_fv_loss: 12.0920 - val_loss: 12.1658 - val_r2_inet_coefficient_loss: 0.0086 - val_r2_inet_lambda_fv_loss: 7.3391 - val_mae_inet_coefficient_loss: 390.5077 - val_mae_inet_lambda_fv_loss: 12.1708\n",
      "Epoch 177/500\n",
      "35/35 [==============================] - 21s 605ms/step - loss: 12.0554 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 129.1655 - mae_inet_coefficient_loss: 391.5569 - mae_inet_lambda_fv_loss: 12.0552 - val_loss: 12.1321 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 7.2506 - val_mae_inet_coefficient_loss: 390.5135 - val_mae_inet_lambda_fv_loss: 12.1371\n",
      "Epoch 178/500\n",
      "35/35 [==============================] - 22s 626ms/step - loss: 12.0202 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 128.9349 - mae_inet_coefficient_loss: 391.5606 - mae_inet_lambda_fv_loss: 12.0200 - val_loss: 12.1000 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 7.1685 - val_mae_inet_coefficient_loss: 390.5187 - val_mae_inet_lambda_fv_loss: 12.1051\n",
      "Epoch 179/500\n",
      "35/35 [==============================] - 20s 577ms/step - loss: 11.9866 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 128.6523 - mae_inet_coefficient_loss: 391.5638 - mae_inet_lambda_fv_loss: 11.9864 - val_loss: 12.0692 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 7.0888 - val_mae_inet_coefficient_loss: 390.5234 - val_mae_inet_lambda_fv_loss: 12.0743\n",
      "Epoch 180/500\n",
      "35/35 [==============================] - 18s 529ms/step - loss: 11.9546 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 128.3294 - mae_inet_coefficient_loss: 391.5664 - mae_inet_lambda_fv_loss: 11.9544 - val_loss: 12.0393 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 7.0116 - val_mae_inet_coefficient_loss: 390.5274 - val_mae_inet_lambda_fv_loss: 12.0445\n",
      "Epoch 181/500\n",
      "35/35 [==============================] - 17s 499ms/step - loss: 11.9238 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 128.0491 - mae_inet_coefficient_loss: 391.5684 - mae_inet_lambda_fv_loss: 11.9236 - val_loss: 12.0104 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.9361 - val_mae_inet_coefficient_loss: 390.5303 - val_mae_inet_lambda_fv_loss: 12.0155\n",
      "Epoch 182/500\n",
      "35/35 [==============================] - 33s 945ms/step - loss: 11.8942 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 127.7449 - mae_inet_coefficient_loss: 391.5697 - mae_inet_lambda_fv_loss: 11.8940 - val_loss: 11.9823 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.8640 - val_mae_inet_coefficient_loss: 390.5323 - val_mae_inet_lambda_fv_loss: 11.9875\n",
      "Epoch 183/500\n",
      "35/35 [==============================] - 28s 785ms/step - loss: 11.8655 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 127.4731 - mae_inet_coefficient_loss: 391.5701 - mae_inet_lambda_fv_loss: 11.8653 - val_loss: 11.9552 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.7931 - val_mae_inet_coefficient_loss: 390.5341 - val_mae_inet_lambda_fv_loss: 11.9604\n",
      "Epoch 184/500\n",
      "35/35 [==============================] - 30s 865ms/step - loss: 11.8376 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 127.1563 - mae_inet_coefficient_loss: 391.5704 - mae_inet_lambda_fv_loss: 11.8374 - val_loss: 11.9290 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.7259 - val_mae_inet_coefficient_loss: 390.5355 - val_mae_inet_lambda_fv_loss: 11.9342\n",
      "Epoch 185/500\n",
      "35/35 [==============================] - 29s 828ms/step - loss: 11.8108 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 126.7480 - mae_inet_coefficient_loss: 391.5701 - mae_inet_lambda_fv_loss: 11.8106 - val_loss: 11.9036 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.6606 - val_mae_inet_coefficient_loss: 390.5361 - val_mae_inet_lambda_fv_loss: 11.9089\n",
      "Epoch 186/500\n",
      "35/35 [==============================] - 30s 848ms/step - loss: 11.7847 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 126.4089 - mae_inet_coefficient_loss: 391.5691 - mae_inet_lambda_fv_loss: 11.7845 - val_loss: 11.8789 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.5971 - val_mae_inet_coefficient_loss: 390.5362 - val_mae_inet_lambda_fv_loss: 11.8843\n",
      "Epoch 187/500\n",
      "35/35 [==============================] - 30s 858ms/step - loss: 11.7594 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 126.0442 - mae_inet_coefficient_loss: 391.5677 - mae_inet_lambda_fv_loss: 11.7592 - val_loss: 11.8548 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.5353 - val_mae_inet_coefficient_loss: 390.5361 - val_mae_inet_lambda_fv_loss: 11.8603\n",
      "Epoch 188/500\n",
      "35/35 [==============================] - 31s 893ms/step - loss: 11.7347 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 125.7212 - mae_inet_coefficient_loss: 391.5661 - mae_inet_lambda_fv_loss: 11.7346 - val_loss: 11.8315 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.4747 - val_mae_inet_coefficient_loss: 390.5353 - val_mae_inet_lambda_fv_loss: 11.8371\n",
      "Epoch 189/500\n",
      "35/35 [==============================] - 28s 806ms/step - loss: 11.7107 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 125.3376 - mae_inet_coefficient_loss: 391.5639 - mae_inet_lambda_fv_loss: 11.7105 - val_loss: 11.8088 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.4159 - val_mae_inet_coefficient_loss: 390.5342 - val_mae_inet_lambda_fv_loss: 11.8145\n",
      "Epoch 190/500\n",
      "35/35 [==============================] - 31s 895ms/step - loss: 11.6872 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 124.9977 - mae_inet_coefficient_loss: 391.5613 - mae_inet_lambda_fv_loss: 11.6871 - val_loss: 11.7867 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.3596 - val_mae_inet_coefficient_loss: 390.5328 - val_mae_inet_lambda_fv_loss: 11.7924\n",
      "Epoch 191/500\n",
      "35/35 [==============================] - 33s 949ms/step - loss: 11.6645 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 124.6480 - mae_inet_coefficient_loss: 391.5584 - mae_inet_lambda_fv_loss: 11.6643 - val_loss: 11.7652 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.3043 - val_mae_inet_coefficient_loss: 390.5310 - val_mae_inet_lambda_fv_loss: 11.7710\n",
      "Epoch 192/500\n",
      "35/35 [==============================] - 28s 818ms/step - loss: 11.6423 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 124.2776 - mae_inet_coefficient_loss: 391.5552 - mae_inet_lambda_fv_loss: 11.6422 - val_loss: 11.7440 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.2533 - val_mae_inet_coefficient_loss: 390.5291 - val_mae_inet_lambda_fv_loss: 11.7499\n",
      "Epoch 193/500\n",
      "35/35 [==============================] - 29s 828ms/step - loss: 11.6207 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 123.9236 - mae_inet_coefficient_loss: 391.5519 - mae_inet_lambda_fv_loss: 11.6206 - val_loss: 11.7236 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.2046 - val_mae_inet_coefficient_loss: 390.5268 - val_mae_inet_lambda_fv_loss: 11.7295\n",
      "Epoch 194/500\n",
      "35/35 [==============================] - 34s 965ms/step - loss: 11.5997 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 123.7191 - mae_inet_coefficient_loss: 391.5484 - mae_inet_lambda_fv_loss: 11.5996 - val_loss: 11.7035 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.1576 - val_mae_inet_coefficient_loss: 390.5245 - val_mae_inet_lambda_fv_loss: 11.7094\n",
      "Epoch 195/500\n",
      "35/35 [==============================] - 30s 858ms/step - loss: 11.5791 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 123.5652 - mae_inet_coefficient_loss: 391.5448 - mae_inet_lambda_fv_loss: 11.5789 - val_loss: 11.6837 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.1121 - val_mae_inet_coefficient_loss: 390.5220 - val_mae_inet_lambda_fv_loss: 11.6897\n",
      "Epoch 196/500\n",
      "35/35 [==============================] - 37s 1s/step - loss: 11.5587 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 123.3900 - mae_inet_coefficient_loss: 391.5409 - mae_inet_lambda_fv_loss: 11.5586 - val_loss: 11.6641 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.0673 - val_mae_inet_coefficient_loss: 390.5193 - val_mae_inet_lambda_fv_loss: 11.6701\n",
      "Epoch 197/500\n",
      "35/35 [==============================] - 46s 1s/step - loss: 11.5387 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 123.1837 - mae_inet_coefficient_loss: 391.5369 - mae_inet_lambda_fv_loss: 11.5386 - val_loss: 11.6450 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 6.0234 - val_mae_inet_coefficient_loss: 390.5166 - val_mae_inet_lambda_fv_loss: 11.6511\n",
      "Epoch 198/500\n",
      "35/35 [==============================] - 45s 1s/step - loss: 11.5191 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 122.9661 - mae_inet_coefficient_loss: 391.5328 - mae_inet_lambda_fv_loss: 11.5190 - val_loss: 11.6263 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.9801 - val_mae_inet_coefficient_loss: 390.5138 - val_mae_inet_lambda_fv_loss: 11.6324\n",
      "Epoch 199/500\n",
      "35/35 [==============================] - 48s 1s/step - loss: 11.4999 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 122.8038 - mae_inet_coefficient_loss: 391.5286 - mae_inet_lambda_fv_loss: 11.4998 - val_loss: 11.6080 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.9381 - val_mae_inet_coefficient_loss: 390.5108 - val_mae_inet_lambda_fv_loss: 11.6142\n",
      "Epoch 200/500\n",
      "35/35 [==============================] - 20s 545ms/step - loss: 11.4811 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 122.5914 - mae_inet_coefficient_loss: 391.5242 - mae_inet_lambda_fv_loss: 11.4810 - val_loss: 11.5901 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.8980 - val_mae_inet_coefficient_loss: 390.5074 - val_mae_inet_lambda_fv_loss: 11.5963\n",
      "Epoch 201/500\n",
      "35/35 [==============================] - 17s 492ms/step - loss: 11.4627 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 122.3555 - mae_inet_coefficient_loss: 391.5196 - mae_inet_lambda_fv_loss: 11.4625 - val_loss: 11.5724 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.8596 - val_mae_inet_coefficient_loss: 390.5038 - val_mae_inet_lambda_fv_loss: 11.5786\n",
      "Epoch 202/500\n",
      "35/35 [==============================] - 17s 476ms/step - loss: 11.4445 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 122.0304 - mae_inet_coefficient_loss: 391.5147 - mae_inet_lambda_fv_loss: 11.4443 - val_loss: 11.5551 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.8224 - val_mae_inet_coefficient_loss: 390.4999 - val_mae_inet_lambda_fv_loss: 11.5613\n",
      "Epoch 203/500\n",
      "35/35 [==============================] - 17s 477ms/step - loss: 11.4265 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 121.7135 - mae_inet_coefficient_loss: 391.5098 - mae_inet_lambda_fv_loss: 11.4264 - val_loss: 11.5380 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.7858 - val_mae_inet_coefficient_loss: 390.4961 - val_mae_inet_lambda_fv_loss: 11.5442\n",
      "Epoch 204/500\n",
      "35/35 [==============================] - 17s 480ms/step - loss: 11.4089 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 121.4092 - mae_inet_coefficient_loss: 391.5049 - mae_inet_lambda_fv_loss: 11.4088 - val_loss: 11.5211 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.7508 - val_mae_inet_coefficient_loss: 390.4922 - val_mae_inet_lambda_fv_loss: 11.5274\n",
      "Epoch 205/500\n",
      "35/35 [==============================] - 24s 678ms/step - loss: 11.3917 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 121.1205 - mae_inet_coefficient_loss: 391.5000 - mae_inet_lambda_fv_loss: 11.3915 - val_loss: 11.5042 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.7158 - val_mae_inet_coefficient_loss: 390.4882 - val_mae_inet_lambda_fv_loss: 11.5105\n",
      "Epoch 206/500\n",
      "35/35 [==============================] - 28s 798ms/step - loss: 11.3747 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 120.8371 - mae_inet_coefficient_loss: 391.4951 - mae_inet_lambda_fv_loss: 11.3746 - val_loss: 11.4876 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.6820 - val_mae_inet_coefficient_loss: 390.4843 - val_mae_inet_lambda_fv_loss: 11.4939\n",
      "Epoch 207/500\n",
      "35/35 [==============================] - 23s 652ms/step - loss: 11.3580 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 120.5855 - mae_inet_coefficient_loss: 391.4902 - mae_inet_lambda_fv_loss: 11.3578 - val_loss: 11.4713 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.6493 - val_mae_inet_coefficient_loss: 390.4803 - val_mae_inet_lambda_fv_loss: 11.4777\n",
      "Epoch 208/500\n",
      "35/35 [==============================] - 23s 644ms/step - loss: 11.3415 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 120.3379 - mae_inet_coefficient_loss: 391.4853 - mae_inet_lambda_fv_loss: 11.3414 - val_loss: 11.4555 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.6176 - val_mae_inet_coefficient_loss: 390.4762 - val_mae_inet_lambda_fv_loss: 11.4619\n",
      "Epoch 209/500\n",
      "35/35 [==============================] - 19s 534ms/step - loss: 11.3253 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 120.0450 - mae_inet_coefficient_loss: 391.4805 - mae_inet_lambda_fv_loss: 11.3252 - val_loss: 11.4399 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.5871 - val_mae_inet_coefficient_loss: 390.4722 - val_mae_inet_lambda_fv_loss: 11.4463\n",
      "Epoch 210/500\n",
      "35/35 [==============================] - 30s 846ms/step - loss: 11.3093 - r2_inet_coefficient_loss: 0.0089 - r2_inet_lambda_fv_loss: 119.7799 - mae_inet_coefficient_loss: 391.4756 - mae_inet_lambda_fv_loss: 11.3092 - val_loss: 11.4243 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.5572 - val_mae_inet_coefficient_loss: 390.4680 - val_mae_inet_lambda_fv_loss: 11.4307\n",
      "Epoch 211/500\n",
      "35/35 [==============================] - 29s 833ms/step - loss: 11.2936 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 119.5193 - mae_inet_coefficient_loss: 391.4707 - mae_inet_lambda_fv_loss: 11.2935 - val_loss: 11.4089 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.5276 - val_mae_inet_coefficient_loss: 390.4639 - val_mae_inet_lambda_fv_loss: 11.4153\n",
      "Epoch 212/500\n",
      "35/35 [==============================] - 28s 814ms/step - loss: 11.2781 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 119.2719 - mae_inet_coefficient_loss: 391.4659 - mae_inet_lambda_fv_loss: 11.2780 - val_loss: 11.3937 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.4983 - val_mae_inet_coefficient_loss: 390.4597 - val_mae_inet_lambda_fv_loss: 11.4001\n",
      "Epoch 213/500\n",
      "35/35 [==============================] - 36s 1s/step - loss: 11.2629 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 119.0275 - mae_inet_coefficient_loss: 391.4610 - mae_inet_lambda_fv_loss: 11.2628 - val_loss: 11.3786 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.4697 - val_mae_inet_coefficient_loss: 390.4553 - val_mae_inet_lambda_fv_loss: 11.3851\n",
      "Epoch 214/500\n",
      "35/35 [==============================] - 29s 851ms/step - loss: 11.2479 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 118.7494 - mae_inet_coefficient_loss: 391.4558 - mae_inet_lambda_fv_loss: 11.2477 - val_loss: 11.3638 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.4426 - val_mae_inet_coefficient_loss: 390.4505 - val_mae_inet_lambda_fv_loss: 11.3703\n",
      "Epoch 215/500\n",
      "35/35 [==============================] - 30s 859ms/step - loss: 11.2330 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 118.4523 - mae_inet_coefficient_loss: 391.4503 - mae_inet_lambda_fv_loss: 11.2328 - val_loss: 11.3493 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.4162 - val_mae_inet_coefficient_loss: 390.4457 - val_mae_inet_lambda_fv_loss: 11.3558\n",
      "Epoch 216/500\n",
      "35/35 [==============================] - 31s 893ms/step - loss: 11.2182 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 118.1714 - mae_inet_coefficient_loss: 391.4449 - mae_inet_lambda_fv_loss: 11.2181 - val_loss: 11.3344 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.3897 - val_mae_inet_coefficient_loss: 390.4408 - val_mae_inet_lambda_fv_loss: 11.3409\n",
      "Epoch 217/500\n",
      "35/35 [==============================] - 31s 903ms/step - loss: 11.2036 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 117.8971 - mae_inet_coefficient_loss: 391.4394 - mae_inet_lambda_fv_loss: 11.2035 - val_loss: 11.3197 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.3636 - val_mae_inet_coefficient_loss: 390.4358 - val_mae_inet_lambda_fv_loss: 11.3263\n",
      "Epoch 218/500\n",
      "35/35 [==============================] - 33s 913ms/step - loss: 11.1892 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 117.6467 - mae_inet_coefficient_loss: 391.4338 - mae_inet_lambda_fv_loss: 11.1891 - val_loss: 11.3051 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.3372 - val_mae_inet_coefficient_loss: 390.4307 - val_mae_inet_lambda_fv_loss: 11.3116\n",
      "Epoch 219/500\n",
      "35/35 [==============================] - 31s 883ms/step - loss: 11.1749 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 117.4039 - mae_inet_coefficient_loss: 391.4282 - mae_inet_lambda_fv_loss: 11.1748 - val_loss: 11.2905 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.3101 - val_mae_inet_coefficient_loss: 390.4257 - val_mae_inet_lambda_fv_loss: 11.2970\n",
      "Epoch 220/500\n",
      "35/35 [==============================] - 28s 823ms/step - loss: 11.1607 - r2_inet_coefficient_loss: 0.0088 - r2_inet_lambda_fv_loss: 117.1403 - mae_inet_coefficient_loss: 391.4226 - mae_inet_lambda_fv_loss: 11.1606 - val_loss: 11.2760 - val_r2_inet_coefficient_loss: 0.0087 - val_r2_inet_lambda_fv_loss: 5.2834 - val_mae_inet_coefficient_loss: 390.4207 - val_mae_inet_lambda_fv_loss: 11.2826\n",
      "Epoch 221/500\n",
      "10/35 [=======>......................] - ETA: 36s - loss: 11.1632 - r2_inet_coefficient_loss: 0.0091 - r2_inet_lambda_fv_loss: 7.5695 - mae_inet_coefficient_loss: 381.1121 - mae_inet_lambda_fv_loss: 11.1632"
     ]
    }
   ],
   "source": [
    "(history_list, \n",
    "\n",
    "#scores_valid_list,\n",
    "scores_test_list, \n",
    "\n",
    "#function_values_valid_list, \n",
    "function_values_test_list, \n",
    "\n",
    "#polynomial_dict_valid_list,\n",
    "polynomial_dict_test_list,\n",
    "\n",
    "#distrib_dict_valid_list,\n",
    "distrib_dict_test_list,\n",
    "\n",
    "model_list) = calculate_interpretation_net_results(lambda_net_train_dataset_list, \n",
    "                                                   lambda_net_valid_dataset_list, \n",
    "                                                   lambda_net_test_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize = tf.constant([float(i) for i in range(interpretation_net_output_shape)])\n",
    "\n",
    "if interpretation_net_output_monomials != None:\n",
    "    poly_optimize_coeffs = poly_optimize[:interpretation_net_output_monomials]\n",
    "\n",
    "    poly_optimize_identifiers_list = []\n",
    "    for i in range(interpretation_net_output_monomials):\n",
    "        poly_optimize_identifiers = tf.math.softmax(poly_optimize[sparsity*i+interpretation_net_output_monomials:sparsity*(i+1)+interpretation_net_output_monomials])\n",
    "        poly_optimize_identifiers_list.append(poly_optimize_identifiers)\n",
    "    poly_optimize_identifiers_list = tf.keras.backend.flatten(poly_optimize_identifiers_list)\n",
    "    poly_optimize = tf.concat([poly_optimize_coeffs, poly_optimize_identifiers_list], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.196958Z",
     "start_time": "2021-01-07T20:33:18.177611Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "polynomial_inet = polynomial_dict_test_list[-1]['inet_polynomials'][index_min]\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][index_min])\n",
    "\n",
    "print_polynomial_from_coefficients(polynomial_inet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lstsq_lambda_pred_polynomials_VS_inet_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.820457Z",
     "start_time": "2021-01-07T20:33:18.813628Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "polynomial_target = lambda_net_test_dataset.target_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_target, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:22.149541Z",
     "start_time": "2021-01-07T20:33:22.141264Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "polynomial_inet = polynomial_dict_test_list[-1]['inet_polynomials'][index_max]\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][index_max])\n",
    "\n",
    "print_polynomial_from_coefficients(polynomial_inet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:22.526765Z",
     "start_time": "2021-01-07T20:33:22.518702Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][index_max])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_max]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:23.155159Z",
     "start_time": "2021-01-07T20:33:23.146225Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "polynomial_target = lambda_net_test_dataset.target_polynomial_list[index_max]\n",
    "print_polynomial_from_coefficients(polynomial_target, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['target_polynomials_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'] > -50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.417509Z",
     "start_time": "2021-01-07T15:49:44.181928Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'] > -50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.935810Z",
     "start_time": "2021-01-07T15:49:44.419772Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'] > -50], binwidth=0.2)\n",
    "p.set(xlim=(-30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:45.795559Z",
     "start_time": "2021-01-07T15:49:44.938329Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'] > -50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:47.784878Z",
     "start_time": "2021-01-07T15:49:45.797362Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'] > -50], binwidth=0.2)\n",
    "p.set(xlim=(-10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.252121Z",
     "start_time": "2021-01-07T15:49:47.786575Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'] > -50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == train_features_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED+1], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    x = tf.linspace(0.0, 1, 250)#tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    if length_plt >= 2:\n",
    "        fig, ax = plt.subplots(length_plt//2, 2, figsize=(30,20))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "        \n",
    "        plot_scores_dict = {}\n",
    "        for key in evaluation_key_list:\n",
    "            try:\n",
    "                scores_list[-1][metric].loc[key]\n",
    "                plot_scores_dict[key] = []\n",
    "            except:\n",
    "                #print(key + 'not in scores_list')\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        for scores in scores_list:\n",
    "            for key in evaluation_key_list:\n",
    "                try:\n",
    "                    plot_scores_dict[key].append(scores[metric].loc[key])\n",
    "                except:\n",
    "                    #print(key + 'not in scores_list')\n",
    "                    continue\n",
    "                                        \n",
    "            \n",
    "        plot_df = pd.DataFrame(data=np.vstack(plot_scores_dict.values()).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=plot_scores_dict.keys())\n",
    "\n",
    "        if length_plt >= 2:\n",
    "            ax[index//2, index%2].set_title(metric)\n",
    "            sns.set(font_scale = 1.25)\n",
    "            p = sns.lineplot(data=plot_df, ax=ax[index//2, index%2])\n",
    "        else:\n",
    "            ax.set_title(metric)\n",
    "            sns.set(font_scale = 1.25)\n",
    "            p = sns.lineplot(data=plot_df, ax=ax)\n",
    "\n",
    "        if ylim != None:\n",
    "            p.set(ylim=ylim)\n",
    "\n",
    "        p.set_yticklabels(np.round(p.get_yticks(), 2), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)     \n",
    "        \n",
    "        #p.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        p.legend(loc='upper center', bbox_to_anchor=(0.47, -0.1),\n",
    "          fancybox=False, shadow=False, ncol=2, fontsize=12)   \n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
