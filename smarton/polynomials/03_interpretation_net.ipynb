{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:16.416747Z",
     "start_time": "2020-12-22T13:42:16.410816Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:16.435323Z",
     "start_time": "2020-12-22T13:42:16.425085Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3  \n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "n_jobs = 5\n",
    "\n",
    "\n",
    "data_size = 10000 #for loading lambda models\n",
    "\n",
    "#specify interpretation net structure\n",
    "optimizer = 'adam'\n",
    "dropout = 0\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "interpretation_network_layers = [2048]\n",
    "\n",
    "random_evaluation_dataset_size = 500\n",
    "\n",
    "#lambda net specifications for loading (need to be set according to lambda net training to load correct weights)\n",
    "epochs_lambda = 200\n",
    "batch_lambda = 64\n",
    "lambda_network_layers = [5*sparsity]\n",
    "optimizer_lambda = '_' + 'SGD'\n",
    "\n",
    "\n",
    "lambda_dataset_size = 1000\n",
    "\n",
    "#set if multi_epoch_analysis should be performed\n",
    "multi_epoch_analysis = False\n",
    "each_epochs_save_lambda = 50\n",
    "epoch_start = 0 #use to skip first epochs in multi_epoch_analysis\n",
    "\n",
    "#set if samples analysis should be performed\n",
    "samples_list = [100, 500, 1000]#None#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "\n",
    "evaluate_with_real_function = False\n",
    "consider_labels_training = False\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 10000\n",
    "\n",
    "inet_holdout_seed_evaluation = False\n",
    "seed_in_inet_training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:16.447308Z",
     "start_time": "2020-12-22T13:42:16.437129Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "structure = '_' + layers_str + str(epochs_lambda) + 'e' + str(batch_lambda) + 'b' + optimizer_lambda\n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure\n",
    "\n",
    "interpretation_network_string = 'drop' + str(dropout) + 'e' + str(epochs) + 'b' + str(batch_size) + '_' + str(interpretation_network_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:21.836189Z",
     "start_time": "2020-12-22T13:42:16.448715Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('./data/plotting/' + interpretation_network_string + filename + '/')\n",
    "    os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:21.843093Z",
     "start_time": "2020-12-22T13:42:21.838750Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:21.861009Z",
     "start_time": "2020-12-22T13:42:21.845024Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#General Utility Functions\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "\n",
    "def return_float_tensor_representation(some_representation, dtype=tf.float32):\n",
    "    if tf.is_tensor(some_representation):\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "    else:\n",
    "        some_representation = tf.convert_to_tensor(some_representation)\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "        \n",
    "    if not tf.is_tensor(some_representation):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(dtype) + ':' + str(some_representation))\n",
    "     \n",
    "    return some_representation\n",
    "\n",
    "\n",
    "def return_numpy_representation(some_representation):\n",
    "    if isinstance(some_representation, pd.DataFrame):\n",
    "        some_representation = some_representation.values\n",
    "        \n",
    "    if isinstance(some_representation, list):\n",
    "        some_representation = np.array(some_representation)\n",
    "    \n",
    "    if not isinstance(some_representation, np.ndarray):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(np.ndarray) + ':' + str(some_representation))\n",
    "    \n",
    "    return some_representation\n",
    "\n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict\n",
    "\n",
    "def return_callbacks_from_string(callback_string_list):\n",
    "    callbacks = [] if len(callback_string_list) > 0 else None\n",
    "    #if 'plot_losses_callback' in callback_string_list:\n",
    "        #callbacks.append(PlotLossesCallback())\n",
    "    if 'reduce_lr_loss' in callback_string_list:\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=epochs/10, verbose=0, min_delta=0, mode='min') #epsilon\n",
    "        callbacks.append(reduce_lr_loss)\n",
    "    if 'early_stopping' in callback_string_list:\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0, verbose=0, mode='min')\n",
    "        callbacks.append(earlyStopping)\n",
    "        \n",
    "    #if not multi_epoch_analysis and samples_list == None: \n",
    "        #callbacks.append(TQDMNotebookCallback())\n",
    "        \n",
    "    return callbacks\n",
    "\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def generate_random_x_values(size, x_max, x_min, x_step, numnber_of_variables, seed=42):\n",
    "    \n",
    "    if random.seed != None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    x_values_list = []\n",
    "    \n",
    "    for j in range(size):\n",
    "        values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))\n",
    "        while arreq_in_list(values, x_values_list):\n",
    "                values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))         \n",
    "        x_values_list.append(values)\n",
    "    \n",
    "    return np.array(x_values_list)\n",
    "\n",
    "def flatten(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, Iterable) and not isinstance(el, (str, bytes)):\n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:21.904933Z",
     "start_time": "2020-12-22T13:42:21.862759Z"
    },
    "code_folding": [
     229,
     246,
     258
    ]
   },
   "outputs": [],
   "source": [
    "class LambdaNetDataset():\n",
    "    lambda_net_list = None\n",
    "    \n",
    "    weight_list = None\n",
    "    \n",
    "    train_settings_list = None\n",
    "    index_list = None\n",
    "    \n",
    "    target_polynomial_list = None\n",
    "    lstsq_lambda_pred_polynomial_list = None\n",
    "    lstsq_target_polynomial_list = None    \n",
    "        \n",
    "    test_data_list = None\n",
    "    \n",
    "    def __init__(self, lambda_net_list):\n",
    "        self.lambda_net_list = lambda_net_list\n",
    "        \n",
    "        self.weight_list = [lambda_net.weights for lambda_net in lambda_net_list]\n",
    "        \n",
    "        self.train_settings_list = {}\n",
    "        for key in lambda_net_list[0].train_settings.keys():\n",
    "            self.train_settings_list[key] = []   \n",
    "        for lambda_net in lambda_net_list:\n",
    "            for key in lambda_net.train_settings.keys():\n",
    "                self.train_settings_list[key].append(lambda_net.train_settings[key])\n",
    "        \n",
    "        self.index_list = [lambda_net.index for lambda_net in lambda_net_list]\n",
    "        \n",
    "        self.target_polynomial_list = [lambda_net.target_polynomial for lambda_net in lambda_net_list]\n",
    "        self.lstsq_lambda_pred_polynomial_list = [lambda_net.lstsq_lambda_pred_polynomial for lambda_net in lambda_net_list]\n",
    "        self.lstsq_target_polynomial_list = [lambda_net.lstsq_target_polynomial for lambda_net in lambda_net_list]\n",
    "      \n",
    "        self.test_data_list = [lambda_net.test_data for lambda_net in lambda_net_list]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.as_pandas().head())\n",
    "    def __str__(self):\n",
    "        return str(self.as_pandas().head())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lambda_net_list)\n",
    "    \n",
    "    \n",
    "    def make_prediction_on_dataset(self, evaluation_dataset):  \n",
    "        assert(evaluation_dataset.shape[1] == n)\n",
    "        lambda_network_preds_list = []\n",
    "        \n",
    "        for weights in self.weight_list:\n",
    "            lambda_network_preds = weights_to_pred(weights, evaluation_dataset)\n",
    "            lambda_network_preds_list.append(lambda_network_preds)\n",
    "        \n",
    "        return np.array(lambda_network_preds_list)\n",
    "    \n",
    "    def make_prediction_on_test_data(self):\n",
    "        lambda_network_preds_list = []\n",
    "        for lambda_net in self.lambda_net_list:\n",
    "            lambda_network_preds = lambda_net.make_prediction_on_test_data()\n",
    "            lambda_network_preds_list.append(lambda_network_preds)\n",
    "            \n",
    "        return np.array(lambda_network_preds_list)\n",
    "                \n",
    "        \n",
    "    def return_target_poly_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape + ' but required (x, ' + str(n) + ')'))     \n",
    "        target_poly_fvs_list = parallel_fv_calculation_from_polynomial(self.target_polynomial_list, [evaluation_dataset for _ in range(len(self.target_polynomial_list))])\n",
    "            \n",
    "        return np.array(target_poly_fvs_list)\n",
    "    \n",
    "    def return_target_poly_fvs_on_test_data(self):        \n",
    "        target_poly_fvs_list = parallel_fv_calculation_from_polynomial(self.target_polynomial_list, self.test_data_list)\n",
    "        \n",
    "        return np.array(target_poly_fvs_list)\n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape + ' but required (x, ' + str(n) + ')'))    \n",
    "        lstsq_lambda_pred_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_lambda_pred_polynomial_list, [evaluation_dataset for _ in range(len(self.target_polynomial_list))])\n",
    "            \n",
    "        return np.array(lstsq_lambda_pred_polynomial_fvs_list)\n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_lambda_pred_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_lambda_pred_polynomial_list, self.test_data_list)\n",
    "            \n",
    "        return np.array(lstsq_lambda_pred_polynomial_fvs_list)\n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape + ' but required (x, ' + str(n) + ')'))\n",
    "        lstsq_target_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_target_polynomial_list, [evaluation_dataset for _ in range(len(self.target_polynomial_list))])\n",
    "            \n",
    "        return np.array(lstsq_target_polynomial_fvs_list)\n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_target_polynomial_fvs_list = parallel_fv_calculation_from_polynomial(self.lstsq_target_polynomial_list, self.test_data_list)\n",
    "            \n",
    "        return np.array(lstsq_target_polynomial_fvs_list)\n",
    "    \n",
    "    def as_pandas(self):  \n",
    "        lambda_dataframe = pd.DataFrame(data=[lambda_net.as_array() for lambda_net in self.lambda_net_list], \n",
    "                                columns=self.lambda_net_list[0].return_column_names(), \n",
    "                                index=[lambda_net.index for lambda_net in self.lambda_net_list])\n",
    "        lambda_dataframe['seed'] = lambda_dataframe['seed'].astype(int)\n",
    "        \n",
    "        return lambda_dataframe\n",
    "\n",
    "    \n",
    "    def get_lambda_nets_by_seed(self, seed_list):\n",
    "        lambda_nets_by_seed = []\n",
    "        for lambda_net in self.lambda_net_list:\n",
    "            if lambda_net.train_settings['seed'] in seed_list:\n",
    "                lambda_nets_by_seed.append(lambda_net)\n",
    "    \n",
    "        return LambdaNetDataset(lambda_nets_by_seed)\n",
    "    \n",
    "    def get_lambda_nets_by_lambda_index(self, lambda_index_list):\n",
    "        lambda_nets_by_lambda_index = []\n",
    "        for lambda_net in self.lambda_net_list:\n",
    "            if lambda_net.index in self.lambda_index_list:\n",
    "                lambda_nets_by_lambda_index.append(lambda_net)\n",
    "    \n",
    "        return LambdaNetDataset(lambda_nets_by_lambda_index) \n",
    "    \n",
    "    def sample(self, size, seed=42):\n",
    "        \n",
    "        assert(isinstance(size, int) or isinstance(size, float), 'Wrong sample size specified')\n",
    "        \n",
    "        random.seed(seed)\n",
    "        \n",
    "        sample_lambda_net_list = None\n",
    "        if isinstance(size, int):\n",
    "            sample_lambda_net_list = random.sample(self.lambda_net_list, size)\n",
    "        elif isinstance(size, float):\n",
    "            size = int(np.round(len(self.lambda_net_list)*size))\n",
    "            sample_lambda_net_list = random.sample(self.lambda_net_list, size)\n",
    "            \n",
    "        return LambdaNetDataset(sample_lambda_net_list)\n",
    "    \n",
    "\n",
    "class LambdaNet():\n",
    "    weights = None\n",
    "    model = None\n",
    "    \n",
    "    train_settings = None\n",
    "    index = None\n",
    "    \n",
    "    target_polynomial = None\n",
    "    lstsq_lambda_pred_polynomial = None\n",
    "    lstsq_target_polynomial = None\n",
    "    \n",
    "    test_data = None\n",
    "    \n",
    "    def __init__(self, line):\n",
    "        assert(isinstance(line, np.ndarray), 'line is no array: ' + str(line))\n",
    "        \n",
    "        self.index = int(line[0])\n",
    "        self.train_settings = {'seed': int(line[1])}\n",
    "        \n",
    "        self.target_polynomial = line[range(2, nCr(n+d, d)+2)]\n",
    "        self.lstsq_lambda_pred_polynomial = line[range(nCr(n+d, d)+2, nCr(n+d, d)*2+2)]\n",
    "        self.lstsq_target_polynomial = line[range(nCr(n+d, d)*2+2, nCr(n+d, d)*3+2)] \n",
    "        assert(self.target_polynomial.shape[0] == sparsity, 'target polynomial has incorrect shape ' + str(self.target_polynomial.shape[0]) + ' but should be ' + str(sparsity))\n",
    "        assert(self.lstsq_lambda_pred_polynomial.shape[0] == sparsity, 'lstsq lambda pred polynomial has incorrect shape ' + str(self.lstsq_lambda_pred_polynomial.shape[0]) + ' but should be ' + str(sparsity))\n",
    "        assert(self.lstsq_target_polynomial.shape[0] == sparsity, 'lstsq target polynomial has incorrect shape ' + str(self.lstsq_target_polynomial.shape[0]) + ' but should be ' + str(sparsity))    \n",
    "        \n",
    "        self.weights = line[nCr(n+d, d)*3+2:]\n",
    "        assert(self.weights.shape[0] == number_of_lambda_weights, 'weights have incorrect shape ' + str(self.weights.shape[0]) + ' but should be ' + str(number_of_lambda_weights))\n",
    "        \n",
    "        directory = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "        path = directory + 'lambda_' + str(self.index) + '_test_data.npy'\n",
    "        \n",
    "        self.test_data = np.load(path)\n",
    "        assert(self.test_data.shape[1] == n, 'test data has wrong shape ' + str(self.test_data.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.weights)\n",
    "    def __str__(self):\n",
    "        return str(self.weights)\n",
    "        \n",
    "    def make_prediction_on_dataset(self, evaluation_dataset):  \n",
    "        assert(evaluation_dataset.shape[1] == n) \n",
    "        lambda_network_preds = weights_to_pred(self.weights, evaluation_dataset)\n",
    "        \n",
    "        return lambda_network_preds\n",
    "    \n",
    "    def make_prediction_on_test_data(self):        \n",
    "        lambda_network_preds = weights_to_pred(self.weights, self.test_data)\n",
    "        \n",
    "        return lambda_network_preds               \n",
    "        \n",
    "    def return_target_poly_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        target_poly_fvs = parallel_fv_calculation_from_polynomial([self.target_polynomial], [evaluation_dataset])\n",
    "    \n",
    "        return target_poly_fvs\n",
    "    \n",
    "    def return_target_poly_fvs_on_test_data(self):\n",
    "        target_poly_fvs = parallel_fv_calculation_from_polynomial([self.target_polynomial], [self.test_data])\n",
    "    \n",
    "        return target_poly_fvs    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        lstsq_lambda_pred_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_lambda_pred_polynomial], [evaluation_dataset])\n",
    "    \n",
    "        return lstsq_lambda_pred_polynomial_fvs\n",
    "    \n",
    "    def return_lstsq_lambda_pred_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_lambda_pred_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_lambda_pred_polynomial], [self.test_data])\n",
    "    \n",
    "        return lstsq_lambda_pred_polynomial_fvs     \n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_dataset(self, evaluation_dataset):\n",
    "        assert(evaluation_dataset.shape[1] == n, 'evaluation dataset has wrong shape ' + str(evaluation_dataset.shape) + ' but required (x, ' + str(n) + ')')\n",
    "        lstsq_target_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_target_polynomial], [evaluation_dataset])\n",
    "    \n",
    "        return lstsq_target_polynomial_fvs\n",
    "    \n",
    "    def return_lstsq_target_polynomial_fvs_on_test_data(self):\n",
    "        lstsq_target_polynomial_fvs = parallel_fv_calculation_from_polynomial([self.lstsq_target_polynomial], [self.test_data])\n",
    "    \n",
    "        return lstsq_target_polynomial_fvs  \n",
    "    \n",
    "    def as_pandas(self): \n",
    "        columns = return_column_names(self)\n",
    "        data = as_array(self)\n",
    "        \n",
    "        df = pd.DataFrame(data=data, columns=columns, index=[self.index])\n",
    "        df['seed'] = df['seed'].astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def as_array(self):\n",
    "        data = np.hstack([self.train_settings['seed'], self.target_polynomial, self.lstsq_lambda_pred_polynomial, self.lstsq_target_polynomial, self.weights])\n",
    "        return data\n",
    "    \n",
    "    def return_column_names(self):\n",
    "        target_polynomial_identifiers = [monomial_identifiers + str('-target') for monomial_identifiers in list_of_monomial_identifiers]\n",
    "        lstsq_lambda_pred_polynomial_identifiers = [monomial_identifiers + str('-lstsq_lambda') for monomial_identifiers in list_of_monomial_identifiers]\n",
    "        lstsq_target_polynomial_identifiers = [monomial_identifiers + str('-lstsq_target') for monomial_identifiers in list_of_monomial_identifiers]\n",
    "\n",
    "        weight_identifiers = ['wb_' + str(i) for i in range(self.weights.shape[0])]\n",
    "        \n",
    "        columns = list(flatten(['seed', target_polynomial_identifiers, lstsq_lambda_pred_polynomial_identifiers, lstsq_target_polynomial_identifiers, weight_identifiers]))\n",
    "                \n",
    "        return columns \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def split_LambdaNetDataset(dataset, test_split, random_seed=RANDOM_SEED):\n",
    "    assert(isinstance(dataset, LambdaNetDataset))\n",
    "    \n",
    "    lambda_nets_list = dataset.lambda_net_list\n",
    "    \n",
    "    if isinstance(test_split, int) or isinstance(test_split, float):\n",
    "        lambda_nets_train_list, lambda_nets_test_list = train_test_split(lambda_nets_list, test_size=test_split, random_state=random_seed)     \n",
    "    elif isinstance(test_split, list):\n",
    "        lambda_nets_test_list = [lambda_nets_list[i] for i in test_split]\n",
    "        lambda_nets_train_list = list(set(lambda_nets_list) - set(lambda_nets_test_list))\n",
    "        #lambda_nets_train_list = lambda_nets_list.copy()\n",
    "        #for i in sorted(test_split, reverse=True):\n",
    "        #    del lambda_nets_train_list[i]           \n",
    "    assert(len(lambda_nets_list) == len(lambda_nets_train_list) + len(lambda_nets_test_list))\n",
    "    \n",
    "    return LambdaNetDataset(lambda_nets_train_list), LambdaNetDataset(lambda_nets_test_list)\n",
    "                                                                                                 \n",
    "def generate_base_model(): #without dropout\n",
    "    base_model = Sequential()\n",
    "\n",
    "    base_model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=n))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        base_model.add(Dense(neurons, activation='relu'))\n",
    "\n",
    "    base_model.add(Dense(1))\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "def shape_flat_weights(flat_weights, target_weights):\n",
    "    \n",
    "    shaped_weights =[]\n",
    "    start = 0\n",
    "    for el in target_weights:\n",
    "        target_shape = el.shape\n",
    "        size = len(list(flatten(el)))\n",
    "        shaped_el = np.reshape(flat_weights[start:start+size], target_shape)\n",
    "        shaped_weights.append(shaped_el)\n",
    "        start += size\n",
    "\n",
    "    return shaped_weights\n",
    "\n",
    "def weights_to_pred(weights, x, base_model=None):\n",
    "\n",
    "    if base_model is None:\n",
    "        base_model = generate_base_model()\n",
    "    else:\n",
    "        base_model = keras.models.clone_model(base_model)\n",
    "    \n",
    "    # Shape weights (flat) into correct model structure\n",
    "    shaped_weights = shape_flat_weights(weights, base_model.get_weights())\n",
    "    \n",
    "    # Make prediction\n",
    "    base_model.set_weights(shaped_weights)\n",
    "    y = base_model.predict(x).ravel()\n",
    "    return y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:21.955826Z",
     "start_time": "2020-12-22T13:42:21.906682Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n",
    "\n",
    "\n",
    "layers_with_input_output = list(flatten([[n], lambda_network_layers, [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:21.978914Z",
     "start_time": "2020-12-22T13:42:21.957141Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with lambda-net prediction based (predictions made in loss function)\n",
    "\n",
    "\n",
    "def mean_absolute_error_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers, base_model):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)    \n",
    "    \n",
    "    model_lambda_placeholder = keras.models.clone_model(base_model)  \n",
    "    \n",
    "    weights_structure = base_model.get_weights()\n",
    "    dims = [np_arrays.shape for np_arrays in weights_structure]\n",
    "    \n",
    "    def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        if seed_in_inet_training:\n",
    "            network_parameters = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "            polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "        else:\n",
    "            network_parameters = polynomial_true_with_lambda_fv[:,sparsity+1:]\n",
    "            polynomial_true = polynomial_true_with_lambda_fv[:,1:sparsity+1]\n",
    "\n",
    "        network_parameters = return_float_tensor_representation(network_parameters)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        \n",
    "        assert(polynomial_true.shape[1] == sparsity)\n",
    "        assert(polynomial_pred.shape[1] == sparsity)        \n",
    "        \n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder), (polynomial_pred, network_parameters), fn_output_signature=tf.float32))\n",
    "    return mean_absolute_error_tf_fv_lambda_extended\n",
    "\n",
    "def calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder):\n",
    "\n",
    "    def calculate_mae_fv_lambda(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        #polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[0]\n",
    "        network_parameters = input_list[1]\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        #CALCULATE LAMBDA FV HERE FOR EVALUATION DATASET\n",
    "        # build models\n",
    "        start = 0\n",
    "        layers = []\n",
    "        for i in range(len(dims)//2):\n",
    "            \n",
    "            # set weights of layer\n",
    "            index = i*2\n",
    "            size = np.product(dims[index])\n",
    "            weights_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[0].assign(weights_tf_true)\n",
    "            start += size\n",
    "            \n",
    "            # set biases of layer\n",
    "            index += 1\n",
    "            size = np.product(dims[index])\n",
    "            biases_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[1].assign(biases_tf_true)\n",
    "            start += size\n",
    "\n",
    "        \n",
    "        lambda_fv = tf.keras.backend.flatten(model_lambda_placeholder(evaluation_dataset))\n",
    "        \n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_lambda\n",
    "\n",
    "\n",
    "\n",
    "#Manual TF Loss function for fv comparison of real and predicted polynomial\n",
    "\n",
    "def mean_absolute_error_tf_fv_poly_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)        \n",
    "    \n",
    "    def mean_absolute_error_tf_fv_poly_extended(polynomial_true, polynomial_pred):\n",
    "\n",
    "        if seed_in_inet_training:\n",
    "            polynomial_true = polynomial_true[:,1:]\n",
    "\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        \n",
    "        assert(polynomial_true.shape[1] == sparsity)\n",
    "        assert(polynomial_pred.shape[1] == sparsity)        \n",
    "        \n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_mae_fv_poly_wrapper(evaluation_dataset, list_of_monomial_identifiers), (polynomial_true, polynomial_pred), fn_output_signature=tf.float32))\n",
    "    return mean_absolute_error_tf_fv_poly_extended\n",
    "\n",
    "def calculate_mae_fv_poly_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "\n",
    "    def calculate_mae_fv_poly(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[1]\n",
    "        \n",
    "        polynomial_true_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_true), (evaluation_dataset))\n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "        \n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (polynomial_true_fv_list, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_poly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#GENERAL LOSS UTILITY FUNCTIONS\n",
    "def calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred):\n",
    "\n",
    "\n",
    "    def calculate_fv_from_data(evaluation_entry):\n",
    "\n",
    "\n",
    "        value_without_coefficient = tf.vectorized_map(calculate_value_without_coefficient_wrapper(evaluation_entry), (list_of_monomial_identifiers))\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        \n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)     \n",
    "        \n",
    "        return polynomial_pred_fv\n",
    "    return calculate_fv_from_data\n",
    "\n",
    "\n",
    "#calculate intermediate term (without coefficient multiplication)\n",
    "def calculate_value_without_coefficient_wrapper(evaluation_entry):\n",
    "    def calculate_value_without_coefficient(coefficient_multiplier_term):      \n",
    "   \n",
    "        return tf.math.reduce_prod(tf.vectorized_map(lambda x: x[0]**x[1], (evaluation_entry, coefficient_multiplier_term)))\n",
    "    return calculate_value_without_coefficient\n",
    "\n",
    "#calculate MAE at the end ---> general:REPLACE FUNCTION WITH LOSS CALL OR LAMBDA\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BASIC COEFFICIENT-BASED LOSS IF X_DATA IS APPENDED\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    \n",
    "    if seed_in_inet_training:\n",
    "        polynomial_true = polynomial_true_with_lambda_fv[:,1:sparsity+1]\n",
    "    else:\n",
    "        polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]    \n",
    "    \n",
    "    assert(polynomial_true.shape[1] == sparsity)\n",
    "    assert(polynomial_pred.shape[1] == sparsity)\n",
    "    \n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n",
    "65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:21.992952Z",
     "start_time": "2020-12-22T13:42:21.980128Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Basic Keras/TF Loss functions\n",
    "def root_mean_squared_error(y_true, y_pred):   \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "        \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)           \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))      \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)        \n",
    "    epsilon = return_float_tensor_representation(epsilon)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:22.005194Z",
     "start_time": "2020-12-22T13:42:21.994216Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Manual calculations for comparison of polynomials based on function values (no TF!)\n",
    "\n",
    "def calcualate_function_value(coefficient_list, lambda_input_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0   \n",
    "        \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        value_without_coefficient = [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multipliers, lambda_input_entry)]\n",
    "\n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, value_without_coefficient)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_function_values_from_polynomial(polynomial, lambda_input_data):        \n",
    "    function_value_list = []\n",
    "        \n",
    "    for lambda_input_entry in lambda_input_data:\n",
    "        function_value = calcualate_function_value(polynomial, lambda_input_entry)\n",
    "        function_value_list.append(function_value)\n",
    "\n",
    "    return np.array(function_value_list)\n",
    "\n",
    "\n",
    "def parallel_fv_calculation_from_polynomial(polynomial_list, lambda_input_list):\n",
    "    \n",
    "    polynomial_list = return_numpy_representation(polynomial_list)\n",
    "    lambda_input_list = return_numpy_representation(lambda_input_list)\n",
    "    \n",
    "    assert(polynomial_list.shape[0] == lambda_input_list.shape[0])\n",
    "    assert(polynomial_list.shape[1] == sparsity)\n",
    "    assert(lambda_input_list.shape[2] == n)\n",
    "    \n",
    "    n_jobs_parallel_fv = 10 if polynomial_list.shape[0] > 10 else polynomial_list.shape[0]\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs_parallel_fv, verbose=0, backend='threading')\n",
    "    polynomial_true_fv = parallel(delayed(calculate_function_values_from_polynomial)(polynomial, lambda_inputs) for polynomial, lambda_inputs in zip(polynomial_list, lambda_input_list))  \n",
    "    del parallel   \n",
    "    \n",
    "\n",
    "    return np.array(polynomial_true_fv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:22.028671Z",
     "start_time": "2020-12-22T13:42:22.006869Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Standard Metrics (no TF!)\n",
    "\n",
    "def mean_absolute_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)      \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(true_values-pred_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))  \n",
    "\n",
    "def root_mean_squared_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)         \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sqrt(np.mean((true_values-pred_values)**2)))\n",
    "    \n",
    "    return np.mean(np.array(result_list)) \n",
    "\n",
    "def mean_absolute_percentage_error_function_values(y_true, y_pred, epsilon=10e-3):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred) \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(((true_values-pred_values)/(true_values+epsilon)))))\n",
    "\n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def r2_score_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(r2_score(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_absolute_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sum(np.abs(true_values-pred_values))/(true_values.shape[0]*np.std(true_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_maximum_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.max(true_values-pred_values)/np.std(true_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_area_between_two_curves_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "      \n",
    "    assert(number_of_variables==1)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(area_between_two_curves(true_values, pred_values))\n",
    " \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_dtw_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "\n",
    "    result_list_single = []\n",
    "    result_list_array = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_single_value, result_single_array = dtw(true_values, pred_values)\n",
    "        result_list_single.append(result_single_value)\n",
    "        result_list_array.append(result_single_array)\n",
    "    \n",
    "    return np.mean(np.array(result_list_single)), np.mean(np.array(result_list_array), axis=1)\n",
    "\n",
    "def mean_frechet_dist_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(frechet_dist(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:22.043164Z",
     "start_time": "2020-12-22T13:42:22.030262Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_interpretation_net(y_data_real, \n",
    "                                y_data_pred, \n",
    "                                polynomial_true_fv, \n",
    "                                polynomial_pred_inet_fv):\n",
    "    \n",
    "    if type(y_data_real) != type(None) and type(y_data_pred) != type(None):\n",
    "        y_data_real = return_numpy_representation(y_data_real)\n",
    "        y_data_pred = return_numpy_representation(y_data_pred)     \n",
    "        \n",
    "        assert(y_data_real.shape[1] == sparsity)\n",
    "        assert(y_data_pred.shape[1] == sparsity)\n",
    "        \n",
    "        mae_coeff = np.round(mean_absolute_error(y_data_real, y_data_pred), 4)\n",
    "        rmse_coeff = np.round(root_mean_squared_error(y_data_real, y_data_pred), 4)\n",
    "        mape_coeff = np.round(mean_absolute_percentage_error_keras(y_data_real, y_data_pred), 4)\n",
    "        accuracy_coeff = np.round(accuracy_single(y_data_real, y_data_pred), 4)\n",
    "        accuracy_multi_coeff = np.round(accuracy_multilabel(y_data_real, y_data_pred), 4)\n",
    "    else:\n",
    "        mae_coeff = np.nan\n",
    "        rmse_coeff = np.nan\n",
    "        mape_coeff = np.nan\n",
    "        accuracy_coeff = np.nan\n",
    "        accuracy_multi_coeff = np.nan\n",
    "        \n",
    "    polynomial_true_fv = return_numpy_representation(polynomial_true_fv)\n",
    "    polynomial_pred_inet_fv = return_numpy_representation(polynomial_pred_inet_fv)\n",
    "    \n",
    "    mae_fv = np.round(mean_absolute_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmse_fv = np.round(root_mean_squared_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    mape_fv = np.round(mean_absolute_percentage_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    r2_fv = np.round(r2_score_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    raae_fv = np.round(relative_absolute_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmae_fv = np.round(relative_maximum_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4) \n",
    "\n",
    "    std_fv = np.std(mae_fv)\n",
    "    mean_fv = np.mean(mae_fv)\n",
    "\n",
    "    return pd.Series(data=[mae_coeff,\n",
    "                          rmse_coeff,\n",
    "                          mape_coeff,\n",
    "                          accuracy_coeff,\n",
    "                          accuracy_multi_coeff,\n",
    "                          \n",
    "                          mae_fv,\n",
    "                          rmse_fv,\n",
    "                          mape_fv,\n",
    "                          r2_fv,\n",
    "                          raae_fv,\n",
    "                          rmae_fv,\n",
    "                          \n",
    "                          std_fv,\n",
    "                          mean_fv],\n",
    "                     index=['MAE',\n",
    "                           'RMSE',\n",
    "                           'MAPE',\n",
    "                           'Accuracy',\n",
    "                           'Accuracy Multilabel',\n",
    "                           \n",
    "                           'MAE FV',\n",
    "                           'RMSE FV',\n",
    "                           'MAPE FV',\n",
    "                           'R2 FV',\n",
    "                           'RAAE FV',\n",
    "                           'RMAE FV',\n",
    "                            \n",
    "                           'STD FV ERROR',\n",
    "                           'MEAN FV ERROR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:22.054330Z",
     "start_time": "2020-12-22T13:42:22.046399Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3) + filename + '.txt'\n",
    "\n",
    "    \n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "    \n",
    "    lambda_nets = []\n",
    "    for _, row in weight_data.iterrows():\n",
    "        lambda_net = LambdaNet(row.values)\n",
    "        lambda_nets.append(lambda_net)\n",
    "      \n",
    "    if data_size < len(lambda_nets):\n",
    "        random.seed(RANDOM_SEED)\n",
    "        lambda_nets = random.sample(lambda_nets, data_size)\n",
    "    \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:39.075829Z",
     "start_time": "2020-12-22T13:42:22.056783Z"
    }
   },
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "\n",
    "parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:42.050074Z",
     "start_time": "2020-12-22T13:42:39.079345Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_dataset.as_pandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:47.592524Z",
     "start_time": "2020-12-22T13:42:42.051654Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:47.638633Z",
     "start_time": "2020-12-22T13:42:47.594458Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "for lambda_net_dataset in lambda_net_dataset_list:\n",
    "    \n",
    "    \n",
    "    if inet_holdout_seed_evaluation:\n",
    "        complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "        random.seed(RANDOM_SEED)\n",
    "        test_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/0.75)))\n",
    "        lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "        \n",
    "        random.seed(RANDOM_SEED)\n",
    "        valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/0.75)))\n",
    "        lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "        train_seeds = complete_seed_list\n",
    "        lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "        \n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "        \n",
    "        del lambda_net_dataset\n",
    "    else:\n",
    "        lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=0.25)\n",
    "        lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.25)\n",
    "\n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "    \n",
    "        del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "        \n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:49.264853Z",
     "start_time": "2020-12-22T13:42:47.640269Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:49.790251Z",
     "start_time": "2020-12-22T13:42:49.266440Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:50.488459Z",
     "start_time": "2020-12-22T13:42:49.791879Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:52.377528Z",
     "start_time": "2020-12-22T13:42:50.490150Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:53.339474Z",
     "start_time": "2020-12-22T13:42:52.379145Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:54.435875Z",
     "start_time": "2020-12-22T13:42:53.340841Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:54.459023Z",
     "start_time": "2020-12-22T13:42:54.437339Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(lambda_net_valid_dataset_list[-1].test_data_list)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:54.475766Z",
     "start_time": "2020-12-22T13:42:54.460839Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(lambda_net_valid_dataset_list[-1].weight_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T13:42:54.502262Z",
     "start_time": "2020-12-22T13:42:54.477620Z"
    },
    "code_folding": [
     136,
     140,
     146,
     150,
     156,
     160,
     166,
     170,
     176,
     180,
     186,
     190,
     196,
     200,
     206,
     210,
     216,
     220,
     226,
     230,
     235
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn_and_pred(lambda_net_train_dataset,\n",
    "                      lambda_net_valid_dataset,\n",
    "                      lambda_net_test_dataset, \n",
    "                      callback_names=[], \n",
    "                      return_model=False):       \n",
    "   \n",
    "\n",
    "    ############################## DATA PREPARATION ###############################\n",
    "\n",
    "    if seed_in_inet_training:\n",
    "        X_train = np.array(lambda_net_train_dataset.weight_list)\n",
    "        X_valid = np.array(lambda_net_valid_dataset.weight_list)\n",
    "        X_test = np.array(lambda_net_test_dataset.weight_list)\n",
    "    else:   #normalize if included in training   \n",
    "        normalizer = Normalizer().fit([np.array(lambda_net_train_dataset.train_settings_list['seed'])])\n",
    "        train_seed_list = normalizer.transform([np.array(lambda_net_train_dataset.train_settings_list['seed'])])[0]\n",
    "        valid_seed_list = normalizer.transform([np.array(lambda_net_valid_dataset.train_settings_list['seed'])])[0]\n",
    "        test_seed_list = normalizer.transform([np.array(lambda_net_test_dataset.train_settings_list['seed'])])[0]\n",
    "\n",
    "        X_train = np.hstack([np.expand_dims(train_seed_list, axis=1), np.array(lambda_net_train_dataset.weight_list)])\n",
    "        X_valid = np.hstack([np.expand_dims(valid_seed_list, axis=1), np.array(lambda_net_valid_dataset.weight_list)])\n",
    "        X_test = np.hstack([np.expand_dims(test_seed_list, axis=1), np.array(lambda_net_test_dataset.weight_list)])\n",
    "    \n",
    "    if evaluate_with_real_function: #target polynomial as inet target\n",
    "        y_train = np.array(lambda_net_train_dataset.target_polynomial_list)\n",
    "        y_valid = np.array(lambda_net_valid_dataset.target_polynomial_list)\n",
    "        y_test = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "    else: #lstsq lambda pred polynomial as inet target\n",
    "        y_train = np.array(lambda_net_train_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "        y_valid = np.array(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "        y_test = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "        \n",
    "        \n",
    "    ############################## OBJECTIVE SPECIFICATION AND LOSS FUNCTION ADJUSTMENTS ###############################\n",
    "        \n",
    "    if consider_labels_training: #coefficient-based evaluation\n",
    "        loss_function = mean_absolute_error_tf_fv\n",
    "        metrics = ['mean_absolute_error']\n",
    "        valid_data = (X_valid, y_valid)\n",
    "        y_train_model = y_train\n",
    "    else: #fv-based evaluation\n",
    "        if evaluate_with_real_function: #based on in-loss fv calculation of real and predicted polynomial\n",
    "            random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "            list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "            loss_function = mean_absolute_error_tf_fv_poly_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers)\n",
    "            metrics = ['mean_absolute_error']\n",
    "            valid_data = (X_valid, y_valid)\n",
    "            y_train_model = y_train\n",
    "        else: #in-loss prediction of lambda-nets\n",
    "            base_model = generate_base_model()\n",
    "            random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "            list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "            loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "            metrics = [mean_absolute_error_extended]\n",
    "            y_train_model = np.hstack((y_train, X_train))   \n",
    "            valid_data = (X_valid, np.hstack((y_valid, X_valid)))   \n",
    "            \n",
    "#TODO ADD ALTERNATIVE: FV COMPARISON WITH LSTSQ POLYNOMIAL INSTEAD OF DIRECTLY LAMBDA NET PREDS\n",
    "        \n",
    "    ############################## BUILD MODEL ###############################\n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(interpretation_network_layers[0], activation='relu', input_dim=X_train.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in interpretation_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(nCr(n+d, d))) \n",
    "    \n",
    "    callbacks = return_callbacks_from_string(callback_names)            \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=metrics\n",
    "                 )\n",
    "\n",
    "        \n",
    "        \n",
    "    ############################## PREDICTION ###############################\n",
    "        \n",
    "    history = model.fit(X_train,\n",
    "              y_train_model,\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              validation_data=valid_data,\n",
    "              callbacks=callbacks,\n",
    "              verbose=10)\n",
    "    \n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    pred_list = [y_valid_pred, y_test_pred]\n",
    "              \n",
    "        \n",
    "    ############################## FUNCTION VALUE CALCULATION ###############################\n",
    "    \n",
    "    lambda_test_data_preds_valid = lambda_net_valid_dataset.make_prediction_on_test_data()\n",
    "    lambda_test_data_preds_test = lambda_net_test_dataset.make_prediction_on_test_data() \n",
    "              \n",
    "    target_poly_test_data_fvs_valid = lambda_net_valid_dataset.return_target_poly_fvs_on_test_data()\n",
    "    target_poly_test_data_fvs_test = lambda_net_test_dataset.return_target_poly_fvs_on_test_data() \n",
    "                \n",
    "    lstsq_lambda_pred_polynomial_test_data_fvs_valid = lambda_net_valid_dataset.return_lstsq_lambda_pred_polynomial_fvs_on_test_data()\n",
    "    lstsq_lambda_pred_polynomial_test_data_fvs_test = lambda_net_test_dataset.return_lstsq_lambda_pred_polynomial_fvs_on_test_data() \n",
    "             \n",
    "    lstsq_target_polynomial_test_data_fvs_valid = lambda_net_valid_dataset.return_lstsq_target_polynomial_fvs_on_test_data()\n",
    "    lstsq_target_polynomial_test_data_fvs_test = lambda_net_test_dataset.return_lstsq_target_polynomial_fvs_on_test_data() \n",
    "        \n",
    "    inet_poly_test_data_fvs_valid = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_net_valid_dataset.test_data_list)\n",
    "    inet_poly_test_data_fvs_test = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_net_test_dataset.test_data_list) \n",
    "    \n",
    "    \n",
    "    function_values_valid = [lambda_test_data_preds_valid, \n",
    "                            target_poly_test_data_fvs_valid, \n",
    "                            lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                            lstsq_target_polynomial_test_data_fvs_valid,\n",
    "                            inet_poly_test_data_fvs_valid]\n",
    "    \n",
    "    function_values_test = [lambda_test_data_preds_test, \n",
    "                            target_poly_test_data_fvs_test, \n",
    "                            lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                            lstsq_target_polynomial_test_data_fvs_test,\n",
    "                            inet_poly_test_data_fvs_test]\n",
    "    \n",
    "    function_values = [function_values_valid, function_values_test]    \n",
    "    \n",
    "    \n",
    "    ############################## EVALUATION ###############################\n",
    "    \n",
    "    #evaluate inet poly against target polynomial on fv-basis\n",
    "    scores_inetPoly_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred,\n",
    "                                                                                   lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                   inet_poly_test_data_fvs_valid, \n",
    "                                                                                   target_poly_test_data_fvs_valid)  \n",
    "    scores_inetPoly_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                  lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                  inet_poly_test_data_fvs_test, \n",
    "                                                                                  target_poly_test_data_fvs_test)\n",
    "\n",
    "    #evaluate inet poly against lambda-net preds on fv-basis\n",
    "    scores_inetPoly_VS_predLambda_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred, \n",
    "                                                                                   None, \n",
    "                                                                                   inet_poly_test_data_fvs_valid, \n",
    "                                                                                   lambda_test_data_preds_valid)\n",
    "    scores_inetPoly_VS_predLambda_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                  None, \n",
    "                                                                                  inet_poly_test_data_fvs_test, \n",
    "                                                                                  lambda_test_data_preds_test)       \n",
    "        \n",
    "    #evaluate inet poly against lstsq target poly on fv-basis\n",
    "    scores_inetPoly_VS_lstsqTarget_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred, \n",
    "                                                                                    lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                    inet_poly_test_data_fvs_valid, \n",
    "                                                                                    lstsq_target_polynomial_test_data_fvs_valid)\n",
    "    scores_inetPoly_VS_lstsqTarget_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                   lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                   inet_poly_test_data_fvs_test, \n",
    "                                                                                   lstsq_target_polynomial_test_data_fvs_test)  \n",
    "\n",
    "    #evaluate inet poly against lstsq lambda poly on fv-basis\n",
    "    scores_inetPoly_VS_lstsqLambda_test_data_fv_valid = evaluate_interpretation_net(y_valid_pred, \n",
    "                                                                                    lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                    inet_poly_test_data_fvs_valid, \n",
    "                                                                                    lstsq_lambda_pred_polynomial_test_data_fvs_valid)\n",
    "    scores_inetPoly_VS_lstsqLambda_test_data_fv_test = evaluate_interpretation_net(y_test_pred, \n",
    "                                                                                   lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                   inet_poly_test_data_fvs_test, \n",
    "                                                                                   lstsq_lambda_pred_polynomial_test_data_fvs_test)     \n",
    "      \n",
    "    #evaluate lstsq lambda pred poly against lambda-net preds on fv-basis\n",
    "    scores_lstsqLambda_VS_predLambda_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                      None, \n",
    "                                                                                      lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                                                                                      lambda_test_data_preds_valid)\n",
    "    scores_lstsqLambda_VS_predLambda_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                     None, \n",
    "                                                                                     lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                                                                                     lambda_test_data_preds_test)\n",
    "    \n",
    "    #evaluate lstsq lambda pred poly against lstsq target poly on fv-basis\n",
    "    scores_lstsqLambda_VS_lstsqTarget_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                       lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                       lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                                                                                       lstsq_target_polynomial_test_data_fvs_valid)\n",
    "    scores_lstsqLambda_VS_lstsqTarget_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                      lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                      lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                                                                                      lstsq_target_polynomial_test_data_fvs_test)    \n",
    "    \n",
    "    #evaluate lstsq lambda pred poly against target poly on fv-basis\n",
    "    scores_lstsqLambda_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                      lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                      lstsq_lambda_pred_polynomial_test_data_fvs_valid, \n",
    "                                                                                      target_poly_test_data_fvs_valid)\n",
    "    scores_lstsqLambda_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list, \n",
    "                                                                                     lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                     lstsq_lambda_pred_polynomial_test_data_fvs_test, \n",
    "                                                                                     target_poly_test_data_fvs_test)    \n",
    "    \n",
    "    #evaluate lambda-net preds against lstsq target poly on fv-basis\n",
    "    scores_predLambda_VS_lstsqTarget_test_data_fv_valid = evaluate_interpretation_net(None, \n",
    "                                                                                      lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                      lambda_test_data_preds_valid, \n",
    "                                                                                      lstsq_target_polynomial_test_data_fvs_valid)\n",
    "    scores_predLambda_VS_lstsqTarget_test_data_fv_test = evaluate_interpretation_net(None, \n",
    "                                                                                     lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                     lambda_test_data_preds_test, \n",
    "                                                                                     lstsq_target_polynomial_test_data_fvs_test)\n",
    "        \n",
    "    #evaluate lambda-net preds against target poly on fv-basis\n",
    "    scores_predLambda_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(None, \n",
    "                                                                                     lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                     lambda_test_data_preds_valid, \n",
    "                                                                                     target_poly_test_data_fvs_valid)\n",
    "    scores_predLambda_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(None, \n",
    "                                                                                    lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                    lambda_test_data_preds_test, \n",
    "                                                                                    target_poly_test_data_fvs_test)\n",
    "      \n",
    "    #evaluate lstsq target poly against target poly on fv-basis\n",
    "    scores_lstsqTarget_VS_targetPoly_test_data_fv_valid = evaluate_interpretation_net(lambda_net_valid_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                      lambda_net_valid_dataset.target_polynomial_list, \n",
    "                                                                                      lstsq_target_polynomial_test_data_fvs_valid, \n",
    "                                                                                      target_poly_test_data_fvs_valid)\n",
    "    scores_lstsqTarget_VS_targetPoly_test_data_fv_test = evaluate_interpretation_net(lambda_net_test_dataset.lstsq_target_polynomial_list, \n",
    "                                                                                     lambda_net_test_dataset.target_polynomial_list, \n",
    "                                                                                     lstsq_target_polynomial_test_data_fvs_test, \n",
    "                                                                                     target_poly_test_data_fvs_test)\n",
    "        \n",
    "    scores_dict = pd.DataFrame(data=[scores_inetPoly_VS_targetPoly_test_data_fv_valid, \n",
    "                                     scores_inetPoly_VS_targetPoly_test_data_fv_test, \n",
    "                                     scores_inetPoly_VS_predLambda_test_data_fv_valid,\n",
    "                                     scores_inetPoly_VS_predLambda_test_data_fv_test,\n",
    "                                     scores_inetPoly_VS_lstsqTarget_test_data_fv_valid,\n",
    "                                     scores_inetPoly_VS_lstsqTarget_test_data_fv_test,\n",
    "                                     scores_inetPoly_VS_lstsqLambda_test_data_fv_valid,\n",
    "                                     scores_inetPoly_VS_lstsqLambda_test_data_fv_test,\n",
    "                                     scores_lstsqLambda_VS_predLambda_test_data_fv_valid,\n",
    "                                     scores_lstsqLambda_VS_predLambda_test_data_fv_test,\n",
    "                                     scores_lstsqLambda_VS_lstsqTarget_test_data_fv_valid,\n",
    "                                     scores_lstsqLambda_VS_lstsqTarget_test_data_fv_test,\n",
    "                                     scores_lstsqLambda_VS_targetPoly_test_data_fv_valid,\n",
    "                                     scores_lstsqLambda_VS_targetPoly_test_data_fv_test,\n",
    "                                     scores_predLambda_VS_lstsqTarget_test_data_fv_valid,\n",
    "                                     scores_predLambda_VS_lstsqTarget_test_data_fv_test,\n",
    "                                     scores_predLambda_VS_targetPoly_test_data_fv_valid,\n",
    "                                     scores_predLambda_VS_targetPoly_test_data_fv_test,\n",
    "                                     scores_lstsqTarget_VS_targetPoly_test_data_fv_valid,\n",
    "                                     scores_lstsqTarget_VS_targetPoly_test_data_fv_test],\n",
    "                               index=['inetPoly_VS_targetPoly_valid', \n",
    "                                      'inetPoly_VS_targetPoly_test', \n",
    "                                      'inetPoly_VS_predLambda_valid',\n",
    "                                      'inetPoly_VS_predLambda_test',\n",
    "                                      'inetPoly_VS_lstsqTarget_valid',\n",
    "                                      'inetPoly_VS_lstsqTarget_test',\n",
    "                                      'inetPoly_VS_lstsqLambda_valid',\n",
    "                                      'inetPoly_VS_lstsqLambda_test',\n",
    "                                      'lstsqLambda_VS_predLambda_valid',\n",
    "                                      'lstsqLambda_VS_predLambda_test',\n",
    "                                      'lstsqLambda_VS_lstsqTarget_valid',\n",
    "                                      'lstsqLambda_VS_lstsqTarget_test',\n",
    "                                      'lstsqLambda_VS_targetPoly_valid',\n",
    "                                      'lstsqLambda_VS_targetPoly_test',\n",
    "                                      'predLambda_VS_lstsqTarget_valid',\n",
    "                                      'predLambda_VS_lstsqTarget_test',\n",
    "                                      'predLambda_VS_targetPoly_valid',\n",
    "                                      'predLambda_VS_targetPoly_test',\n",
    "                                      'lstsqTarget_VS_targetPoly_valid',\n",
    "                                      'lstsqTarget_VS_targetPoly_test'])\n",
    "    \n",
    "\n",
    "    if return_model:\n",
    "        return history.history, scores_dict, function_values, pred_list, model         \n",
    "    else: \n",
    "        return history.history, scores_dict, function_values, pred_list       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T14:09:51.386457Z",
     "start_time": "2020-12-22T13:42:54.503814Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if samples_list == None: \n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, \n",
    "                            verbose=11, \n",
    "                            backend='loky')(delayed(train_nn_and_pred)(lambda_net_train_dataset,\n",
    "                                                                       lambda_net_valid_dataset,\n",
    "                                                                       lambda_net_test_dataset, \n",
    "                                                                       callback_names=['early_stopping']) for lambda_net_train_dataset,\n",
    "                                                                                                              lambda_net_valid_dataset,\n",
    "                                                                                                              lambda_net_test_dataset  in zip(lambda_net_train_dataset_list,\n",
    "                                                                                                                                              lambda_net_valid_dataset_list,\n",
    "                                                                                                                                              lambda_net_test_dataset_list))      \n",
    "\n",
    "    history_list = [result[0] for result in results_list]\n",
    "    \n",
    "    scores_list = [result[1] for result in results_list]\n",
    "    \n",
    "    function_values_complete_list = [result[2] for result in results_list]\n",
    "    function_values_valid_list = [function_values[0] for function_values in function_values_complete_list]\n",
    "    function_values_test_list = [function_values[1] for function_values in function_values_complete_list]\n",
    "\n",
    "    inet_preds_list = [result[3] for result in results_list]\n",
    "\n",
    "    for i, history in enumerate(history_list):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "        \n",
    "        plt.plot(history[list(history.keys())[1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_list, f, protocol=2)   \n",
    "        \n",
    "else:\n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, verbose=11, backend='loky')(delayed(train_nn_and_pred)(lambda_net_train_dataset.sample(samples),\n",
    "                                                                                                  lambda_net_valid_dataset,\n",
    "                                                                                                  lambda_net_test_dataset, \n",
    "                                                                                                  callback_names=['early_stopping']) for samples in samples_list)     \n",
    "    \n",
    "    history_list = [result[0] for result in results_list]\n",
    "    \n",
    "    scores_list = [result[1] for result in results_list]\n",
    "    \n",
    "    function_values_complete_list = [result[2] for result in results_list]\n",
    "    function_values_valid_list = [function_values[0] for function_values in function_values_complete_list]\n",
    "    function_values_test_list = [function_values[1] for function_values in function_values_complete_list]\n",
    "\n",
    "    inet_preds_list = [result[3] for result in results_list]\n",
    "    \n",
    "    for i, history in enumerate(history_list):       \n",
    "        \n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_list, f, protocol=2)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T14:59:56.738504Z",
     "start_time": "2020-12-22T14:59:56.719852Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T14:59:57.703697Z",
     "start_time": "2020-12-22T14:59:57.518048Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T14:59:58.105104Z",
     "start_time": "2020-12-22T14:59:57.810063Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:00.131388Z",
     "start_time": "2020-12-22T14:59:58.314228Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "if samples_list == None:\n",
    "    x_axis_steps = [(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]\n",
    "    x_max = epochs_lambda\n",
    "else:\n",
    "    x_axis_steps = samples_list\n",
    "    x_max = samples_list[-1]\n",
    "    \n",
    "if evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(length_plt//2, 2, figsize=(30,20))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        inetPoly_VS_targetPoly_test = []\n",
    "        #inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        #inetPoly_VS_lstsqLambda_test = []\n",
    "        #lstsqLambda_VS_predLambda_test = []\n",
    "        lstsqLambda_VS_lstsqTarget_test = []\n",
    "        #lstsqLambda_VS_targetPoly_test = []\n",
    "        predLambda_VS_lstsqTarget_test = []\n",
    "        #predLambda_VS_targetPoly_test = []\n",
    "        lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_targetPoly_test.append(scores[metric].loc['inetPoly_VS_targetPoly_test'])\n",
    "            predLambda_VS_lstsqTarget_test.append(scores[metric].loc['predLambda_VS_lstsqTarget_test'])\n",
    "            lstsqLambda_VS_lstsqTarget_test.append(scores[metric].loc['lstsqLambda_VS_lstsqTarget_test'])     \n",
    "            lstsqTarget_VS_targetPoly_test.append(scores[metric].loc['lstsqTarget_VS_targetPoly_test'])\n",
    "        \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_targetPoly_test, predLambda_VS_lstsqTarget_test, lstsqLambda_VS_lstsqTarget_test, lstsqTarget_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_targetPoly_test', 'predLambda_VS_lstsqTarget_test', 'lstsqLambda_VS_lstsqTarget_test', 'lstsqTarget_VS_targetPoly_test'])\n",
    "\n",
    "        ax[index//2, index%2].set_title(metric)\n",
    "        sns.lineplot(data=plot_df, ax=ax[index//2, index%2])\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_REAL_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else:\n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(length_plt//2, 2, figsize=(30,20))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        #inetPoly_VS_targetPoly_test = []\n",
    "        inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        inetPoly_VS_lstsqLambda_test = []\n",
    "        lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        #lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        #predLambda_VS_targetPoly_test = []\n",
    "        #lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_lstsqLambda_test.append(scores[metric].loc['inetPoly_VS_lstsqLambda_test'])\n",
    "            inetPoly_VS_predLambda_test.append(scores[metric].loc['inetPoly_VS_predLambda_test'])\n",
    "            lstsqLambda_VS_predLambda_test.append(scores[metric].loc['lstsqLambda_VS_predLambda_test'])     \n",
    "\n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_predLambda_test, inetPoly_VS_lstsqLambda_test, lstsqLambda_VS_predLambda_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test'])\n",
    "\n",
    "        ax[index//2, index%2].set_title(metric)\n",
    "        sns.lineplot(data=plot_df, ax=ax[index//2, index%2])\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else: \n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:00.516568Z",
     "start_time": "2020-12-22T15:00:00.133384Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "plot_metric_list = ['MAE FV']\n",
    "\n",
    "if samples_list == None:\n",
    "    x_axis_steps = [(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]\n",
    "    x_max = epochs_lambda\n",
    "else:\n",
    "    x_axis_steps = samples_list\n",
    "    x_max = samples_list[-1]\n",
    "    \n",
    "if evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        inetPoly_VS_targetPoly_test = []\n",
    "        #inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        #inetPoly_VS_lstsqLambda_test = []\n",
    "        #lstsqLambda_VS_predLambda_test = []\n",
    "        lstsqLambda_VS_lstsqTarget_test = []\n",
    "        #lstsqLambda_VS_targetPoly_test = []\n",
    "        predLambda_VS_lstsqTarget_test = []\n",
    "        #predLambda_VS_targetPoly_test = []\n",
    "        lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_targetPoly_test.append(scores[metric].loc['inetPoly_VS_targetPoly_test'])\n",
    "            predLambda_VS_lstsqTarget_test.append(scores[metric].loc['predLambda_VS_lstsqTarget_test'])\n",
    "            lstsqLambda_VS_lstsqTarget_test.append(scores[metric].loc['lstsqLambda_VS_lstsqTarget_test'])     \n",
    "            lstsqTarget_VS_targetPoly_test.append(scores[metric].loc['lstsqTarget_VS_targetPoly_test'])\n",
    "        \n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_targetPoly_test, predLambda_VS_lstsqTarget_test, lstsqLambda_VS_lstsqTarget_test, lstsqTarget_VS_targetPoly_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_targetPoly_test', 'predLambda_VS_lstsqTarget_test', 'lstsqLambda_VS_lstsqTarget_test', 'lstsqTarget_VS_targetPoly_test'])\n",
    "\n",
    "        ax.set_title(metric)\n",
    "        sns.lineplot(data=plot_df, ax=ax)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_REAL_' + metric + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else:\n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "\n",
    "        #inetPoly_VS_targetPoly_test = []\n",
    "        inetPoly_VS_predLambda_test = []\n",
    "        #inetPoly_VS_lstsqTarget_test = []\n",
    "        inetPoly_VS_lstsqLambda_test = []\n",
    "        lstsqLambda_VS_predLambda_test = []\n",
    "        #lstsqLambda_VS_lstsqTarget_test = []\n",
    "        #lstsqLambda_VS_targetPoly_test = []\n",
    "        #predLambda_VS_lstsqTarget_test = []\n",
    "        #predLambda_VS_targetPoly_test = []\n",
    "        #lstsqTarget_VS_targetPoly_test = []\n",
    "\n",
    "        for scores in scores_list:\n",
    "            inetPoly_VS_lstsqLambda_test.append(scores[metric].loc['inetPoly_VS_lstsqLambda_test'])\n",
    "            inetPoly_VS_predLambda_test.append(scores[metric].loc['inetPoly_VS_predLambda_test'])\n",
    "            lstsqLambda_VS_predLambda_test.append(scores[metric].loc['lstsqLambda_VS_predLambda_test'])     \n",
    "\n",
    "        plot_df = pd.DataFrame(data=np.vstack([inetPoly_VS_predLambda_test, inetPoly_VS_lstsqLambda_test, lstsqLambda_VS_predLambda_test]).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test'])\n",
    "\n",
    "        ax.set_title(metric)\n",
    "        sns.lineplot(data=plot_df, ax=ax)\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    if samples_list == None:\n",
    "        file = 'multi_epoch_MODEL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    else: \n",
    "        file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + metric + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:00.530747Z",
     "start_time": "2020-12-22T15:00:00.518390Z"
    }
   },
   "outputs": [],
   "source": [
    "rand_index = 42\n",
    "\n",
    "lambda_model_preds = function_values_test_list[-1][0][rand_index].ravel()\n",
    "real_poly_fvs = function_values_test_list[-1][1][rand_index]\n",
    "lstsq_lambda_preds_poly = function_values_test_list[-1][2][rand_index]\n",
    "lstsq_target_poly = function_values_test_list[-1][3][rand_index]\n",
    "inet_poly_fvs = function_values_test_list[-1][4][rand_index]\n",
    "\n",
    "    \n",
    "x_vars = ['x' + str(i) for i in range(1, n+1)]\n",
    "\n",
    "columns = x_vars.copy()\n",
    "columns.append('FVs')\n",
    "\n",
    "columns_single = x_vars.copy()\n",
    "\n",
    "eval_size_plot = inet_poly_fvs.shape[0]\n",
    "vars_plot = lambda_net_test_dataset_list[-1].test_data_list[rand_index]\n",
    "\n",
    "    \n",
    "if evaluate_with_real_function:\n",
    "    columns_single.extend(['Lambda Model Preds', 'Target Poly FVs', 'LSTSQ Target Poly FVs', 'I-Net Poly FVs'])\n",
    "    plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, lambda_model_preds, real_poly_fvs, lstsq_target_poly, inet_poly_fvs]), columns=columns_single)\n",
    "    preds_plot_all = np.vstack([lambda_model_preds, real_poly_fvs, lstsq_target_poly, inet_poly_fvs]).ravel()\n",
    "    vars_plot_all_preds = np.vstack([vars_plot for i in range(len(columns_single[n:]))])\n",
    "    \n",
    "    lambda_model_preds_str = np.array(['Lambda Model Preds' for i in range(eval_size_plot)])\n",
    "    real_poly_fvs_str = np.array(['Target Poly FVs' for i in range(eval_size_plot)])\n",
    "    lstsq_target_poly_str = np.array(['LSTSQ Target Poly FVs' for i in range(eval_size_plot)])\n",
    "    inet_poly_fvs_str = np.array(['I-Net Poly FVs' for i in range(eval_size_plot)])\n",
    "    \n",
    "    identifier = np.concatenate([lambda_model_preds_str, real_poly_fvs_str, lstsq_target_poly_str, inet_poly_fvs_str])\n",
    "else:\n",
    "    columns_single.extend(['Lambda Model Preds', 'Target Poly FVs', 'LSTSQ Lambda Poly FVs', 'I-Net Poly FVs'])\n",
    "    plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, lambda_model_preds, real_poly_fvs, lstsq_lambda_preds_poly, inet_poly_fvs]), columns=columns_single)\n",
    "    preds_plot_all = np.vstack([lambda_model_preds, real_poly_fvs, lstsq_lambda_preds_poly, inet_poly_fvs]).ravel()\n",
    "    vars_plot_all_preds = np.vstack([vars_plot for i in range(len(columns_single[n:]))])\n",
    "    \n",
    "    lambda_model_preds_str = np.array(['Lambda Model Preds' for i in range(eval_size_plot)])\n",
    "    real_poly_fvs_str = np.array(['Target Poly FVs' for i in range(eval_size_plot)])\n",
    "    lstsq_lambda_preds_poly_str = np.array(['LSTSQ Lambda Poly FVs' for i in range(eval_size_plot)])\n",
    "    inet_poly_fvs_str = np.array(['I-Net Poly FVs' for i in range(eval_size_plot)])\n",
    "    \n",
    "    identifier = np.concatenate([lambda_model_preds_str, real_poly_fvs_str, lstsq_lambda_preds_poly_str, inet_poly_fvs_str])\n",
    "\n",
    "plot_data = pd.DataFrame(data=np.column_stack([vars_plot_all_preds, preds_plot_all]), columns=columns)\n",
    "plot_data['Identifier'] = identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:01.186880Z",
     "start_time": "2020-12-22T15:00:00.532412Z"
    }
   },
   "outputs": [],
   "source": [
    "pp1 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  y_vars=['FVs'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:05.010043Z",
     "start_time": "2020-12-22T15:00:01.189813Z"
    }
   },
   "outputs": [],
   "source": [
    "pp2 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  #y_vars=['FVs'],\n",
    "                  #x_vars=x_vars\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:06.301459Z",
     "start_time": "2020-12-22T15:00:05.012758Z"
    }
   },
   "outputs": [],
   "source": [
    "pp3 = sns.pairplot(data=plot_data_single,\n",
    "                  #kind='reg',\n",
    "                  y_vars=columns_single[n:],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:08.052138Z",
     "start_time": "2020-12-22T15:00:06.303248Z"
    }
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_REAL_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')\n",
    "else:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_PRED_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:08.173748Z",
     "start_time": "2020-12-22T15:00:08.055014Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = []\n",
    "for i in range(len(lambda_net_test_dataset_list[-1])):\n",
    "    random_polynomial = list(random_product([i*a_step for i in range(int(a_min*10**int(-np.log10(a_step))), int(a_max*10**int(-np.log10(a_step))))], repeat=nCr(n+d, d)))\n",
    "    list_of_random_polynomials.append(random_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:21.650736Z",
     "start_time": "2020-12-22T15:00:08.175588Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].test_data_list)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:21.656786Z",
     "start_time": "2020-12-22T14:59:59.826Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:21.658023Z",
     "start_time": "2020-12-22T14:59:59.987Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:21.659027Z",
     "start_time": "2020-12-22T15:00:00.318Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].test_data_list)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:21.660186Z",
     "start_time": "2020-12-22T15:00:00.502Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T15:00:21.661196Z",
     "start_time": "2020-12-22T15:00:00.676Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
