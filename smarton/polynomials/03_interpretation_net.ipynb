{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:31.809670Z",
     "start_time": "2020-11-20T12:16:31.803211Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:31.825043Z",
     "start_time": "2020-11-20T12:16:31.813952Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3  \n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "n_jobs = -3\n",
    "\n",
    "\n",
    "data_size = 10000 #for loading lambda models\n",
    "\n",
    "#specify interpretation net structure\n",
    "optimizer = 'adam'\n",
    "dropout = 0\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "interpretation_network_layers = [2048]\n",
    "\n",
    "\n",
    "#lambda net specifications for loading (need to be set according to lambda net training to load correct weights)\n",
    "epochs_lambda = 200\n",
    "batch_lambda = 64\n",
    "lambda_network_layers = [5*sparsity]\n",
    "optimizer_lambda = '_' + 'SGD'\n",
    "\n",
    "\n",
    "lambda_dataset_size = 1000\n",
    "\n",
    "#set if multi_epoch_analysis should be performed\n",
    "multi_epoch_analysis = True\n",
    "each_epochs_save_lambda = 10 #None if no checkpointing (otherwise set according to lambda-net training)\n",
    "epoch_start = 0 #use to skip first epochs in multi_epoch_analysis\n",
    "\n",
    "#set if samples analysis should be performed\n",
    "samples_list = None#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "\n",
    "evaluate_with_real_function = False\n",
    "consider_labels_training = False\n",
    "\n",
    "same_training_all_lambda_nets = True\n",
    "fixed_seed_lambda_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:31.835684Z",
     "start_time": "2020-11-20T12:16:31.826819Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1)\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_SeedMethod'\n",
    "elif not fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_NoSeedMethod'\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "structure = '_' + layers_str + str(epochs_lambda) + 'e' + str(batch_lambda) + 'b' + optimizer_lambda\n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure\n",
    "\n",
    "interpretation_network_string = 'drop' + str(dropout) + 'e' + str(epochs) + 'b' + str(batch_size) + '_' + str(interpretation_network_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:36.888279Z",
     "start_time": "2020-11-20T12:16:31.837648Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:36.893913Z",
     "start_time": "2020-11-20T12:16:36.890106Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:36.903363Z",
     "start_time": "2020-11-20T12:16:36.895661Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:36.912032Z",
     "start_time": "2020-11-20T12:16:36.905383Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Given variable is no instance of ' + str(np.ndarray) + ':' + str(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:36.933545Z",
     "start_time": "2020-11-20T12:16:36.913900Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "\n",
    "def return_float_tensor_representation(some_representation):\n",
    "    if tf.is_tensor(some_representation):\n",
    "        some_representation = tf.dtypes.cast(some_representation, tf.float32) \n",
    "    else:\n",
    "        some_representation = tf.convert_to_tensor(some_representation)\n",
    "        some_representation = tf.dtypes.cast(some_representation, tf.float32) \n",
    "        \n",
    "    if not tf.is_tensor(some_representation):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(tf.float32) + ':' + str(some_representation))\n",
    "     \n",
    "    return some_representation\n",
    "\n",
    "\n",
    "def return_numpy_representation(some_representation):\n",
    "    if isinstance(some_representation, pd.DataFrame):\n",
    "        some_representation = some_representation.values\n",
    "        \n",
    "    if isinstance(some_representation, list):\n",
    "        some_representation = np.array(some_representation)\n",
    "    \n",
    "    if not isinstance(some_representation, np.ndarray):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(np.ndarray) + ':' + str(some_representation))\n",
    "    \n",
    "    return some_representation\n",
    "\n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict\n",
    "\n",
    "def return_callbacks_from_string(callback_string_list):\n",
    "    callbacks = [] if len(callback_string_list) > 0 else None\n",
    "    if 'plot_losses_callback' in callback_string_list:\n",
    "        callbacks.append(PlotLossesKerasTF())\n",
    "    if 'reduce_lr_loss' in callback_string_list:\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=epochs/10, verbose=0, min_delta=0, mode='min') #epsilon\n",
    "        callbacks.append(reduce_lr_loss)\n",
    "    if 'early_stopping' in callback_string_list:\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0, verbose=0, mode='min')\n",
    "        callbacks.append(earlyStopping)\n",
    "        \n",
    "    if not multi_epoch_analysis and samples_list == None: \n",
    "        callbacks.append(TQDMNotebookCallback())\n",
    "        \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:36.981747Z",
     "start_time": "2020-11-20T12:16:36.935043Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:36.989248Z",
     "start_time": "2020-11-20T12:16:36.982967Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with polynomials on function value basis\n",
    "\n",
    "def calculate_mae_fv(polynomial_true_pred):\n",
    "    polynomial_true = polynomial_true_pred[0]\n",
    "    polynomial_pred = polynomial_true_pred[1]\n",
    "    \n",
    "    global lambda_train_input\n",
    "    lambda_input = lambda_train_input\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "            \n",
    "        polynomial_true_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_true))\n",
    "        polynomial_true_fv = tf.reduce_sum(polynomial_true_value_per_term)\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "        \n",
    "        if index == 0:   \n",
    "            result = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "        else:           \n",
    "            current_valiue = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "            result = tf.math.add(result, current_valiue)    \n",
    "            \n",
    "    return  tf.math.divide(result, lambda_input.shape[0]) #tf.random.uniform(shape=[1], minval=0.1, maxval=10.0)\n",
    "\n",
    "def mean_absolute_error_tf_fv(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv, (y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.001527Z",
     "start_time": "2020-11-20T12:16:36.990589Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with pre-saved lambda-net predictions\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def calculate_mae_fv_lambda(input_list):\n",
    "    \n",
    "    polynomial_true = input_list[0]\n",
    "    polynomial_pred = input_list[1]\n",
    "    lambda_fv = input_list[2]\n",
    "\n",
    "    global lambda_train_input\n",
    "    lambda_input = lambda_train_input\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "\n",
    "        if index == 0:\n",
    "            polynomial_pred_fv_list = tf.convert_to_tensor([polynomial_pred_fv])\n",
    "        else:\n",
    "            polynomial_pred_fv_list = tf.concat([polynomial_pred_fv_list, tf.convert_to_tensor([polynomial_pred_fv])], 0)\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "  \n",
    "    \n",
    "def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "    lambda_fv = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda, (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "\n",
    "\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.016823Z",
     "start_time": "2020-11-20T12:16:37.002881Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Basic Keras/TF Loss functions\n",
    "def root_mean_squared_error(y_true, y_pred):   \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "        \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)           \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))      \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)        \n",
    "    epsilon = return_float_tensor_representation(epsilon)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.028435Z",
     "start_time": "2020-11-20T12:16:37.018215Z"
    }
   },
   "outputs": [],
   "source": [
    "#Manual calculations for comparison of polynomials based on function values (no TF!)\n",
    "\n",
    "def calcualate_function_value(coefficient_list, lambda_input_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0   \n",
    "        \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        value_without_coefficient = [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multipliers, lambda_input_entry)]\n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, value_without_coefficient)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_function_values_from_polynomial(polynomial, lambda_input_data):\n",
    "    polynomial = return_numpy_representation(polynomial)\n",
    "    \n",
    "    function_value_list = []\n",
    "        \n",
    "    for lambda_input_entry in lambda_input_data:\n",
    "        function_value = calcualate_function_value(polynomial, lambda_input_entry)\n",
    "        function_value_list.append(function_value)\n",
    "\n",
    "    return np.array(function_value_list)\n",
    "\n",
    "\n",
    "def parallel_fv_calculation_from_polynomial(polynomial_list, lambda_input_list):\n",
    "    parallel = Parallel(n_jobs=10, verbose=0, backend='threading')\n",
    "    polynomial_true_fv = parallel(delayed(calculate_function_values_from_polynomial)(polynomial, lambda_input_list) for polynomial in polynomial_list)  \n",
    "    del parallel   \n",
    "    \n",
    "    return np.array(polynomial_true_fv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.049860Z",
     "start_time": "2020-11-20T12:16:37.030054Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Standard Metrics (no TF!)\n",
    "\n",
    "def mean_absolute_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)      \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(true_values-pred_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))  \n",
    "\n",
    "def root_mean_squared_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)         \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sqrt(np.mean((true_values-pred_values)**2)))\n",
    "    \n",
    "    return np.mean(np.array(result_list)) \n",
    "\n",
    "def mean_absolute_percentage_error_function_values(y_true, y_pred, epsilon=10e-3):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred) \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(((true_values-pred_values)/(true_values+epsilon)))))\n",
    "\n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def r2_score_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(r2_score(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_absolute_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sum(np.abs(true_values-pred_values))/(true_values.shape[0]*np.std(true_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_maximum_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.max(true_values-pred_values)/np.std(true_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_area_between_two_curves_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "      \n",
    "    assert(number_of_variables==1)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(area_between_two_curves(true_values, pred_values))\n",
    " \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_dtw_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "\n",
    "    result_list_single = []\n",
    "    result_list_array = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_single_value, result_single_array = dtw(true_values, pred_values)\n",
    "        result_list_single.append(result_single_value)\n",
    "        result_list_array.append(result_single_array)\n",
    "    \n",
    "    return np.mean(np.array(result_list_single)), np.mean(np.array(result_list_array), axis=1)\n",
    "\n",
    "def mean_frechet_dist_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(frechet_dist(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.062391Z",
     "start_time": "2020-11-20T12:16:37.052063Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_interpretation_net(y_data_real, \n",
    "                                y_data_pred, \n",
    "                                polynomial_true_fv, \n",
    "                                polynomial_pred_inet_fv):\n",
    "    \n",
    "    mae_coeff = np.round(mean_absolute_error(y_data_real, y_data_pred), 4)\n",
    "    rmse_coeff = np.round(root_mean_squared_error(y_data_real, y_data_pred), 4)\n",
    "    mape_coeff = np.round(mean_absolute_percentage_error_keras(y_data_real, y_data_pred), 4)\n",
    "    accuracy_coeff = np.round(accuracy_single(y_data_real, y_data_pred), 4)\n",
    "    accuracy_multi_coeff = np.round(accuracy_multilabel(y_data_real, y_data_pred), 4)\n",
    "    \n",
    "    mae_fv = np.round(mean_absolute_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmse_fv = np.round(root_mean_squared_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    mape_fv = np.round(mean_absolute_percentage_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    r2_fv = np.round(r2_score_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    raae_fv = np.round(relative_absolute_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmae_fv = np.round(relative_maximum_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4) \n",
    "\n",
    "    std_fv = np.std(polynomial_pred_inet_fv)\n",
    "    mean_fv = np.mean(polynomial_pred_inet_fv)\n",
    "\n",
    "    return {\n",
    "             'MAE': mae_coeff,\n",
    "             'RMSE': rmse_coeff, \n",
    "             'MAPE': mape_coeff,\n",
    "             'Accuracy': accuracy_coeff, \n",
    "             'Accuracy Multilabel': accuracy_multi_coeff, \n",
    "\n",
    "             'MAE FV': mae_fv,\n",
    "             'RMSE FV': rmse_fv,\n",
    "             'MAPE FV': mape_fv,\n",
    "             'R2 FV': r2_fv,\n",
    "             'RAAE FV': raae_fv,\n",
    "             'RMAE FV': rmae_fv,         \n",
    "             'STD FV PRED': std_fv,   \n",
    "             'MEAN FV PRED': mean_fv\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.075433Z",
     "start_time": "2020-11-20T12:16:37.066512Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    return weight_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.083125Z",
     "start_time": "2020-11-20T12:16:37.076950Z"
    }
   },
   "outputs": [],
   "source": [
    "#epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda)\n",
    "print(epoch_start)\n",
    "print(each_epochs_save_lambda)\n",
    "print(epochs_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.091816Z",
     "start_time": "2020-11-20T12:16:37.084687Z"
    }
   },
   "outputs": [],
   "source": [
    "[index for index in range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.099625Z",
     "start_time": "2020-11-20T12:16:37.093398Z"
    }
   },
   "outputs": [],
   "source": [
    "index = each_epochs_save_lambda if i > 1 else max(each_epochs_save_lambda-1, 1) if i==1 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.112224Z",
     "start_time": "2020-11-20T12:16:37.101366Z"
    }
   },
   "outputs": [],
   "source": [
    "[each_epochs_save_lambda if i > 1 else max(each_epochs_save_lambda-1, 1) if i==1 else 1 for i in range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.118962Z",
     "start_time": "2020-11-20T12:16:37.114972Z"
    }
   },
   "outputs": [],
   "source": [
    "index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.130264Z",
     "start_time": "2020-11-20T12:16:37.121112Z"
    }
   },
   "outputs": [],
   "source": [
    "[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.138280Z",
     "start_time": "2020-11-20T12:16:37.131839Z"
    }
   },
   "outputs": [],
   "source": [
    "index = i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.149564Z",
     "start_time": "2020-11-20T12:16:37.140554Z"
    }
   },
   "outputs": [],
   "source": [
    "[i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:37.159973Z",
     "start_time": "2020-11-20T12:16:37.152151Z"
    }
   },
   "outputs": [],
   "source": [
    "[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:38.838869Z",
     "start_time": "2020-11-20T12:16:37.161763Z"
    }
   },
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "if multi_epoch_analysis:  \n",
    "    weight_data_list = []\n",
    "    \n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    weight_data_list = parallel(delayed(load_data)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    weight_data = weight_data_list[-1]\n",
    "else:\n",
    "\n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "                \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:41.857945Z",
     "start_time": "2020-11-20T12:16:38.841325Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis == False:\n",
    "    path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    lambda_train_fv = pd.read_csv(path_lambda_train_fv, sep=',')\n",
    "    lambda_valid_fv = pd.read_csv(path_lambda_valid_fv, sep=',')\n",
    "    lambda_test_fv = pd.read_csv(path_lambda_test_fv, sep=',') \n",
    "    \n",
    "    lambda_train_fv = lambda_train_fv.sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "    lambda_valid_fv = lambda_valid_fv.sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "    lambda_test_fv = lambda_test_fv.sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "        \n",
    "    #ADJUSTMENT: CANT SAVE IN DATAFRAME COLUMN IF FALSE\n",
    "    if same_training_all_lambda_nets:\n",
    "        lambda_train_input = []\n",
    "        for lambda_input_entry in lambda_train_fv.columns:\n",
    "            lambda_train_input.append(np.array(lambda_input_entry[1:-1].split()).astype('float'))\n",
    "        lambda_train_input = np.array(lambda_train_input)\n",
    "\n",
    "        lambda_valid_input = []\n",
    "        for lambda_input_entry in lambda_valid_fv.columns:\n",
    "            lambda_valid_input.append(np.array(lambda_input_entry[1:-1].split()).astype('float'))\n",
    "        lambda_valid_input = np.array(lambda_valid_input)\n",
    "\n",
    "        lambda_test_input = []\n",
    "        for lambda_input_entry in lambda_test_fv.columns:\n",
    "            lambda_test_input.append(np.array(lambda_input_entry[1:-1].split()).astype('float'))\n",
    "        lambda_test_input  = np.array(lambda_test_input)\n",
    "\n",
    "else:\n",
    "    lambda_train_fv_list = []\n",
    "    lambda_valid_fv_list = []\n",
    "    lambda_test_fv_list = []\n",
    "\n",
    "    for i in tqdm(epochs_save_range_lambda):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "        path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "        path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "        path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        lambda_train_fv = pd.read_csv(path_lambda_train_fv, sep=',')\n",
    "        lambda_valid_fv = pd.read_csv(path_lambda_valid_fv, sep=',')\n",
    "        lambda_test_fv = pd.read_csv(path_lambda_test_fv, sep=',')     \n",
    "        \n",
    "        lambda_train_fv = lambda_train_fv.sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "        lambda_valid_fv = lambda_valid_fv.sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "        lambda_test_fv = lambda_test_fv.sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "  \n",
    "        lambda_train_fv_list.append(lambda_train_fv)\n",
    "        lambda_valid_fv_list.append(lambda_valid_fv)\n",
    "        lambda_test_fv_list.append(lambda_test_fv)\n",
    "   \n",
    "    #ADJUSTMENT: CANT SAVE IN DATAFRAME COLUMN IF FALSE\n",
    "    if same_training_all_lambda_nets:\n",
    "        lambda_train_input = []\n",
    "        for lambda_input_entry in lambda_train_fv.columns:\n",
    "            lambda_train_input.append(np.array(lambda_input_entry[1:-1].split()).astype('float'))\n",
    "        lambda_train_input = np.array(lambda_train_input)\n",
    "\n",
    "        lambda_valid_input = []\n",
    "        for lambda_input_entry in lambda_valid_fv.columns:\n",
    "            lambda_valid_input.append(np.array(lambda_input_entry[1:-1].split()).astype('float'))\n",
    "        lambda_valid_input = np.array(lambda_valid_input)\n",
    "\n",
    "        lambda_test_input = []\n",
    "        for lambda_input_entry in lambda_test_fv.columns:\n",
    "            lambda_test_input.append(np.array(lambda_input_entry[1:-1].split()).astype('float'))\n",
    "        lambda_test_input  = np.array(lambda_test_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:41.886600Z",
     "start_time": "2020-11-20T12:16:41.859822Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:41.902195Z",
     "start_time": "2020-11-20T12:16:41.888142Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_test_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:41.923089Z",
     "start_time": "2020-11-20T12:16:41.903495Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_valid_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:44.170050Z",
     "start_time": "2020-11-20T12:16:41.924429Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:44.541125Z",
     "start_time": "2020-11-20T12:16:44.171611Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "if multi_epoch_analysis:    \n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    X_valid_list = []\n",
    "    y_valid_list = []\n",
    "    \n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    \n",
    "    lambda_test_fv_valid_split_list = []\n",
    "    lambda_test_fv_test_split_list = []\n",
    "    lambda_test_fv_train_split_list = []\n",
    "    \n",
    "    lambda_valid_fv_valid_split_list = []\n",
    "    lambda_valid_fv_test_split_list = []\n",
    "    lambda_valid_fv_train_split_list = []\n",
    "    \n",
    "    lambda_train_fv_valid_split_list = []\n",
    "    lambda_train_fv_test_split_list = []\n",
    "    lambda_train_fv_train_split_list = []\n",
    "    \n",
    "    for weight_data, lambda_train_fv, lambda_valid_fv, lambda_test_fv in tqdm(zip(weight_data_list, lambda_train_fv_list, lambda_valid_fv_list, lambda_test_fv_list), total=len(weight_data_list)): \n",
    "        \n",
    "        if psutil.virtual_memory().percent > 80:\n",
    "            raise SystemExit(\"Out of RAM!\")\n",
    "        \n",
    "        X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3)], axis=1)\n",
    "        y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3)]].astype(float)\n",
    "        \n",
    "        lambda_train_fv = lambda_train_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv = lambda_valid_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv = lambda_test_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "        y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "        y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "        y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)      \n",
    "        \n",
    "        #y_data_polynomial = y_data_polynomial_true\n",
    "        #y_data_polynomial_pred_lstsq = y_data_polynomial_lstsq_pred\n",
    "        #y_data_polynomial_true_lstsq = y_data_polynomial_lstsq_true\n",
    "        \n",
    "        if evaluate_with_real_function:\n",
    "            y_data = y_data_polynomial_true\n",
    "        else:\n",
    "            y_data = y_data_polynomial_lstsq_pred  \n",
    "                                         \n",
    "        X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    \n",
    "        X_train_list.append(X_train)\n",
    "        y_train_list.append(y_train)\n",
    "\n",
    "        X_valid_list.append(X_valid)\n",
    "        y_valid_list.append(y_valid)\n",
    "\n",
    "        X_test_list.append(X_test)\n",
    "        y_test_list.append(y_test)   \n",
    "        \n",
    "\n",
    "        lambda_train_fv_with_valid_split, lambda_train_fv_test_split = train_test_split(lambda_train_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_train_fv_train_split, lambda_train_fv_valid_split = train_test_split(lambda_train_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_train_fv_valid_split_list.append(lambda_train_fv_valid_split)\n",
    "        lambda_train_fv_test_split_list.append(lambda_train_fv_test_split)\n",
    "        lambda_train_fv_train_split_list.append(lambda_train_fv_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_valid_fv_with_valid_split, lambda_valid_fv_test_split = train_test_split(lambda_valid_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv_train_split, lambda_valid_fv_valid_split = train_test_split(lambda_valid_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_valid_fv_valid_split_list.append(lambda_valid_fv_valid_split)\n",
    "        lambda_valid_fv_test_split_list.append(lambda_valid_fv_test_split)\n",
    "        lambda_valid_fv_train_split_list.append(lambda_valid_fv_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_test_fv_with_valid_split, lambda_test_fv_test_split = train_test_split(lambda_test_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv_train_split, lambda_test_fv_valid_split = train_test_split(lambda_test_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_test_fv_valid_split_list.append(lambda_test_fv_valid_split)\n",
    "        lambda_test_fv_test_split_list.append(lambda_test_fv_test_split)\n",
    "        lambda_test_fv_train_split_list.append(lambda_test_fv_train_split)   \n",
    "            \n",
    "else:    \n",
    "    lambda_test_fv_valid_split_list = []\n",
    "    lambda_test_fv_test_split_list = []\n",
    "    lambda_test_fv_train_split_list = []\n",
    "    \n",
    "    lambda_valid_fv_valid_split_list = []\n",
    "    lambda_valid_fv_test_split_list = []\n",
    "    lambda_valid_fv_train_split_list = []\n",
    "    \n",
    "    lambda_train_fv_valid_split_list = []\n",
    "    lambda_train_fv_test_split_list = []\n",
    "    lambda_train_fv_train_split_list = []\n",
    "    \n",
    "    X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3)], axis=1)\n",
    "    y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3)]].astype(float)\n",
    "    \n",
    "    y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "    y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "    y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)\n",
    "    \n",
    "    lambda_train_fv = lambda_train_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_valid_fv = lambda_valid_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_test_fv = lambda_test_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        y_data = y_data_polynomial_true\n",
    "    else:\n",
    "        y_data = y_data_polynomial_lstsq_pred                                     \n",
    "    \n",
    "        lambda_train_fv_with_valid_split, lambda_train_fv_test_split = train_test_split(lambda_train_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_train_fv_train_split, lambda_train_fv_valid_split = train_test_split(lambda_train_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_valid_fv_with_valid_split, lambda_valid_fv_test_split = train_test_split(lambda_valid_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv_train_split, lambda_valid_fv_valid_split = train_test_split(lambda_valid_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "        lambda_test_fv_with_valid_split, lambda_test_fv_test_split = train_test_split(lambda_test_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv_train_split, lambda_test_fv_valid_split = train_test_split(lambda_test_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:44.556939Z",
     "start_time": "2020-11-20T12:16:44.542768Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = X_train_list[-1].head()\n",
    "else:\n",
    "    print_head = X_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:44.578000Z",
     "start_time": "2020-11-20T12:16:44.558216Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = y_train_list[-1].head()\n",
    "else:\n",
    "    print_head = y_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:16:44.594347Z",
     "start_time": "2020-11-20T12:16:44.579479Z"
    },
    "code_folding": [
     94
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn_and_pred(X_train, \n",
    "                      X_valid, \n",
    "                      X_test, \n",
    "                      y_train, \n",
    "                      y_valid, \n",
    "                      y_test,\n",
    "                      lambda_train_fv_valid_split, \n",
    "                      lambda_train_fv_test_split, \n",
    "                      lambda_train_fv_train_split, \n",
    "                      lambda_valid_fv_valid_split, \n",
    "                      lambda_valid_fv_test_split, \n",
    "                      lambda_valid_fv_train_split, \n",
    "                      lambda_test_fv_valid_split, \n",
    "                      lambda_test_fv_test_split, \n",
    "                      lambda_test_fv_train_split, \n",
    "                      callback_names=[], \n",
    "                      return_model=False):       \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(interpretation_network_layers[0], activation='relu', input_dim=X_train.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in interpretation_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(nCr(n+d, d))) \n",
    "    \n",
    "    #decide whether to use lambda preds for evaluation or polynomial from lstsq lambda preds\n",
    "    if not consider_labels_training and not evaluate_with_real_function:\n",
    "        loss_function = mean_absolute_error_tf_fv_lambda_extended\n",
    "        metrics = [mean_absolute_error_tf_fv_lambda_extended, mean_absolute_error_extended]\n",
    "        valid_data = None\n",
    "        y_train_model = np.hstack((y_train, lambda_train_fv_train_split))\n",
    "    else:\n",
    "        loss_function = mean_absolute_error_tf_fv\n",
    "        metrics = ['mean_absolute_error']\n",
    "        valid_data = (X_valid, y_valid)\n",
    "        y_train_model = y_train\n",
    "      \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=metrics\n",
    "                 )\n",
    "\n",
    "    #Callbacks\n",
    "    callbacks = return_callbacks_from_string(callback_names)\n",
    "        \n",
    "    history = model.fit(X_train,\n",
    "              y_train_model,\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              validation_data=valid_data,\n",
    "              callbacks=callbacks,\n",
    "              verbose=0)\n",
    "\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    \n",
    "    polynomial_true_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_valid_input)\n",
    "    polynomial_pred_inet_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_valid_input)\n",
    "    \n",
    "    polynomial_true_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_valid_input)\n",
    "    polynomial_pred_inet_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_valid_input)\n",
    "\n",
    "    \n",
    "    polynomial_true_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_test_input)\n",
    "    polynomial_pred_inet_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_test_input)\n",
    "    \n",
    "    polynomial_true_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_test_input)\n",
    "    polynomial_pred_inet_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_test_input)\n",
    "    \n",
    "    \n",
    "    polynomial_test_fv = [polynomial_true_test_fv_valid_split, \n",
    "                            polynomial_pred_inet_test_fv_valid_split, \n",
    "                            polynomial_true_test_fv_test_split, \n",
    "                            polynomial_pred_inet_test_fv_test_split]\n",
    "    \n",
    "    polynomial_valid_fv = [polynomial_true_valid_fv_valid_split, \n",
    "                             polynomial_pred_inet_valid_fv_valid_split, \n",
    "                             polynomial_true_valid_fv_test_split, \n",
    "                             polynomial_pred_inet_valid_fv_test_split]\n",
    "    \n",
    "    polynomial_fv = [polynomial_valid_fv, polynomial_test_fv]\n",
    "    \n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_test_fv_valid_split, \n",
    "                                polynomial_pred_inet_test_fv_valid_split)\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_test_fv_test_split, \n",
    "                                polynomial_pred_inet_test_fv_test_split)\n",
    "    \n",
    "    \n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_valid_fv_valid_split, \n",
    "                                polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_valid_fv_test_split, \n",
    "                                polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv_valid_split, scores_truePoly_VS_inetPoly_test_fv_test_split)\n",
    "    scores_truePoly_VS_inetPoly_valid_fv = mergeDict(scores_truePoly_VS_inetPoly_valid_fv_valid_split, scores_truePoly_VS_inetPoly_valid_fv_test_split)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        scores_dict = [scores_truePoly_VS_inetPoly_test_fv, scores_truePoly_VS_inetPoly_valid_fv]\n",
    "    else:   \n",
    "        scores_predLambda_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_test_fv_valid_split, \n",
    "                                    polynomial_pred_inet_test_fv_valid_split)\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_test_fv_test_split, \n",
    "                                    polynomial_pred_inet_test_fv_test_split)\n",
    "\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_valid_fv_valid_split, \n",
    "                                    polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_valid_fv_test_split, \n",
    "                                    polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "        metrics = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'STD FV PRED', 'MEAN FV PRED']\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_test_fv = mergeDict(scores_predLambda_VS_inetPoly_test_fv_valid_split, scores_predLambda_VS_inetPoly_test_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_test_fv = {key: scores_predLambda_VS_inetPoly_test_fv[key] for key in metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv, scores_predLambda_VS_inetPoly_test_fv)\n",
    "        \n",
    "        scores_predLambda_VS_inetPoly_valid_fv = mergeDict(scores_predLambda_VS_inetPoly_valid_fv_valid_split, scores_predLambda_VS_inetPoly_valid_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_valid_fv = {key: scores_predLambda_VS_inetPoly_valid_fv[key] for key in metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_valid_fv =mergeDict(scores_truePoly_VS_inetPoly_valid_fv, scores_predLambda_VS_inetPoly_valid_fv)\n",
    "\n",
    "        scores_dict = [scores_truePoly_and_predLambda_VS_inetPoly_test_fv, scores_truePoly_and_predLambda_VS_inetPoly_valid_fv]\n",
    "\n",
    "    if return_model:\n",
    "        return history.history, scores_dict, polynomial_fv, model         \n",
    "    else: \n",
    "        return history.history, scores_dict, polynomial_fv       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:26.348541Z",
     "start_time": "2020-11-20T12:16:44.595972Z"
    },
    "code_folding": [
     2,
     47,
     142
    ]
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results = train_nn_and_pred(X_train.values, \n",
    "                                X_valid.values, \n",
    "                                X_test.values, \n",
    "                                y_train.values, \n",
    "                                y_valid.values, \n",
    "                                y_test.values, \n",
    "                                lambda_train_fv_valid_split.values, \n",
    "                                lambda_train_fv_test_split.values, \n",
    "                                lambda_train_fv_train_split.values, \n",
    "                                lambda_valid_fv_valid_split.values, \n",
    "                                lambda_valid_fv_test_split.values, \n",
    "                                lambda_valid_fv_train_split.values, \n",
    "                                lambda_test_fv_valid_split.values, \n",
    "                                lambda_test_fv_test_split.values, \n",
    "                                lambda_test_fv_train_split.values, \n",
    "                                callback_names=['plot_losses_callback', 'early_stopping'], \n",
    "                                return_model=True)\n",
    "    \n",
    "    history = results[0]    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    scores_complete = results[1]\n",
    "    scores_with_valid_fv = scores_complete[0]\n",
    "    scores_with_test_fv = scores_complete[1]\n",
    "    \n",
    "    polynomial_fv_complete = results[2]\n",
    "    polynomial_valid_fv = polynomial_fv_complete[0]\n",
    "    polynomial_test_fv = polynomial_fv_complete[1]\n",
    "    \n",
    "    model = results[3]\n",
    "    \n",
    "    x = PrettyTable()\n",
    "\n",
    "    x.field_names = [\"Error Name\", \"Valid Error Int\", \"Test Error Int\"]\n",
    "\n",
    "    for error, value in scores_with_test_fv.items():\n",
    "\n",
    "        x.add_row([error, value[0], value[1]])\n",
    "\n",
    "    print(x)    \n",
    "    \n",
    "elif multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, \n",
    "                            verbose=11, \n",
    "                            backend='loky')(delayed(train_nn_and_pred)(X_train.values, \n",
    "                                                                      X_valid.values, \n",
    "                                                                      X_test.values, \n",
    "                                                                      y_train.values, \n",
    "                                                                      y_valid.values, \n",
    "                                                                      y_test.values, \n",
    "                                                                      lambda_train_fv_valid_split.values, \n",
    "                                                                      lambda_train_fv_test_split.values, \n",
    "                                                                      lambda_train_fv_train_split.values, \n",
    "                                                                      lambda_valid_fv_valid_split.values, \n",
    "                                                                      lambda_valid_fv_test_split.values, \n",
    "                                                                      lambda_valid_fv_train_split.values, \n",
    "                                                                      lambda_test_fv_valid_split.values, \n",
    "                                                                      lambda_test_fv_test_split.values, \n",
    "                                                                      lambda_test_fv_train_split.values, \n",
    "                                                                      callback_names=['early_stopping']) for X_train, \n",
    "                                                                                                               X_valid, \n",
    "                                                                                                               X_test, \n",
    "                                                                                                               y_train, \n",
    "                                                                                                               y_valid, \n",
    "                                                                                                               y_test, \n",
    "                                                                                                               lambda_train_fv_valid_split, \n",
    "                                                                                                               lambda_train_fv_test_split, \n",
    "                                                                                                               lambda_train_fv_train_split,                                            \n",
    "                                                                                                               lambda_valid_fv_valid_split, \n",
    "                                                                                                               lambda_valid_fv_test_split, \n",
    "                                                                                                               lambda_valid_fv_train_split, \n",
    "                                                                                                               lambda_test_fv_valid_split, \n",
    "                                                                                                               lambda_test_fv_test_split, \n",
    "                                                                                                               lambda_test_fv_train_split in zip(X_train_list, \n",
    "                                                                                                                                                 X_valid_list, \n",
    "                                                                                                                                                 X_test_list, \n",
    "                                                                                                                                                 y_train_list, \n",
    "                                                                                                                                                 y_valid_list, \n",
    "                                                                                                                                                 y_test_list, \n",
    "                                                                                                                                                 lambda_train_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_train_split_list,                                                                                                                                                  \n",
    "                                                                                                                                                 lambda_valid_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_train_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_train_split_list))      \n",
    "\n",
    "    history_list = [result[0] for result in results_list]\n",
    "    \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "\n",
    "    for i, history in enumerate(history_list):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        plt.plot(history[list(history.keys())[1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)   \n",
    "        \n",
    "elif not multi_epoch_analysis and  samples_list != None:\n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, verbose=11, backend='loky')(delayed(train_nn_and_pred)(X_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  X_valid.values, \n",
    "                                                                                                  X_test.values, \n",
    "                                                                                                  y_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  y_valid.values, \n",
    "                                                                                                  y_test.values, \n",
    "                                                                                                  lambda_train_fv_valid_split.values, \n",
    "                                                                                                  lambda_train_fv_test_split.values, \n",
    "                                                                                                  lambda_train_fv_train_split.values, \n",
    "                                                                                                  lambda_valid_fv_valid_split.values, \n",
    "                                                                                                  lambda_valid_fv_test_split.values, \n",
    "                                                                                                  lambda_valid_fv_train_split.values, \n",
    "                                                                                                  lambda_test_fv_valid_split.values, \n",
    "                                                                                                  lambda_test_fv_test_split.values, \n",
    "                                                                                                  lambda_test_fv_train_split.values, \n",
    "                                                                                                  callback_names=['early_stopping']) for samples in samples_list)     \n",
    "    \n",
    "    history_list = [result[0] for result in results_list]\n",
    "     \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "    for i, history in enumerate(history_list):       \n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:26.356766Z",
     "start_time": "2020-11-20T12:35:26.350570Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_valid_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_valid_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:26.406600Z",
     "start_time": "2020-11-20T12:35:26.358551Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_test_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_test_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:26.626540Z",
     "start_time": "2020-11-20T12:35:26.408400Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:26.814016Z",
     "start_time": "2020-11-20T12:35:26.628481Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:26.822131Z",
     "start_time": "2020-11-20T12:35:26.815838Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss = []\n",
    "    plot_history_metric = []\n",
    "    plot_history_val_loss = []\n",
    "    plot_history_val_metric = []\n",
    "        \n",
    "    for history in history_list:\n",
    "        plot_history_loss.append(history['loss'][-1])\n",
    "        plot_history_metric.append(history[list(history.keys())[1]][-1])\n",
    "\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plot_history_val_loss.append(history['val_loss'][-1])\n",
    "            plot_history_val_metric.append(history[list(history.keys())[len(history.keys())//2+1]][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.074923Z",
     "start_time": "2020-11-20T12:35:26.827877Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss_df = pd.DataFrame(data=plot_history_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_loss))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_loss_df = pd.DataFrame(data=plot_history_val_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_loss))])\n",
    "    \n",
    "    plt.plot(plot_history_loss_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_loss_df)\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.255157Z",
     "start_time": "2020-11-20T12:35:27.077544Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_metric_df = pd.DataFrame(data=plot_history_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_metric))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_metric_df = pd.DataFrame(data=plot_history_val_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_metric))])\n",
    "    \n",
    "    plt.plot(plot_history_metric_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_metric_df)\n",
    "    plt.title('Metric')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure Interpretation-Net Socres for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.488217Z",
     "start_time": "2020-11-20T12:35:27.257276Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_coeff_keys = ['MAE', 'RMSE', 'MAPE', 'Accuracy', 'Accuracy Multilabel']\n",
    "metrics_fv_keys = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "new_row_identifiers_coeff = ['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT']\n",
    "new_row_identifiers_fv = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "\n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "  \n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)\n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "\n",
    "\n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'means_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)  \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "    \n",
    "elif not multi_epoch_analysis and samples_list != None and evaluate_with_real_function:\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "\n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "        \n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "\n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1) \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)     \n",
    "            \n",
    "elif not multi_epoch_analysis and  samples_list != None and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "\n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "\n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)      \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)    \n",
    "            \n",
    "if multi_epoch_analysis:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')  \n",
    "elif samples_list != None:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_valid_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_test_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Lambda Scores for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.528944Z",
     "start_time": "2020-11-20T12:35:27.489868Z"
    }
   },
   "outputs": [],
   "source": [
    "path_scores_valid_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_scores_test_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_stds_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_means_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "df_mean_scores_valid_lambda = pd.read_csv(path_scores_valid_lambda, sep=',', index_col=0)\n",
    "df_mean_scores_test_lambda = pd.read_csv(path_scores_test_lambda, sep=',', index_col=0)\n",
    "df_stds_lambda = pd.read_csv(path_stds_lambda, sep=',', index_col=0)\n",
    "df_means_lambda = pd.read_csv(path_means_lambda, sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.546084Z",
     "start_time": "2020-11-20T12:35:27.530401Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_test_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.565785Z",
     "start_time": "2020-11-20T12:35:27.547431Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stds_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.593864Z",
     "start_time": "2020-11-20T12:35:27.567553Z"
    }
   },
   "outputs": [],
   "source": [
    "df_means_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns to Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.605584Z",
     "start_time": "2020-11-20T12:35:27.596613Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = df_mean_scores_test_lambda.index#scores_df.index.intersection(df_mean_scores_test_lambda.index)\n",
    "    scores_lambda = df_mean_scores_test_lambda.loc[plot_cols]\n",
    "    scores_int = scores_test_list.loc[plot_cols]    \n",
    "elif samples_list != None:\n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV']\n",
    "    scores_int = scores_test_list.loc[plot_cols] \n",
    "    scores_lambda = df_mean_scores_test_lambda.loc[plot_cols].iloc[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.628355Z",
     "start_time": "2020-11-20T12:35:27.608094Z"
    }
   },
   "outputs": [],
   "source": [
    "print_head = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    print_head = scores_int\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.645310Z",
     "start_time": "2020-11-20T12:35:27.629912Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_test_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:27.669422Z",
     "start_time": "2020-11-20T12:35:27.647228Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_valid_fv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:29.579231Z",
     "start_time": "2020-11-20T12:35:27.672977Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('./data/plotting/' + interpretation_network_string + filename + '/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "        vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "            \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "                    \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "        vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "        \n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:29.587065Z",
     "start_time": "2020-11-20T12:35:29.581612Z"
    }
   },
   "outputs": [],
   "source": [
    "vals_int_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:29.593643Z",
     "start_time": "2020-11-20T12:35:29.589184Z"
    }
   },
   "outputs": [],
   "source": [
    "vals_int_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:29.952308Z",
     "start_time": "2020-11-20T12:35:29.595434Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "    vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file \n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "       \n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "    vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "    \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:29.965660Z",
     "start_time": "2020-11-20T12:35:29.953705Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split_list[-1]\n",
    "elif samples_list != None:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "else:\n",
    "    plot_preds = polynomial_test_fv\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "\n",
    "x_vars = ['x' + str(i) for i in range(1, n+1)]\n",
    "\n",
    "columns = x_vars.copy()\n",
    "columns.append('FVs')\n",
    "\n",
    "columns_single = x_vars.copy()\n",
    "columns_single.extend(['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'])\n",
    "\n",
    "eval_size_plot = plot_preds[2].shape[1]\n",
    "rand_index = 2#42#random.randint(0, plot_preds[2].shape[0]-1)\n",
    "vars_plot = np.column_stack([lambda_test_input[::,i] for i in range(n)])\n",
    "plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, plot_preds[2][rand_index], plot_preds[3][rand_index], plot_eval.values[rand_index]]), columns=columns_single)\n",
    "\n",
    "vars_plot_all_preds = np.append(np.append(vars_plot, vars_plot, axis=0), vars_plot, axis=0)\n",
    "preds_plot_all = np.append(np.append(plot_preds[2][rand_index], plot_preds[3][rand_index], axis=0), plot_eval.values[rand_index], axis=0)\n",
    "\n",
    "if evaluate_with_real_function:\n",
    "    real_str = np.array(['Real Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds LSTSQ\n",
    "else:\n",
    "    real_str = np.array(['Lambda Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds\n",
    "    \n",
    "identifier = np.concatenate([real_str, int_str, lambda_str])\n",
    "\n",
    "plot_data = pd.DataFrame(data=np.column_stack([vars_plot_all_preds, preds_plot_all]), columns=columns)\n",
    "plot_data['Identifier'] = identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:30.774383Z",
     "start_time": "2020-11-20T12:35:29.966989Z"
    }
   },
   "outputs": [],
   "source": [
    "pp1 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  y_vars=['FVs'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:34.296375Z",
     "start_time": "2020-11-20T12:35:30.775775Z"
    }
   },
   "outputs": [],
   "source": [
    "pp2 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  #y_vars=['FVs'],\n",
    "                  #x_vars=x_vars\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:35.447649Z",
     "start_time": "2020-11-20T12:35:34.299339Z"
    }
   },
   "outputs": [],
   "source": [
    "pp3 = sns.pairplot(data=plot_data_single,\n",
    "                  #kind='reg',\n",
    "                  y_vars=['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:37.093510Z",
     "start_time": "2020-11-20T12:35:35.449316Z"
    }
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_REAL_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')\n",
    "else:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_PRED_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:37.115847Z",
     "start_time": "2020-11-20T12:35:37.095540Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n)+ '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "    loss_df_lambda = pd.read_csv(path_loss, sep=',')\n",
    "    metric_df_lambda = pd.read_csv(path_metric, sep=',')\n",
    "    val_loss_df_lambda = pd.read_csv(path_val_loss, sep=',')\n",
    "    val_metric_df_lambda = pd.read_csv(path_val_metric, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:37.123766Z",
     "start_time": "2020-11-20T12:35:37.118211Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "\n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_metric = 0\n",
    "\n",
    "    metric_df_adjusted = metric_df_lambda.copy(deep=True)\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "\n",
    "    val_metric_df_adjusted = val_metric_df_lambda.copy(deep=True)\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_metric'])\n",
    "    plt.title('model metric')\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:37.131845Z",
     "start_time": "2020-11-20T12:35:37.125894Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_loss = 1000\n",
    "\n",
    "    loss_df_adjusted = loss_df_lambda.copy(deep=True)\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "\n",
    "    val_loss_df_adjusted = val_loss_df_lambda.copy(deep=True)\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:37.137934Z",
     "start_time": "2020-11-20T12:35:37.133573Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    preds = model.predict(X_test)\n",
    "    preds_rounded = np.round(preds, 1)\n",
    "    #preds_true = pd.DataFrame(data=[np.round(preds, 1), y_test.values])\n",
    "    for pred, y in tqdm(zip(preds_rounded, y_test.values)):\n",
    "        if (pred == y).all():\n",
    "            print(pred)\n",
    "    \n",
    "    #print(preds_rounded)\n",
    "    #print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:37.143871Z",
     "start_time": "2020-11-20T12:35:37.139526Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#summarize history for loss\n",
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/loss_' + interpretation_network_string + filename + '.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:37.152413Z",
     "start_time": "2020-11-20T12:35:37.145197Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    random_polynomial = list(random_product([i*a_step for i in range(int(a_min*10**int(-np.log10(a_step))), int(a_max*10**int(-np.log10(a_step))))], repeat=nCr(n+d, d)))\n",
    "    list_of_random_polynomials.append(random_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:40.200812Z",
     "start_time": "2020-11-20T12:35:37.154701Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(y_test.values, lambda_test_input)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:40.215441Z",
     "start_time": "2020-11-20T12:35:40.203755Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Benchmark Error Coefficients: ' + str(np.round(mean_absolute_error(y_test, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T12:35:40.222865Z",
     "start_time": "2020-11-20T12:35:40.217177Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Benchmark Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
