{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:10:34.261978Z",
     "start_time": "2020-12-09T08:10:34.254229Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:10:34.564352Z",
     "start_time": "2020-12-09T08:10:34.549369Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3  \n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "n_jobs = 21\n",
    "\n",
    "\n",
    "data_size = 1000 #for loading lambda models\n",
    "\n",
    "#specify interpretation net structure\n",
    "optimizer = 'adam'\n",
    "dropout = 0\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "interpretation_network_layers = [2048]\n",
    "\n",
    "random_evaluation_dataset_size = 500\n",
    "\n",
    "#lambda net specifications for loading (need to be set according to lambda net training to load correct weights)\n",
    "epochs_lambda = 200\n",
    "batch_lambda = 64\n",
    "lambda_network_layers = [5*sparsity]\n",
    "optimizer_lambda = '_' + 'SGD'\n",
    "\n",
    "\n",
    "lambda_dataset_size = 1000\n",
    "\n",
    "#set if multi_epoch_analysis should be performed\n",
    "multi_epoch_analysis = True\n",
    "each_epochs_save_lambda = 10 #None if no checkpointing (otherwise set according to lambda-net training)\n",
    "epoch_start = 0 #use to skip first epochs in multi_epoch_analysis\n",
    "\n",
    "#set if samples analysis should be performed\n",
    "samples_list = None#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "\n",
    "evaluate_with_real_function = False\n",
    "consider_labels_training = False\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 20\n",
    "\n",
    "inet_holdout_seed_evaluation = False\n",
    "seed_in_inet_training = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:10:34.897674Z",
     "start_time": "2020-12-09T08:10:34.882693Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else None\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "structure = '_' + layers_str + str(epochs_lambda) + 'e' + str(batch_lambda) + 'b' + optimizer_lambda\n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure\n",
    "\n",
    "interpretation_network_string = 'drop' + str(dropout) + 'e' + str(epochs) + 'b' + str(batch_size) + '_' + str(interpretation_network_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:13.936630Z",
     "start_time": "2020-12-09T08:10:35.772678Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('./data/plotting/' + interpretation_network_string + filename + '/')\n",
    "    os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:13.973090Z",
     "start_time": "2020-12-09T08:11:13.938657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:13.998332Z",
     "start_time": "2020-12-09T08:11:13.974916Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "\n",
    "def return_float_tensor_representation(some_representation, dtype=tf.float32):\n",
    "    if tf.is_tensor(some_representation):\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "    else:\n",
    "        some_representation = tf.convert_to_tensor(some_representation)\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "        \n",
    "    if not tf.is_tensor(some_representation):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(dtype) + ':' + str(some_representation))\n",
    "     \n",
    "    return some_representation\n",
    "\n",
    "\n",
    "def return_numpy_representation(some_representation):\n",
    "    if isinstance(some_representation, pd.DataFrame):\n",
    "        some_representation = some_representation.values\n",
    "        \n",
    "    if isinstance(some_representation, list):\n",
    "        some_representation = np.array(some_representation)\n",
    "    \n",
    "    if not isinstance(some_representation, np.ndarray):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(np.ndarray) + ':' + str(some_representation))\n",
    "    \n",
    "    return some_representation\n",
    "\n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict\n",
    "\n",
    "def return_callbacks_from_string(callback_string_list):\n",
    "    callbacks = [] if len(callback_string_list) > 0 else None\n",
    "    #if 'plot_losses_callback' in callback_string_list:\n",
    "        #callbacks.append(PlotLossesCallback())\n",
    "    if 'reduce_lr_loss' in callback_string_list:\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=epochs/10, verbose=0, min_delta=0, mode='min') #epsilon\n",
    "        callbacks.append(reduce_lr_loss)\n",
    "    if 'early_stopping' in callback_string_list:\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0, verbose=0, mode='min')\n",
    "        callbacks.append(earlyStopping)\n",
    "        \n",
    "    #if not multi_epoch_analysis and samples_list == None: \n",
    "        #callbacks.append(TQDMNotebookCallback())\n",
    "        \n",
    "    return callbacks\n",
    "\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def generate_random_x_values(size, x_max, x_min, x_step, numnber_of_variables):\n",
    "    x_values_list = []\n",
    "    \n",
    "    for j in range(size):\n",
    "        values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))\n",
    "        while arreq_in_list(values, x_values_list):\n",
    "                values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))         \n",
    "        x_values_list.append(values)\n",
    "    \n",
    "    return np.array(x_values_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.081256Z",
     "start_time": "2020-12-09T08:11:14.000461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522e1afa42374f52834e0edd8b3feae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7411380ea34c4abb97f2bdea2d9f8118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.094587Z",
     "start_time": "2020-12-09T08:11:14.083017Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with polynomials on function value basis\n",
    "\n",
    "def calculate_mae_fv(polynomial_true_pred):\n",
    "    polynomial_true = polynomial_true_pred[0]\n",
    "    polynomial_pred = polynomial_true_pred[1]\n",
    "    \n",
    "    global lambda_train_input_train_split \n",
    "    lambda_input = lambda_train_input_train_split[0]\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "            \n",
    "        polynomial_true_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_true))\n",
    "        polynomial_true_fv = tf.reduce_sum(polynomial_true_value_per_term)\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "        \n",
    "        if index == 0:   \n",
    "            result = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "        else:           \n",
    "            current_valiue = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "            result = tf.math.add(result, current_valiue)    \n",
    "            \n",
    "    return  tf.math.divide(result, lambda_input.shape[0]) #tf.random.uniform(shape=[1], minval=0.1, maxval=10.0)\n",
    "\n",
    "def mean_absolute_error_tf_fv(y_true, y_pred):\n",
    "        \n",
    "    y_true = return_float_tensor_representation(y_true)\n",
    "    y_pred = return_float_tensor_representation(y_pred)\n",
    "    \n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv, (y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.122596Z",
     "start_time": "2020-12-09T08:11:14.096399Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with lambda-net prediction based (predictions made in loss function)\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "\n",
    "def mean_absolute_error_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers, base_model):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)    \n",
    "    \n",
    "    model_lambda_placeholder = keras.models.clone_model(base_model)  \n",
    "    \n",
    "    weights_structure = base_model.get_weights()\n",
    "    dims = [np_arrays.shape for np_arrays in weights_structure]\n",
    "    \n",
    "    def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        network_parameters = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "        polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "        network_parameters = return_float_tensor_representation(network_parameters)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder), (polynomial_pred, network_parameters), fn_output_signature=tf.float32))\n",
    "    return mean_absolute_error_tf_fv_lambda_extended\n",
    "\n",
    "\n",
    "#CHANGES NEEDED\n",
    "def calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder):\n",
    "\n",
    "    def calculate_mae_fv_lambda(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        #polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[0]\n",
    "        network_parameters = input_list[1]\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        #CALCULATE LAMBDA FV HERE FOR EVALUATION DATASET\n",
    "        # build models\n",
    "        start = 0\n",
    "        layers = []\n",
    "        for i in range(len(dims)//2):\n",
    "            \n",
    "            # set weights of layer\n",
    "            index = i*2\n",
    "            size = np.product(dims[index])\n",
    "            weights_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[0].assign(weights_tf_true)\n",
    "            start += size\n",
    "            \n",
    "            # set biases of layer\n",
    "            index += 1\n",
    "            size = np.product(dims[index])\n",
    "            biases_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[1].assign(biases_tf_true)\n",
    "            start += size\n",
    "\n",
    "        \n",
    "        lambda_fv = tf.keras.backend.flatten(model_lambda_placeholder(evaluation_dataset))\n",
    "        \n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_lambda\n",
    "\n",
    "#nothing to change here (just fv calculation for evaluation entry for single polynomial)\n",
    "def calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred):\n",
    "\n",
    "\n",
    "    def calculate_fv_from_data(evaluation_entry):\n",
    "\n",
    "\n",
    "        value_without_coefficient = tf.vectorized_map(calculate_value_without_coefficient_wrapper(evaluation_entry), (list_of_monomial_identifiers))\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        \n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)     \n",
    "        \n",
    "        return polynomial_pred_fv\n",
    "    return calculate_fv_from_data\n",
    "\n",
    "\n",
    "#calculate intermediate term (without coefficient multiplication)\n",
    "def calculate_value_without_coefficient_wrapper(evaluation_entry):\n",
    "    def calculate_value_without_coefficient(coefficient_multiplier_term):      \n",
    "   \n",
    "        return tf.math.reduce_prod(tf.vectorized_map(lambda x: x[0]**x[1], (evaluation_entry, coefficient_multiplier_term)))\n",
    "    return calculate_value_without_coefficient\n",
    "\n",
    "#calculate MAE at the end ---> general:REPLACE FUNCTION WITH LOSS CALL OR LAMBDA\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.158810Z",
     "start_time": "2020-12-09T08:11:14.124587Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with pre-saved lambda-net predictions\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "def calculate_mae_single_input_preds_appended(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def calculate_mae_fv_lambda_preds_appended(input_list):\n",
    "    \n",
    "    polynomial_true = input_list[0]\n",
    "    polynomial_pred = input_list[1]\n",
    "    lambda_fv = input_list[2]\n",
    "\n",
    "    global lambda_train_input_train_split \n",
    "    lambda_input = lambda_train_input_train_split[0] #[0] correct here?\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "\n",
    "        if index == 0:\n",
    "            polynomial_pred_fv_list = tf.convert_to_tensor([polynomial_pred_fv])\n",
    "        else:\n",
    "            polynomial_pred_fv_list = tf.concat([polynomial_pred_fv_list, tf.convert_to_tensor([polynomial_pred_fv])], 0)\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input_preds_appended, (lambda_fv, polynomial_pred_fv_list)))\n",
    "  \n",
    "    \n",
    "def mean_absolute_error_tf_fv_lambda_preds_appended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "    lambda_fv = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda_preds_appended, (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.224083Z",
     "start_time": "2020-12-09T08:11:14.202147Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Basic Keras/TF Loss functions\n",
    "def root_mean_squared_error(y_true, y_pred):   \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "        \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)           \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))      \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)        \n",
    "    epsilon = return_float_tensor_representation(epsilon)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.238868Z",
     "start_time": "2020-12-09T08:11:14.225798Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual calculations for comparison of polynomials based on function values (no TF!)\n",
    "\n",
    "def calcualate_function_value(coefficient_list, lambda_input_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0   \n",
    "        \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        value_without_coefficient = [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multipliers, lambda_input_entry)]\n",
    "\n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, value_without_coefficient)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_function_values_from_polynomial(polynomial, lambda_input_data):\n",
    "    polynomial = return_numpy_representation(polynomial)\n",
    "    \n",
    "    function_value_list = []\n",
    "        \n",
    "    for lambda_input_entry in lambda_input_data:\n",
    "        function_value = calcualate_function_value(polynomial, lambda_input_entry)\n",
    "        function_value_list.append(function_value)\n",
    "\n",
    "    return np.array(function_value_list)\n",
    "\n",
    "\n",
    "def parallel_fv_calculation_from_polynomial(polynomial_list, lambda_input_list):\n",
    "    parallel = Parallel(n_jobs=10, verbose=0, backend='threading')\n",
    "    polynomial_true_fv = parallel(delayed(calculate_function_values_from_polynomial)(polynomial, lambda_inputs) for polynomial, lambda_inputs in zip(polynomial_list, lambda_input_list))  \n",
    "    del parallel   \n",
    "    \n",
    "\n",
    "    return np.array(polynomial_true_fv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.268990Z",
     "start_time": "2020-12-09T08:11:14.240635Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Standard Metrics (no TF!)\n",
    "\n",
    "def mean_absolute_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)      \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(true_values-pred_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))  \n",
    "\n",
    "def root_mean_squared_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)         \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sqrt(np.mean((true_values-pred_values)**2)))\n",
    "    \n",
    "    return np.mean(np.array(result_list)) \n",
    "\n",
    "def mean_absolute_percentage_error_function_values(y_true, y_pred, epsilon=10e-3):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred) \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(((true_values-pred_values)/(true_values+epsilon)))))\n",
    "\n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def r2_score_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(r2_score(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_absolute_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sum(np.abs(true_values-pred_values))/(true_values.shape[0]*np.std(true_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_maximum_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.max(true_values-pred_values)/np.std(true_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_area_between_two_curves_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "      \n",
    "    assert(number_of_variables==1)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(area_between_two_curves(true_values, pred_values))\n",
    " \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_dtw_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "\n",
    "    result_list_single = []\n",
    "    result_list_array = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_single_value, result_single_array = dtw(true_values, pred_values)\n",
    "        result_list_single.append(result_single_value)\n",
    "        result_list_array.append(result_single_array)\n",
    "    \n",
    "    return np.mean(np.array(result_list_single)), np.mean(np.array(result_list_array), axis=1)\n",
    "\n",
    "def mean_frechet_dist_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(frechet_dist(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.285382Z",
     "start_time": "2020-12-09T08:11:14.270737Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_interpretation_net(y_data_real, \n",
    "                                y_data_pred, \n",
    "                                polynomial_true_fv, \n",
    "                                polynomial_pred_inet_fv):\n",
    "    \n",
    "    mae_coeff = np.round(mean_absolute_error(y_data_real, y_data_pred), 4)\n",
    "    rmse_coeff = np.round(root_mean_squared_error(y_data_real, y_data_pred), 4)\n",
    "    mape_coeff = np.round(mean_absolute_percentage_error_keras(y_data_real, y_data_pred), 4)\n",
    "    accuracy_coeff = np.round(accuracy_single(y_data_real, y_data_pred), 4)\n",
    "    accuracy_multi_coeff = np.round(accuracy_multilabel(y_data_real, y_data_pred), 4)\n",
    "    \n",
    "    print(polynomial_true_fv.shape)\n",
    "    print(polynomial_pred_inet_fv.shape)\n",
    "    \n",
    "    mae_fv = np.round(mean_absolute_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmse_fv = np.round(root_mean_squared_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    mape_fv = np.round(mean_absolute_percentage_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    r2_fv = np.round(r2_score_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    raae_fv = np.round(relative_absolute_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmae_fv = np.round(relative_maximum_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4) \n",
    "\n",
    "    std_fv = np.std(polynomial_pred_inet_fv)\n",
    "    mean_fv = np.mean(polynomial_pred_inet_fv)\n",
    "\n",
    "    return {\n",
    "             'MAE': mae_coeff,\n",
    "             'RMSE': rmse_coeff, \n",
    "             'MAPE': mape_coeff,\n",
    "             'Accuracy': accuracy_coeff, \n",
    "             'Accuracy Multilabel': accuracy_multi_coeff, \n",
    "\n",
    "             'MAE FV': mae_fv,\n",
    "             'RMSE FV': rmse_fv,\n",
    "             'MAPE FV': mape_fv,\n",
    "             'R2 FV': r2_fv,\n",
    "             'RAAE FV': raae_fv,\n",
    "             'RMAE FV': rmae_fv,         \n",
    "             'STD FV PRED': std_fv,   \n",
    "             'MEAN FV PRED': mean_fv\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:14.313961Z",
     "start_time": "2020-12-09T08:11:14.301055Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_weight_data(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    return weight_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_lambda_pred_data(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    lambda_train_fv_with_lambda_input = pd.read_csv(path_lambda_train_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_valid_fv_with_lambda_input = pd.read_csv(path_lambda_valid_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_test_fv_with_lambda_input = pd.read_csv(path_lambda_test_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "   \n",
    "    return lambda_train_fv_with_lambda_input, lambda_valid_fv_with_lambda_input, lambda_test_fv_with_lambda_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:20.884434Z",
     "start_time": "2020-12-09T08:11:14.315740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=21)]: Using backend LokyBackend with 21 concurrent workers.\n",
      "[Parallel(n_jobs=21)]: Done   4 out of  21 | elapsed:    3.1s remaining:   13.0s\n",
      "[Parallel(n_jobs=21)]: Done  12 out of  21 | elapsed:    4.2s remaining:    3.2s\n",
      "[Parallel(n_jobs=21)]: Done  21 out of  21 | elapsed:    6.5s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if multi_epoch_analysis:  \n",
    "    weight_data_list = []\n",
    "    \n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    weight_data_list = parallel(delayed(load_weight_data)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    weight_data = weight_data_list[-1]\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    lambda_fv_with_lambda_input_list = parallel(delayed(load_lambda_pred_data)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    lambda_train_fv_with_lambda_input_list = [lambda_fvs[0] for lambda_fvs in lambda_fv_with_lambda_input_list]\n",
    "    lambda_valid_fv_with_lambda_input_list = [lambda_fvs[1] for lambda_fvs in lambda_fv_with_lambda_input_list]\n",
    "    lambda_test_fv_with_lambda_input_list = [lambda_fvs[2] for lambda_fvs in lambda_fv_with_lambda_input_list]    \n",
    "else:\n",
    "\n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "                \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "        \n",
    "    path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    lambda_train_fv_with_lambda_input = pd.read_csv(path_lambda_train_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_valid_fv_with_lambda_input = pd.read_csv(path_lambda_valid_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_test_fv_with_lambda_input = pd.read_csv(path_lambda_test_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:30.580043Z",
     "start_time": "2020-12-09T08:11:30.532235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "      <th>1156</th>\n",
       "      <th>1157</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>1181241943</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-9.700</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>3.800</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-7.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>2746317213</td>\n",
       "      <td>5.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>9.200</td>\n",
       "      <td>8.300</td>\n",
       "      <td>8.900</td>\n",
       "      <td>3.900</td>\n",
       "      <td>-3.400</td>\n",
       "      <td>-9.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>2746317213</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.767</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>2536146025</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-8.400</td>\n",
       "      <td>-3.300</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-9.000</td>\n",
       "      <td>7.100</td>\n",
       "      <td>-4.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.374</td>\n",
       "      <td>-1.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>958682846</td>\n",
       "      <td>-4.700</td>\n",
       "      <td>-7.000</td>\n",
       "      <td>2.500</td>\n",
       "      <td>8.900</td>\n",
       "      <td>5.400</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>9.100</td>\n",
       "      <td>-1.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1      2      3      4      5      6      7      8      9     \\\n",
       "0   200  1181241943 -0.700 -9.700  9.800 -1.500  3.800  1.400 -7.400  7.500   \n",
       "1   200  2746317213  5.400 -6.500  9.200  8.300  8.900  3.900 -3.400 -9.800   \n",
       "2   200  2746317213  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "3   200  2536146025 -4.100 -8.400 -3.300 -0.600 -4.100 -9.000  7.100 -4.800   \n",
       "4   200   958682846 -4.700 -7.000  2.500  8.900  5.400 -0.900  9.100 -1.900   \n",
       "\n",
       "   ...   1148   1149   1150   1151   1152   1153   1154   1155   1156   1157  \n",
       "0  ... -0.040 -0.031 -0.041  0.737  0.601  0.044  0.748  0.218 -0.031  0.643  \n",
       "1  ...  0.443 -0.254  0.355  0.264 -0.036  0.143  0.571  0.507  0.123  0.660  \n",
       "2  ...  0.217 -0.028  0.111  0.433 -0.071  0.216 -0.099  0.767 -0.324 -0.584  \n",
       "3  ...  0.606 -0.112 -0.263  0.264 -0.554  0.096  0.055 -0.273  0.374 -1.962  \n",
       "4  ... -0.018 -0.022  0.291 -0.008 -0.327 -0.047 -0.165 -0.044 -0.031 -0.239  \n",
       "\n",
       "[5 rows x 1158 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:33.695415Z",
     "start_time": "2020-12-09T08:11:30.581565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "      <th>1156</th>\n",
       "      <th>1157</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>200.000</td>\n",
       "      <td>1578077718.850</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1243352707.987</td>\n",
       "      <td>5.591</td>\n",
       "      <td>5.743</td>\n",
       "      <td>5.904</td>\n",
       "      <td>5.769</td>\n",
       "      <td>5.793</td>\n",
       "      <td>5.794</td>\n",
       "      <td>5.758</td>\n",
       "      <td>5.755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.304</td>\n",
       "      <td>1.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>200.000</td>\n",
       "      <td>107420369.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>-1.053</td>\n",
       "      <td>-1.291</td>\n",
       "      <td>-1.231</td>\n",
       "      <td>-1.054</td>\n",
       "      <td>-1.046</td>\n",
       "      <td>-1.303</td>\n",
       "      <td>-1.066</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-4.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>200.000</td>\n",
       "      <td>430764563.750</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>-4.625</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-1.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>200.000</td>\n",
       "      <td>1116522227.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>200.000</td>\n",
       "      <td>2786338449.000</td>\n",
       "      <td>4.800</td>\n",
       "      <td>4.800</td>\n",
       "      <td>5.325</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.300</td>\n",
       "      <td>5.100</td>\n",
       "      <td>4.625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000</td>\n",
       "      <td>3831882064.000</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>...</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.875</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.303</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.322</td>\n",
       "      <td>0.850</td>\n",
       "      <td>2.790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 1158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0              1        2        3        4        5        6     \\\n",
       "count 1000.000       1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean   200.000 1578077718.850    0.014   -0.142    0.058   -0.020    0.021   \n",
       "std      0.000 1243352707.987    5.591    5.743    5.904    5.769    5.793   \n",
       "min    200.000  107420369.000  -10.000  -10.000  -10.000  -10.000  -10.000   \n",
       "25%    200.000  430764563.750   -4.500   -5.500   -5.100   -5.000   -5.100   \n",
       "50%    200.000 1116522227.500    0.000    0.000   -0.200    0.100    0.100   \n",
       "75%    200.000 2786338449.000    4.800    4.800    5.325    4.900    5.000   \n",
       "max    200.000 3831882064.000    9.900    9.900    9.900    9.900    9.900   \n",
       "\n",
       "          7        8        9     ...     1148     1149     1150     1151  \\\n",
       "count 1000.000 1000.000 1000.000  ... 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.446    0.062   -0.242  ...    0.011   -0.033   -0.018   -0.041   \n",
       "std      5.794    5.758    5.755  ...    0.327    0.338    0.324    0.312   \n",
       "min    -10.000   -9.900  -10.000  ...   -1.168   -1.053   -1.291   -1.231   \n",
       "25%     -5.500   -4.625   -5.300  ...   -0.164   -0.223   -0.192   -0.204   \n",
       "50%     -0.500    0.000   -0.100  ...   -0.024   -0.034   -0.023   -0.045   \n",
       "75%      4.300    5.100    4.625  ...    0.189    0.183    0.173    0.073   \n",
       "max      9.900    9.900    9.900  ...    1.068    1.082    0.875    1.034   \n",
       "\n",
       "          1152     1153     1154     1155     1156     1157  \n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000  \n",
       "mean     0.015    0.029    0.060   -0.081   -0.051   -0.653  \n",
       "std      0.351    0.347    0.372    0.270    0.304    1.446  \n",
       "min     -1.054   -1.046   -1.303   -1.066   -1.000   -4.556  \n",
       "25%     -0.173   -0.171   -0.175   -0.219   -0.212   -1.494  \n",
       "50%     -0.031   -0.025    0.009   -0.049   -0.033   -0.585  \n",
       "75%      0.243    0.225    0.332    0.045    0.088    0.163  \n",
       "max      1.014    1.303    0.936    1.322    0.850    2.790  \n",
       "\n",
       "[8 rows x 1158 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:11:33.715974Z",
     "start_time": "2020-12-09T08:11:33.697112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>d_1</th>\n",
       "      <th>FV_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>c_2</th>\n",
       "      <th>d_2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_561</th>\n",
       "      <th>b_561</th>\n",
       "      <th>c_561</th>\n",
       "      <th>d_561</th>\n",
       "      <th>FV_561</th>\n",
       "      <th>a_562</th>\n",
       "      <th>b_562</th>\n",
       "      <th>c_562</th>\n",
       "      <th>d_562</th>\n",
       "      <th>FV_562</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>288.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.760</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>2.281</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>1.101</td>\n",
       "      <td>-0.830</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>8.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>105.000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>-8.990</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-12.048</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-3.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>706.000</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>0.390</td>\n",
       "      <td>4.901</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.610</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>7.061</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>965.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-9.708</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>2.810</td>\n",
       "      <td>-0.910</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-8.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>314.000</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.670</td>\n",
       "      <td>14.022</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.470</td>\n",
       "      <td>0.840</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>24.804</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.130</td>\n",
       "      <td>14.447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 2811 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lambda_index    a_1    b_1    c_1    d_1   FV_1    a_2    b_2    c_2  \\\n",
       "521       288.000 -0.050  0.760 -0.670 -0.510  2.281 -0.930  0.220 -0.480   \n",
       "737       105.000  0.930 -0.940 -0.400 -0.690 -8.990  0.550 -0.450  0.320   \n",
       "740       706.000 -0.880 -0.180 -0.510  0.390  4.901 -0.150 -0.410 -0.270   \n",
       "660       965.000 -0.090 -0.670  0.460  0.040 -9.708  0.500  0.020 -0.730   \n",
       "411       314.000 -0.230 -0.700  0.920  0.670 14.022 -0.200 -0.110 -0.680   \n",
       "\n",
       "       d_2  ...  a_561  b_561  c_561  d_561  FV_561  a_562  b_562  c_562  \\\n",
       "521 -0.260  ...  0.520 -0.820 -0.530 -0.410   1.101 -0.830 -0.260  0.950   \n",
       "737 -0.050  ... -0.140 -0.860 -0.720  0.450 -12.048 -0.800 -0.070 -0.250   \n",
       "740 -0.880  ...  0.690  0.610 -0.110 -0.870   7.061 -0.410 -0.420  0.830   \n",
       "660 -0.480  ...  0.270 -0.370 -0.680 -0.580   2.810 -0.910  0.020  0.690   \n",
       "411 -0.510  ... -0.130 -0.470  0.840 -0.690  24.804  0.090 -0.450  0.770   \n",
       "\n",
       "     d_562  FV_562  \n",
       "521 -0.460   8.066  \n",
       "737  0.060  -3.819  \n",
       "740  0.010   2.047  \n",
       "660  0.180  -8.470  \n",
       "411  0.130  14.447  \n",
       "\n",
       "[5 rows x 2811 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = lambda_train_fv_with_lambda_input_list[-1].head()\n",
    "else:\n",
    "    print_head = lambda_train_fv_with_lambda_input.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.936785Z",
     "start_time": "2020-12-07T13:35:07.987Z"
    }
   },
   "outputs": [],
   "source": [
    "TODO: FIRST VALUE IN WEIGHT_DATA IS SEED\n",
    "    --> MAKE OPTION TO INCLUDE SEED IN I-NET OR NOT TO INCLUDE \n",
    "    --> MAKE TWO DIFFERENT TEST SET CREATOION TYPES\n",
    "            1. ALL SEEDS ARE INCLUDED IN TRAIN, VALID AND TEST\n",
    "            2. SEEDS IN VALID AND TEST ARE NOT THE SAME AS IN TRAINING\n",
    "\n",
    "MÃSSEN SEEDS IN ADEREN FILES GEADDED WERDEN, DASS RICHTIG GESPLITTED WIRD? \n",
    "WO MUSS SPLITTEN ALLES ANGEPASST WERDEN, DASS ALLE EVALUATIONS STIMMEN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T21:17:45.334117Z",
     "start_time": "2020-12-07T21:17:45.326530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(20/(1/0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.939468Z",
     "start_time": "2020-12-07T13:35:07.993Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "if multi_epoch_analysis:    \n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    X_valid_list = []\n",
    "    y_valid_list = []\n",
    "    \n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    \n",
    "    lambda_test_fv_valid_split_list = []\n",
    "    lambda_test_fv_test_split_list = []\n",
    "    lambda_test_fv_train_split_list = []\n",
    "    \n",
    "    lambda_valid_fv_valid_split_list = []\n",
    "    lambda_valid_fv_test_split_list = []\n",
    "    lambda_valid_fv_train_split_list = []\n",
    "    \n",
    "    lambda_train_fv_valid_split_list = []\n",
    "    lambda_train_fv_test_split_list = []\n",
    "    lambda_train_fv_train_split_list = []\n",
    "    \n",
    "    lambda_test_input_valid_split_list = []\n",
    "    lambda_test_input_test_split_list = []\n",
    "    lambda_test_input_train_split_list = []\n",
    "    \n",
    "    lambda_valid_input_valid_split_list = []\n",
    "    lambda_valid_input_test_split_list = []\n",
    "    lambda_valid_input_train_split_list = []\n",
    "    \n",
    "    lambda_train_input_valid_split_list = []\n",
    "    lambda_train_input_test_split_list = []\n",
    "    lambda_train_input_train_split_list = []\n",
    "    \n",
    "    for i, (weight_data, lambda_train_fv, lambda_valid_fv, lambda_test_fv) in tqdm(enumerate(zip(weight_data_list, lambda_train_fv_with_lambda_input_list, lambda_valid_fv_with_lambda_input_list, lambda_test_fv_with_lambda_input_list)), total=len(weight_data_list)): \n",
    "        \n",
    "        if psutil.virtual_memory().percent > 80:\n",
    "            raise SystemExit(\"Out of RAM!\")\n",
    "        \n",
    "        \n",
    "        X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3+2)], axis=1)\n",
    "        y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3+2)]].astype(float)\n",
    "        \n",
    "        y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "        y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "        y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)      \n",
    "        \n",
    "        if evaluate_with_real_function:\n",
    "            y_data = y_data_polynomial_true\n",
    "        else:\n",
    "            y_data = y_data_polynomial_lstsq_pred  \n",
    "           \n",
    "        if inet_holdout_seed_evaluation:\n",
    "            complete_seed_list = list(weight_data.iloc[:,0].unique())\n",
    "            print(len(complete_seed_list))\n",
    "            \n",
    "            test_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)/(1/0.75)))\n",
    "            test_lambda_indices = X_data[X_data[1].isin(test_seeds)][0].values\n",
    "            test_indices = list(X_data[X_data[1].isin(test_seeds)][0].index)\n",
    "            complete_seed_list.remove(test_seeds)\n",
    "            print(len(test_seeds))\n",
    "            \n",
    "            valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)/(1/0.75)))\n",
    "            valid_lambda_indices = X_data[X_data[1].isin(valid_seeds)][0].values\n",
    "            valid_indices = list(X_data[X_data[1].isin(valid_seeds)][0].index)\n",
    "            complete_seed_list.remove(valid_seeds)\n",
    "            print(len(valid_seeds))\n",
    "            \n",
    "            train_seeds = complete_seed_list\n",
    "            train_lambda_indices = X_data[X_data[1].isin(train_seeds)][0].values\n",
    "            train_indices = list(X_data[X_data[1].isin(train_seeds)][0].index)\n",
    "            print(len(test_seeds))\n",
    "            \n",
    "            X_train = X_data.loc[train_indices]\n",
    "            y_train = y_data.loc[train_indices]\n",
    "            \n",
    "            X_valid = X_data.loc[valid_indices]\n",
    "            y_valid = y_data.loc[valid_indices]\n",
    "            \n",
    "            X_test = X_data.loc[test_indices]\n",
    "            y_test = y_data.loc[test_indices]\n",
    "            \n",
    "            print(X_train.shape, y_train.shape)\n",
    "            print(X_valid.shape, y_valid.shape)\n",
    "            print(X_test.shape, y_test.shape)\n",
    "            \n",
    "        else:     \n",
    "            X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "            \n",
    "        X_train_list.append(X_train)\n",
    "        y_train_list.append(y_train)\n",
    "\n",
    "        X_valid_list.append(X_valid)\n",
    "        y_valid_list.append(y_valid)\n",
    "\n",
    "        X_test_list.append(X_test)\n",
    "        y_test_list.append(y_test)                 \n",
    "        \n",
    "        if inet_holdout_seed_evaluation:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            lambda_train_indices = lambda_train_fv_with_lambda_input[['lambda_index']]\n",
    "            lambda_valid_indices = lambda_valid_fv_with_lambda_input[['lambda_index']]\n",
    "            lambda_test_indices = lambda_test_fv_with_lambda_input[['lambda_index']]\n",
    "\n",
    "            lambda_train_fv_with_lambda_input = lambda_train_fv_with_lambda_input.drop('lambda_index')\n",
    "            lambda_valid_fv_with_lambda_input = lambda_valid_fv_with_lambda_input.drop('lambda_index')\n",
    "            lambda_test_fv_with_lambda_input = lambda_test_fv_with_lambda_input.drop('lambda_index')\n",
    "\n",
    "            lambda_train_fv_with_lambda_input_with_valid_split, lambda_train_fv_with_lambda_input_test_split = train_test_split(lambda_train_fv_with_lambda_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "            lambda_train_fv_with_lambda_input_train_split, lambda_train_fv_with_lambda_input_valid_split = train_test_split(lambda_train_fv_with_lambda_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "            lambda_valid_fv_with_lambda_input_with_valid_split, lambda_valid_fv_with_lambda_input_test_split = train_test_split(lambda_valid_fv_with_lambda_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "            lambda_valid_fv_with_lambda_input_train_split, lambda_valid_fv_with_lambda_input_valid_split = train_test_split(lambda_valid_fv_with_lambda_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "            lambda_test_fv_with_lambda_input_with_valid_split, lambda_test_fv_with_lambda_input_test_split = train_test_split(lambda_test_fv_with_lambda_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "            lambda_test_fv_with_lambda_input_train_split, lambda_test_fv_with_lambda_input_valid_split = train_test_split(lambda_test_fv_with_lambda_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)                 \n",
    "        \n",
    "        lambda_train_fv_train_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_train_split[lambda_train_fv_with_lambda_input_train_split.columns[n::n+1]]], axis=1)\n",
    "        lambda_valid_fv_train_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_train_split[lambda_valid_fv_with_lambda_input_train_split.columns[n::n+1]]], axis=1)\n",
    "        lambda_test_fv_train_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_train_split[lambda_test_fv_with_lambda_input_train_split.columns[n::n+1]]], axis=1)\n",
    "\n",
    "        if i == 0:\n",
    "            lambda_train_input_train_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_train_split.drop(lambda_train_fv_with_lambda_input_train_split.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input_train_split.shape[0], int((lambda_train_fv_with_lambda_input_train_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "            lambda_valid_input_train_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_train_split.drop(lambda_valid_fv_with_lambda_input_train_split.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input_train_split.shape[0], int((lambda_valid_fv_with_lambda_input_train_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "            lambda_test_input_train_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_train_split.drop(lambda_test_fv_with_lambda_input_train_split.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input_train_split.shape[0], int((lambda_test_fv_with_lambda_input_train_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "\n",
    "            \n",
    "        lambda_train_fv_valid_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_valid_split[lambda_train_fv_with_lambda_input_valid_split.columns[n::n+1]]], axis=1)\n",
    "        lambda_valid_fv_valid_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_valid_split[lambda_valid_fv_with_lambda_input_valid_split.columns[n::n+1]]], axis=1)\n",
    "        lambda_test_fv_valid_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_valid_split[lambda_test_fv_with_lambda_input_valid_split.columns[n::n+1]]], axis=1)\n",
    "\n",
    "        if i == 0:\n",
    "            lambda_train_input_valid_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_valid_split.drop(lambda_train_fv_with_lambda_input_valid_split.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input_valid_split.shape[0], int((lambda_train_fv_with_lambda_input_valid_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "            lambda_valid_input_valid_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_valid_split.drop(lambda_valid_fv_with_lambda_input_valid_split.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input_valid_split.shape[0], int((lambda_valid_fv_with_lambda_input_valid_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "            lambda_test_input_valid_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_valid_split.drop(lambda_test_fv_with_lambda_input_valid_split.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input_valid_split.shape[0], int((lambda_test_fv_with_lambda_input_validsplit.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "\n",
    "            \n",
    "        lambda_train_fv_test_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_test_split[lambda_train_fv_with_lambda_input_test_split.columns[n::n+1]]], axis=1)\n",
    "        lambda_valid_fv_test_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_test_split[lambda_valid_fv_with_lambda_input_test_split.columns[n::n+1]]], axis=1)\n",
    "        lambda_test_fv_test_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_test_split[lambda_test_fv_with_lambda_input_test_split.columns[n::n+1]]], axis=1)\n",
    "\n",
    "        if i == 0:\n",
    "            lambda_train_input_test_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_test_split.drop(lambda_train_fv_with_lambda_input_test_split.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input_test_split.shape[0], int((lambda_train_fv_with_lambda_input_test_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "            lambda_valid_input_test_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_test_split.drop(lambda_valid_fv_with_lambda_input_test_split.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input_test_split.shape[0], int((lambda_valid_fv_with_lambda_input_test_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "            lambda_test_input_test_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_test_split.drop(lambda_test_fv_with_lambda_input_test_split.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input_test_split.shape[0], int((lambda_test_fv_with_lambda_input_test_split.shape[1]*(n/(n+1)))/n), n)], axis=1)    \n",
    "        \n",
    "        lambda_train_fv_train_split_list.append(lambda_train_fv_train_split)\n",
    "        lambda_valid_fv_train_split_list.append(lambda_valid_fv_train_split)\n",
    "        lambda_test_fv_train_split_list.append(lambda_test_fv_train_split)   \n",
    "\n",
    "        lambda_train_fv_valid_split_list.append(lambda_train_fv_valid_split)\n",
    "        lambda_valid_fv_valid_split_list.append(lambda_valid_fv_valid_split)\n",
    "        lambda_test_fv_valid_split_list.append(lambda_test_fv_valid_split)\n",
    "\n",
    "        lambda_train_fv_test_split_list.append(lambda_train_fv_test_split)         \n",
    "        lambda_valid_fv_test_split_list.append(lambda_valid_fv_test_split) \n",
    "        lambda_test_fv_test_split_list.append(lambda_test_fv_test_split)\n",
    "           \n",
    "            \n",
    "        lambda_train_input_train_split_list.append(lambda_train_input_train_split)\n",
    "        lambda_valid_input_train_split_list.append(lambda_valid_input_train_split)\n",
    "        lambda_test_input_train_split_list.append(lambda_test_input_train_split)   \n",
    "       \n",
    "        lambda_train_input_valid_split_list.append(lambda_train_input_valid_split)\n",
    "        lambda_valid_input_valid_split_list.append(lambda_valid_input_valid_split)\n",
    "        lambda_test_input_valid_split_list.append(lambda_test_input_valid_split)\n",
    "\n",
    "        lambda_train_input_test_split_list.append(lambda_train_input_test_split)\n",
    "        lambda_valid_input_test_split_list.append(lambda_valid_input_test_split)\n",
    "        lambda_test_input_test_split_list.append(lambda_test_input_test_split)\n",
    "        \n",
    "else:        \n",
    "    X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3+2)], axis=1)\n",
    "    y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3+2)]].astype(float)\n",
    "    \n",
    "    y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "    y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "    y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        y_data = y_data_polynomial_true\n",
    "    else:\n",
    "        y_data = y_data_polynomial_lstsq_pred      \n",
    "        \n",
    "    if inet_holdout_seed_evaluation:\n",
    "        pass\n",
    "    else:     \n",
    "        X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)                   \n",
    "\n",
    "    if inet_holdout_seed_evaluation:\n",
    "        pass\n",
    "    else:\n",
    "        lambda_train_indices = lambda_train_fv_with_lambda_input[['lambda_index']]\n",
    "        lambda_valid_indices = lambda_valid_fv_with_lambda_input[['lambda_index']]\n",
    "        lambda_test_indices = lambda_test_fv_with_lambda_input[['lambda_index']]\n",
    "\n",
    "        lambda_train_fv_with_lambda_input = lambda_train_fv_with_lambda_input.drop('lambda_index')\n",
    "        lambda_valid_fv_with_lambda_input = lambda_valid_fv_with_lambda_input.drop('lambda_index')\n",
    "        lambda_test_fv_with_lambda_input = lambda_test_fv_with_lambda_input.drop('lambda_index')\n",
    "\n",
    "        lambda_train_fv_with_lambda_input_with_valid_split, lambda_train_fv_with_lambda_input_test_split = train_test_split(lambda_train_fv_with_lambda_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_train_fv_with_lambda_input_train_split, lambda_train_fv_with_lambda_input_valid_split = train_test_split(lambda_train_fv_with_lambda_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "        lambda_valid_fv_with_lambda_input_with_valid_split, lambda_valid_fv_with_lambda_input_test_split = train_test_split(lambda_valid_fv_with_lambda_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv_with_lambda_input_train_split, lambda_valid_fv_with_lambda_input_valid_split = train_test_split(lambda_valid_fv_with_lambda_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "        lambda_test_fv_with_lambda_input_with_valid_split, lambda_test_fv_with_lambda_input_test_split = train_test_split(lambda_test_fv_with_lambda_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv_with_lambda_input_train_split, lambda_test_fv_with_lambda_input_valid_split = train_test_split(lambda_test_fv_with_lambda_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)                 \n",
    "\n",
    "    lambda_train_fv_train_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_train_split[lambda_train_fv_with_lambda_input_train_split.columns[n::n+1]]], axis=1)\n",
    "    lambda_valid_fv_train_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_train_split[lambda_valid_fv_with_lambda_input_train_split.columns[n::n+1]]], axis=1)\n",
    "    lambda_test_fv_train_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_train_split[lambda_test_fv_with_lambda_input_train_split.columns[n::n+1]]], axis=1)\n",
    "\n",
    "    lambda_train_input_train_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_train_split.drop(lambda_train_fv_with_lambda_input_train_split.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input_train_split.shape[0], int((lambda_train_fv_with_lambda_input_train_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "    lambda_valid_input_train_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_train_split.drop(lambda_valid_fv_with_lambda_input_train_split.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input_train_split.shape[0], int((lambda_valid_fv_with_lambda_input_train_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "    lambda_test_input_train_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_train_split.drop(lambda_test_fv_with_lambda_input_train_split.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input_train_split.shape[0], int((lambda_test_fv_with_lambda_input_train_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "\n",
    "\n",
    "    lambda_train_fv_valid_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_valid_split[lambda_train_fv_with_lambda_input_valid_split.columns[n::n+1]]], axis=1)\n",
    "    lambda_valid_fv_valid_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_valid_split[lambda_valid_fv_with_lambda_input_valid_split.columns[n::n+1]]], axis=1)\n",
    "    lambda_test_fv_valid_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_valid_split[lambda_test_fv_with_lambda_input_valid_split.columns[n::n+1]]], axis=1)\n",
    "\n",
    "    lambda_train_input_valid_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_valid_split.drop(lambda_train_fv_with_lambda_input_valid_split.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input_valid_split.shape[0], int((lambda_train_fv_with_lambda_input_valid_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "    lambda_valid_input_valid_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_valid_split.drop(lambda_valid_fv_with_lambda_input_valid_split.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input_valid_split.shape[0], int((lambda_valid_fv_with_lambda_input_valid_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "    lambda_test_input_valid_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_valid_split.drop(lambda_test_fv_with_lambda_input_valid_split.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input_valid_split.shape[0], int((lambda_test_fv_with_lambda_input_validsplit.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "\n",
    "\n",
    "    lambda_train_fv_test_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_test_split[lambda_train_fv_with_lambda_input_test_split.columns[n::n+1]]], axis=1)\n",
    "    lambda_valid_fv_test_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_test_split[lambda_valid_fv_with_lambda_input_test_split.columns[n::n+1]]], axis=1)\n",
    "    lambda_test_fv_test_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_test_split[lambda_test_fv_with_lambda_input_test_split.columns[n::n+1]]], axis=1)\n",
    "\n",
    "    lambda_train_input_test_split = pd.concat([lambda_train_indices, lambda_train_fv_with_lambda_input_test_split.drop(lambda_train_fv_with_lambda_input_test_split.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input_test_split.shape[0], int((lambda_train_fv_with_lambda_input_test_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "    lambda_valid_input_test_split = pd.concat([lambda_valid_indices, lambda_valid_fv_with_lambda_input_test_split.drop(lambda_valid_fv_with_lambda_input_test_split.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input_test_split.shape[0], int((lambda_valid_fv_with_lambda_input_test_split.shape[1]*(n/(n+1)))/n), n)], axis=1)\n",
    "    lambda_test_input_test_split = pd.concat([lambda_test_indices, lambda_test_fv_with_lambda_input_test_split.drop(lambda_test_fv_with_lambda_input_test_split.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input_test_split.shape[0], int((lambda_test_fv_with_lambda_input_test_split.shape[1]*(n/(n+1)))/n), n)], axis=1)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.940639Z",
     "start_time": "2020-12-07T13:35:07.995Z"
    }
   },
   "outputs": [],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.941724Z",
     "start_time": "2020-12-07T13:35:07.997Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.942820Z",
     "start_time": "2020-12-07T13:35:07.999Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_with_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_train_fv_train_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_train_input_train_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.943932Z",
     "start_time": "2020-12-07T13:35:08.002Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = X_train_list[-1].head()\n",
    "else:\n",
    "    print_head = X_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.945025Z",
     "start_time": "2020-12-07T13:35:08.004Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = y_train_list[-1].head()\n",
    "else:\n",
    "    print_head = y_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.946151Z",
     "start_time": "2020-12-07T13:35:08.006Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_base_model():\n",
    "    base_model = Sequential()\n",
    "\n",
    "    base_model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=lambda_train_input_train_split[0].shape[1])) #1024\n",
    "\n",
    "    if dropout > 0:\n",
    "        base_model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        base_model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            base_model.add(Dropout(dropout))   \n",
    "\n",
    "    base_model.add(Dense(1))\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.947400Z",
     "start_time": "2020-12-07T13:35:08.009Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn_and_pred(X_train, \n",
    "                      X_valid, \n",
    "                      X_test, \n",
    "                      y_train, \n",
    "                      y_valid, \n",
    "                      y_test,\n",
    "                      lambda_train_fv_valid_split, \n",
    "                      lambda_train_fv_test_split, \n",
    "                      lambda_train_fv_train_split, \n",
    "                      lambda_valid_fv_valid_split, \n",
    "                      lambda_valid_fv_test_split, \n",
    "                      lambda_valid_fv_train_split, \n",
    "                      lambda_test_fv_valid_split, \n",
    "                      lambda_test_fv_test_split, \n",
    "                      lambda_test_fv_train_split, \n",
    "                      callback_names=[], \n",
    "                      return_model=False):       \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(interpretation_network_layers[0], activation='relu', input_dim=X_train.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in interpretation_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(nCr(n+d, d))) \n",
    "    \n",
    "    #decide whether to use lambda preds for evaluation or polynomial from lstsq lambda preds\n",
    "    if not consider_labels_training and not evaluate_with_real_function:\n",
    "        if True: #implementation with direct lambda net prediction\n",
    "            base_model = generate_base_model()\n",
    "            random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "            #random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "            list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "            loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "            metrics = [loss_function, mean_absolute_error_extended]\n",
    "            valid_data = None\n",
    "            y_train_model = np.hstack((y_train, X_train))\n",
    "        else: #old implementation with preloaded lambda preds\n",
    "            loss_function = mean_absolute_error_tf_fv_lambda_extended_preds_appended    \n",
    "            metrics = [mean_absolute_error_tf_fv_lambda_extended_preds_appended, mean_absolute_error_extended]\n",
    "            valid_data = None\n",
    "            y_train_model = np.hstack((y_train, lambda_train_fv_train_split))\n",
    "            \n",
    "    else:\n",
    "        loss_function = mean_absolute_error_tf_fv\n",
    "        metrics = ['mean_absolute_error']\n",
    "        valid_data = (X_valid, y_valid)\n",
    "        y_train_model = y_train\n",
    "     \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=metrics\n",
    "                 )\n",
    "\n",
    "    #Callbacks\n",
    "    callbacks = return_callbacks_from_string(callback_names)\n",
    "        \n",
    "    if seed_in_inet_training: #drop lambda_index\n",
    "        X_train_inet = X_data.drop([0])\n",
    "        X_valid_inet = X_data.drop([0])\n",
    "        X_test_inet = X_data.drop([0])\n",
    "    else: #drop lambda_index and seed\n",
    "        X_train_inet = X_data.drop([0, 1])\n",
    "        X_valid_inet = X_data.drop([0, 1])\n",
    "        X_test_inet = X_data.drop([0, 1])\n",
    "        \n",
    "        \n",
    "    history = model.fit(X_train_inet,\n",
    "              y_train_model,\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              validation_data=valid_data,\n",
    "              callbacks=callbacks,\n",
    "              verbose=10)\n",
    "    \n",
    "    y_valid_pred = model.predict(X_valid_inet)\n",
    "    y_test_pred = model.predict(X_test_inet)\n",
    "    \n",
    "    polynomial_true_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_valid_input_valid_split) #USE SPLIT HERE CORRECT?\n",
    "    polynomial_pred_inet_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_valid_input_valid_split)\n",
    "    \n",
    "    polynomial_true_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_valid_input_test_split)\n",
    "    polynomial_pred_inet_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_valid_input_test_split)\n",
    "\n",
    "    \n",
    "    polynomial_true_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_test_input_valid_split)\n",
    "    polynomial_pred_inet_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_test_input_valid_split)\n",
    "    \n",
    "    polynomial_true_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_test_input_test_split)\n",
    "    polynomial_pred_inet_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_test_input_test_split)\n",
    "    \n",
    "    \n",
    "    polynomial_test_fv = [polynomial_true_test_fv_valid_split, \n",
    "                            polynomial_pred_inet_test_fv_valid_split, \n",
    "                            polynomial_true_test_fv_test_split, \n",
    "                            polynomial_pred_inet_test_fv_test_split]\n",
    "    \n",
    "    polynomial_valid_fv = [polynomial_true_valid_fv_valid_split, \n",
    "                             polynomial_pred_inet_valid_fv_valid_split, \n",
    "                             polynomial_true_valid_fv_test_split, \n",
    "                             polynomial_pred_inet_valid_fv_test_split]\n",
    "    \n",
    "    polynomial_fv = [polynomial_valid_fv, polynomial_test_fv]\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_test_fv_valid_split, \n",
    "                                polynomial_pred_inet_test_fv_valid_split)\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_test_fv_test_split, \n",
    "                                polynomial_pred_inet_test_fv_test_split)\n",
    "    \n",
    "    \n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_valid_fv_valid_split, \n",
    "                                polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_valid_fv_test_split, \n",
    "                                polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv_valid_split, scores_truePoly_VS_inetPoly_test_fv_test_split)\n",
    "    scores_truePoly_VS_inetPoly_valid_fv = mergeDict(scores_truePoly_VS_inetPoly_valid_fv_valid_split, scores_truePoly_VS_inetPoly_valid_fv_test_split)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        scores_dict = [scores_truePoly_VS_inetPoly_test_fv, scores_truePoly_VS_inetPoly_valid_fv]\n",
    "    else:   \n",
    "        scores_predLambda_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_test_fv_valid_split, \n",
    "                                    polynomial_pred_inet_test_fv_valid_split)\n",
    "        \n",
    "        scores_predLambda_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_test_fv_test_split, \n",
    "                                    polynomial_pred_inet_test_fv_test_split)\n",
    "\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_valid_fv_valid_split, \n",
    "                                    polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_valid_fv_test_split, \n",
    "                                    polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "        eval_metrics = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'STD FV PRED', 'MEAN FV PRED']\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_test_fv = mergeDict(scores_predLambda_VS_inetPoly_test_fv_valid_split, scores_predLambda_VS_inetPoly_test_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_test_fv = {key: scores_predLambda_VS_inetPoly_test_fv[key] for key in eval_metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv, scores_predLambda_VS_inetPoly_test_fv)\n",
    "        \n",
    "        scores_predLambda_VS_inetPoly_valid_fv = mergeDict(scores_predLambda_VS_inetPoly_valid_fv_valid_split, scores_predLambda_VS_inetPoly_valid_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_valid_fv = {key: scores_predLambda_VS_inetPoly_valid_fv[key] for key in eval_metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_valid_fv =mergeDict(scores_truePoly_VS_inetPoly_valid_fv, scores_predLambda_VS_inetPoly_valid_fv)\n",
    "\n",
    "        scores_dict = [scores_truePoly_and_predLambda_VS_inetPoly_test_fv, scores_truePoly_and_predLambda_VS_inetPoly_valid_fv]\n",
    "\n",
    "    if return_model:\n",
    "        return history.history, scores_dict, polynomial_fv, model         \n",
    "    else: \n",
    "        return history.history, scores_dict, polynomial_fv       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.948826Z",
     "start_time": "2020-12-07T13:35:08.011Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results = train_nn_and_pred(X_train.values, \n",
    "                                X_valid.values, \n",
    "                                X_test.values, \n",
    "                                y_train.values, \n",
    "                                y_valid.values, \n",
    "                                y_test.values, \n",
    "                                lambda_train_fv_valid_split.values, \n",
    "                                lambda_train_fv_test_split.values, \n",
    "                                lambda_train_fv_train_split.values, \n",
    "                                lambda_valid_fv_valid_split.values, \n",
    "                                lambda_valid_fv_test_split.values, \n",
    "                                lambda_valid_fv_train_split.values, \n",
    "                                lambda_test_fv_valid_split.values, \n",
    "                                lambda_test_fv_test_split.values, \n",
    "                                lambda_test_fv_train_split.values, \n",
    "                                callback_names=['plot_losses_callback', 'early_stopping'], \n",
    "                                return_model=True)\n",
    "    \n",
    "    history = results[0]\n",
    "    \n",
    "    scores_complete = results[1]\n",
    "    scores_with_valid_fv = scores_complete[0]\n",
    "    scores_with_test_fv = scores_complete[1]\n",
    "    \n",
    "    polynomial_fv_complete = results[2]\n",
    "    polynomial_valid_fv = polynomial_fv_complete[0]\n",
    "    polynomial_test_fv = polynomial_fv_complete[1]\n",
    "    \n",
    "    model = results[3]\n",
    "    \n",
    "    x = PrettyTable()\n",
    "\n",
    "    x.field_names = [\"Error Name\", \"Valid Error Int\", \"Test Error Int\"]\n",
    "\n",
    "    for error, value in scores_with_test_fv.items():\n",
    "\n",
    "        x.add_row([error, value[0], value[1]])\n",
    "\n",
    "    print(x)    \n",
    "    \n",
    "elif multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, \n",
    "                            verbose=11, \n",
    "                            backend='loky')(delayed(train_nn_and_pred)(X_train.values, \n",
    "                                                                      X_valid.values, \n",
    "                                                                      X_test.values, \n",
    "                                                                      y_train.values, \n",
    "                                                                      y_valid.values, \n",
    "                                                                      y_test.values, \n",
    "                                                                      lambda_train_fv_valid_split.values, \n",
    "                                                                      lambda_train_fv_test_split.values, \n",
    "                                                                      lambda_train_fv_train_split.values, \n",
    "                                                                      lambda_valid_fv_valid_split.values, \n",
    "                                                                      lambda_valid_fv_test_split.values, \n",
    "                                                                      lambda_valid_fv_train_split.values, \n",
    "                                                                      lambda_test_fv_valid_split.values, \n",
    "                                                                      lambda_test_fv_test_split.values, \n",
    "                                                                      lambda_test_fv_train_split.values, \n",
    "                                                                      callback_names=['early_stopping']) for X_train, \n",
    "                                                                                                               X_valid, \n",
    "                                                                                                               X_test, \n",
    "                                                                                                               y_train, \n",
    "                                                                                                               y_valid, \n",
    "                                                                                                               y_test, \n",
    "                                                                                                               lambda_train_fv_valid_split, \n",
    "                                                                                                               lambda_train_fv_test_split, \n",
    "                                                                                                               lambda_train_fv_train_split,                                            \n",
    "                                                                                                               lambda_valid_fv_valid_split, \n",
    "                                                                                                               lambda_valid_fv_test_split, \n",
    "                                                                                                               lambda_valid_fv_train_split, \n",
    "                                                                                                               lambda_test_fv_valid_split, \n",
    "                                                                                                               lambda_test_fv_test_split, \n",
    "                                                                                                               lambda_test_fv_train_split in zip(X_train_list, \n",
    "                                                                                                                                                 X_valid_list, \n",
    "                                                                                                                                                 X_test_list, \n",
    "                                                                                                                                                 y_train_list, \n",
    "                                                                                                                                                 y_valid_list, \n",
    "                                                                                                                                                 y_test_list, \n",
    "                                                                                                                                                 lambda_train_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_train_split_list,                                                                                                                                                  \n",
    "                                                                                                                                                 lambda_valid_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_train_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_train_split_list))      \n",
    "\n",
    "    history_list = [result[0] for result in results_list]\n",
    "    \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "\n",
    "    for i, history in enumerate(history_list):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "        \n",
    "        plt.plot(history[list(history.keys())[1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)   \n",
    "        \n",
    "elif not multi_epoch_analysis and  samples_list != None:\n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, verbose=11, backend='loky')(delayed(train_nn_and_pred)(X_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  X_valid.values, \n",
    "                                                                                                  X_test.values, \n",
    "                                                                                                  y_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  y_valid.values, \n",
    "                                                                                                  y_test.values, \n",
    "                                                                                                  lambda_train_fv_valid_split.values, \n",
    "                                                                                                  lambda_train_fv_test_split.values, \n",
    "                                                                                                  lambda_train_fv_train_split.values, \n",
    "                                                                                                  lambda_valid_fv_valid_split.values, \n",
    "                                                                                                  lambda_valid_fv_test_split.values, \n",
    "                                                                                                  lambda_valid_fv_train_split.values, \n",
    "                                                                                                  lambda_test_fv_valid_split.values, \n",
    "                                                                                                  lambda_test_fv_test_split.values, \n",
    "                                                                                                  lambda_test_fv_train_split.values, \n",
    "                                                                                                  callback_names=['early_stopping']) for samples in samples_list)     \n",
    "    \n",
    "    history_list = [result[0] for result in results_list]\n",
    "     \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "    for i, history in enumerate(history_list):       \n",
    "        \n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.950008Z",
     "start_time": "2020-12-07T13:35:08.013Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_valid_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_valid_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.951149Z",
     "start_time": "2020-12-07T13:35:08.015Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_test_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_test_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.952223Z",
     "start_time": "2020-12-07T13:35:08.017Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.953260Z",
     "start_time": "2020-12-07T13:35:08.019Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.954420Z",
     "start_time": "2020-12-07T13:35:08.021Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss = []\n",
    "    plot_history_metric = []\n",
    "    plot_history_val_loss = []\n",
    "    plot_history_val_metric = []\n",
    "        \n",
    "    for history in history_list:\n",
    "        plot_history_loss.append(history['loss'][-1])\n",
    "        plot_history_metric.append(history[list(history.keys())[1]][-1])\n",
    "\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plot_history_val_loss.append(history['val_loss'][-1])\n",
    "            plot_history_val_metric.append(history[list(history.keys())[len(history.keys())//2+1]][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.955626Z",
     "start_time": "2020-12-07T13:35:08.023Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss_df = pd.DataFrame(data=plot_history_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_loss))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_loss_df = pd.DataFrame(data=plot_history_val_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_loss))])\n",
    "    \n",
    "    plt.plot(plot_history_loss_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_loss_df)\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.956834Z",
     "start_time": "2020-12-07T13:35:08.025Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_metric_df = pd.DataFrame(data=plot_history_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_metric))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_metric_df = pd.DataFrame(data=plot_history_val_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_metric))])\n",
    "    \n",
    "    plt.plot(plot_history_metric_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_metric_df)\n",
    "    plt.title('Metric')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure Interpretation-Net Socres for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.958122Z",
     "start_time": "2020-12-07T13:35:08.028Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_coeff_keys = ['MAE', 'RMSE', 'MAPE', 'Accuracy', 'Accuracy Multilabel']\n",
    "metrics_fv_keys = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "new_row_identifiers_coeff = ['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT']\n",
    "new_row_identifiers_fv = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "\n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "  \n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)\n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "\n",
    "\n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'means_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)  \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "    \n",
    "elif not multi_epoch_analysis and samples_list != None and evaluate_with_real_function:\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "\n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "        \n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "\n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1) \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)     \n",
    "            \n",
    "elif not multi_epoch_analysis and  samples_list != None and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "\n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "\n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)      \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)    \n",
    "            \n",
    "if multi_epoch_analysis:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')  \n",
    "elif samples_list != None:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_valid_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_test_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Lambda Scores for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.959340Z",
     "start_time": "2020-12-07T13:35:08.030Z"
    }
   },
   "outputs": [],
   "source": [
    "path_scores_valid_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_scores_test_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_stds_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_means_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "df_mean_scores_valid_lambda = pd.read_csv(path_scores_valid_lambda, sep=',', index_col=0)\n",
    "df_mean_scores_test_lambda = pd.read_csv(path_scores_test_lambda, sep=',', index_col=0)\n",
    "df_stds_lambda = pd.read_csv(path_stds_lambda, sep=',', index_col=0)\n",
    "df_means_lambda = pd.read_csv(path_means_lambda, sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.960494Z",
     "start_time": "2020-12-07T13:35:08.032Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_test_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.961684Z",
     "start_time": "2020-12-07T13:35:08.034Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stds_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.962768Z",
     "start_time": "2020-12-07T13:35:08.036Z"
    }
   },
   "outputs": [],
   "source": [
    "df_means_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns to Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.964015Z",
     "start_time": "2020-12-07T13:35:08.039Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:   \n",
    "    select_columns = []\n",
    "\n",
    "    for column in df_mean_scores_test_lambda.columns:\n",
    "        if int(column.split(' ')[-1][1:]) in [(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]:\n",
    "            select_columns.append(column)\n",
    "    \n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "    scores_lambda = df_mean_scores_test_lambda[select_columns].loc[plot_cols]\n",
    "    scores_int = scores_test_list.loc[plot_cols]    \n",
    "elif samples_list != None:\n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "    scores_int = scores_test_list.loc[plot_cols] \n",
    "    scores_lambda = df_mean_scores_test_lambda.loc[plot_cols].iloc[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.965141Z",
     "start_time": "2020-12-07T13:35:08.041Z"
    }
   },
   "outputs": [],
   "source": [
    "print_head = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    print_head = scores_int\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.966255Z",
     "start_time": "2020-12-07T13:35:08.043Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_test_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.967365Z",
     "start_time": "2020-12-07T13:35:08.045Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_valid_fv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.968553Z",
     "start_time": "2020-12-07T13:35:08.047Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "        vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "            \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "                    \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "        vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "        \n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.969737Z",
     "start_time": "2020-12-07T13:35:08.050Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.970918Z",
     "start_time": "2020-12-07T13:35:08.052Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "    vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file \n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "       \n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "    vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "    \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.972072Z",
     "start_time": "2020-12-07T13:35:08.054Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split_list[-1]\n",
    "elif samples_list != None:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "else:\n",
    "    plot_preds = polynomial_test_fv\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "\n",
    "x_vars = ['x' + str(i) for i in range(1, n+1)]\n",
    "\n",
    "columns = x_vars.copy()\n",
    "columns.append('FVs')\n",
    "\n",
    "columns_single = x_vars.copy()\n",
    "columns_single.extend(['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'])\n",
    "\n",
    "eval_size_plot = plot_preds[2].shape[1]\n",
    "rand_index = 2#42#random.randint(0, plot_preds[2].shape[0]-1)\n",
    "vars_plot = np.column_stack([lambda_test_input_test_split[rand_index][::,i] for i in range(n)])\n",
    "plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, plot_preds[2][rand_index], plot_preds[3][rand_index], plot_eval.values[rand_index]]), columns=columns_single)\n",
    "\n",
    "vars_plot_all_preds = np.append(np.append(vars_plot, vars_plot, axis=0), vars_plot, axis=0)\n",
    "preds_plot_all = np.append(np.append(plot_preds[2][rand_index], plot_preds[3][rand_index], axis=0), plot_eval.values[rand_index], axis=0)\n",
    "\n",
    "if evaluate_with_real_function:\n",
    "    real_str = np.array(['Real Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds LSTSQ\n",
    "else:\n",
    "    real_str = np.array(['Lambda Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds\n",
    "    \n",
    "identifier = np.concatenate([real_str, int_str, lambda_str])\n",
    "\n",
    "plot_data = pd.DataFrame(data=np.column_stack([vars_plot_all_preds, preds_plot_all]), columns=columns)\n",
    "plot_data['Identifier'] = identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.973224Z",
     "start_time": "2020-12-07T13:35:08.056Z"
    }
   },
   "outputs": [],
   "source": [
    "pp1 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  y_vars=['FVs'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.974260Z",
     "start_time": "2020-12-07T13:35:08.058Z"
    }
   },
   "outputs": [],
   "source": [
    "pp2 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  #y_vars=['FVs'],\n",
    "                  #x_vars=x_vars\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.975369Z",
     "start_time": "2020-12-07T13:35:08.059Z"
    }
   },
   "outputs": [],
   "source": [
    "pp3 = sns.pairplot(data=plot_data_single,\n",
    "                  #kind='reg',\n",
    "                  y_vars=['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.976581Z",
     "start_time": "2020-12-07T13:35:08.062Z"
    }
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_REAL_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')\n",
    "else:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_PRED_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.977792Z",
     "start_time": "2020-12-07T13:35:08.064Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n)+ '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "    loss_df_lambda = pd.read_csv(path_loss, sep=',')\n",
    "    metric_df_lambda = pd.read_csv(path_metric, sep=',')\n",
    "    val_loss_df_lambda = pd.read_csv(path_val_loss, sep=',')\n",
    "    val_metric_df_lambda = pd.read_csv(path_val_metric, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.978954Z",
     "start_time": "2020-12-07T13:35:08.066Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "\n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_metric = 0\n",
    "\n",
    "    metric_df_adjusted = metric_df_lambda.copy(deep=True)\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "\n",
    "    val_metric_df_adjusted = val_metric_df_lambda.copy(deep=True)\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_metric'])\n",
    "    plt.title('model metric')\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.980124Z",
     "start_time": "2020-12-07T13:35:08.068Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_loss = 1000\n",
    "\n",
    "    loss_df_adjusted = loss_df_lambda.copy(deep=True)\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "\n",
    "    val_loss_df_adjusted = val_loss_df_lambda.copy(deep=True)\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.981303Z",
     "start_time": "2020-12-07T13:35:08.070Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    preds = model.predict(X_test)\n",
    "    preds_rounded = np.round(preds, 1)\n",
    "    #preds_true = pd.DataFrame(data=[np.round(preds, 1), y_test.values])\n",
    "    for pred, y in tqdm(zip(preds_rounded, y_test.values)):\n",
    "        if (pred == y).all():\n",
    "            print(pred)\n",
    "    \n",
    "    #print(preds_rounded)\n",
    "    #print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.982396Z",
     "start_time": "2020-12-07T13:35:08.072Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#summarize history for loss\n",
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except KeyError:\n",
    "        print('no val_loss in keys')\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/loss_' + interpretation_network_string + filename + '.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.983627Z",
     "start_time": "2020-12-07T13:35:08.074Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    random_polynomial = list(random_product([i*a_step for i in range(int(a_min*10**int(-np.log10(a_step))), int(a_max*10**int(-np.log10(a_step))))], repeat=nCr(n+d, d)))\n",
    "    list_of_random_polynomials.append(random_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.984745Z",
     "start_time": "2020-12-07T13:35:08.076Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(y_test.values, lambda_test_input_test_split)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_test_input_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.985913Z",
     "start_time": "2020-12-07T13:35:08.079Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(y_test, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.987060Z",
     "start_time": "2020-12-07T13:35:08.081Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.988200Z",
     "start_time": "2020-12-07T13:35:08.083Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(y_train.values, lambda_train_input_train_split)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:35:16.989228Z",
     "start_time": "2020-12-07T13:35:08.085Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
