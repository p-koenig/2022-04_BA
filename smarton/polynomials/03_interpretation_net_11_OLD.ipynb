{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:31.601655Z",
     "start_time": "2020-11-27T17:42:31.594280Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:31.615618Z",
     "start_time": "2020-11-27T17:42:31.604131Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3  \n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "n_jobs = 20\n",
    "\n",
    "\n",
    "data_size = 10000 #for loading lambda models\n",
    "\n",
    "#specify interpretation net structure\n",
    "optimizer = 'adam'\n",
    "dropout = 0\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "interpretation_network_layers = [2048]\n",
    "\n",
    "random_evaluation_dataset_size = 500\n",
    "\n",
    "#lambda net specifications for loading (need to be set according to lambda net training to load correct weights)\n",
    "epochs_lambda = 200\n",
    "batch_lambda = 64\n",
    "lambda_network_layers = [5*sparsity]\n",
    "optimizer_lambda = '_' + 'SGD'\n",
    "\n",
    "\n",
    "lambda_dataset_size = 1000\n",
    "\n",
    "#set if multi_epoch_analysis should be performed\n",
    "multi_epoch_analysis = True\n",
    "each_epochs_save_lambda = 10 #None if no checkpointing (otherwise set according to lambda-net training)\n",
    "epoch_start = 0 #use to skip first epochs in multi_epoch_analysis\n",
    "\n",
    "#set if samples analysis should be performed\n",
    "samples_list = None#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "\n",
    "evaluate_with_real_function = False\n",
    "consider_labels_training = False\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "fixed_seed_lambda_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:31.650310Z",
     "start_time": "2020-11-27T17:42:31.617566Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1)\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_SeedMethod'\n",
    "elif not fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_NoSeedMethod'\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "structure = '_' + layers_str + str(epochs_lambda) + 'e' + str(batch_lambda) + 'b' + optimizer_lambda\n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure\n",
    "\n",
    "interpretation_network_string = 'drop' + str(dropout) + 'e' + str(epochs) + 'b' + str(batch_size) + '_' + str(interpretation_network_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.791499Z",
     "start_time": "2020-11-27T17:42:31.652644Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.801344Z",
     "start_time": "2020-11-27T17:42:50.794475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.827274Z",
     "start_time": "2020-11-27T17:42:50.804216Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "\n",
    "def return_float_tensor_representation(some_representation, dtype=tf.float32):\n",
    "    if tf.is_tensor(some_representation):\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "    else:\n",
    "        some_representation = tf.convert_to_tensor(some_representation)\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "        \n",
    "    if not tf.is_tensor(some_representation):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(dtype) + ':' + str(some_representation))\n",
    "     \n",
    "    return some_representation\n",
    "\n",
    "\n",
    "def return_numpy_representation(some_representation):\n",
    "    if isinstance(some_representation, pd.DataFrame):\n",
    "        some_representation = some_representation.values\n",
    "        \n",
    "    if isinstance(some_representation, list):\n",
    "        some_representation = np.array(some_representation)\n",
    "    \n",
    "    if not isinstance(some_representation, np.ndarray):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(np.ndarray) + ':' + str(some_representation))\n",
    "    \n",
    "    return some_representation\n",
    "\n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict\n",
    "\n",
    "def return_callbacks_from_string(callback_string_list):\n",
    "    callbacks = [] if len(callback_string_list) > 0 else None\n",
    "    if 'plot_losses_callback' in callback_string_list:\n",
    "        callbacks.append(PlotLossesKerasTF())\n",
    "    if 'reduce_lr_loss' in callback_string_list:\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=epochs/10, verbose=0, min_delta=0, mode='min') #epsilon\n",
    "        callbacks.append(reduce_lr_loss)\n",
    "    if 'early_stopping' in callback_string_list:\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0, verbose=0, mode='min')\n",
    "        callbacks.append(earlyStopping)\n",
    "        \n",
    "    if not multi_epoch_analysis and samples_list == None: \n",
    "        callbacks.append(TQDMNotebookCallback())\n",
    "        \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.895992Z",
     "start_time": "2020-11-27T17:42:50.829881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec10c47852bf4f53ba999f8c66d89bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10879e3a627d482f8d479b16766c7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.909136Z",
     "start_time": "2020-11-27T17:42:50.897918Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with polynomials on function value basis\n",
    "\n",
    "def calculate_mae_fv(polynomial_true_pred):\n",
    "    polynomial_true = polynomial_true_pred[0]\n",
    "    polynomial_pred = polynomial_true_pred[1]\n",
    "    \n",
    "    global lambda_train_input\n",
    "    lambda_input = lambda_train_input\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "            \n",
    "        polynomial_true_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_true))\n",
    "        polynomial_true_fv = tf.reduce_sum(polynomial_true_value_per_term)\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "        \n",
    "        if index == 0:   \n",
    "            result = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "        else:           \n",
    "            current_valiue = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "            result = tf.math.add(result, current_valiue)    \n",
    "            \n",
    "    return  tf.math.divide(result, lambda_input.shape[0]) #tf.random.uniform(shape=[1], minval=0.1, maxval=10.0)\n",
    "\n",
    "def mean_absolute_error_tf_fv(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv, (y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.934379Z",
     "start_time": "2020-11-27T17:42:50.911375Z"
    }
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with lambda-net prediction based (predictions made in loss function)\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "\n",
    "def mean_absolute_error_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers, base_model):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)    \n",
    "    \n",
    "    model_lambda_placeholder = keras.models.clone_model(base_model)  \n",
    "    \n",
    "    weights_structure = base_model.get_weights()\n",
    "    dims = [np_arrays.shape for np_arrays in weights_structure]\n",
    "    \n",
    "    def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        network_parameters = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "        polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "        network_parameters = return_float_tensor_representation(network_parameters)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder), (polynomial_pred, network_parameters), fn_output_signature=tf.float32))\n",
    "    return mean_absolute_error_tf_fv_lambda_extended\n",
    "\n",
    "\n",
    "#CHANGES NEEDED\n",
    "def calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder):\n",
    "\n",
    "    def calculate_mae_fv_lambda(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        #polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[0]\n",
    "        network_parameters = input_list[1]\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        #CALCULATE LAMBDA FV HERE FOR EVALUATION DATASET\n",
    "        # build models\n",
    "        start = 0\n",
    "        layers = []\n",
    "        for i in range(len(dims)//2):\n",
    "            \n",
    "            # set weights of layer\n",
    "            index = i*2\n",
    "            size = np.product(dims[index])\n",
    "            weights_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[0].assign(weights_tf_true)\n",
    "            start += size\n",
    "            \n",
    "            # set biases of layer\n",
    "            index += 1\n",
    "            size = np.product(dims[index])\n",
    "            biases_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[1].assign(biases_tf_true)\n",
    "            start += size\n",
    "\n",
    "        \n",
    "        lambda_fv = tf.keras.backend.flatten(model_lambda_placeholder(evaluation_dataset))\n",
    "        \n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_lambda\n",
    "\n",
    "#nothing to change here (just fv calculation for evaluation entry for single polynomial)\n",
    "def calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred):\n",
    "\n",
    "\n",
    "    def calculate_fv_from_data(evaluation_entry):\n",
    "\n",
    "\n",
    "        value_without_coefficient = tf.vectorized_map(calculate_value_without_coefficient_wrapper(evaluation_entry), (list_of_monomial_identifiers))\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        \n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)     \n",
    "        \n",
    "        return polynomial_pred_fv\n",
    "    return calculate_fv_from_data\n",
    "\n",
    "\n",
    "#calculate intermediate term (without coefficient multiplication)\n",
    "def calculate_value_without_coefficient_wrapper(evaluation_entry):\n",
    "    def calculate_value_without_coefficient(coefficient_multiplier_term):      \n",
    "   \n",
    "        return tf.math.reduce_prod(tf.vectorized_map(lambda x: x[0]**x[1], (evaluation_entry, coefficient_multiplier_term)))\n",
    "    return calculate_value_without_coefficient\n",
    "\n",
    "#calculate MAE at the end ---> general:REPLACE FUNCTION WITH LOSS CALL OR LAMBDA\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.955683Z",
     "start_time": "2020-11-27T17:42:50.936469Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error #DEPRECATED, MAYBE NOT WORKING\n",
    "\n",
    "#Manual TF Loss function for comparison with pre-saved lambda-net predictions\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def calculate_value_without_coefficient_wrapper(evaluation_entry):\n",
    "    def calculate_value_without_coefficient(coefficient_multiplier_term):      \n",
    "        #return tf.math.reduce_prod(tf.vectorized_map(calculate_single_value_without_coefficient, (tf.strings.to_number(tf.strings.bytes_split(coefficient_multiplier_term), tf.float64), evaluation_entry)))\n",
    "        \n",
    "\n",
    "        \n",
    "        #coefficient_multiplier_term_integers = tf.reshape(tf.strings.to_number(tf.strings.bytes_split(coefficient_multiplier_term), tf.float64), [n,])\n",
    "        \n",
    "        print('calculate_value_without_coefficient_wrapper')\n",
    "        print(coefficient_multiplier_term.get_shape())\n",
    "        print(evaluation_entry.get_shape())\n",
    "        \n",
    "        return tf.math.reduce_prod(tf.vectorized_map(lambda x: x[0]**x[1], (evaluation_entry, coefficient_multiplier_term)))\n",
    "    return calculate_value_without_coefficient\n",
    "\n",
    "\n",
    "def calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred):\n",
    "    \n",
    "    #print('calculate_fv_from_data_wrapper')\n",
    "    #print(list_of_monomial_identifiers)\n",
    "    #print(list_of_monomial_identifiers.get_shape())\n",
    "    #print(polynomial_pred)\n",
    "    #print(polynomial_pred.get_shape())\n",
    "\n",
    "    def calculate_fv_from_data(evaluation_entry):\n",
    "        \n",
    "        #print('calculate_fv_from_data')\n",
    "        #print(evaluation_entry)\n",
    "        #print(evaluation_entry.get_shape())\n",
    "\n",
    "        value_without_coefficient = tf.vectorized_map(calculate_value_without_coefficient_wrapper(evaluation_entry), (list_of_monomial_identifiers))\n",
    "        \n",
    "        #print(value_without_coefficient)\n",
    "        #print(value_without_coefficient.get_shape())\n",
    "        \n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        \n",
    "        #print(polynomial_pred_value_per_term)\n",
    "        #print(polynomial_pred_value_per_term.get_shape().ndims)\n",
    "        \n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)     \n",
    "        \n",
    "        return polynomial_pred_fv\n",
    "    return calculate_fv_from_data\n",
    "\n",
    "def calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "\n",
    "    def calculate_mae_fv_lambda(input_list):\n",
    "\n",
    "        polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[1]\n",
    "        lambda_fv = input_list[2]\n",
    "        #evaluation_dataset = input_list[3]\n",
    "        #list_of_monomial_identifiers = input_list[4]\n",
    "\n",
    "        #print('calculate_mae_fv_lambda')\n",
    "        #print(list_of_monomial_identifiers)\n",
    "        #print(list_of_monomial_identifiers.get_shape())\n",
    "        #print(polynomial_pred)\n",
    "        #print(polynomial_pred.get_shape())\n",
    "        #print(evaluation_dataset)\n",
    "        #print(evaluation_dataset.get_shape())\n",
    "\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_lambda\n",
    "    \n",
    "def mean_absolute_error_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset, tf.float64)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers, tf.float64)    \n",
    "    \n",
    "    def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        lambda_fv = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "        polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "        lambda_fv = return_float_tensor_representation(lambda_fv, tf.float64)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true, tf.float64)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred, tf.float64)\n",
    "        #return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers), (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers), (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "    return mean_absolute_error_tf_fv_lambda_extended\n",
    "\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.972115Z",
     "start_time": "2020-11-27T17:42:50.958573Z"
    }
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with pre-saved lambda-net predictions\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "def calculate_mae_single_input_preds_appended(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def calculate_mae_fv_lambda_preds_appended(input_list):\n",
    "    \n",
    "    polynomial_true = input_list[0]\n",
    "    polynomial_pred = input_list[1]\n",
    "    lambda_fv = input_list[2]\n",
    "\n",
    "    global lambda_train_input\n",
    "    lambda_input = lambda_train_input\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "\n",
    "        if index == 0:\n",
    "            polynomial_pred_fv_list = tf.convert_to_tensor([polynomial_pred_fv])\n",
    "        else:\n",
    "            polynomial_pred_fv_list = tf.concat([polynomial_pred_fv_list, tf.convert_to_tensor([polynomial_pred_fv])], 0)\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input_preds_appended, (lambda_fv, polynomial_pred_fv_list)))\n",
    "  \n",
    "    \n",
    "def mean_absolute_error_tf_fv_lambda_preds_appended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "    lambda_fv = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda_preds_appended, (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:50.990719Z",
     "start_time": "2020-11-27T17:42:50.974295Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Basic Keras/TF Loss functions\n",
    "def root_mean_squared_error(y_true, y_pred):   \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "        \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)           \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))      \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)        \n",
    "    epsilon = return_float_tensor_representation(epsilon)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:51.005178Z",
     "start_time": "2020-11-27T17:42:50.992782Z"
    }
   },
   "outputs": [],
   "source": [
    "#Manual calculations for comparison of polynomials based on function values (no TF!)\n",
    "\n",
    "def calcualate_function_value(coefficient_list, lambda_input_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0   \n",
    "        \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        value_without_coefficient = [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multipliers, lambda_input_entry)]\n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, value_without_coefficient)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_function_values_from_polynomial(polynomial, lambda_input_data):\n",
    "    polynomial = return_numpy_representation(polynomial)\n",
    "    \n",
    "    function_value_list = []\n",
    "        \n",
    "    for lambda_input_entry in lambda_input_data:\n",
    "        function_value = calcualate_function_value(polynomial, lambda_input_entry)\n",
    "        function_value_list.append(function_value)\n",
    "\n",
    "    return np.array(function_value_list)\n",
    "\n",
    "\n",
    "def parallel_fv_calculation_from_polynomial(polynomial_list, lambda_input_list):\n",
    "    parallel = Parallel(n_jobs=10, verbose=0, backend='threading')\n",
    "    polynomial_true_fv = parallel(delayed(calculate_function_values_from_polynomial)(polynomial, lambda_input_list) for polynomial in polynomial_list)  \n",
    "    del parallel   \n",
    "    \n",
    "    return np.array(polynomial_true_fv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:51.035724Z",
     "start_time": "2020-11-27T17:42:51.007450Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Standard Metrics (no TF!)\n",
    "\n",
    "def mean_absolute_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)      \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(true_values-pred_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))  \n",
    "\n",
    "def root_mean_squared_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)         \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sqrt(np.mean((true_values-pred_values)**2)))\n",
    "    \n",
    "    return np.mean(np.array(result_list)) \n",
    "\n",
    "def mean_absolute_percentage_error_function_values(y_true, y_pred, epsilon=10e-3):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred) \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(((true_values-pred_values)/(true_values+epsilon)))))\n",
    "\n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def r2_score_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(r2_score(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_absolute_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sum(np.abs(true_values-pred_values))/(true_values.shape[0]*np.std(true_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_maximum_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.max(true_values-pred_values)/np.std(true_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_area_between_two_curves_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "      \n",
    "    assert(number_of_variables==1)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(area_between_two_curves(true_values, pred_values))\n",
    " \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_dtw_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "\n",
    "    result_list_single = []\n",
    "    result_list_array = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_single_value, result_single_array = dtw(true_values, pred_values)\n",
    "        result_list_single.append(result_single_value)\n",
    "        result_list_array.append(result_single_array)\n",
    "    \n",
    "    return np.mean(np.array(result_list_single)), np.mean(np.array(result_list_array), axis=1)\n",
    "\n",
    "def mean_frechet_dist_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(frechet_dist(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T17:42:51.050842Z",
     "start_time": "2020-11-27T17:42:51.039705Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_interpretation_net(y_data_real, \n",
    "                                y_data_pred, \n",
    "                                polynomial_true_fv, \n",
    "                                polynomial_pred_inet_fv):\n",
    "    \n",
    "    mae_coeff = np.round(mean_absolute_error(y_data_real, y_data_pred), 4)\n",
    "    rmse_coeff = np.round(root_mean_squared_error(y_data_real, y_data_pred), 4)\n",
    "    mape_coeff = np.round(mean_absolute_percentage_error_keras(y_data_real, y_data_pred), 4)\n",
    "    accuracy_coeff = np.round(accuracy_single(y_data_real, y_data_pred), 4)\n",
    "    accuracy_multi_coeff = np.round(accuracy_multilabel(y_data_real, y_data_pred), 4)\n",
    "    \n",
    "    mae_fv = np.round(mean_absolute_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmse_fv = np.round(root_mean_squared_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    mape_fv = np.round(mean_absolute_percentage_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    r2_fv = np.round(r2_score_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    raae_fv = np.round(relative_absolute_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmae_fv = np.round(relative_maximum_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4) \n",
    "\n",
    "    std_fv = np.std(polynomial_pred_inet_fv)\n",
    "    mean_fv = np.mean(polynomial_pred_inet_fv)\n",
    "\n",
    "    return {\n",
    "             'MAE': mae_coeff,\n",
    "             'RMSE': rmse_coeff, \n",
    "             'MAPE': mape_coeff,\n",
    "             'Accuracy': accuracy_coeff, \n",
    "             'Accuracy Multilabel': accuracy_multi_coeff, \n",
    "\n",
    "             'MAE FV': mae_fv,\n",
    "             'RMSE FV': rmse_fv,\n",
    "             'MAPE FV': mape_fv,\n",
    "             'R2 FV': r2_fv,\n",
    "             'RAAE FV': raae_fv,\n",
    "             'RMAE FV': rmae_fv,         \n",
    "             'STD FV PRED': std_fv,   \n",
    "             'MEAN FV PRED': mean_fv\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.590Z"
    }
   },
   "outputs": [],
   "source": [
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def generate_random_x_values(size, x_max, x_min, x_step, numnber_of_variables):\n",
    "    x_values_list = []\n",
    "    \n",
    "    for j in range(size):\n",
    "        values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))\n",
    "        while arreq_in_list(values, x_values_list):\n",
    "                values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))         \n",
    "        x_values_list.append(values)\n",
    "    \n",
    "    return np.array(x_values_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T11:20:25.146804Z",
     "start_time": "2020-11-27T11:20:23.569840Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.595Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    return weight_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   6 out of  21 | elapsed:    7.2s remaining:   17.9s\n",
      "[Parallel(n_jobs=20)]: Done  14 out of  21 | elapsed:    8.6s remaining:    4.3s\n",
      "[Parallel(n_jobs=20)]: Done  21 out of  21 | elapsed:   10.0s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if multi_epoch_analysis:  \n",
    "    weight_data_list = []\n",
    "    \n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    weight_data_list = parallel(delayed(load_data)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    weight_data = weight_data_list[-1]\n",
    "else:\n",
    "\n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "                \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031f699e2468427c90cf1776c8718adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if multi_epoch_analysis == False:\n",
    "    path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    lambda_train_fv_with_lambda_input = pd.read_csv(path_lambda_train_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "    lambda_valid_fv_with_lambda_input = pd.read_csv(path_lambda_valid_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "    lambda_test_fv_with_lambda_input = pd.read_csv(path_lambda_test_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "    \n",
    "    lambda_train_fv = lambda_train_fv_with_lambda_input[lambda_train_fv_with_lambda_input.columns[n::n+1]]\n",
    "    lambda_valid_fv = lambda_valid_fv_with_lambda_input[lambda_valid_fv_with_lambda_input.columns[n::n+1]]\n",
    "    lambda_test_fv = lambda_test_fv_with_lambda_input[lambda_test_fv_with_lambda_input.columns[n::n+1]]\n",
    "    \n",
    "    lambda_train_input = lambda_train_fv_with_lambda_input.drop(lambda_train_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input.shape[0], int((lambda_train_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    lambda_valid_input = lambda_valid_fv_with_lambda_input.drop(lambda_valid_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input.shape[0], int((lambda_valid_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    lambda_test_input = lambda_test_fv_with_lambda_input.drop(lambda_test_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input.shape[0], int((lambda_test_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "\n",
    "else:\n",
    "    lambda_train_fv_list = []\n",
    "    lambda_valid_fv_list = []\n",
    "    lambda_test_fv_list = []\n",
    "\n",
    "    for i in tqdm(epochs_save_range_lambda):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "        path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "        path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "        path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        lambda_train_fv_with_lambda_input = pd.read_csv(path_lambda_train_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "        lambda_valid_fv_with_lambda_input = pd.read_csv(path_lambda_valid_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "        lambda_test_fv_with_lambda_input = pd.read_csv(path_lambda_test_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1)\n",
    "\n",
    "        lambda_train_fv = lambda_train_fv_with_lambda_input[lambda_train_fv_with_lambda_input.columns[n::n+1]]\n",
    "        lambda_valid_fv = lambda_valid_fv_with_lambda_input[lambda_valid_fv_with_lambda_input.columns[n::n+1]]\n",
    "        lambda_test_fv = lambda_test_fv_with_lambda_input[lambda_test_fv_with_lambda_input.columns[n::n+1]]\n",
    "\n",
    "        if i == 0:\n",
    "            lambda_train_input = lambda_train_fv_with_lambda_input.drop(lambda_train_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input.shape[0], int((lambda_train_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "            lambda_valid_input = lambda_valid_fv_with_lambda_input.drop(lambda_valid_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input.shape[0], int((lambda_valid_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "            lambda_test_input = lambda_test_fv_with_lambda_input.drop(lambda_test_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input.shape[0], int((lambda_test_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "\n",
    "        lambda_train_fv_list.append(lambda_train_fv)\n",
    "        lambda_valid_fv_list.append(lambda_valid_fv)\n",
    "        lambda_test_fv_list.append(lambda_test_fv)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FV_1</th>\n",
       "      <th>FV_2</th>\n",
       "      <th>FV_3</th>\n",
       "      <th>FV_4</th>\n",
       "      <th>FV_5</th>\n",
       "      <th>FV_6</th>\n",
       "      <th>FV_7</th>\n",
       "      <th>FV_8</th>\n",
       "      <th>FV_9</th>\n",
       "      <th>FV_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FV_553</th>\n",
       "      <th>FV_554</th>\n",
       "      <th>FV_555</th>\n",
       "      <th>FV_556</th>\n",
       "      <th>FV_557</th>\n",
       "      <th>FV_558</th>\n",
       "      <th>FV_559</th>\n",
       "      <th>FV_560</th>\n",
       "      <th>FV_561</th>\n",
       "      <th>FV_562</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.010</td>\n",
       "      <td>-20.718</td>\n",
       "      <td>3.605</td>\n",
       "      <td>-26.109</td>\n",
       "      <td>-3.041</td>\n",
       "      <td>19.062</td>\n",
       "      <td>-15.976</td>\n",
       "      <td>9.950</td>\n",
       "      <td>-13.630</td>\n",
       "      <td>-22.753</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.526</td>\n",
       "      <td>-15.202</td>\n",
       "      <td>1.169</td>\n",
       "      <td>-16.629</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>16.116</td>\n",
       "      <td>14.976</td>\n",
       "      <td>11.760</td>\n",
       "      <td>-15.216</td>\n",
       "      <td>1.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.656</td>\n",
       "      <td>-9.384</td>\n",
       "      <td>18.926</td>\n",
       "      <td>-6.291</td>\n",
       "      <td>-9.114</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-3.660</td>\n",
       "      <td>-10.399</td>\n",
       "      <td>7.295</td>\n",
       "      <td>6.911</td>\n",
       "      <td>...</td>\n",
       "      <td>14.812</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>1.294</td>\n",
       "      <td>-2.015</td>\n",
       "      <td>-7.005</td>\n",
       "      <td>20.969</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-2.520</td>\n",
       "      <td>3.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.213</td>\n",
       "      <td>-9.835</td>\n",
       "      <td>-7.436</td>\n",
       "      <td>-7.448</td>\n",
       "      <td>-14.421</td>\n",
       "      <td>-17.642</td>\n",
       "      <td>-10.262</td>\n",
       "      <td>-9.901</td>\n",
       "      <td>-17.747</td>\n",
       "      <td>-9.046</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.866</td>\n",
       "      <td>11.381</td>\n",
       "      <td>-5.145</td>\n",
       "      <td>17.385</td>\n",
       "      <td>-2.739</td>\n",
       "      <td>-17.517</td>\n",
       "      <td>-13.108</td>\n",
       "      <td>-2.332</td>\n",
       "      <td>1.055</td>\n",
       "      <td>-4.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.762</td>\n",
       "      <td>-10.718</td>\n",
       "      <td>8.977</td>\n",
       "      <td>-13.743</td>\n",
       "      <td>-10.800</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-11.094</td>\n",
       "      <td>-7.907</td>\n",
       "      <td>-8.935</td>\n",
       "      <td>17.057</td>\n",
       "      <td>...</td>\n",
       "      <td>32.858</td>\n",
       "      <td>7.353</td>\n",
       "      <td>-12.187</td>\n",
       "      <td>-9.882</td>\n",
       "      <td>8.750</td>\n",
       "      <td>-1.438</td>\n",
       "      <td>-4.458</td>\n",
       "      <td>-10.305</td>\n",
       "      <td>21.515</td>\n",
       "      <td>-0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.205</td>\n",
       "      <td>-1.930</td>\n",
       "      <td>3.679</td>\n",
       "      <td>7.892</td>\n",
       "      <td>-2.534</td>\n",
       "      <td>-3.439</td>\n",
       "      <td>1.079</td>\n",
       "      <td>8.915</td>\n",
       "      <td>-6.922</td>\n",
       "      <td>-12.812</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-14.948</td>\n",
       "      <td>2.923</td>\n",
       "      <td>-8.961</td>\n",
       "      <td>-7.088</td>\n",
       "      <td>-2.568</td>\n",
       "      <td>-3.648</td>\n",
       "      <td>4.801</td>\n",
       "      <td>-11.915</td>\n",
       "      <td>-9.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FV_1    FV_2   FV_3    FV_4    FV_5    FV_6    FV_7    FV_8    FV_9  \\\n",
       "0  4.010 -20.718  3.605 -26.109  -3.041  19.062 -15.976   9.950 -13.630   \n",
       "1 -5.656  -9.384 18.926  -6.291  -9.114   0.483  -3.660 -10.399   7.295   \n",
       "2  8.213  -9.835 -7.436  -7.448 -14.421 -17.642 -10.262  -9.901 -17.747   \n",
       "3 -9.762 -10.718  8.977 -13.743 -10.800   0.398 -11.094  -7.907  -8.935   \n",
       "4 11.205  -1.930  3.679   7.892  -2.534  -3.439   1.079   8.915  -6.922   \n",
       "\n",
       "    FV_10  ...  FV_553  FV_554  FV_555  FV_556  FV_557  FV_558  FV_559  \\\n",
       "0 -22.753  ... -14.526 -15.202   1.169 -16.629  -0.220  16.116  14.976   \n",
       "1   6.911  ...  14.812  -0.893  -0.555   1.294  -2.015  -7.005  20.969   \n",
       "2  -9.046  ...  -4.866  11.381  -5.145  17.385  -2.739 -17.517 -13.108   \n",
       "3  17.057  ...  32.858   7.353 -12.187  -9.882   8.750  -1.438  -4.458   \n",
       "4 -12.812  ...  -1.180 -14.948   2.923  -8.961  -7.088  -2.568  -3.648   \n",
       "\n",
       "   FV_560  FV_561  FV_562  \n",
       "0  11.760 -15.216   1.435  \n",
       "1  -0.955  -2.520   3.605  \n",
       "2  -2.332   1.055  -4.495  \n",
       "3 -10.305  21.515  -0.109  \n",
       "4   4.801 -11.915  -9.008  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_train_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.03, -0.04, -0.93,  0.62],\n",
       "        [-0.92, -0.17,  0.59,  0.83],\n",
       "        [ 0.56, -0.7 ,  0.14, -0.45],\n",
       "        ...,\n",
       "        [ 0.94,  0.47,  0.22,  0.12],\n",
       "        [-0.85,  0.99,  0.84,  0.65],\n",
       "        [ 0.12, -0.87, -0.79,  0.64]],\n",
       "\n",
       "       [[-0.53,  0.22, -0.89, -0.3 ],\n",
       "        [-0.48,  0.39, -1.  ,  0.24],\n",
       "        [ 0.75,  0.42,  0.16,  0.39],\n",
       "        ...,\n",
       "        [ 0.11, -0.55,  0.61, -0.26],\n",
       "        [ 0.46, -0.11, -0.75,  0.6 ],\n",
       "        [-0.51,  0.64,  0.2 ,  0.45]],\n",
       "\n",
       "       [[ 0.86, -0.64, -0.03, -0.14],\n",
       "        [ 0.96,  0.61, -0.23,  0.15],\n",
       "        [-0.61, -0.4 ,  0.31,  0.65],\n",
       "        ...,\n",
       "        [ 0.91,  0.04,  0.29, -0.93],\n",
       "        [-0.14, -0.59,  0.06, -0.8 ],\n",
       "        [ 0.94,  0.49,  0.36,  0.37]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.36,  0.2 ,  0.97,  0.84],\n",
       "        [-0.11, -0.51,  0.09, -0.07],\n",
       "        [ 0.2 , -0.92,  0.42, -0.6 ],\n",
       "        ...,\n",
       "        [ 0.65,  0.58, -0.74,  0.47],\n",
       "        [ 0.33, -0.38, -0.35, -0.86],\n",
       "        [ 0.24, -0.2 , -0.99,  0.18]],\n",
       "\n",
       "       [[ 0.29, -0.89,  0.02, -0.66],\n",
       "        [ 0.28, -0.22, -0.03,  0.31],\n",
       "        [ 0.38, -0.25, -1.  , -0.62],\n",
       "        ...,\n",
       "        [ 0.74, -0.24, -0.03, -0.79],\n",
       "        [ 0.92,  0.53, -0.07, -0.69],\n",
       "        [-0.5 ,  0.99, -0.41, -0.25]],\n",
       "\n",
       "       [[-0.21, -0.17,  0.29,  0.36],\n",
       "        [ 0.21,  0.42,  0.51,  0.25],\n",
       "        [ 0.75,  0.53, -0.35,  0.75],\n",
       "        ...,\n",
       "        [ 0.28,  0.36,  0.5 ,  0.76],\n",
       "        [ 0.39,  0.66, -0.28, -0.48],\n",
       "        [-0.34, -0.18, -0.56, -0.15]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1146</th>\n",
       "      <th>1147</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-4.900</td>\n",
       "      <td>8.700</td>\n",
       "      <td>3.200</td>\n",
       "      <td>4.100</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-7.000</td>\n",
       "      <td>-6.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>0.551</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.631</td>\n",
       "      <td>-0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.500</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>5.300</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>7.100</td>\n",
       "      <td>5.500</td>\n",
       "      <td>2.200</td>\n",
       "      <td>-7.600</td>\n",
       "      <td>2.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.500</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-3.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>8.600</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>9.100</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.635</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.632</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.400</td>\n",
       "      <td>-3.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-8.200</td>\n",
       "      <td>3.900</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>5.100</td>\n",
       "      <td>7.100</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>3.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.900</td>\n",
       "      <td>4.300</td>\n",
       "      <td>-3.600</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.200</td>\n",
       "      <td>8.500</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>6.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.335</td>\n",
       "      <td>-0.377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1      2      3     4      5      6      7      8      9     ...  \\\n",
       "0  2.600  0.600 -4.900  8.700 3.200  4.100 -7.700 -9.900 -7.000 -6.900  ...   \n",
       "1 -0.500 -2.600  5.300  7.000 6.000  7.100  5.500  2.200 -7.600  2.800  ...   \n",
       "2 -6.500  9.300 -3.300 -8.800 8.600  1.400 -5.700  9.100 -9.200 -0.900  ...   \n",
       "3 -0.400 -3.900 -4.200 -8.200 3.900 -9.900  5.100  7.100 -4.400  3.000  ...   \n",
       "4 -0.900  4.300 -3.600  7.000 1.000  6.000  5.200  8.500 -3.800  6.400  ...   \n",
       "\n",
       "    1146  1147   1148  1149   1150   1151   1152  1153  1154   1155  \n",
       "0 -0.379 0.638 -0.696 0.551 -0.795 -0.002 -0.776 0.143 0.631 -0.465  \n",
       "1  0.429 0.770 -0.260 0.529 -0.423  0.507 -0.275 0.653 0.504  0.202  \n",
       "2 -0.270 0.439 -0.635 0.123 -0.632  0.560 -0.318 0.106 0.399 -0.753  \n",
       "3  0.143 0.201 -0.235 0.083 -0.198  0.139 -0.376 0.528 0.352 -0.131  \n",
       "4  0.416 0.395 -0.190 0.055 -0.157  0.520 -0.247 0.063 0.335 -0.377  \n",
       "\n",
       "[5 rows x 1156 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FV_1</th>\n",
       "      <th>FV_2</th>\n",
       "      <th>FV_3</th>\n",
       "      <th>FV_4</th>\n",
       "      <th>FV_5</th>\n",
       "      <th>FV_6</th>\n",
       "      <th>FV_7</th>\n",
       "      <th>FV_8</th>\n",
       "      <th>FV_9</th>\n",
       "      <th>FV_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FV_241</th>\n",
       "      <th>FV_242</th>\n",
       "      <th>FV_243</th>\n",
       "      <th>FV_244</th>\n",
       "      <th>FV_245</th>\n",
       "      <th>FV_246</th>\n",
       "      <th>FV_247</th>\n",
       "      <th>FV_248</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.312</td>\n",
       "      <td>-1.403</td>\n",
       "      <td>-11.285</td>\n",
       "      <td>-2.955</td>\n",
       "      <td>-5.382</td>\n",
       "      <td>12.354</td>\n",
       "      <td>8.547</td>\n",
       "      <td>-22.219</td>\n",
       "      <td>2.398</td>\n",
       "      <td>-19.916</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.188</td>\n",
       "      <td>-21.404</td>\n",
       "      <td>-15.419</td>\n",
       "      <td>7.503</td>\n",
       "      <td>1.854</td>\n",
       "      <td>-2.010</td>\n",
       "      <td>3.063</td>\n",
       "      <td>11.027</td>\n",
       "      <td>-14.065</td>\n",
       "      <td>13.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.066</td>\n",
       "      <td>10.303</td>\n",
       "      <td>21.278</td>\n",
       "      <td>7.156</td>\n",
       "      <td>1.987</td>\n",
       "      <td>1.398</td>\n",
       "      <td>2.308</td>\n",
       "      <td>23.343</td>\n",
       "      <td>7.409</td>\n",
       "      <td>16.175</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.553</td>\n",
       "      <td>-3.436</td>\n",
       "      <td>20.195</td>\n",
       "      <td>-7.659</td>\n",
       "      <td>6.948</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-5.156</td>\n",
       "      <td>1.648</td>\n",
       "      <td>19.422</td>\n",
       "      <td>26.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-29.427</td>\n",
       "      <td>-8.638</td>\n",
       "      <td>-8.046</td>\n",
       "      <td>-9.191</td>\n",
       "      <td>-8.297</td>\n",
       "      <td>-16.062</td>\n",
       "      <td>-22.782</td>\n",
       "      <td>-12.213</td>\n",
       "      <td>-1.717</td>\n",
       "      <td>-8.311</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.047</td>\n",
       "      <td>4.378</td>\n",
       "      <td>-6.187</td>\n",
       "      <td>-11.045</td>\n",
       "      <td>-11.538</td>\n",
       "      <td>4.907</td>\n",
       "      <td>-6.183</td>\n",
       "      <td>-11.311</td>\n",
       "      <td>-1.912</td>\n",
       "      <td>10.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.527</td>\n",
       "      <td>1.404</td>\n",
       "      <td>9.842</td>\n",
       "      <td>8.374</td>\n",
       "      <td>-9.375</td>\n",
       "      <td>3.295</td>\n",
       "      <td>9.987</td>\n",
       "      <td>-7.101</td>\n",
       "      <td>-6.340</td>\n",
       "      <td>-6.972</td>\n",
       "      <td>...</td>\n",
       "      <td>1.244</td>\n",
       "      <td>-5.114</td>\n",
       "      <td>22.690</td>\n",
       "      <td>19.175</td>\n",
       "      <td>-6.837</td>\n",
       "      <td>1.020</td>\n",
       "      <td>-3.810</td>\n",
       "      <td>-13.406</td>\n",
       "      <td>3.706</td>\n",
       "      <td>-1.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.948</td>\n",
       "      <td>-4.710</td>\n",
       "      <td>10.571</td>\n",
       "      <td>-1.245</td>\n",
       "      <td>7.456</td>\n",
       "      <td>1.572</td>\n",
       "      <td>3.961</td>\n",
       "      <td>-5.325</td>\n",
       "      <td>1.545</td>\n",
       "      <td>-3.583</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.494</td>\n",
       "      <td>-6.858</td>\n",
       "      <td>-2.255</td>\n",
       "      <td>6.051</td>\n",
       "      <td>-11.454</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>-1.860</td>\n",
       "      <td>-7.472</td>\n",
       "      <td>-6.902</td>\n",
       "      <td>9.203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FV_1   FV_2    FV_3   FV_4   FV_5    FV_6    FV_7    FV_8   FV_9   FV_10  \\\n",
       "0   0.312 -1.403 -11.285 -2.955 -5.382  12.354   8.547 -22.219  2.398 -19.916   \n",
       "1   2.066 10.303  21.278  7.156  1.987   1.398   2.308  23.343  7.409  16.175   \n",
       "2 -29.427 -8.638  -8.046 -9.191 -8.297 -16.062 -22.782 -12.213 -1.717  -8.311   \n",
       "3  -3.527  1.404   9.842  8.374 -9.375   3.295   9.987  -7.101 -6.340  -6.972   \n",
       "4  -4.948 -4.710  10.571 -1.245  7.456   1.572   3.961  -5.325  1.545  -3.583   \n",
       "\n",
       "   ...  FV_241  FV_242  FV_243  FV_244  FV_245  FV_246  FV_247  FV_248  \\\n",
       "0  ... -19.188 -21.404 -15.419   7.503   1.854  -2.010   3.063  11.027   \n",
       "1  ...  -2.553  -3.436  20.195  -7.659   6.948  -0.140  -5.156   1.648   \n",
       "2  ... -31.047   4.378  -6.187 -11.045 -11.538   4.907  -6.183 -11.311   \n",
       "3  ...   1.244  -5.114  22.690  19.175  -6.837   1.020  -3.810 -13.406   \n",
       "4  ... -11.494  -6.858  -2.255   6.051 -11.454  -0.753  -1.860  -7.472   \n",
       "\n",
       "   FV_249  FV_250  \n",
       "0 -14.065  13.010  \n",
       "1  19.422  26.665  \n",
       "2  -1.912  10.237  \n",
       "3   3.706  -1.082  \n",
       "4  -6.902   9.203  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_test_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FV_1</th>\n",
       "      <th>FV_2</th>\n",
       "      <th>FV_3</th>\n",
       "      <th>FV_4</th>\n",
       "      <th>FV_5</th>\n",
       "      <th>FV_6</th>\n",
       "      <th>FV_7</th>\n",
       "      <th>FV_8</th>\n",
       "      <th>FV_9</th>\n",
       "      <th>FV_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FV_179</th>\n",
       "      <th>FV_180</th>\n",
       "      <th>FV_181</th>\n",
       "      <th>FV_182</th>\n",
       "      <th>FV_183</th>\n",
       "      <th>FV_184</th>\n",
       "      <th>FV_185</th>\n",
       "      <th>FV_186</th>\n",
       "      <th>FV_187</th>\n",
       "      <th>FV_188</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.423</td>\n",
       "      <td>-29.479</td>\n",
       "      <td>-10.103</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>13.992</td>\n",
       "      <td>-17.252</td>\n",
       "      <td>-7.308</td>\n",
       "      <td>-15.575</td>\n",
       "      <td>-19.016</td>\n",
       "      <td>12.607</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.654</td>\n",
       "      <td>-17.112</td>\n",
       "      <td>5.844</td>\n",
       "      <td>12.375</td>\n",
       "      <td>2.850</td>\n",
       "      <td>12.249</td>\n",
       "      <td>-9.283</td>\n",
       "      <td>-13.222</td>\n",
       "      <td>-16.577</td>\n",
       "      <td>-19.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.307</td>\n",
       "      <td>-2.064</td>\n",
       "      <td>8.493</td>\n",
       "      <td>23.725</td>\n",
       "      <td>2.598</td>\n",
       "      <td>-5.554</td>\n",
       "      <td>22.810</td>\n",
       "      <td>1.625</td>\n",
       "      <td>11.758</td>\n",
       "      <td>14.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.225</td>\n",
       "      <td>6.115</td>\n",
       "      <td>2.751</td>\n",
       "      <td>5.284</td>\n",
       "      <td>1.706</td>\n",
       "      <td>10.944</td>\n",
       "      <td>14.798</td>\n",
       "      <td>28.712</td>\n",
       "      <td>-3.451</td>\n",
       "      <td>-7.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.821</td>\n",
       "      <td>8.119</td>\n",
       "      <td>-11.548</td>\n",
       "      <td>-10.384</td>\n",
       "      <td>-11.432</td>\n",
       "      <td>-15.929</td>\n",
       "      <td>-19.657</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>9.695</td>\n",
       "      <td>-10.683</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.436</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-11.221</td>\n",
       "      <td>-2.716</td>\n",
       "      <td>-8.613</td>\n",
       "      <td>-8.111</td>\n",
       "      <td>-8.999</td>\n",
       "      <td>-4.765</td>\n",
       "      <td>17.039</td>\n",
       "      <td>-6.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-11.683</td>\n",
       "      <td>12.666</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-5.710</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-6.506</td>\n",
       "      <td>18.404</td>\n",
       "      <td>-8.574</td>\n",
       "      <td>11.517</td>\n",
       "      <td>-7.825</td>\n",
       "      <td>...</td>\n",
       "      <td>3.397</td>\n",
       "      <td>3.187</td>\n",
       "      <td>12.490</td>\n",
       "      <td>-5.419</td>\n",
       "      <td>5.231</td>\n",
       "      <td>12.319</td>\n",
       "      <td>12.889</td>\n",
       "      <td>15.733</td>\n",
       "      <td>-2.802</td>\n",
       "      <td>-11.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.859</td>\n",
       "      <td>-4.771</td>\n",
       "      <td>-6.228</td>\n",
       "      <td>-2.706</td>\n",
       "      <td>-1.562</td>\n",
       "      <td>5.004</td>\n",
       "      <td>-2.494</td>\n",
       "      <td>2.376</td>\n",
       "      <td>-8.323</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.891</td>\n",
       "      <td>0.546</td>\n",
       "      <td>-3.210</td>\n",
       "      <td>-11.186</td>\n",
       "      <td>-6.632</td>\n",
       "      <td>-3.888</td>\n",
       "      <td>8.873</td>\n",
       "      <td>-2.157</td>\n",
       "      <td>6.177</td>\n",
       "      <td>-5.099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FV_1    FV_2    FV_3    FV_4    FV_5    FV_6    FV_7    FV_8    FV_9  \\\n",
       "0  -4.423 -29.479 -10.103  -2.624  13.992 -17.252  -7.308 -15.575 -19.016   \n",
       "1   6.307  -2.064   8.493  23.725   2.598  -5.554  22.810   1.625  11.758   \n",
       "2  -8.821   8.119 -11.548 -10.384 -11.432 -15.929 -19.657  -2.834   9.695   \n",
       "3 -11.683  12.666  -0.953  -5.710   0.127  -6.506  18.404  -8.574  11.517   \n",
       "4  -7.859  -4.771  -6.228  -2.706  -1.562   5.004  -2.494   2.376  -8.323   \n",
       "\n",
       "    FV_10  ...  FV_179  FV_180  FV_181  FV_182  FV_183  FV_184  FV_185  \\\n",
       "0  12.607  ...  -7.654 -17.112   5.844  12.375   2.850  12.249  -9.283   \n",
       "1  14.336  ...  -2.225   6.115   2.751   5.284   1.706  10.944  14.798   \n",
       "2 -10.683  ...  -5.436  -0.155 -11.221  -2.716  -8.613  -8.111  -8.999   \n",
       "3  -7.825  ...   3.397   3.187  12.490  -5.419   5.231  12.319  12.889   \n",
       "4  -0.313  ...  -8.891   0.546  -3.210 -11.186  -6.632  -3.888   8.873   \n",
       "\n",
       "   FV_186  FV_187  FV_188  \n",
       "0 -13.222 -16.577 -19.120  \n",
       "1  28.712  -3.451  -7.348  \n",
       "2  -4.765  17.039  -6.177  \n",
       "3  15.733  -2.802 -11.310  \n",
       "4  -2.157   6.177  -5.099  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_valid_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1146</th>\n",
       "      <th>1147</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.456</td>\n",
       "      <td>-0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.753</td>\n",
       "      <td>5.723</td>\n",
       "      <td>5.832</td>\n",
       "      <td>5.809</td>\n",
       "      <td>5.787</td>\n",
       "      <td>5.782</td>\n",
       "      <td>5.760</td>\n",
       "      <td>5.761</td>\n",
       "      <td>5.811</td>\n",
       "      <td>5.758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-1.826</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-2.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.900</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-4.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.237</td>\n",
       "      <td>-0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.435</td>\n",
       "      <td>-0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>4.800</td>\n",
       "      <td>5.100</td>\n",
       "      <td>4.800</td>\n",
       "      <td>4.900</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5.100</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.417</td>\n",
       "      <td>1.428</td>\n",
       "      <td>1.444</td>\n",
       "      <td>2.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 1156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.058    -0.075    -0.071    -0.213    -0.094    -0.057     0.040   \n",
       "std       5.753     5.723     5.832     5.809     5.787     5.782     5.760   \n",
       "min     -10.000   -10.000   -10.000   -10.000   -10.000   -10.000   -10.000   \n",
       "25%      -4.900    -5.000    -5.200    -5.300    -5.200    -5.100    -5.000   \n",
       "50%       0.100    -0.100    -0.100    -0.400    -0.100     0.000     0.100   \n",
       "75%       5.000     4.800     5.100     4.800     4.900     4.900     5.100   \n",
       "max       9.900     9.900     9.900     9.900     9.900     9.900     9.900   \n",
       "\n",
       "           7         8         9     ...      1146      1147      1148  \\\n",
       "count 10000.000 10000.000 10000.000  ... 10000.000 10000.000 10000.000   \n",
       "mean     -0.119    -0.036     0.034  ...     0.010     0.318    -0.291   \n",
       "std       5.761     5.811     5.758  ...     0.315     0.174     0.179   \n",
       "min     -10.000   -10.000   -10.000  ...    -0.904     0.030    -0.902   \n",
       "25%      -5.100    -5.200    -4.900  ...    -0.247     0.172    -0.422   \n",
       "50%      -0.100     0.100     0.100  ...     0.013     0.296    -0.274   \n",
       "75%       4.900     5.000     5.100  ...     0.265     0.439    -0.138   \n",
       "max       9.900     9.900     9.900  ...     0.827     0.922     0.020   \n",
       "\n",
       "           1149      1150      1151      1152      1153      1154      1155  \n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000  \n",
       "mean      0.250    -0.248     0.121    -0.389     0.443     0.456    -0.192  \n",
       "std       0.171     0.264     0.286     0.369     0.259     0.256     0.873  \n",
       "min      -0.105    -1.214    -0.646    -1.826    -0.014    -0.008    -2.389  \n",
       "25%       0.102    -0.438    -0.112    -0.660     0.221     0.237    -0.837  \n",
       "50%       0.241    -0.240     0.128    -0.372     0.417     0.435    -0.181  \n",
       "75%       0.377    -0.034     0.336    -0.087     0.627     0.641     0.429  \n",
       "max       0.797     0.358     0.948     0.417     1.428     1.444     2.055  \n",
       "\n",
       "[8 rows x 1156 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.629Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddca9f0d52c949d8815ffadea69d8b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#generate train, test and validation data for training\n",
    "if multi_epoch_analysis:    \n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    X_valid_list = []\n",
    "    y_valid_list = []\n",
    "    \n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    \n",
    "    lambda_test_fv_valid_split_list = []\n",
    "    lambda_test_fv_test_split_list = []\n",
    "    lambda_test_fv_train_split_list = []\n",
    "    \n",
    "    lambda_valid_fv_valid_split_list = []\n",
    "    lambda_valid_fv_test_split_list = []\n",
    "    lambda_valid_fv_train_split_list = []\n",
    "    \n",
    "    lambda_train_fv_valid_split_list = []\n",
    "    lambda_train_fv_test_split_list = []\n",
    "    lambda_train_fv_train_split_list = []\n",
    "    \n",
    "    for weight_data, lambda_train_fv, lambda_valid_fv, lambda_test_fv in tqdm(zip(weight_data_list, lambda_train_fv_list, lambda_valid_fv_list, lambda_test_fv_list), total=len(weight_data_list)): \n",
    "        \n",
    "        if psutil.virtual_memory().percent > 80:\n",
    "            raise SystemExit(\"Out of RAM!\")\n",
    "        \n",
    "        X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3)], axis=1)\n",
    "        y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3)]].astype(float)\n",
    "        \n",
    "        lambda_train_fv = lambda_train_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv = lambda_valid_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv = lambda_test_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "        y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "        y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "        y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)      \n",
    "        \n",
    "        #y_data_polynomial = y_data_polynomial_true\n",
    "        #y_data_polynomial_pred_lstsq = y_data_polynomial_lstsq_pred\n",
    "        #y_data_polynomial_true_lstsq = y_data_polynomial_lstsq_true\n",
    "        \n",
    "        if evaluate_with_real_function:\n",
    "            y_data = y_data_polynomial_true\n",
    "        else:\n",
    "            y_data = y_data_polynomial_lstsq_pred  \n",
    "                                         \n",
    "        X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    \n",
    "        X_train_list.append(X_train)\n",
    "        y_train_list.append(y_train)\n",
    "\n",
    "        X_valid_list.append(X_valid)\n",
    "        y_valid_list.append(y_valid)\n",
    "\n",
    "        X_test_list.append(X_test)\n",
    "        y_test_list.append(y_test)   \n",
    "        \n",
    "\n",
    "        lambda_train_fv_with_valid_split, lambda_train_fv_test_split = train_test_split(lambda_train_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_train_fv_train_split, lambda_train_fv_valid_split = train_test_split(lambda_train_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_train_fv_valid_split_list.append(lambda_train_fv_valid_split)\n",
    "        lambda_train_fv_test_split_list.append(lambda_train_fv_test_split)\n",
    "        lambda_train_fv_train_split_list.append(lambda_train_fv_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_valid_fv_with_valid_split, lambda_valid_fv_test_split = train_test_split(lambda_valid_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv_train_split, lambda_valid_fv_valid_split = train_test_split(lambda_valid_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_valid_fv_valid_split_list.append(lambda_valid_fv_valid_split)\n",
    "        lambda_valid_fv_test_split_list.append(lambda_valid_fv_test_split)\n",
    "        lambda_valid_fv_train_split_list.append(lambda_valid_fv_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_test_fv_with_valid_split, lambda_test_fv_test_split = train_test_split(lambda_test_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv_train_split, lambda_test_fv_valid_split = train_test_split(lambda_test_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_test_fv_valid_split_list.append(lambda_test_fv_valid_split)\n",
    "        lambda_test_fv_test_split_list.append(lambda_test_fv_test_split)\n",
    "        lambda_test_fv_train_split_list.append(lambda_test_fv_train_split)   \n",
    "            \n",
    "else:    \n",
    "    lambda_test_fv_valid_split_list = []\n",
    "    lambda_test_fv_test_split_list = []\n",
    "    lambda_test_fv_train_split_list = []\n",
    "    \n",
    "    lambda_valid_fv_valid_split_list = []\n",
    "    lambda_valid_fv_test_split_list = []\n",
    "    lambda_valid_fv_train_split_list = []\n",
    "    \n",
    "    lambda_train_fv_valid_split_list = []\n",
    "    lambda_train_fv_test_split_list = []\n",
    "    lambda_train_fv_train_split_list = []\n",
    "    \n",
    "    X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3)], axis=1)\n",
    "    y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3)]].astype(float)\n",
    "    \n",
    "    y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "    y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "    y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)\n",
    "    \n",
    "    lambda_train_fv = lambda_train_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_valid_fv = lambda_valid_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_test_fv = lambda_test_fv.sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        y_data = y_data_polynomial_true\n",
    "    else:\n",
    "        y_data = y_data_polynomial_lstsq_pred                                     \n",
    "    \n",
    "        lambda_train_fv_with_valid_split, lambda_train_fv_test_split = train_test_split(lambda_train_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_train_fv_train_split, lambda_train_fv_valid_split = train_test_split(lambda_train_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_valid_fv_with_valid_split, lambda_valid_fv_test_split = train_test_split(lambda_valid_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv_train_split, lambda_valid_fv_valid_split = train_test_split(lambda_valid_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "        lambda_test_fv_with_valid_split, lambda_test_fv_test_split = train_test_split(lambda_test_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv_train_split, lambda_test_fv_valid_split = train_test_split(lambda_test_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.633Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>...</th>\n",
       "      <th>1146</th>\n",
       "      <th>1147</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.402</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.627</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.346</td>\n",
       "      <td>1.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>0.259</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.545</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>0.491</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-1.127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       105    106    107    108    109    110   111   112    113    114   ...  \\\n",
       "6374  0.209 -0.010  0.025  0.154 -0.189  0.167 0.035 0.152 -0.519 -0.076  ...   \n",
       "2657 -0.046 -0.000 -0.001  0.103 -0.251 -0.061 0.056 0.110 -0.088 -0.244  ...   \n",
       "3076 -0.034 -0.093  0.028 -0.113 -0.512  0.109 0.679 0.512 -0.102 -0.309  ...   \n",
       "8178  0.259  0.005  0.096 -0.034 -0.134  0.345 0.327 0.292 -0.295 -0.125  ...   \n",
       "7311  0.131 -0.032 -0.064  0.042 -0.492  0.006 0.086 0.130  0.067 -0.149  ...   \n",
       "\n",
       "      1146  1147   1148  1149   1150  1151   1152  1153  1154   1155  \n",
       "6374 0.276 0.402 -0.571 0.271 -0.688 0.018 -0.889 0.382 0.098 -0.130  \n",
       "2657 0.363 0.126 -0.062 0.214 -0.172 0.051 -0.360 0.924 0.408 -0.946  \n",
       "3076 0.608 0.444 -0.131 0.191 -0.162 0.627 -0.304 1.017 0.346  1.528  \n",
       "8178 0.126 0.545 -0.629 0.491 -0.608 0.347 -0.203 0.382 0.345  0.642  \n",
       "7311 0.275 0.169 -0.158 0.006 -0.380 0.167 -0.640 0.689 0.120 -1.127  \n",
       "\n",
       "[5 rows x 1051 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = X_train_list[-1].head()\n",
    "else:\n",
    "    print_head = X_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>1.301</td>\n",
       "      <td>-7.944</td>\n",
       "      <td>-1.383</td>\n",
       "      <td>0.799</td>\n",
       "      <td>1.811</td>\n",
       "      <td>2.550</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-3.464</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>-3.478</td>\n",
       "      <td>0.517</td>\n",
       "      <td>1.886</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-1.465</td>\n",
       "      <td>2.865</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-4.025</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>1.726</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>9.112</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.763</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>8.338</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>1.359</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-7.273</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.445</td>\n",
       "      <td>5.146</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>4.142</td>\n",
       "      <td>-4.195</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.592</td>\n",
       "      <td>1.561</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>6.792</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.223</td>\n",
       "      <td>-1.813</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-1.009</td>\n",
       "      <td>-0.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>-10.153</td>\n",
       "      <td>-3.950</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>0.183</td>\n",
       "      <td>7.239</td>\n",
       "      <td>0.594</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581</td>\n",
       "      <td>-4.019</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>-0.506</td>\n",
       "      <td>1.182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          35     36     37     38     39     40     41     42     43     44  \\\n",
       "6374   1.301 -7.944 -1.383  0.799  1.811  2.550 -0.182 -0.317  0.459  0.020   \n",
       "2657  -3.478  0.517  1.886  0.093 -1.465  2.865 -0.281  0.419  0.099  0.007   \n",
       "3076   9.112 -0.136  0.763 -0.033  8.338 -0.051 -0.188  1.359  0.192 -0.213   \n",
       "8178   4.142 -4.195 -0.569  0.076  0.046  0.152 -0.592  1.561 -0.061  0.389   \n",
       "7311 -10.153 -3.950 -0.539  0.183  7.239  0.594 -0.291 -0.202  0.186 -0.077   \n",
       "\n",
       "      ...     60     61     62     63     64     65     66     67     68  \\\n",
       "6374  ... -0.222 -0.155  0.384 -0.243 -0.389 -3.464  0.220 -0.590 -0.004   \n",
       "2657  ...  0.086 -4.025 -0.023  0.326 -0.006  1.726 -0.010 -0.029  0.553   \n",
       "3076  ... -0.192 -7.273  0.178  0.865  0.445  5.146 -0.417 -0.818  0.068   \n",
       "8178  ... -0.435  6.792  0.332  0.100  0.223 -1.813 -0.401  0.568 -1.009   \n",
       "7311  ...  0.581 -4.019 -0.235  0.550  0.417 -0.334  0.522 -0.664 -0.506   \n",
       "\n",
       "         69  \n",
       "6374 -0.743  \n",
       "2657  0.260  \n",
       "3076 -0.548  \n",
       "8178 -0.258  \n",
       "7311  1.182  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = y_train_list[-1].head()\n",
    "else:\n",
    "    print_head = y_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.640Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_base_model():\n",
    "    base_model = Sequential()\n",
    "\n",
    "    base_model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=lambda_train_input[0].shape[1])) #1024\n",
    "\n",
    "    if dropout > 0:\n",
    "        base_model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        base_model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            base_model.add(Dropout(dropout))   \n",
    "\n",
    "    base_model.add(Dense(1))\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.643Z"
    },
    "code_folding": [
     94
    ]
   },
   "outputs": [],
   "source": [
    "def train_nn_and_pred(X_train, \n",
    "                      X_valid, \n",
    "                      X_test, \n",
    "                      y_train, \n",
    "                      y_valid, \n",
    "                      y_test,\n",
    "                      lambda_train_fv_valid_split, \n",
    "                      lambda_train_fv_test_split, \n",
    "                      lambda_train_fv_train_split, \n",
    "                      lambda_valid_fv_valid_split, \n",
    "                      lambda_valid_fv_test_split, \n",
    "                      lambda_valid_fv_train_split, \n",
    "                      lambda_test_fv_valid_split, \n",
    "                      lambda_test_fv_test_split, \n",
    "                      lambda_test_fv_train_split, \n",
    "                      callback_names=[], \n",
    "                      return_model=False):       \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(interpretation_network_layers[0], activation='relu', input_dim=X_train.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in interpretation_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(nCr(n+d, d))) \n",
    "    \n",
    "    #decide whether to use lambda preds for evaluation or polynomial from lstsq lambda preds\n",
    "    if not consider_labels_training and not evaluate_with_real_function:\n",
    "        if True: #implementation with direct lambda net prediction\n",
    "            base_model = generate_base_model()\n",
    "            random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "            #random_evaluation_dataset = lambda_train_input[0]\n",
    "            list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "            loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "            metrics = [mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model), mean_absolute_error_extended]\n",
    "            valid_data = None\n",
    "            y_train_model = np.hstack((y_train, X_train))\n",
    "        else: #old implementation with preloaded lambda preds\n",
    "            loss_function = mean_absolute_error_tf_fv_lambda_preds_appended    \n",
    "            metrics = [mean_absolute_error_tf_fv_lambda_preds_appended, mean_absolute_error_extended]\n",
    "            valid_data = None\n",
    "            y_train_model = np.hstack((y_train, lambda_train_fv_train_split))\n",
    "            \n",
    "    else:\n",
    "        loss_function = mean_absolute_error_tf_fv\n",
    "        metrics = ['mean_absolute_error']\n",
    "        valid_data = (X_valid, y_valid)\n",
    "        y_train_model = y_train\n",
    "      \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=metrics\n",
    "                 )\n",
    "\n",
    "    #Callbacks\n",
    "    callbacks = return_callbacks_from_string(callback_names)\n",
    "        \n",
    "    history = model.fit(X_train,\n",
    "              y_train_model,\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              validation_data=valid_data,\n",
    "              callbacks=callbacks,\n",
    "              verbose=0)\n",
    "\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    \n",
    "    polynomial_true_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_valid_input)\n",
    "    polynomial_pred_inet_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_valid_input)\n",
    "    \n",
    "    polynomial_true_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_valid_input)\n",
    "    polynomial_pred_inet_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_valid_input)\n",
    "\n",
    "    \n",
    "    polynomial_true_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_test_input)\n",
    "    polynomial_pred_inet_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_test_input)\n",
    "    \n",
    "    polynomial_true_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_test_input)\n",
    "    polynomial_pred_inet_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_test_input)\n",
    "    \n",
    "    \n",
    "    polynomial_test_fv = [polynomial_true_test_fv_valid_split, \n",
    "                            polynomial_pred_inet_test_fv_valid_split, \n",
    "                            polynomial_true_test_fv_test_split, \n",
    "                            polynomial_pred_inet_test_fv_test_split]\n",
    "    \n",
    "    polynomial_valid_fv = [polynomial_true_valid_fv_valid_split, \n",
    "                             polynomial_pred_inet_valid_fv_valid_split, \n",
    "                             polynomial_true_valid_fv_test_split, \n",
    "                             polynomial_pred_inet_valid_fv_test_split]\n",
    "    \n",
    "    polynomial_fv = [polynomial_valid_fv, polynomial_test_fv]\n",
    "    \n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_test_fv_valid_split, \n",
    "                                polynomial_pred_inet_test_fv_valid_split)\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_test_fv_test_split, \n",
    "                                polynomial_pred_inet_test_fv_test_split)\n",
    "    \n",
    "    \n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_valid_fv_valid_split, \n",
    "                                polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_valid_fv_test_split, \n",
    "                                polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv_valid_split, scores_truePoly_VS_inetPoly_test_fv_test_split)\n",
    "    scores_truePoly_VS_inetPoly_valid_fv = mergeDict(scores_truePoly_VS_inetPoly_valid_fv_valid_split, scores_truePoly_VS_inetPoly_valid_fv_test_split)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        scores_dict = [scores_truePoly_VS_inetPoly_test_fv, scores_truePoly_VS_inetPoly_valid_fv]\n",
    "    else:   \n",
    "        scores_predLambda_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_test_fv_valid_split, \n",
    "                                    polynomial_pred_inet_test_fv_valid_split)\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_test_fv_test_split, \n",
    "                                    polynomial_pred_inet_test_fv_test_split)\n",
    "\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_valid_fv_valid_split, \n",
    "                                    polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_valid_fv_test_split, \n",
    "                                    polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "        metrics = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'STD FV PRED', 'MEAN FV PRED']\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_test_fv = mergeDict(scores_predLambda_VS_inetPoly_test_fv_valid_split, scores_predLambda_VS_inetPoly_test_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_test_fv = {key: scores_predLambda_VS_inetPoly_test_fv[key] for key in metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv, scores_predLambda_VS_inetPoly_test_fv)\n",
    "        \n",
    "        scores_predLambda_VS_inetPoly_valid_fv = mergeDict(scores_predLambda_VS_inetPoly_valid_fv_valid_split, scores_predLambda_VS_inetPoly_valid_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_valid_fv = {key: scores_predLambda_VS_inetPoly_valid_fv[key] for key in metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_valid_fv =mergeDict(scores_truePoly_VS_inetPoly_valid_fv, scores_predLambda_VS_inetPoly_valid_fv)\n",
    "\n",
    "        scores_dict = [scores_truePoly_and_predLambda_VS_inetPoly_test_fv, scores_truePoly_and_predLambda_VS_inetPoly_valid_fv]\n",
    "\n",
    "    if return_model:\n",
    "        return history.history, scores_dict, polynomial_fv, model         \n",
    "    else: \n",
    "        return history.history, scores_dict, polynomial_fv       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.646Z"
    },
    "code_folding": [
     2,
     47,
     142
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results = train_nn_and_pred(X_train.values, \n",
    "                                X_valid.values, \n",
    "                                X_test.values, \n",
    "                                y_train.values, \n",
    "                                y_valid.values, \n",
    "                                y_test.values, \n",
    "                                lambda_train_fv_valid_split.values, \n",
    "                                lambda_train_fv_test_split.values, \n",
    "                                lambda_train_fv_train_split.values, \n",
    "                                lambda_valid_fv_valid_split.values, \n",
    "                                lambda_valid_fv_test_split.values, \n",
    "                                lambda_valid_fv_train_split.values, \n",
    "                                lambda_test_fv_valid_split.values, \n",
    "                                lambda_test_fv_test_split.values, \n",
    "                                lambda_test_fv_train_split.values, \n",
    "                                callback_names=['plot_losses_callback', 'early_stopping'], \n",
    "                                return_model=True)\n",
    "    \n",
    "    history = results[0]\n",
    "    \n",
    "    scores_complete = results[1]\n",
    "    scores_with_valid_fv = scores_complete[0]\n",
    "    scores_with_test_fv = scores_complete[1]\n",
    "    \n",
    "    polynomial_fv_complete = results[2]\n",
    "    polynomial_valid_fv = polynomial_fv_complete[0]\n",
    "    polynomial_test_fv = polynomial_fv_complete[1]\n",
    "    \n",
    "    model = results[3]\n",
    "    \n",
    "    x = PrettyTable()\n",
    "\n",
    "    x.field_names = [\"Error Name\", \"Valid Error Int\", \"Test Error Int\"]\n",
    "\n",
    "    for error, value in scores_with_test_fv.items():\n",
    "\n",
    "        x.add_row([error, value[0], value[1]])\n",
    "\n",
    "    print(x)    \n",
    "    \n",
    "elif multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, \n",
    "                            verbose=11, \n",
    "                            backend='loky')(delayed(train_nn_and_pred)(X_train.values, \n",
    "                                                                      X_valid.values, \n",
    "                                                                      X_test.values, \n",
    "                                                                      y_train.values, \n",
    "                                                                      y_valid.values, \n",
    "                                                                      y_test.values, \n",
    "                                                                      lambda_train_fv_valid_split.values, \n",
    "                                                                      lambda_train_fv_test_split.values, \n",
    "                                                                      lambda_train_fv_train_split.values, \n",
    "                                                                      lambda_valid_fv_valid_split.values, \n",
    "                                                                      lambda_valid_fv_test_split.values, \n",
    "                                                                      lambda_valid_fv_train_split.values, \n",
    "                                                                      lambda_test_fv_valid_split.values, \n",
    "                                                                      lambda_test_fv_test_split.values, \n",
    "                                                                      lambda_test_fv_train_split.values, \n",
    "                                                                      callback_names=['early_stopping']) for X_train, \n",
    "                                                                                                               X_valid, \n",
    "                                                                                                               X_test, \n",
    "                                                                                                               y_train, \n",
    "                                                                                                               y_valid, \n",
    "                                                                                                               y_test, \n",
    "                                                                                                               lambda_train_fv_valid_split, \n",
    "                                                                                                               lambda_train_fv_test_split, \n",
    "                                                                                                               lambda_train_fv_train_split,                                            \n",
    "                                                                                                               lambda_valid_fv_valid_split, \n",
    "                                                                                                               lambda_valid_fv_test_split, \n",
    "                                                                                                               lambda_valid_fv_train_split, \n",
    "                                                                                                               lambda_test_fv_valid_split, \n",
    "                                                                                                               lambda_test_fv_test_split, \n",
    "                                                                                                               lambda_test_fv_train_split in zip(X_train_list, \n",
    "                                                                                                                                                 X_valid_list, \n",
    "                                                                                                                                                 X_test_list, \n",
    "                                                                                                                                                 y_train_list, \n",
    "                                                                                                                                                 y_valid_list, \n",
    "                                                                                                                                                 y_test_list, \n",
    "                                                                                                                                                 lambda_train_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_train_split_list,                                                                                                                                                  \n",
    "                                                                                                                                                 lambda_valid_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_train_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_train_split_list))      \n",
    "\n",
    "    history_list = [result[0] for result in results_list]\n",
    "    \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "\n",
    "    for i, history in enumerate(history_list):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        plt.plot(history[list(history.keys())[1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)   \n",
    "        \n",
    "elif not multi_epoch_analysis and  samples_list != None:\n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, verbose=11, backend='loky')(delayed(train_nn_and_pred)(X_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  X_valid.values, \n",
    "                                                                                                  X_test.values, \n",
    "                                                                                                  y_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  y_valid.values, \n",
    "                                                                                                  y_test.values, \n",
    "                                                                                                  lambda_train_fv_valid_split.values, \n",
    "                                                                                                  lambda_train_fv_test_split.values, \n",
    "                                                                                                  lambda_train_fv_train_split.values, \n",
    "                                                                                                  lambda_valid_fv_valid_split.values, \n",
    "                                                                                                  lambda_valid_fv_test_split.values, \n",
    "                                                                                                  lambda_valid_fv_train_split.values, \n",
    "                                                                                                  lambda_test_fv_valid_split.values, \n",
    "                                                                                                  lambda_test_fv_test_split.values, \n",
    "                                                                                                  lambda_test_fv_train_split.values, \n",
    "                                                                                                  callback_names=['early_stopping']) for samples in samples_list)     \n",
    "    \n",
    "    history_list = [result[0] for result in results_list]\n",
    "     \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "    for i, history in enumerate(history_list):       \n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.649Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_valid_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_valid_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.652Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_test_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_test_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.654Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.657Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.660Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss = []\n",
    "    plot_history_metric = []\n",
    "    plot_history_val_loss = []\n",
    "    plot_history_val_metric = []\n",
    "        \n",
    "    for history in history_list:\n",
    "        plot_history_loss.append(history['loss'][-1])\n",
    "        plot_history_metric.append(history[list(history.keys())[1]][-1])\n",
    "\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plot_history_val_loss.append(history['val_loss'][-1])\n",
    "            plot_history_val_metric.append(history[list(history.keys())[len(history.keys())//2+1]][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.663Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss_df = pd.DataFrame(data=plot_history_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_loss))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_loss_df = pd.DataFrame(data=plot_history_val_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_loss))])\n",
    "    \n",
    "    plt.plot(plot_history_loss_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_loss_df)\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.666Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_metric_df = pd.DataFrame(data=plot_history_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_metric))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_metric_df = pd.DataFrame(data=plot_history_val_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_metric))])\n",
    "    \n",
    "    plt.plot(plot_history_metric_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_metric_df)\n",
    "    plt.title('Metric')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure Interpretation-Net Socres for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.669Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_coeff_keys = ['MAE', 'RMSE', 'MAPE', 'Accuracy', 'Accuracy Multilabel']\n",
    "metrics_fv_keys = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "new_row_identifiers_coeff = ['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT']\n",
    "new_row_identifiers_fv = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "\n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "  \n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)\n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "\n",
    "\n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'means_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)  \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "    \n",
    "elif not multi_epoch_analysis and samples_list != None and evaluate_with_real_function:\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "\n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "        \n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "\n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1) \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)     \n",
    "            \n",
    "elif not multi_epoch_analysis and  samples_list != None and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "\n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "\n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)      \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)    \n",
    "            \n",
    "if multi_epoch_analysis:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')  \n",
    "elif samples_list != None:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_valid_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_test_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Lambda Scores for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.676Z"
    }
   },
   "outputs": [],
   "source": [
    "path_scores_valid_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_scores_test_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_stds_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_means_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "df_mean_scores_valid_lambda = pd.read_csv(path_scores_valid_lambda, sep=',', index_col=0)\n",
    "df_mean_scores_test_lambda = pd.read_csv(path_scores_test_lambda, sep=',', index_col=0)\n",
    "df_stds_lambda = pd.read_csv(path_stds_lambda, sep=',', index_col=0)\n",
    "df_means_lambda = pd.read_csv(path_means_lambda, sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.680Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_test_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.683Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stds_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.687Z"
    }
   },
   "outputs": [],
   "source": [
    "df_means_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns to Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.690Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "    scores_lambda = df_mean_scores_test_lambda.loc[plot_cols]\n",
    "    scores_int = scores_test_list.loc[plot_cols]    \n",
    "elif samples_list != None:\n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "    scores_int = scores_test_list.loc[plot_cols] \n",
    "    scores_lambda = df_mean_scores_test_lambda.loc[plot_cols].iloc[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.693Z"
    }
   },
   "outputs": [],
   "source": [
    "print_head = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    print_head = scores_int\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.697Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_test_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.702Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_valid_fv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.706Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('./data/plotting/' + interpretation_network_string + filename + '/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "        vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "            \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "                    \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "        vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "        \n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.710Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "    vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file \n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "       \n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "    vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "    \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.714Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split_list[-1]\n",
    "elif samples_list != None:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "else:\n",
    "    plot_preds = polynomial_test_fv\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "\n",
    "x_vars = ['x' + str(i) for i in range(1, n+1)]\n",
    "\n",
    "columns = x_vars.copy()\n",
    "columns.append('FVs')\n",
    "\n",
    "columns_single = x_vars.copy()\n",
    "columns_single.extend(['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'])\n",
    "\n",
    "eval_size_plot = plot_preds[2].shape[1]\n",
    "rand_index = 2#42#random.randint(0, plot_preds[2].shape[0]-1)\n",
    "vars_plot = np.column_stack([lambda_test_input[::,i] for i in range(n)])\n",
    "plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, plot_preds[2][rand_index], plot_preds[3][rand_index], plot_eval.values[rand_index]]), columns=columns_single)\n",
    "\n",
    "vars_plot_all_preds = np.append(np.append(vars_plot, vars_plot, axis=0), vars_plot, axis=0)\n",
    "preds_plot_all = np.append(np.append(plot_preds[2][rand_index], plot_preds[3][rand_index], axis=0), plot_eval.values[rand_index], axis=0)\n",
    "\n",
    "if evaluate_with_real_function:\n",
    "    real_str = np.array(['Real Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds LSTSQ\n",
    "else:\n",
    "    real_str = np.array(['Lambda Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds\n",
    "    \n",
    "identifier = np.concatenate([real_str, int_str, lambda_str])\n",
    "\n",
    "plot_data = pd.DataFrame(data=np.column_stack([vars_plot_all_preds, preds_plot_all]), columns=columns)\n",
    "plot_data['Identifier'] = identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.718Z"
    }
   },
   "outputs": [],
   "source": [
    "pp1 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  y_vars=['FVs'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.721Z"
    }
   },
   "outputs": [],
   "source": [
    "pp2 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  #y_vars=['FVs'],\n",
    "                  #x_vars=x_vars\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.724Z"
    }
   },
   "outputs": [],
   "source": [
    "pp3 = sns.pairplot(data=plot_data_single,\n",
    "                  #kind='reg',\n",
    "                  y_vars=['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.726Z"
    }
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_REAL_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')\n",
    "else:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_PRED_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.728Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n)+ '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "    loss_df_lambda = pd.read_csv(path_loss, sep=',')\n",
    "    metric_df_lambda = pd.read_csv(path_metric, sep=',')\n",
    "    val_loss_df_lambda = pd.read_csv(path_val_loss, sep=',')\n",
    "    val_metric_df_lambda = pd.read_csv(path_val_metric, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.731Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "\n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_metric = 0\n",
    "\n",
    "    metric_df_adjusted = metric_df_lambda.copy(deep=True)\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "\n",
    "    val_metric_df_adjusted = val_metric_df_lambda.copy(deep=True)\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_metric'])\n",
    "    plt.title('model metric')\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.733Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_loss = 1000\n",
    "\n",
    "    loss_df_adjusted = loss_df_lambda.copy(deep=True)\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "\n",
    "    val_loss_df_adjusted = val_loss_df_lambda.copy(deep=True)\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.735Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    preds = model.predict(X_test)\n",
    "    preds_rounded = np.round(preds, 1)\n",
    "    #preds_true = pd.DataFrame(data=[np.round(preds, 1), y_test.values])\n",
    "    for pred, y in tqdm(zip(preds_rounded, y_test.values)):\n",
    "        if (pred == y).all():\n",
    "            print(pred)\n",
    "    \n",
    "    #print(preds_rounded)\n",
    "    #print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.739Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#summarize history for loss\n",
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/loss_' + interpretation_network_string + filename + '.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.742Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    random_polynomial = list(random_product([i*a_step for i in range(int(a_min*10**int(-np.log10(a_step))), int(a_max*10**int(-np.log10(a_step))))], repeat=nCr(n+d, d)))\n",
    "    list_of_random_polynomials.append(random_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.744Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(y_test.values, lambda_test_input)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.746Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Benchmark Error Coefficients: ' + str(np.round(mean_absolute_error(y_test, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-27T17:42:31.749Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Benchmark Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
