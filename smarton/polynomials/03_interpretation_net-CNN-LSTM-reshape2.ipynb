{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 3, #degree\n",
    "        'n': 1, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': True, #use Laurent polynomials (negative degree with up to -d)  \n",
    "        'neg_d': 1,#int or None\n",
    "        'neg_d_prob': 0.2,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': None,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0.05,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'lambda_nets_total': 50000,\n",
    "        'noise': 0,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'border_min': 0.2, #needs to be between 0 and (x_max-x_min)/2\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5,\n",
    "        'a_zero_prob': 0.25,\n",
    "        'a_random_prob': 0.1,      \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0.25,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 500,\n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': None, #(None, int)\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number\n",
    "        \n",
    "        'normalize_inet_data': True,\n",
    "\n",
    "        'evaluate_with_real_function': False,\n",
    "        'consider_labels_training': False,\n",
    "                      \n",
    "        'data_reshape_version': 2, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': True,\n",
    "        'nas_type': 'CNN-LSTM', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': True,\n",
    "        'gpu_numbers': '1',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 5\n",
      "[[0], [1], [2], [3], [-1]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebb95c95a9c412a874fb693634003ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 5\n",
      "[[0], [1], [2], [3], [-1]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    print(list_of_monomial_identifiers_extended)        \n",
    "else:\n",
    "    variable_sets = [[_d for _d in range(d+1)] for _ in range(n)]  \n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)\n",
    "\n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity: ' + str(sparsity))\n",
    "    print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    if np.sum(monomial_identifier) <= d:\n",
    "        if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "            list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']\n",
    "\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sparsity')*config['data']['sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense512-1024-output_5_drop0.25e500b256_custom/lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_3_laurent_negd_1_prob_0.2_spars_5_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n",
      "lnets_50000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_3_laurent_negd_1_prob_0.2_spars_5_amin_-1_amax_1_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend MultiprocessingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:   32.0s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "\n",
    "parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>-1-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>-1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>-1-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.837</td>\n",
       "      <td>-1.094</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>1.037</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.203</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.727</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-1.187</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-2.434</td>\n",
       "      <td>10.741</td>\n",
       "      <td>-17.272</td>\n",
       "      <td>8.838</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-1.971</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-1.315</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-2.480</td>\n",
       "      <td>-1.824</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.892</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-3.153</td>\n",
       "      <td>-4.373</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-1.413</td>\n",
       "      <td>-2.751</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-1.612</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-3.497</td>\n",
       "      <td>-1.771</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-1.160</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-4.583</td>\n",
       "      <td>-5.191</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-1.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.319</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.884</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>0.357</td>\n",
       "      <td>3.564</td>\n",
       "      <td>-10.730</td>\n",
       "      <td>15.290</td>\n",
       "      <td>-7.919</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.884</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.423</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-1.462</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-2.703</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-2.583</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.561</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.553</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>2.007</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>4.056</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.412</td>\n",
       "      <td>3.100</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>1.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  0-target  1-target  2-target  3-target  -1-target  \\\n",
       "33553  1373158606     0.110     0.904     0.347    -0.851      0.000   \n",
       "9427   1373158606    -0.294     0.859    -0.018    -0.458      0.000   \n",
       "199    1373158606     0.000     0.000    -0.124     0.000     -0.588   \n",
       "12447  1373158606    -0.145     0.000     0.000     0.000      0.000   \n",
       "39489  1373158606     0.884    -0.248     0.000    -0.571      0.357   \n",
       "\n",
       "       0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "33553           0.130           0.649           0.837          -1.094   \n",
       "9427           -0.324           1.037          -0.447          -0.157   \n",
       "199            -2.434          10.741         -17.272           8.838   \n",
       "12447          -0.145           0.002          -0.006           0.004   \n",
       "39489           3.564         -10.730          15.290          -7.919   \n",
       "\n",
       "       -1-lstsq_lambda  0-lstsq_target  1-lstsq_target  2-lstsq_target  \\\n",
       "33553            0.004           0.110           0.904           0.347   \n",
       "9427             0.003          -0.294           0.859          -0.018   \n",
       "199             -0.419          -0.000           0.000          -0.124   \n",
       "12447            0.000          -0.145           0.000          -0.000   \n",
       "39489            0.153           0.884          -0.248          -0.000   \n",
       "\n",
       "       3-lstsq_target  -1-lstsq_target   wb_0   wb_1   wb_2   wb_3   wb_4  \\\n",
       "33553          -0.851            0.000 -0.010 -0.248  0.312  0.267  0.296   \n",
       "9427           -0.458           -0.000 -0.010 -0.248  0.156 -0.230  0.203   \n",
       "199             0.000           -0.588 -0.010 -0.248 -0.952 -1.971  0.306   \n",
       "12447           0.000            0.000 -0.010 -0.248  0.127  0.049  0.121   \n",
       "39489          -0.571            0.357 -0.010 -0.248  0.338  0.264 -0.371   \n",
       "\n",
       "        wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  \\\n",
       "33553  0.251  0.344 -0.324  0.247  0.525  0.414 -0.096 -0.021  0.354  0.282   \n",
       "9427   0.153  0.203 -0.324 -0.374  0.402  0.358 -0.096 -0.021  0.249  0.179   \n",
       "199    0.272 -1.315 -0.324 -2.480 -1.824 -0.768 -0.096 -0.021 -0.892  0.292   \n",
       "12447  0.079  0.146 -0.324  0.027  0.275  0.319 -0.096 -0.021  0.185  0.080   \n",
       "39489 -0.369  0.423 -0.324  0.270 -0.872  0.507 -0.096 -0.021  0.418 -1.462   \n",
       "\n",
       "       wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  \\\n",
       "33553  0.394  0.153  0.410  0.335  0.388  0.301  0.048  0.010 -0.046 -0.441   \n",
       "9427   0.286  0.063  0.314  0.255  0.302  0.173  0.351  0.010 -0.046 -0.441   \n",
       "199   -0.650  0.009  0.470  0.349  0.420  0.289 -3.153 -4.373 -0.046 -0.441   \n",
       "12447  0.212  0.023  0.251  0.170  0.234  0.042 -0.006 -0.054 -0.046 -0.441   \n",
       "39489  0.448 -2.703 -0.433 -0.359 -0.409 -2.583  0.221  0.010 -0.046 -0.441   \n",
       "\n",
       "       wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  \\\n",
       "33553  0.000  0.000 -0.229 -0.193 -0.032 -0.028 -0.210  0.000 -0.184 -0.056   \n",
       "9427   0.000  0.000 -0.099  0.116 -0.021 -0.014  0.005  0.000  0.149 -0.031   \n",
       "199    0.000  0.000  0.404  0.433 -0.135 -0.148  0.449  0.000  0.427  0.494   \n",
       "12447  0.000  0.000 -0.111 -0.046 -0.090 -0.075 -0.014  0.000 -0.026  0.050   \n",
       "39489  0.000  0.000 -0.127 -0.097  0.332  0.325 -0.156  0.000 -0.126  0.421   \n",
       "\n",
       "       wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  \\\n",
       "33553 -0.251  0.000  0.000 -0.218 -0.030 -0.245 -0.015 -0.043 -0.039 -0.042   \n",
       "9427  -0.027  0.000  0.000 -0.016 -0.016 -0.019 -0.005 -0.025 -0.023 -0.027   \n",
       "199    0.434  0.000  0.000  0.446 -0.147  0.432 -0.025 -0.233 -0.172 -0.211   \n",
       "12447 -0.037  0.000  0.000 -0.034 -0.061 -0.154 -0.028 -0.041 -0.146 -0.069   \n",
       "39489 -0.159  0.000  0.000 -0.144  0.537 -0.159  0.419  0.397  0.326  0.368   \n",
       "\n",
       "       wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  \\\n",
       "33553 -0.034 -0.049 -0.024  0.000  0.000 -0.278 -0.392 -0.510 -0.621  0.271   \n",
       "9427  -0.014 -0.255 -0.026  0.000  0.000 -0.278 -0.392 -0.205 -0.418  0.158   \n",
       "199   -0.144  0.476  0.533  0.000  0.000 -0.278 -0.392 -1.413 -2.751  0.310   \n",
       "12447 -0.027 -0.000  0.005  0.000  0.000 -0.278 -0.392 -0.191 -0.208  0.103   \n",
       "39489  0.561 -0.086 -0.024  0.000  0.000 -0.278 -0.392 -0.368 -0.445  0.497   \n",
       "\n",
       "       wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  \\\n",
       "33553  0.319 -0.389 -0.445 -0.700  0.155 -0.291 -0.385 -0.397 -0.356  0.280   \n",
       "9427   0.212  0.064 -0.445 -0.727  0.072  0.066 -0.385 -0.397  0.079  0.171   \n",
       "199    0.463 -1.612 -0.445 -3.497 -1.771 -0.895 -0.385 -0.397 -1.160  0.358   \n",
       "12447  0.174 -0.000 -0.445 -0.245 -0.001  0.001 -0.385 -0.397  0.000  0.088   \n",
       "39489  0.553 -0.290 -0.445 -0.607  0.515 -0.204 -0.385 -0.397 -0.247  2.007   \n",
       "\n",
       "       wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  \\\n",
       "33553 -0.382  0.536  0.198  0.264  0.208  0.260 -0.340 -0.050 -0.285 -0.418   \n",
       "9427   0.087  0.462  0.091  0.147  0.103  0.164 -1.187 -0.049 -0.285 -0.418   \n",
       "199   -0.890  0.430  0.229  0.293  0.245  0.340 -4.583 -5.191 -0.285 -0.418   \n",
       "12447 -0.085  0.443  0.000  0.137 -0.001  0.033 -0.316 -0.066 -0.285 -0.418   \n",
       "39489 -0.264  4.056  0.405  0.490  0.412  3.100 -0.676 -0.049 -0.285 -0.418   \n",
       "\n",
       "       wb_75  \n",
       "33553  0.243  \n",
       "9427  -0.083  \n",
       "199   -1.059  \n",
       "12447 -0.145  \n",
       "39489  1.166  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>-1-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>-1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>-1-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.447</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.722</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.245</td>\n",
       "      <td>1.506</td>\n",
       "      <td>5.943</td>\n",
       "      <td>9.179</td>\n",
       "      <td>4.637</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.781</td>\n",
       "      <td>1.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.303</td>\n",
       "      <td>1.211</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-10.315</td>\n",
       "      <td>-34.270</td>\n",
       "      <td>-57.037</td>\n",
       "      <td>-25.940</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-4.502</td>\n",
       "      <td>-4.520</td>\n",
       "      <td>-4.104</td>\n",
       "      <td>-3.809</td>\n",
       "      <td>-4.508</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-3.864</td>\n",
       "      <td>-4.110</td>\n",
       "      <td>-2.690</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-3.265</td>\n",
       "      <td>-3.756</td>\n",
       "      <td>-5.257</td>\n",
       "      <td>-6.754</td>\n",
       "      <td>-1.728</td>\n",
       "      <td>-3.641</td>\n",
       "      <td>-3.023</td>\n",
       "      <td>-5.372</td>\n",
       "      <td>-4.572</td>\n",
       "      <td>-7.117</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-5.378</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>-1.349</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>-5.028</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-5.420</td>\n",
       "      <td>-2.448</td>\n",
       "      <td>-2.407</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-3.221</td>\n",
       "      <td>-1.168</td>\n",
       "      <td>-5.835</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>-1.543</td>\n",
       "      <td>-1.047</td>\n",
       "      <td>-6.543</td>\n",
       "      <td>-8.843</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-2.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.243</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.293</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.474</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>9.216</td>\n",
       "      <td>40.050</td>\n",
       "      <td>51.573</td>\n",
       "      <td>28.791</td>\n",
       "      <td>0.827</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.618</td>\n",
       "      <td>1.079</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.572</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.222</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.504</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0.882</td>\n",
       "      <td>1.113</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.631</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.711</td>\n",
       "      <td>1.012</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>1.477</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>4.445</td>\n",
       "      <td>5.304</td>\n",
       "      <td>1.457</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>4.401</td>\n",
       "      <td>1.553</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>1.328</td>\n",
       "      <td>5.035</td>\n",
       "      <td>1.072</td>\n",
       "      <td>9.205</td>\n",
       "      <td>2.169</td>\n",
       "      <td>3.926</td>\n",
       "      <td>2.988</td>\n",
       "      <td>6.545</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>2.637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  0-target  1-target  2-target  3-target  -1-target  \\\n",
       "count      10000.000 10000.000 10000.000 10000.000 10000.000  10000.000   \n",
       "mean  1373158606.000    -0.001     0.005     0.004    -0.002     -0.001   \n",
       "std            0.000     0.504     0.484     0.483     0.487      0.245   \n",
       "min   1373158606.000    -1.000    -1.000    -1.000    -1.000     -0.999   \n",
       "25%   1373158606.000    -0.351    -0.280    -0.278    -0.302      0.000   \n",
       "50%   1373158606.000     0.000     0.000     0.000     0.000      0.000   \n",
       "75%   1373158606.000     0.348     0.298     0.294     0.290      0.000   \n",
       "max   1373158606.000     1.000     1.000     1.000     1.000      0.999   \n",
       "\n",
       "       0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean           -0.021           0.094          -0.132           0.063   \n",
       "std             1.506           5.943           9.179           4.637   \n",
       "min           -10.315         -34.270         -57.037         -25.940   \n",
       "25%            -0.460          -0.442          -0.619          -0.310   \n",
       "50%            -0.001          -0.001           0.002           0.000   \n",
       "75%             0.449           0.439           0.658           0.302   \n",
       "max             9.216          40.050          51.573          28.791   \n",
       "\n",
       "       -1-lstsq_lambda  0-lstsq_target  1-lstsq_target  2-lstsq_target  \\\n",
       "count        10000.000       10000.000       10000.000       10000.000   \n",
       "mean            -0.000          -0.001           0.005           0.004   \n",
       "std              0.145           0.504           0.484           0.483   \n",
       "min             -0.779          -1.000          -1.000          -1.000   \n",
       "25%             -0.003          -0.351          -0.280          -0.278   \n",
       "50%              0.000          -0.000           0.000           0.000   \n",
       "75%              0.003           0.348           0.298           0.294   \n",
       "max              0.827           1.000           1.000           1.000   \n",
       "\n",
       "       3-lstsq_target  -1-lstsq_target      wb_0      wb_1      wb_2  \\\n",
       "count       10000.000        10000.000 10000.000 10000.000 10000.000   \n",
       "mean           -0.002           -0.001    -0.010    -0.248     0.124   \n",
       "std             0.487            0.245     0.000     0.000     0.354   \n",
       "min            -1.000           -0.999    -0.010    -0.248    -4.502   \n",
       "25%            -0.302           -0.000    -0.010    -0.248     0.099   \n",
       "50%            -0.000            0.000    -0.010    -0.248     0.178   \n",
       "75%             0.290            0.000    -0.010    -0.248     0.297   \n",
       "max             1.000            0.999    -0.010    -0.248     0.660   \n",
       "\n",
       "           wb_3      wb_4      wb_5      wb_6      wb_7      wb_8      wb_9  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.006     0.115     0.024     0.141    -0.324    -0.040     0.182   \n",
       "std       0.518     0.378     0.506     0.466     0.000     0.617     0.730   \n",
       "min      -4.520    -4.104    -3.809    -4.508    -0.324    -3.864    -4.110   \n",
       "25%       0.030     0.087     0.015     0.133    -0.324     0.028     0.221   \n",
       "50%       0.093     0.193     0.112     0.240    -0.324     0.074     0.386   \n",
       "75%       0.221     0.296     0.243     0.345    -0.324     0.194     0.514   \n",
       "max       0.569     0.686     0.618     1.079    -0.324     0.572     1.162   \n",
       "\n",
       "          wb_10     wb_11     wb_12     wb_13     wb_14     wb_15     wb_16  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.353    -0.096    -0.021     0.218     0.028     0.274    -0.207   \n",
       "std       0.295     0.000     0.000     0.331     0.540     0.298     0.941   \n",
       "min      -2.690    -0.096    -0.021    -3.265    -3.756    -5.257    -6.754   \n",
       "25%       0.328    -0.096    -0.021     0.190     0.049     0.243    -0.003   \n",
       "50%       0.394    -0.096    -0.021     0.281     0.142     0.327     0.010   \n",
       "75%       0.474    -0.096    -0.021     0.377     0.257     0.406     0.107   \n",
       "max       1.222    -0.096    -0.021     1.001     0.632     0.860     0.504   \n",
       "\n",
       "          wb_17     wb_18     wb_19     wb_20     wb_21     wb_22     wb_23  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.273     0.207     0.262    -0.123    -0.121    -0.228    -0.046   \n",
       "std       0.369     0.290     0.326     0.900     0.781     1.067     0.000   \n",
       "min      -1.728    -3.641    -3.023    -5.372    -4.572    -7.117    -0.046   \n",
       "25%       0.244     0.147     0.229     0.012     0.025     0.008    -0.046   \n",
       "50%       0.336     0.256     0.323     0.043     0.045     0.010    -0.046   \n",
       "75%       0.439     0.363     0.416     0.234     0.148     0.115    -0.046   \n",
       "max       1.180     0.882     1.113     0.700     0.651     0.631    -0.046   \n",
       "\n",
       "          wb_24     wb_25     wb_26     wb_27     wb_28     wb_29     wb_30  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.441     0.000     0.000    -0.047    -0.017    -0.032    -0.019   \n",
       "std       0.000     0.000     0.000     0.168     0.160     0.166     0.164   \n",
       "min      -0.441     0.000     0.000    -0.510    -0.373    -0.420    -0.438   \n",
       "25%      -0.441     0.000     0.000    -0.145    -0.094    -0.126    -0.098   \n",
       "50%      -0.441     0.000     0.000    -0.087    -0.050    -0.068    -0.045   \n",
       "75%      -0.441     0.000     0.000    -0.003     0.011    -0.005     0.003   \n",
       "max      -0.441     0.000     0.000     0.784     0.737     0.636     0.705   \n",
       "\n",
       "          wb_31     wb_32     wb_33     wb_34     wb_35     wb_36     wb_37  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.023     0.000    -0.014     0.033    -0.054     0.000     0.000   \n",
       "std       0.185     0.000     0.162     0.272     0.196     0.000     0.000   \n",
       "min      -0.779     0.000    -0.399    -0.933    -0.913     0.000     0.000   \n",
       "25%      -0.119     0.000    -0.083    -0.119    -0.164     0.000     0.000   \n",
       "50%      -0.045     0.000    -0.056    -0.036    -0.065     0.000     0.000   \n",
       "75%       0.042     0.000     0.013     0.214     0.005     0.000     0.000   \n",
       "max       0.679     0.000     0.696     0.895     0.932     0.000     0.000   \n",
       "\n",
       "          wb_38     wb_39     wb_40     wb_41     wb_42     wb_43     wb_44  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.028    -0.011    -0.057     0.021    -0.043    -0.058    -0.037   \n",
       "std       0.178     0.165     0.171     0.163     0.215     0.173     0.185   \n",
       "min      -0.783    -0.372    -0.482    -0.352    -0.816    -0.512    -0.738   \n",
       "25%      -0.136    -0.088    -0.174    -0.025    -0.140    -0.166    -0.143   \n",
       "50%      -0.050    -0.044    -0.083    -0.024    -0.052    -0.092    -0.055   \n",
       "75%       0.030     0.017    -0.018     0.037     0.030    -0.017     0.011   \n",
       "max       0.654     0.711     1.012     0.792     0.701     0.999     0.673   \n",
       "\n",
       "          wb_45     wb_46     wb_47     wb_48     wb_49     wb_50     wb_51  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.018    -0.000     0.042     0.000     0.000    -0.278    -0.392   \n",
       "std       0.189     0.163     0.184     0.000     0.000     0.000     0.000   \n",
       "min      -0.502    -0.473    -0.457     0.000     0.000    -0.278    -0.392   \n",
       "25%      -0.048    -0.052    -0.024     0.000     0.000    -0.278    -0.392   \n",
       "50%      -0.036    -0.050    -0.024     0.000     0.000    -0.278    -0.392   \n",
       "75%       0.066     0.017     0.071     0.000     0.000    -0.278    -0.392   \n",
       "max       0.807     0.681     0.889     0.000     0.000    -0.278    -0.392   \n",
       "\n",
       "          wb_52     wb_53     wb_54     wb_55     wb_56     wb_57     wb_58  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean     -0.353    -0.488     0.305     0.447    -0.200    -0.445    -0.580   \n",
       "std       0.373     0.593     0.402     0.568     0.494     0.000     0.740   \n",
       "min      -5.378    -5.500    -1.349    -0.430    -5.028    -0.445    -5.420   \n",
       "25%      -0.423    -0.458     0.127     0.202    -0.253    -0.445    -0.509   \n",
       "50%      -0.264    -0.311     0.234     0.293    -0.144    -0.445    -0.351   \n",
       "75%      -0.180    -0.243     0.365     0.436    -0.020    -0.445    -0.282   \n",
       "max       1.477    -0.037     4.445     5.304     1.457    -0.445    -0.100   \n",
       "\n",
       "          wb_59     wb_60     wb_61     wb_62     wb_63     wb_64     wb_65  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.055    -0.080    -0.385    -0.397    -0.142     0.371    -0.176   \n",
       "std       0.645     0.310     0.000     0.000     0.354     0.595     0.303   \n",
       "min      -2.448    -2.407    -0.385    -0.397    -3.221    -1.168    -5.835   \n",
       "25%      -0.133    -0.208    -0.385    -0.397    -0.244     0.131    -0.288   \n",
       "50%      -0.001    -0.095    -0.385    -0.397    -0.133     0.235    -0.162   \n",
       "75%       0.131     0.017    -0.385    -0.397    -0.002     0.354    -0.016   \n",
       "max       4.401     1.553    -0.385    -0.397     1.328     5.035     1.072   \n",
       "\n",
       "          wb_66     wb_67     wb_68     wb_69     wb_70     wb_71     wb_72  \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.901     0.100     0.266     0.128     0.467    -0.722    -0.497   \n",
       "std       1.211     0.399     0.304     0.351     0.992     0.982     1.204   \n",
       "min       0.224    -1.560    -1.214    -1.543    -1.047    -6.543    -8.843   \n",
       "25%       0.430    -0.006     0.107    -0.001     0.054    -0.619    -0.352   \n",
       "50%       0.507     0.119     0.235     0.139     0.200    -0.401    -0.097   \n",
       "75%       0.773     0.219     0.377     0.246     0.363    -0.339    -0.049   \n",
       "max       9.205     2.169     3.926     2.988     6.545    -0.143     0.100   \n",
       "\n",
       "          wb_73     wb_74     wb_75  \n",
       "count 10000.000 10000.000 10000.000  \n",
       "mean     -0.285    -0.418     0.010  \n",
       "std       0.000     0.000     0.552  \n",
       "min      -0.285    -0.418    -2.605  \n",
       "25%      -0.285    -0.418    -0.265  \n",
       "50%      -0.285    -0.418    -0.001  \n",
       "75%      -0.285    -0.418     0.282  \n",
       "max      -0.285    -0.418     2.637  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46090259],\n",
       "       [0.10577573],\n",
       "       [0.9130468 ],\n",
       "       [0.49852098],\n",
       "       [0.78355729],\n",
       "       [0.70730873],\n",
       "       [0.21320786],\n",
       "       [0.57263504],\n",
       "       [0.62688689],\n",
       "       [0.53872974]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5166591 ],\n",
       "       [0.20806498],\n",
       "       [0.57668708],\n",
       "       [0.54108354],\n",
       "       [0.62168377],\n",
       "       [0.6215422 ],\n",
       "       [0.30984831],\n",
       "       [0.5812829 ],\n",
       "       [0.60306484],\n",
       "       [0.56428691]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "for lambda_net_dataset in lambda_net_dataset_list:\n",
    "    \n",
    "    \n",
    "    if inet_holdout_seed_evaluation:\n",
    "        complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "        random.seed(RANDOM_SEED)\n",
    "        \n",
    "        if isinstance(test_size, float):\n",
    "            test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "        \n",
    "        test_seeds = random.sample(complete_seed_list, test_size)\n",
    "        lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "        \n",
    "        random.seed(RANDOM_SEED)\n",
    "        valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "        lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "        train_seeds = complete_seed_list\n",
    "        lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "        \n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "        \n",
    "        del lambda_net_dataset\n",
    "    else:\n",
    "        lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "        lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "        \n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "    \n",
    "        del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "        \n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8910, 92)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 92)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 92)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>-1-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>-1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>-1-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44759</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.752</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>1.204</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.752</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.171</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25250</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.722</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.966</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.948</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.604</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>0.659</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.707</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22860</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14471</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.636</td>\n",
       "      <td>4.505</td>\n",
       "      <td>-18.133</td>\n",
       "      <td>28.920</td>\n",
       "      <td>-14.519</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>-1.302</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.074</td>\n",
       "      <td>-2.188</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-3.650</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.751</td>\n",
       "      <td>-3.486</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.429</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>1.372</td>\n",
       "      <td>1.797</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>2.038</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>2.020</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>5.117</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.936</td>\n",
       "      <td>3.997</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>1.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44142</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.788</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.213</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  0-target  1-target  2-target  3-target  -1-target  \\\n",
       "44759  1373158606     0.228     0.155     0.752    -0.679     -0.057   \n",
       "25250  1373158606     0.244    -0.784     0.540     0.948      0.000   \n",
       "22860  1373158606     0.000     0.269     0.000     0.000      0.000   \n",
       "14471  1373158606     0.000     0.699     0.000     0.000      0.636   \n",
       "44142  1373158606    -0.842     0.000     0.000     0.000      0.000   \n",
       "\n",
       "       0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "44759          -0.243           1.204          -0.161          -0.403   \n",
       "25250           0.263          -0.722           0.426           0.966   \n",
       "22860          -0.007           0.241           0.099          -0.071   \n",
       "14471           4.505         -18.133          28.920         -14.519   \n",
       "44142          -0.772          -0.446           0.788          -0.416   \n",
       "\n",
       "       -1-lstsq_lambda  0-lstsq_target  1-lstsq_target  2-lstsq_target  \\\n",
       "44759            0.008           0.228           0.155           0.752   \n",
       "25250           -0.006           0.244          -0.784           0.540   \n",
       "22860            0.002           0.000           0.269          -0.000   \n",
       "14471            0.317           0.000           0.699           0.000   \n",
       "44142            0.003          -0.842          -0.000           0.000   \n",
       "\n",
       "       3-lstsq_target  -1-lstsq_target   wb_0   wb_1  wb_2  wb_3   wb_4  \\\n",
       "44759          -0.679           -0.057 -0.010 -0.248 0.287 0.233  0.318   \n",
       "25250           0.948           -0.000 -0.010 -0.248 0.144 0.083  0.413   \n",
       "22860           0.000            0.000 -0.010 -0.248 0.143 0.071  0.203   \n",
       "14471          -0.000            0.636 -0.010 -0.248 0.181 0.001 -1.032   \n",
       "44142          -0.000            0.000 -0.010 -0.248 0.248 0.135  0.291   \n",
       "\n",
       "        wb_5  wb_6   wb_7  wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  \\\n",
       "44759  0.275 0.288 -0.324 0.206  0.548  0.347 -0.096 -0.021  0.307  0.304   \n",
       "25250  0.381 0.208 -0.324 0.066  0.785  0.351 -0.096 -0.021  0.235  0.307   \n",
       "22860  0.155 0.154 -0.324 0.063  0.362  0.308 -0.096 -0.021  0.189  0.174   \n",
       "14471 -1.302 0.302 -0.324 0.074 -2.188  0.466 -0.096 -0.021  0.322 -1.621   \n",
       "44142  0.235 0.213 -0.324 0.100  0.283  0.397 -0.096 -0.021  0.265  0.237   \n",
       "\n",
       "       wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  \\\n",
       "44759  0.344  0.164  0.430  0.354  0.410  0.308  0.185  0.010 -0.046 -0.441   \n",
       "25250  0.276  0.303  0.405  0.474  0.465  0.443  0.042  0.009 -0.046 -0.441   \n",
       "22860  0.252  0.068  0.303  0.306  0.298  0.147  0.045  0.010 -0.046 -0.441   \n",
       "14471  0.351 -3.650 -0.964 -0.571 -0.751 -3.486  0.046  0.010 -0.046 -0.441   \n",
       "44142  0.367  0.010  0.359  0.369  0.363  0.039  0.039 -0.064 -0.046 -0.441   \n",
       "\n",
       "       wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  \\\n",
       "44759  0.000  0.000 -0.177 -0.151 -0.034 -0.028 -0.166  0.000 -0.147 -0.059   \n",
       "25250  0.000  0.000 -0.012 -0.007 -0.222 -0.182 -0.015  0.000 -0.004 -0.604   \n",
       "22860  0.000  0.000 -0.104 -0.071 -0.060 -0.079 -0.071  0.000 -0.063 -0.040   \n",
       "14471  0.000  0.000 -0.076 -0.004  0.413  0.428 -0.127  0.000 -0.075  0.539   \n",
       "44142  0.000  0.000  0.158  0.168 -0.073 -0.055  0.191  0.000  0.166  0.255   \n",
       "\n",
       "       wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  \\\n",
       "44759 -0.186  0.000  0.000 -0.169 -0.032 -0.195 -0.017 -0.050 -0.040 -0.046   \n",
       "25250 -0.025  0.000  0.000 -0.018 -0.130 -0.021 -0.214 -0.170 -0.279 -0.207   \n",
       "22860 -0.116  0.000  0.000 -0.086 -0.021 -0.130 -0.014 -0.038 -0.040 -0.038   \n",
       "14471 -0.229  0.000  0.000 -0.134  0.429 -0.177  0.556  0.461  0.402  0.436   \n",
       "44142  0.172  0.000  0.000  0.179 -0.055  0.151 -0.025 -0.078 -0.093 -0.082   \n",
       "\n",
       "       wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  \\\n",
       "44759 -0.033 -0.130 -0.024  0.000  0.000 -0.278 -0.392 -0.394 -0.482  0.292   \n",
       "25250 -0.294 -0.048 -0.024  0.000  0.000 -0.278 -0.392 -0.136 -0.190  0.503   \n",
       "22860 -0.016 -0.047 -0.024  0.000  0.000 -0.278 -0.392 -0.192 -0.224  0.190   \n",
       "14471  0.586 -0.051 -0.024  0.000  0.000 -0.278 -0.392 -0.144 -0.237  1.372   \n",
       "44142 -0.042  0.170  0.257  0.000  0.000 -0.278 -0.392 -0.336 -0.374  0.297   \n",
       "\n",
       "       wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  \\\n",
       "44759  0.347 -0.259 -0.445 -0.542  0.171 -0.139 -0.385 -0.397 -0.237  0.310   \n",
       "25250  0.554 -0.073 -0.445 -0.230  0.659 -0.054 -0.385 -0.397 -0.066  0.381   \n",
       "22860  0.259 -0.026 -0.445 -0.271  0.058 -0.018 -0.385 -0.397 -0.034  0.181   \n",
       "14471  1.797 -0.095 -0.445 -0.283  2.038 -0.244 -0.385 -0.397 -0.096  2.020   \n",
       "44142  0.346 -0.213 -0.445 -0.414 -0.120 -0.192 -0.385 -0.397 -0.210  0.276   \n",
       "\n",
       "       wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  \\\n",
       "44759 -0.254  0.571  0.221  0.275  0.229  0.302 -0.697 -0.049 -0.285 -0.418   \n",
       "25250 -0.063  1.110  0.268  0.550  0.359  0.707 -0.337 -0.049 -0.285 -0.418   \n",
       "22860 -0.101  0.468  0.100  0.234  0.119  0.142 -0.339 -0.049 -0.285 -0.418   \n",
       "14471 -0.168  5.117  1.110  0.903  0.936  3.997 -0.340 -0.050 -0.285 -0.418   \n",
       "44142 -0.250  0.430  0.168  0.324  0.205  0.054 -0.494 -0.326 -0.285 -0.418   \n",
       "\n",
       "       wb_75  \n",
       "44759 -0.048  \n",
       "25250  0.130  \n",
       "22860  0.038  \n",
       "14471  1.458  \n",
       "44142 -0.196  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>-1-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>-1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>-1-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35627</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.242</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>0.292</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15838</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.685</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-2.268</td>\n",
       "      <td>9.335</td>\n",
       "      <td>-14.987</td>\n",
       "      <td>6.737</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.685</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.866</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-1.171</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-2.079</td>\n",
       "      <td>-4.610</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-1.138</td>\n",
       "      <td>-1.561</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-1.080</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-1.824</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-3.014</td>\n",
       "      <td>-5.260</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-1.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37218</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36762</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.155</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.634</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  0-target  1-target  2-target  3-target  -1-target  \\\n",
       "35627  1373158606     0.000    -0.793     0.098     0.215      0.000   \n",
       "15838  1373158606     0.000    -0.105    -0.393    -0.685     -0.426   \n",
       "37218  1373158606    -0.445     0.553     0.002     0.000      0.000   \n",
       "36762  1373158606     0.000    -0.902     0.390     0.000      0.000   \n",
       "2830   1373158606     0.634     0.000     0.000    -0.484      0.000   \n",
       "\n",
       "       0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "35627           0.006          -0.694          -0.122           0.330   \n",
       "15838          -2.268           9.335         -14.987           6.737   \n",
       "37218          -0.422           0.456           0.144          -0.071   \n",
       "36762           0.003          -0.768           0.095           0.155   \n",
       "2830            0.601           0.118          -0.177          -0.383   \n",
       "\n",
       "       -1-lstsq_lambda  0-lstsq_target  1-lstsq_target  2-lstsq_target  \\\n",
       "35627           -0.004           0.000          -0.793           0.098   \n",
       "15838           -0.259          -0.000          -0.105          -0.393   \n",
       "37218            0.000          -0.445           0.553           0.002   \n",
       "36762           -0.004          -0.000          -0.902           0.390   \n",
       "2830             0.002           0.634          -0.000           0.000   \n",
       "\n",
       "       3-lstsq_target  -1-lstsq_target   wb_0   wb_1   wb_2   wb_3   wb_4  \\\n",
       "35627           0.215           -0.000 -0.010 -0.248  0.242  0.196  0.224   \n",
       "15838          -0.685           -0.426 -0.010 -0.248 -0.678 -0.997  0.264   \n",
       "37218          -0.000           -0.000 -0.010 -0.248 -0.078 -0.146  0.263   \n",
       "36762           0.000            0.000 -0.010 -0.248  0.247  0.193  0.202   \n",
       "2830           -0.484            0.000 -0.010 -0.248  0.309  0.257 -0.070   \n",
       "\n",
       "        wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  \\\n",
       "35627  0.179  0.306 -0.324  0.173  0.459  0.411 -0.096 -0.021  0.328  0.190   \n",
       "15838 -0.003 -0.866 -0.324 -1.171 -0.400  0.089 -0.096 -0.021 -0.637  0.191   \n",
       "37218  0.201 -0.025 -0.324 -0.165  0.380  0.347 -0.096 -0.021  0.154  0.206   \n",
       "36762  0.161  0.300 -0.324  0.165  0.457  0.412 -0.096 -0.021  0.323  0.166   \n",
       "2830  -0.080  0.324 -0.324  0.233  0.064  0.420 -0.096 -0.021  0.360 -0.100   \n",
       "\n",
       "       wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  \\\n",
       "35627  0.348  0.011  0.259  0.254  0.268  0.169  0.143  0.242 -0.046 -0.441   \n",
       "15838 -0.084  0.010  0.794  0.393  0.716  0.037 -2.079 -4.610 -0.046 -0.441   \n",
       "37218  0.272  0.091  0.323  0.336  0.320  0.193 -0.218 -0.417 -0.046 -0.441   \n",
       "36762  0.357  0.010  0.240  0.248  0.245  0.132  0.141  0.234 -0.046 -0.441   \n",
       "2830   0.381 -0.119  0.054  0.036  0.085 -0.167  0.218  0.010 -0.046 -0.441   \n",
       "\n",
       "       wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  \\\n",
       "35627  0.000  0.000 -0.026 -0.020 -0.145 -0.131 -0.033  0.000 -0.019 -0.050   \n",
       "15838  0.000  0.000  0.316  0.333 -0.165 -0.002  0.343  0.000  0.330  0.227   \n",
       "37218  0.000  0.000  0.043  0.075 -0.123 -0.087  0.097  0.000  0.078 -0.127   \n",
       "36762  0.000  0.000 -0.030 -0.019 -0.128 -0.107 -0.030  0.000 -0.016 -0.048   \n",
       "2830   0.000  0.000 -0.201 -0.177  0.061  0.059 -0.144  0.000 -0.170  0.197   \n",
       "\n",
       "       wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  \\\n",
       "35627 -0.047  0.000  0.000 -0.035 -0.119 -0.038 -0.024 -0.158 -0.189 -0.163   \n",
       "15838  0.082  0.000  0.000  0.336 -0.121  0.046 -0.025 -0.661 -0.243 -0.500   \n",
       "37218 -0.045  0.000  0.000  0.070 -0.078 -0.035 -0.033 -0.113 -0.166 -0.120   \n",
       "36762 -0.043  0.000  0.000 -0.033 -0.100 -0.040 -0.024 -0.140 -0.164 -0.148   \n",
       "2830  -0.212  0.000  0.000 -0.147  0.072 -0.224  0.079  0.154  0.060  0.136   \n",
       "\n",
       "       wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  \\\n",
       "35627 -0.100 -0.017 -0.024  0.000  0.000 -0.278 -0.392 -0.249 -0.309  0.299   \n",
       "15838 -0.043  0.435  0.664  0.000  0.000 -0.278 -0.392 -1.138 -1.561 -0.410   \n",
       "37218 -0.067  0.100  0.151  0.000  0.000 -0.278 -0.392 -0.126 -0.282  0.274   \n",
       "36762 -0.082 -0.013 -0.022  0.000  0.000 -0.278 -0.392 -0.258 -0.308  0.252   \n",
       "2830   0.120 -0.158 -0.024  0.000  0.000 -0.278 -0.392 -0.445 -0.543  0.123   \n",
       "\n",
       "       wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  \\\n",
       "35627  0.367 -0.188 -0.445 -0.346 -0.130 -0.142 -0.385 -0.397 -0.182  0.292   \n",
       "15838  0.146 -1.080 -0.445 -1.824 -0.090  0.001 -0.385 -0.397 -0.861 -0.381   \n",
       "37218  0.291  0.000 -0.445 -0.363  0.100  0.059 -0.385 -0.397  0.022  0.220   \n",
       "36762  0.309 -0.182 -0.445 -0.340 -0.125 -0.143 -0.385 -0.397 -0.178  0.239   \n",
       "2830   0.209 -0.260 -0.445 -0.610  0.005 -0.236 -0.385 -0.397 -0.271  0.161   \n",
       "\n",
       "       wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  \\\n",
       "35627 -0.172  0.431  0.143  0.298  0.182  0.320 -0.409 -0.248 -0.285 -0.418   \n",
       "15838 -0.032  0.430 -0.982 -0.562 -0.940  0.054 -3.014 -5.260 -0.285 -0.418   \n",
       "37218  0.079  0.513  0.129  0.331  0.151  0.219 -0.558 -0.542 -0.285 -0.418   \n",
       "36762 -0.185  0.430  0.107  0.263  0.144  0.222 -0.409 -0.239 -0.285 -0.418   \n",
       "2830  -0.321  0.524  0.004  0.027  0.004  0.138 -0.773 -0.050 -0.285 -0.418   \n",
       "\n",
       "       wb_75  \n",
       "35627 -0.103  \n",
       "15838 -1.069  \n",
       "37218 -0.229  \n",
       "36762 -0.112  \n",
       "2830   0.558  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>-1-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>-1-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>-1-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25056</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.942</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.487</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30334</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>-2.548</td>\n",
       "      <td>12.578</td>\n",
       "      <td>-20.031</td>\n",
       "      <td>9.715</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-1.494</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-2.423</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-3.481</td>\n",
       "      <td>-4.726</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.383</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-1.404</td>\n",
       "      <td>-2.570</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-1.829</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-3.433</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-1.117</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-1.084</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-5.006</td>\n",
       "      <td>-5.658</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17962</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-1.760</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.259</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39588</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.863</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>-0.948</td>\n",
       "      <td>1.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>-0.863</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>0.937</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.474</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.541</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34107</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.665</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.445</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             seed  0-target  1-target  2-target  3-target  -1-target  \\\n",
       "25056  1373158606    -0.926     0.389     0.206    -0.880      0.000   \n",
       "30334  1373158606     0.259     0.000     0.000    -0.558     -0.629   \n",
       "17962  1373158606    -0.251     0.000    -0.934    -0.158      0.000   \n",
       "39588  1373158606    -0.378    -0.863    -0.784     0.937      0.000   \n",
       "34107  1373158606     0.886     0.156     0.000    -0.665      0.000   \n",
       "\n",
       "       0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "25056          -0.942           0.506          -0.116          -0.619   \n",
       "30334          -2.548          12.578         -20.031           9.715   \n",
       "17962          -0.305           0.380          -1.760           0.382   \n",
       "39588          -0.363          -0.788          -0.948           1.001   \n",
       "34107           0.758           0.723          -1.040          -0.017   \n",
       "\n",
       "       -1-lstsq_lambda  0-lstsq_target  1-lstsq_target  2-lstsq_target  \\\n",
       "25056            0.001          -0.926           0.389           0.206   \n",
       "30334           -0.452           0.259          -0.000           0.000   \n",
       "17962            0.002          -0.251          -0.000          -0.934   \n",
       "39588           -0.005          -0.378          -0.863          -0.784   \n",
       "34107            0.007           0.886           0.156          -0.000   \n",
       "\n",
       "       3-lstsq_target  -1-lstsq_target   wb_0   wb_1   wb_2   wb_3  wb_4  \\\n",
       "25056          -0.880           -0.000 -0.010 -0.248  0.036 -0.059 0.142   \n",
       "30334          -0.558           -0.629 -0.010 -0.248 -0.895 -1.829 0.183   \n",
       "17962          -0.158           -0.000 -0.010 -0.248  0.443  0.292 0.213   \n",
       "39588           0.937           -0.000 -0.010 -0.248  0.307  0.262 0.286   \n",
       "34107          -0.665           -0.000 -0.010 -0.248  0.327  0.250 0.091   \n",
       "\n",
       "       wb_5   wb_6   wb_7   wb_8   wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  \\\n",
       "25056 0.004  0.019 -0.324 -0.119  0.126  0.261 -0.096 -0.021  0.090  0.179   \n",
       "30334 0.096 -1.494 -0.324 -2.423 -1.381 -0.599 -0.096 -0.021 -0.805  0.146   \n",
       "17962 0.081  0.353 -0.324  0.254  0.506  0.509 -0.096 -0.021  0.389  0.080   \n",
       "39588 0.248  0.386 -0.324  0.241  0.539  0.474 -0.096 -0.021  0.406  0.261   \n",
       "34107 0.013  0.317 -0.324  0.230  0.014  0.445 -0.096 -0.021  0.345  0.012   \n",
       "\n",
       "       wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  \\\n",
       "25056  0.192  0.010  0.727  0.196  0.597  0.038 -0.196 -0.432 -0.046 -0.441   \n",
       "30334 -0.610  0.010  0.352  0.230  0.631  0.038 -3.481 -4.726 -0.046 -0.441   \n",
       "17962  0.537  0.010  0.551  0.235  0.373  0.037  0.201  0.259 -0.046 -0.441   \n",
       "39588  0.407  0.010  0.352  0.332  0.341  0.037  0.207  0.334 -0.046 -0.441   \n",
       "34107  0.417 -0.155  0.162  0.190  0.182 -0.158  0.210  0.009 -0.046 -0.441   \n",
       "\n",
       "       wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  \\\n",
       "25056  0.000  0.000 -0.005  0.027 -0.034 -0.006  0.071  0.000  0.050  0.218   \n",
       "30334  0.000  0.000  0.398  0.450 -0.071 -0.043  0.453  0.000  0.495  0.509   \n",
       "17962  0.000  0.000 -0.299 -0.106 -0.020 -0.083 -0.059  0.000 -0.077 -0.070   \n",
       "39588  0.000  0.000 -0.036 -0.032 -0.209 -0.187 -0.044  0.000 -0.029 -0.062   \n",
       "34107  0.000  0.000 -0.177 -0.151  0.186  0.188 -0.188  0.000 -0.143  0.361   \n",
       "\n",
       "       wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  \\\n",
       "25056  0.109  0.000  0.000  0.078 -0.089  0.050 -0.025 -0.547 -0.052 -0.351   \n",
       "30334  0.422  0.000  0.000  0.427 -0.058  0.383 -0.025 -0.156 -0.100 -0.531   \n",
       "17962 -0.210  0.000  0.000 -0.080 -0.081 -0.333 -0.024 -0.272 -0.020 -0.048   \n",
       "39588 -0.055  0.000  0.000 -0.045 -0.197 -0.047 -0.024 -0.223 -0.227 -0.220   \n",
       "34107 -0.216  0.000  0.000 -0.190  0.205 -0.205  0.190  0.244  0.173  0.218   \n",
       "\n",
       "       wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  \\\n",
       "25056 -0.043  0.084  0.141  0.000  0.000 -0.278 -0.392 -0.159 -0.246 -0.074   \n",
       "30334 -0.042  0.504  0.609  0.000  0.000 -0.278 -0.392 -1.404 -2.570  0.087   \n",
       "17962 -0.042 -0.052 -0.031  0.000  0.000 -0.278 -0.392 -0.622 -0.432 -0.174   \n",
       "39588 -0.042 -0.023 -0.036  0.000  0.000 -0.278 -0.392 -0.320 -0.369  0.477   \n",
       "34107  0.262 -0.138 -0.024  0.000  0.000 -0.278 -0.392 -0.429 -0.480  0.258   \n",
       "\n",
       "       wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  \\\n",
       "25056  0.037 -0.033 -0.445 -0.327 -0.004 -0.010 -0.385 -0.397 -0.023 -0.221   \n",
       "30334  0.173 -1.829 -0.445 -3.433 -1.380 -0.843 -0.385 -0.397 -1.117  0.111   \n",
       "17962  0.198 -0.227 -0.445 -0.440 -0.149 -0.301 -0.385 -0.397 -0.242  0.125   \n",
       "39588  0.588 -0.252 -0.445 -0.405 -0.179 -0.207 -0.385 -0.397 -0.249  0.541   \n",
       "34107  0.336 -0.284 -0.445 -0.537  0.146 -0.256 -0.385 -0.397 -0.279  0.286   \n",
       "\n",
       "       wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  \\\n",
       "25056 -0.020  0.430 -0.883 -0.067 -0.627  0.055 -0.487 -0.457 -0.285 -0.418   \n",
       "30334 -0.926  0.430  0.058  0.073 -1.084  0.054 -5.006 -5.658 -0.285 -0.418   \n",
       "17962 -0.527  0.431 -0.421 -0.135 -0.193  0.054 -0.473 -0.251 -0.285 -0.418   \n",
       "39588 -0.242  0.430  0.312  0.444  0.330  0.054 -0.459 -0.292 -0.285 -0.418   \n",
       "34107 -0.333  0.711  0.175  0.245  0.181  0.340 -0.662 -0.049 -0.285 -0.418   \n",
       "\n",
       "       wb_75  \n",
       "25056 -0.776  \n",
       "30334 -0.852  \n",
       "17962 -0.263  \n",
       "39588 -0.496  \n",
       "34107  0.325  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51 Complete [01h 06m 10s]\n",
      "val_loss: 0.024341918528079987\n",
      "\n",
      "Best val_loss So Far: 0.023556137457489967\n",
      "Total elapsed time: 05h 31m 02s\n",
      "\n",
      "Search: Running Trial #52\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "conv_block_1/ke...|3                 |3                 \n",
      "conv_block_1/se...|False             |False             \n",
      "conv_block_1/ma...|False             |False             \n",
      "conv_block_1/dr...|0                 |0                 \n",
      "conv_block_1/nu...|2                 |2                 \n",
      "conv_block_1/nu...|2                 |2                 \n",
      "conv_block_1/fi...|32                |32                \n",
      "conv_block_1/fi...|128               |128               \n",
      "conv_block_1/fi...|512               |512               \n",
      "conv_block_1/fi...|32                |32                \n",
      "rnn_block_1/bid...|True              |True              \n",
      "rnn_block_1/lay...|lstm              |lstm              \n",
      "rnn_block_1/num...|2                 |2                 \n",
      "dense_block_1/u...|False             |False             \n",
      "dense_block_1/n...|2                 |2                 \n",
      "dense_block_1/u...|32                |32                \n",
      "dense_block_1/d...|0                 |0                 \n",
      "dense_block_1/u...|32                |32                \n",
      "regression_head...|0                 |0                 \n",
      "optimizer         |adam              |adam              \n",
      "learning_rate     |0.001             |0.001             \n",
      "dense_block_1/u...|32                |32                \n",
      "conv_block_1/fi...|32                |None              \n",
      "conv_block_1/fi...|512               |None              \n",
      "\n",
      "Epoch 1/500\n",
      "35/35 [==============================] - 28s 621ms/step - loss: 0.3554 - r2_inet_coefficient_loss: 0.0020 - r2_inet_lambda_fv_loss: 1.5125 - mae_inet_coefficient_loss: 1.7107 - mae_inet_lambda_fv_loss: 0.3554 - val_loss: 0.2406 - val_r2_inet_coefficient_loss: 0.0059 - val_r2_inet_lambda_fv_loss: 0.1602 - val_mae_inet_coefficient_loss: 1.8613 - val_mae_inet_lambda_fv_loss: 0.2406\n",
      "Epoch 2/500\n",
      "35/35 [==============================] - 19s 548ms/step - loss: 0.2156 - r2_inet_coefficient_loss: 0.0049 - r2_inet_lambda_fv_loss: -0.0360 - mae_inet_coefficient_loss: 1.7273 - mae_inet_lambda_fv_loss: 0.2155 - val_loss: 0.1273 - val_r2_inet_coefficient_loss: 0.0029 - val_r2_inet_lambda_fv_loss: -0.5707 - val_mae_inet_coefficient_loss: 1.8841 - val_mae_inet_lambda_fv_loss: 0.1275\n",
      "Epoch 3/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.1197 - r2_inet_coefficient_loss: 0.0029 - r2_inet_lambda_fv_loss: -0.5983 - mae_inet_coefficient_loss: 1.7564 - mae_inet_lambda_fv_loss: 0.1197 - val_loss: 0.0789 - val_r2_inet_coefficient_loss: 0.0026 - val_r2_inet_lambda_fv_loss: -0.7765 - val_mae_inet_coefficient_loss: 1.8914 - val_mae_inet_lambda_fv_loss: 0.0791\n",
      "Epoch 4/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0799 - r2_inet_coefficient_loss: 0.0028 - r2_inet_lambda_fv_loss: -0.7702 - mae_inet_coefficient_loss: 1.7655 - mae_inet_lambda_fv_loss: 0.0799 - val_loss: 0.0691 - val_r2_inet_coefficient_loss: 0.0021 - val_r2_inet_lambda_fv_loss: -0.8153 - val_mae_inet_coefficient_loss: 1.8919 - val_mae_inet_lambda_fv_loss: 0.0693\n",
      "Epoch 5/500\n",
      "35/35 [==============================] - 19s 545ms/step - loss: 0.0678 - r2_inet_coefficient_loss: 0.0026 - r2_inet_lambda_fv_loss: -0.8090 - mae_inet_coefficient_loss: 1.7671 - mae_inet_lambda_fv_loss: 0.0678 - val_loss: 0.0633 - val_r2_inet_coefficient_loss: 0.0030 - val_r2_inet_lambda_fv_loss: -0.8337 - val_mae_inet_coefficient_loss: 1.8968 - val_mae_inet_lambda_fv_loss: 0.0635\n",
      "Epoch 6/500\n",
      "35/35 [==============================] - 18s 525ms/step - loss: 0.0638 - r2_inet_coefficient_loss: 0.0030 - r2_inet_lambda_fv_loss: -0.8264 - mae_inet_coefficient_loss: 1.7684 - mae_inet_lambda_fv_loss: 0.0638 - val_loss: 0.0585 - val_r2_inet_coefficient_loss: 0.0027 - val_r2_inet_lambda_fv_loss: -0.8505 - val_mae_inet_coefficient_loss: 1.8961 - val_mae_inet_lambda_fv_loss: 0.0587\n",
      "Epoch 7/500\n",
      "35/35 [==============================] - 19s 546ms/step - loss: 0.0613 - r2_inet_coefficient_loss: 0.0027 - r2_inet_lambda_fv_loss: -0.8347 - mae_inet_coefficient_loss: 1.7675 - mae_inet_lambda_fv_loss: 0.0613 - val_loss: 0.0586 - val_r2_inet_coefficient_loss: 0.0021 - val_r2_inet_lambda_fv_loss: -0.8541 - val_mae_inet_coefficient_loss: 1.8937 - val_mae_inet_lambda_fv_loss: 0.0588\n",
      "Epoch 8/500\n",
      "35/35 [==============================] - 19s 547ms/step - loss: 0.0598 - r2_inet_coefficient_loss: 0.0027 - r2_inet_lambda_fv_loss: -0.8379 - mae_inet_coefficient_loss: 1.7671 - mae_inet_lambda_fv_loss: 0.0598 - val_loss: 0.0563 - val_r2_inet_coefficient_loss: 0.0023 - val_r2_inet_lambda_fv_loss: -0.8557 - val_mae_inet_coefficient_loss: 1.8940 - val_mae_inet_lambda_fv_loss: 0.0565\n",
      "Epoch 9/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0573 - r2_inet_coefficient_loss: 0.0026 - r2_inet_lambda_fv_loss: -0.8468 - mae_inet_coefficient_loss: 1.7659 - mae_inet_lambda_fv_loss: 0.0573 - val_loss: 0.0547 - val_r2_inet_coefficient_loss: 0.0019 - val_r2_inet_lambda_fv_loss: -0.8634 - val_mae_inet_coefficient_loss: 1.8920 - val_mae_inet_lambda_fv_loss: 0.0549\n",
      "Epoch 10/500\n",
      "35/35 [==============================] - 19s 537ms/step - loss: 0.0561 - r2_inet_coefficient_loss: 0.0024 - r2_inet_lambda_fv_loss: -0.8486 - mae_inet_coefficient_loss: 1.7647 - mae_inet_lambda_fv_loss: 0.0561 - val_loss: 0.0522 - val_r2_inet_coefficient_loss: 0.0017 - val_r2_inet_lambda_fv_loss: -0.8666 - val_mae_inet_coefficient_loss: 1.8909 - val_mae_inet_lambda_fv_loss: 0.0523\n",
      "Epoch 11/500\n",
      "35/35 [==============================] - 19s 545ms/step - loss: 0.0550 - r2_inet_coefficient_loss: 0.0021 - r2_inet_lambda_fv_loss: -0.8543 - mae_inet_coefficient_loss: 1.7624 - mae_inet_lambda_fv_loss: 0.0550 - val_loss: 0.0526 - val_r2_inet_coefficient_loss: 0.0015 - val_r2_inet_lambda_fv_loss: -0.8650 - val_mae_inet_coefficient_loss: 1.8900 - val_mae_inet_lambda_fv_loss: 0.0528\n",
      "Epoch 12/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0541 - r2_inet_coefficient_loss: 0.0020 - r2_inet_lambda_fv_loss: -0.8581 - mae_inet_coefficient_loss: 1.7604 - mae_inet_lambda_fv_loss: 0.0541 - val_loss: 0.0520 - val_r2_inet_coefficient_loss: 0.0013 - val_r2_inet_lambda_fv_loss: -0.8712 - val_mae_inet_coefficient_loss: 1.8855 - val_mae_inet_lambda_fv_loss: 0.0522\n",
      "Epoch 13/500\n",
      "35/35 [==============================] - 19s 548ms/step - loss: 0.0541 - r2_inet_coefficient_loss: 0.0018 - r2_inet_lambda_fv_loss: -0.8599 - mae_inet_coefficient_loss: 1.7577 - mae_inet_lambda_fv_loss: 0.0541 - val_loss: 0.0566 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.8593 - val_mae_inet_coefficient_loss: 1.8799 - val_mae_inet_lambda_fv_loss: 0.0567\n",
      "Epoch 14/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0551 - r2_inet_coefficient_loss: 0.0018 - r2_inet_lambda_fv_loss: -0.8563 - mae_inet_coefficient_loss: 1.7551 - mae_inet_lambda_fv_loss: 0.0551 - val_loss: 0.0498 - val_r2_inet_coefficient_loss: 0.0015 - val_r2_inet_lambda_fv_loss: -0.8761 - val_mae_inet_coefficient_loss: 1.8838 - val_mae_inet_lambda_fv_loss: 0.0500\n",
      "Epoch 15/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0512 - r2_inet_coefficient_loss: 0.0017 - r2_inet_lambda_fv_loss: -0.8666 - mae_inet_coefficient_loss: 1.7542 - mae_inet_lambda_fv_loss: 0.0512 - val_loss: 0.0488 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.8811 - val_mae_inet_coefficient_loss: 1.8828 - val_mae_inet_lambda_fv_loss: 0.0490\n",
      "Epoch 16/500\n",
      "35/35 [==============================] - 19s 544ms/step - loss: 0.0505 - r2_inet_coefficient_loss: 0.0016 - r2_inet_lambda_fv_loss: -0.8692 - mae_inet_coefficient_loss: 1.7544 - mae_inet_lambda_fv_loss: 0.0505 - val_loss: 0.0487 - val_r2_inet_coefficient_loss: 0.0012 - val_r2_inet_lambda_fv_loss: -0.8782 - val_mae_inet_coefficient_loss: 1.8830 - val_mae_inet_lambda_fv_loss: 0.0489\n",
      "Epoch 17/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0511 - r2_inet_coefficient_loss: 0.0017 - r2_inet_lambda_fv_loss: -0.8698 - mae_inet_coefficient_loss: 1.7556 - mae_inet_lambda_fv_loss: 0.0511 - val_loss: 0.0526 - val_r2_inet_coefficient_loss: 0.0012 - val_r2_inet_lambda_fv_loss: -0.8789 - val_mae_inet_coefficient_loss: 1.8839 - val_mae_inet_lambda_fv_loss: 0.0528\n",
      "Epoch 18/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0521 - r2_inet_coefficient_loss: 0.0016 - r2_inet_lambda_fv_loss: -0.8700 - mae_inet_coefficient_loss: 1.7555 - mae_inet_lambda_fv_loss: 0.0521 - val_loss: 0.0464 - val_r2_inet_coefficient_loss: 0.0013 - val_r2_inet_lambda_fv_loss: -0.8843 - val_mae_inet_coefficient_loss: 1.8848 - val_mae_inet_lambda_fv_loss: 0.0466\n",
      "Epoch 19/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0481 - r2_inet_coefficient_loss: 0.0015 - r2_inet_lambda_fv_loss: -0.8772 - mae_inet_coefficient_loss: 1.7576 - mae_inet_lambda_fv_loss: 0.0481 - val_loss: 0.0482 - val_r2_inet_coefficient_loss: 0.0014 - val_r2_inet_lambda_fv_loss: -0.8799 - val_mae_inet_coefficient_loss: 1.8888 - val_mae_inet_lambda_fv_loss: 0.0483\n",
      "Epoch 20/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0476 - r2_inet_coefficient_loss: 0.0015 - r2_inet_lambda_fv_loss: -0.8809 - mae_inet_coefficient_loss: 1.7605 - mae_inet_lambda_fv_loss: 0.0476 - val_loss: 0.0460 - val_r2_inet_coefficient_loss: 0.0013 - val_r2_inet_lambda_fv_loss: -0.8846 - val_mae_inet_coefficient_loss: 1.8878 - val_mae_inet_lambda_fv_loss: 0.0462\n",
      "Epoch 21/500\n",
      "35/35 [==============================] - 19s 548ms/step - loss: 0.0477 - r2_inet_coefficient_loss: 0.0016 - r2_inet_lambda_fv_loss: -0.8802 - mae_inet_coefficient_loss: 1.7625 - mae_inet_lambda_fv_loss: 0.0477 - val_loss: 0.0461 - val_r2_inet_coefficient_loss: 0.0014 - val_r2_inet_lambda_fv_loss: -0.8840 - val_mae_inet_coefficient_loss: 1.8897 - val_mae_inet_lambda_fv_loss: 0.0462\n",
      "Epoch 22/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0455 - r2_inet_coefficient_loss: 0.0017 - r2_inet_lambda_fv_loss: -0.8851 - mae_inet_coefficient_loss: 1.7647 - mae_inet_lambda_fv_loss: 0.0455 - val_loss: 0.0457 - val_r2_inet_coefficient_loss: 0.0014 - val_r2_inet_lambda_fv_loss: -0.8872 - val_mae_inet_coefficient_loss: 1.8906 - val_mae_inet_lambda_fv_loss: 0.0458\n",
      "Epoch 23/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0473 - r2_inet_coefficient_loss: 0.0019 - r2_inet_lambda_fv_loss: -0.8830 - mae_inet_coefficient_loss: 1.7671 - mae_inet_lambda_fv_loss: 0.0473 - val_loss: 0.0452 - val_r2_inet_coefficient_loss: 0.0015 - val_r2_inet_lambda_fv_loss: -0.8839 - val_mae_inet_coefficient_loss: 1.8922 - val_mae_inet_lambda_fv_loss: 0.0453\n",
      "Epoch 24/500\n",
      "35/35 [==============================] - 19s 549ms/step - loss: 0.0487 - r2_inet_coefficient_loss: 0.0019 - r2_inet_lambda_fv_loss: -0.8800 - mae_inet_coefficient_loss: 1.7680 - mae_inet_lambda_fv_loss: 0.0487 - val_loss: 0.0477 - val_r2_inet_coefficient_loss: 0.0023 - val_r2_inet_lambda_fv_loss: -0.8887 - val_mae_inet_coefficient_loss: 1.8975 - val_mae_inet_lambda_fv_loss: 0.0478\n",
      "Epoch 25/500\n",
      "35/35 [==============================] - 19s 550ms/step - loss: 0.0466 - r2_inet_coefficient_loss: 0.0024 - r2_inet_lambda_fv_loss: -0.8859 - mae_inet_coefficient_loss: 1.7706 - mae_inet_lambda_fv_loss: 0.0466 - val_loss: 0.0439 - val_r2_inet_coefficient_loss: 0.0020 - val_r2_inet_lambda_fv_loss: -0.8896 - val_mae_inet_coefficient_loss: 1.8972 - val_mae_inet_lambda_fv_loss: 0.0440\n",
      "Epoch 26/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0438 - r2_inet_coefficient_loss: 0.0024 - r2_inet_lambda_fv_loss: -0.8923 - mae_inet_coefficient_loss: 1.7739 - mae_inet_lambda_fv_loss: 0.0438 - val_loss: 0.0421 - val_r2_inet_coefficient_loss: 0.0019 - val_r2_inet_lambda_fv_loss: -0.8903 - val_mae_inet_coefficient_loss: 1.8989 - val_mae_inet_lambda_fv_loss: 0.0423\n",
      "Epoch 27/500\n",
      "35/35 [==============================] - 19s 547ms/step - loss: 0.0448 - r2_inet_coefficient_loss: 0.0026 - r2_inet_lambda_fv_loss: -0.8926 - mae_inet_coefficient_loss: 1.7770 - mae_inet_lambda_fv_loss: 0.0448 - val_loss: 0.0457 - val_r2_inet_coefficient_loss: 0.0024 - val_r2_inet_lambda_fv_loss: -0.8913 - val_mae_inet_coefficient_loss: 1.9034 - val_mae_inet_lambda_fv_loss: 0.0458\n",
      "Epoch 28/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0439 - r2_inet_coefficient_loss: 0.0030 - r2_inet_lambda_fv_loss: -0.8906 - mae_inet_coefficient_loss: 1.7804 - mae_inet_lambda_fv_loss: 0.0439 - val_loss: 0.0408 - val_r2_inet_coefficient_loss: 0.0024 - val_r2_inet_lambda_fv_loss: -0.8968 - val_mae_inet_coefficient_loss: 1.9033 - val_mae_inet_lambda_fv_loss: 0.0409\n",
      "Epoch 29/500\n",
      "35/35 [==============================] - 19s 549ms/step - loss: 0.0416 - r2_inet_coefficient_loss: 0.0035 - r2_inet_lambda_fv_loss: -0.8920 - mae_inet_coefficient_loss: 1.7865 - mae_inet_lambda_fv_loss: 0.0416 - val_loss: 0.0438 - val_r2_inet_coefficient_loss: 0.0027 - val_r2_inet_lambda_fv_loss: -0.8812 - val_mae_inet_coefficient_loss: 1.9101 - val_mae_inet_lambda_fv_loss: 0.0439\n",
      "Epoch 30/500\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0401 - r2_inet_coefficient_loss: 0.0040 - r2_inet_lambda_fv_loss: -0.8983 - mae_inet_coefficient_loss: 1.7919 - mae_inet_lambda_fv_loss: 0.0401 - val_loss: 0.0389 - val_r2_inet_coefficient_loss: 0.0028 - val_r2_inet_lambda_fv_loss: -0.8913 - val_mae_inet_coefficient_loss: 1.9140 - val_mae_inet_lambda_fv_loss: 0.0390\n",
      "Epoch 31/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0388 - r2_inet_coefficient_loss: 0.0042 - r2_inet_lambda_fv_loss: -0.9024 - mae_inet_coefficient_loss: 1.7949 - mae_inet_lambda_fv_loss: 0.0388 - val_loss: 0.0399 - val_r2_inet_coefficient_loss: 0.0025 - val_r2_inet_lambda_fv_loss: -0.8867 - val_mae_inet_coefficient_loss: 1.9103 - val_mae_inet_lambda_fv_loss: 0.0400\n",
      "Epoch 32/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0394 - r2_inet_coefficient_loss: 0.0040 - r2_inet_lambda_fv_loss: -0.9007 - mae_inet_coefficient_loss: 1.7926 - mae_inet_lambda_fv_loss: 0.0394 - val_loss: 0.0407 - val_r2_inet_coefficient_loss: 0.0020 - val_r2_inet_lambda_fv_loss: -0.8958 - val_mae_inet_coefficient_loss: 1.9093 - val_mae_inet_lambda_fv_loss: 0.0408\n",
      "Epoch 33/500\n",
      "35/35 [==============================] - 19s 550ms/step - loss: 0.0385 - r2_inet_coefficient_loss: 0.0036 - r2_inet_lambda_fv_loss: -0.9023 - mae_inet_coefficient_loss: 1.7926 - mae_inet_lambda_fv_loss: 0.0385 - val_loss: 0.0397 - val_r2_inet_coefficient_loss: 0.0018 - val_r2_inet_lambda_fv_loss: -0.8956 - val_mae_inet_coefficient_loss: 1.9086 - val_mae_inet_lambda_fv_loss: 0.0397\n",
      "Epoch 34/500\n",
      "35/35 [==============================] - 19s 545ms/step - loss: 0.0382 - r2_inet_coefficient_loss: 0.0030 - r2_inet_lambda_fv_loss: -0.9045 - mae_inet_coefficient_loss: 1.7927 - mae_inet_lambda_fv_loss: 0.0382 - val_loss: 0.0374 - val_r2_inet_coefficient_loss: 0.0015 - val_r2_inet_lambda_fv_loss: -0.9011 - val_mae_inet_coefficient_loss: 1.9105 - val_mae_inet_lambda_fv_loss: 0.0375\n",
      "Epoch 35/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0383 - r2_inet_coefficient_loss: 0.0025 - r2_inet_lambda_fv_loss: -0.9031 - mae_inet_coefficient_loss: 1.7929 - mae_inet_lambda_fv_loss: 0.0383 - val_loss: 0.0374 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.9026 - val_mae_inet_coefficient_loss: 1.9099 - val_mae_inet_lambda_fv_loss: 0.0375\n",
      "Epoch 36/500\n",
      "35/35 [==============================] - 19s 548ms/step - loss: 0.0384 - r2_inet_coefficient_loss: 0.0018 - r2_inet_lambda_fv_loss: -0.9035 - mae_inet_coefficient_loss: 1.7914 - mae_inet_lambda_fv_loss: 0.0384 - val_loss: 0.0432 - val_r2_inet_coefficient_loss: 0.0011 - val_r2_inet_lambda_fv_loss: -0.8971 - val_mae_inet_coefficient_loss: 1.9177 - val_mae_inet_lambda_fv_loss: 0.0433\n",
      "Epoch 37/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0386 - r2_inet_coefficient_loss: 0.0015 - r2_inet_lambda_fv_loss: -0.9027 - mae_inet_coefficient_loss: 1.7936 - mae_inet_lambda_fv_loss: 0.0386 - val_loss: 0.0353 - val_r2_inet_coefficient_loss: 4.0995e-04 - val_r2_inet_lambda_fv_loss: -0.9049 - val_mae_inet_coefficient_loss: 1.9153 - val_mae_inet_lambda_fv_loss: 0.0354\n",
      "Epoch 38/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0349 - r2_inet_coefficient_loss: 0.0014 - r2_inet_lambda_fv_loss: -0.9104 - mae_inet_coefficient_loss: 1.7944 - mae_inet_lambda_fv_loss: 0.0349 - val_loss: 0.0378 - val_r2_inet_coefficient_loss: 5.9338e-04 - val_r2_inet_lambda_fv_loss: -0.9015 - val_mae_inet_coefficient_loss: 1.9150 - val_mae_inet_lambda_fv_loss: 0.0379\n",
      "Epoch 39/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0357 - r2_inet_coefficient_loss: 0.0013 - r2_inet_lambda_fv_loss: -0.9072 - mae_inet_coefficient_loss: 1.7955 - mae_inet_lambda_fv_loss: 0.0357 - val_loss: 0.0347 - val_r2_inet_coefficient_loss: 5.8500e-04 - val_r2_inet_lambda_fv_loss: -0.9048 - val_mae_inet_coefficient_loss: 1.9162 - val_mae_inet_lambda_fv_loss: 0.0348\n",
      "Epoch 40/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0348 - r2_inet_coefficient_loss: 0.0013 - r2_inet_lambda_fv_loss: -0.9095 - mae_inet_coefficient_loss: 1.7954 - mae_inet_lambda_fv_loss: 0.0348 - val_loss: 0.0345 - val_r2_inet_coefficient_loss: 3.1525e-04 - val_r2_inet_lambda_fv_loss: -0.9084 - val_mae_inet_coefficient_loss: 1.9155 - val_mae_inet_lambda_fv_loss: 0.0346\n",
      "Epoch 41/500\n",
      "35/35 [==============================] - 19s 540ms/step - loss: 0.0355 - r2_inet_coefficient_loss: 0.0011 - r2_inet_lambda_fv_loss: -0.9058 - mae_inet_coefficient_loss: 1.7964 - mae_inet_lambda_fv_loss: 0.0355 - val_loss: 0.0346 - val_r2_inet_coefficient_loss: 4.5162e-04 - val_r2_inet_lambda_fv_loss: -0.9031 - val_mae_inet_coefficient_loss: 1.9204 - val_mae_inet_lambda_fv_loss: 0.0347\n",
      "Epoch 42/500\n",
      "35/35 [==============================] - 19s 543ms/step - loss: 0.0361 - r2_inet_coefficient_loss: 0.0010 - r2_inet_lambda_fv_loss: -0.9089 - mae_inet_coefficient_loss: 1.7969 - mae_inet_lambda_fv_loss: 0.0361 - val_loss: 0.0354 - val_r2_inet_coefficient_loss: -1.3787e-04 - val_r2_inet_lambda_fv_loss: -0.9062 - val_mae_inet_coefficient_loss: 1.9152 - val_mae_inet_lambda_fv_loss: 0.0355\n",
      "Epoch 43/500\n",
      "35/35 [==============================] - 19s 542ms/step - loss: 0.0354 - r2_inet_coefficient_loss: 7.0966e-04 - r2_inet_lambda_fv_loss: -0.9108 - mae_inet_coefficient_loss: 1.7966 - mae_inet_lambda_fv_loss: 0.0354 - val_loss: 0.0377 - val_r2_inet_coefficient_loss: -1.3085e-04 - val_r2_inet_lambda_fv_loss: -0.8935 - val_mae_inet_coefficient_loss: 1.9161 - val_mae_inet_lambda_fv_loss: 0.0378\n",
      "Epoch 44/500\n",
      "35/35 [==============================] - 16s 456ms/step - loss: 0.0355 - r2_inet_coefficient_loss: 5.9897e-04 - r2_inet_lambda_fv_loss: -0.9111 - mae_inet_coefficient_loss: 1.7973 - mae_inet_lambda_fv_loss: 0.0355 - val_loss: 0.0354 - val_r2_inet_coefficient_loss: -8.3801e-04 - val_r2_inet_lambda_fv_loss: -0.9019 - val_mae_inet_coefficient_loss: 1.9146 - val_mae_inet_lambda_fv_loss: 0.0355\n",
      "Epoch 45/500\n",
      "35/35 [==============================] - 18s 522ms/step - loss: 0.0341 - r2_inet_coefficient_loss: 5.2782e-04 - r2_inet_lambda_fv_loss: -0.9095 - mae_inet_coefficient_loss: 1.7990 - mae_inet_lambda_fv_loss: 0.0341 - val_loss: 0.0353 - val_r2_inet_coefficient_loss: 1.4675e-04 - val_r2_inet_lambda_fv_loss: -0.8884 - val_mae_inet_coefficient_loss: 1.9222 - val_mae_inet_lambda_fv_loss: 0.0353\n",
      "Epoch 46/500\n",
      "35/35 [==============================] - 18s 509ms/step - loss: 0.0331 - r2_inet_coefficient_loss: 6.7072e-04 - r2_inet_lambda_fv_loss: -0.9139 - mae_inet_coefficient_loss: 1.7998 - mae_inet_lambda_fv_loss: 0.0331 - val_loss: 0.0339 - val_r2_inet_coefficient_loss: -8.5235e-06 - val_r2_inet_lambda_fv_loss: -0.9088 - val_mae_inet_coefficient_loss: 1.9200 - val_mae_inet_lambda_fv_loss: 0.0340\n",
      "Epoch 47/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0349 - r2_inet_coefficient_loss: 6.2166e-04 - r2_inet_lambda_fv_loss: -0.9098 - mae_inet_coefficient_loss: 1.7994 - mae_inet_lambda_fv_loss: 0.0349 - val_loss: 0.0329 - val_r2_inet_coefficient_loss: 1.5882e-04 - val_r2_inet_lambda_fv_loss: -0.9051 - val_mae_inet_coefficient_loss: 1.9210 - val_mae_inet_lambda_fv_loss: 0.0330\n",
      "Epoch 48/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0326 - r2_inet_coefficient_loss: 6.1395e-04 - r2_inet_lambda_fv_loss: -0.9140 - mae_inet_coefficient_loss: 1.8003 - mae_inet_lambda_fv_loss: 0.0326 - val_loss: 0.0371 - val_r2_inet_coefficient_loss: -2.1474e-04 - val_r2_inet_lambda_fv_loss: -0.9033 - val_mae_inet_coefficient_loss: 1.9227 - val_mae_inet_lambda_fv_loss: 0.0372\n",
      "Epoch 49/500\n",
      "35/35 [==============================] - 19s 549ms/step - loss: 0.0357 - r2_inet_coefficient_loss: 3.1155e-04 - r2_inet_lambda_fv_loss: -0.9092 - mae_inet_coefficient_loss: 1.7995 - mae_inet_lambda_fv_loss: 0.0357 - val_loss: 0.0373 - val_r2_inet_coefficient_loss: -5.3303e-04 - val_r2_inet_lambda_fv_loss: -0.8889 - val_mae_inet_coefficient_loss: 1.9167 - val_mae_inet_lambda_fv_loss: 0.0373\n",
      "Epoch 50/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0350 - r2_inet_coefficient_loss: 2.6028e-04 - r2_inet_lambda_fv_loss: -0.9104 - mae_inet_coefficient_loss: 1.8001 - mae_inet_lambda_fv_loss: 0.0350 - val_loss: 0.0346 - val_r2_inet_coefficient_loss: -1.8236e-04 - val_r2_inet_lambda_fv_loss: -0.8955 - val_mae_inet_coefficient_loss: 1.9220 - val_mae_inet_lambda_fv_loss: 0.0347\n",
      "Epoch 51/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0322 - r2_inet_coefficient_loss: 3.6638e-04 - r2_inet_lambda_fv_loss: -0.9165 - mae_inet_coefficient_loss: 1.8011 - mae_inet_lambda_fv_loss: 0.0322 - val_loss: 0.0365 - val_r2_inet_coefficient_loss: -1.5303e-04 - val_r2_inet_lambda_fv_loss: -0.9015 - val_mae_inet_coefficient_loss: 1.9240 - val_mae_inet_lambda_fv_loss: 0.0366\n",
      "Epoch 52/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0338 - r2_inet_coefficient_loss: 2.6360e-04 - r2_inet_lambda_fv_loss: -0.9127 - mae_inet_coefficient_loss: 1.8019 - mae_inet_lambda_fv_loss: 0.0338 - val_loss: 0.0333 - val_r2_inet_coefficient_loss: -5.9475e-04 - val_r2_inet_lambda_fv_loss: -0.9002 - val_mae_inet_coefficient_loss: 1.9211 - val_mae_inet_lambda_fv_loss: 0.0334\n",
      "Epoch 53/500\n",
      "35/35 [==============================] - 19s 548ms/step - loss: 0.0315 - r2_inet_coefficient_loss: 1.2869e-04 - r2_inet_lambda_fv_loss: -0.9179 - mae_inet_coefficient_loss: 1.8022 - mae_inet_lambda_fv_loss: 0.0315 - val_loss: 0.0326 - val_r2_inet_coefficient_loss: -2.1613e-04 - val_r2_inet_lambda_fv_loss: -0.9041 - val_mae_inet_coefficient_loss: 1.9243 - val_mae_inet_lambda_fv_loss: 0.0327\n",
      "Epoch 54/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0315 - r2_inet_coefficient_loss: 2.5646e-04 - r2_inet_lambda_fv_loss: -0.9135 - mae_inet_coefficient_loss: 1.8032 - mae_inet_lambda_fv_loss: 0.0315 - val_loss: 0.0318 - val_r2_inet_coefficient_loss: -8.3199e-04 - val_r2_inet_lambda_fv_loss: -0.9070 - val_mae_inet_coefficient_loss: 1.9205 - val_mae_inet_lambda_fv_loss: 0.0319\n",
      "Epoch 55/500\n",
      "16/35 [============>.................] - ETA: 9s - loss: 0.0312 - r2_inet_coefficient_loss: 4.4550e-04 - r2_inet_lambda_fv_loss: -0.9212 - mae_inet_coefficient_loss: 1.7322 - mae_inet_lambda_fv_loss: 0.0312 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0288 - r2_inet_coefficient_loss: 0.0194 - r2_inet_lambda_fv_loss: -0.9269 - mae_inet_coefficient_loss: 1.8311 - mae_inet_lambda_fv_loss: 0.0288 - val_loss: 0.0297 - val_r2_inet_coefficient_loss: 0.0170 - val_r2_inet_lambda_fv_loss: -0.9155 - val_mae_inet_coefficient_loss: 1.9536 - val_mae_inet_lambda_fv_loss: 0.0298\n",
      "Epoch 61/500\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0288 - r2_inet_coefficient_loss: 0.0193 - r2_inet_lambda_fv_loss: -0.9248 - mae_inet_coefficient_loss: 1.8318 - mae_inet_lambda_fv_loss: 0.0288 - val_loss: 0.0298 - val_r2_inet_coefficient_loss: 0.0169 - val_r2_inet_lambda_fv_loss: -0.9142 - val_mae_inet_coefficient_loss: 1.9515 - val_mae_inet_lambda_fv_loss: 0.0299\n",
      "Epoch 62/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0279 - r2_inet_coefficient_loss: 0.0194 - r2_inet_lambda_fv_loss: -0.9291 - mae_inet_coefficient_loss: 1.8339 - mae_inet_lambda_fv_loss: 0.0279 - val_loss: 0.0302 - val_r2_inet_coefficient_loss: 0.0168 - val_r2_inet_lambda_fv_loss: -0.9168 - val_mae_inet_coefficient_loss: 1.9516 - val_mae_inet_lambda_fv_loss: 0.0303\n",
      "Epoch 63/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0299 - r2_inet_coefficient_loss: 0.0192 - r2_inet_lambda_fv_loss: -0.9223 - mae_inet_coefficient_loss: 1.8312 - mae_inet_lambda_fv_loss: 0.0299 - val_loss: 0.0286 - val_r2_inet_coefficient_loss: 0.0168 - val_r2_inet_lambda_fv_loss: -0.9221 - val_mae_inet_coefficient_loss: 1.9537 - val_mae_inet_lambda_fv_loss: 0.0287\n",
      "Epoch 64/500\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0291 - r2_inet_coefficient_loss: 0.0192 - r2_inet_lambda_fv_loss: -0.9254 - mae_inet_coefficient_loss: 1.8323 - mae_inet_lambda_fv_loss: 0.0291 - val_loss: 0.0329 - val_r2_inet_coefficient_loss: 0.0162 - val_r2_inet_lambda_fv_loss: -0.9151 - val_mae_inet_coefficient_loss: 1.9456 - val_mae_inet_lambda_fv_loss: 0.0330\n",
      "Epoch 65/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0320 - r2_inet_coefficient_loss: 0.0190 - r2_inet_lambda_fv_loss: -0.9238 - mae_inet_coefficient_loss: 1.8290 - mae_inet_lambda_fv_loss: 0.0320 - val_loss: 0.0354 - val_r2_inet_coefficient_loss: 0.0158 - val_r2_inet_lambda_fv_loss: -0.9104 - val_mae_inet_coefficient_loss: 1.9426 - val_mae_inet_lambda_fv_loss: 0.0355\n",
      "Epoch 66/500\n",
      "35/35 [==============================] - 20s 573ms/step - loss: 0.0348 - r2_inet_coefficient_loss: 0.0186 - r2_inet_lambda_fv_loss: -0.9199 - mae_inet_coefficient_loss: 1.8255 - mae_inet_lambda_fv_loss: 0.0348 - val_loss: 0.0323 - val_r2_inet_coefficient_loss: 0.0169 - val_r2_inet_lambda_fv_loss: -0.9090 - val_mae_inet_coefficient_loss: 1.9542 - val_mae_inet_lambda_fv_loss: 0.0324\n",
      "Epoch 67/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0301 - r2_inet_coefficient_loss: 0.0194 - r2_inet_lambda_fv_loss: -0.9244 - mae_inet_coefficient_loss: 1.8333 - mae_inet_lambda_fv_loss: 0.0301 - val_loss: 0.0349 - val_r2_inet_coefficient_loss: 0.0167 - val_r2_inet_lambda_fv_loss: -0.9088 - val_mae_inet_coefficient_loss: 1.9504 - val_mae_inet_lambda_fv_loss: 0.0350\n",
      "Epoch 68/500\n",
      "35/35 [==============================] - 20s 569ms/step - loss: 0.0311 - r2_inet_coefficient_loss: 0.0194 - r2_inet_lambda_fv_loss: -0.9271 - mae_inet_coefficient_loss: 1.8337 - mae_inet_lambda_fv_loss: 0.0311 - val_loss: 0.0305 - val_r2_inet_coefficient_loss: 0.0168 - val_r2_inet_lambda_fv_loss: -0.9157 - val_mae_inet_coefficient_loss: 1.9535 - val_mae_inet_lambda_fv_loss: 0.0306\n",
      "Epoch 69/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0277 - r2_inet_coefficient_loss: 0.0193 - r2_inet_lambda_fv_loss: -0.9266 - mae_inet_coefficient_loss: 1.8349 - mae_inet_lambda_fv_loss: 0.0277 - val_loss: 0.0275 - val_r2_inet_coefficient_loss: 0.0171 - val_r2_inet_lambda_fv_loss: -0.9203 - val_mae_inet_coefficient_loss: 1.9592 - val_mae_inet_lambda_fv_loss: 0.0276\n",
      "Epoch 70/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0263 - r2_inet_coefficient_loss: 0.0194 - r2_inet_lambda_fv_loss: -0.9296 - mae_inet_coefficient_loss: 1.8375 - mae_inet_lambda_fv_loss: 0.0263 - val_loss: 0.0324 - val_r2_inet_coefficient_loss: 0.0172 - val_r2_inet_lambda_fv_loss: -0.9128 - val_mae_inet_coefficient_loss: 1.9578 - val_mae_inet_lambda_fv_loss: 0.0325\n",
      "Epoch 71/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0264 - r2_inet_coefficient_loss: 0.0196 - r2_inet_lambda_fv_loss: -0.9342 - mae_inet_coefficient_loss: 1.8378 - mae_inet_lambda_fv_loss: 0.0264 - val_loss: 0.0285 - val_r2_inet_coefficient_loss: 0.0166 - val_r2_inet_lambda_fv_loss: -0.9132 - val_mae_inet_coefficient_loss: 1.9541 - val_mae_inet_lambda_fv_loss: 0.0286\n",
      "Epoch 72/500\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0260 - r2_inet_coefficient_loss: 0.0192 - r2_inet_lambda_fv_loss: -0.9338 - mae_inet_coefficient_loss: 1.8371 - mae_inet_lambda_fv_loss: 0.0260 - val_loss: 0.0303 - val_r2_inet_coefficient_loss: 0.0163 - val_r2_inet_lambda_fv_loss: -0.9193 - val_mae_inet_coefficient_loss: 1.9528 - val_mae_inet_lambda_fv_loss: 0.0303\n",
      "Epoch 73/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0293 - r2_inet_coefficient_loss: 0.0191 - r2_inet_lambda_fv_loss: -0.9310 - mae_inet_coefficient_loss: 1.8362 - mae_inet_lambda_fv_loss: 0.0293 - val_loss: 0.0302 - val_r2_inet_coefficient_loss: 0.0165 - val_r2_inet_lambda_fv_loss: -0.9090 - val_mae_inet_coefficient_loss: 1.9557 - val_mae_inet_lambda_fv_loss: 0.0303\n",
      "Epoch 74/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0264 - r2_inet_coefficient_loss: 0.0191 - r2_inet_lambda_fv_loss: -0.9347 - mae_inet_coefficient_loss: 1.8370 - mae_inet_lambda_fv_loss: 0.0264 - val_loss: 0.0292 - val_r2_inet_coefficient_loss: 0.0164 - val_r2_inet_lambda_fv_loss: -0.9171 - val_mae_inet_coefficient_loss: 1.9546 - val_mae_inet_lambda_fv_loss: 0.0293\n",
      "Epoch 75/500\n",
      "35/35 [==============================] - 20s 564ms/step - loss: 0.0262 - r2_inet_coefficient_loss: 0.0190 - r2_inet_lambda_fv_loss: -0.9327 - mae_inet_coefficient_loss: 1.8374 - mae_inet_lambda_fv_loss: 0.0262 - val_loss: 0.0273 - val_r2_inet_coefficient_loss: 0.0167 - val_r2_inet_lambda_fv_loss: -0.9206 - val_mae_inet_coefficient_loss: 1.9576 - val_mae_inet_lambda_fv_loss: 0.0274\n",
      "Epoch 76/500\n",
      "35/35 [==============================] - 20s 565ms/step - loss: 0.0261 - r2_inet_coefficient_loss: 0.0191 - r2_inet_lambda_fv_loss: -0.9312 - mae_inet_coefficient_loss: 1.8380 - mae_inet_lambda_fv_loss: 0.0261 - val_loss: 0.0318 - val_r2_inet_coefficient_loss: 0.0168 - val_r2_inet_lambda_fv_loss: -0.9201 - val_mae_inet_coefficient_loss: 1.9583 - val_mae_inet_lambda_fv_loss: 0.0319\n",
      "Epoch 77/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0285 - r2_inet_coefficient_loss: 0.0190 - r2_inet_lambda_fv_loss: -0.9300 - mae_inet_coefficient_loss: 1.8363 - mae_inet_lambda_fv_loss: 0.0285 - val_loss: 0.0360 - val_r2_inet_coefficient_loss: 0.0160 - val_r2_inet_lambda_fv_loss: -0.8994 - val_mae_inet_coefficient_loss: 1.9495 - val_mae_inet_lambda_fv_loss: 0.0361\n",
      "Epoch 78/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0335 - r2_inet_coefficient_loss: 0.0185 - r2_inet_lambda_fv_loss: -0.9200 - mae_inet_coefficient_loss: 1.8316 - mae_inet_lambda_fv_loss: 0.0335 - val_loss: 0.0315 - val_r2_inet_coefficient_loss: 0.0162 - val_r2_inet_lambda_fv_loss: -0.9164 - val_mae_inet_coefficient_loss: 1.9507 - val_mae_inet_lambda_fv_loss: 0.0317\n",
      "Epoch 79/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0296 - r2_inet_coefficient_loss: 0.0189 - r2_inet_lambda_fv_loss: -0.9282 - mae_inet_coefficient_loss: 1.8352 - mae_inet_lambda_fv_loss: 0.0296 - val_loss: 0.0377 - val_r2_inet_coefficient_loss: 0.0159 - val_r2_inet_lambda_fv_loss: -0.9166 - val_mae_inet_coefficient_loss: 1.9464 - val_mae_inet_lambda_fv_loss: 0.0378\n",
      "Epoch 80/500\n",
      "35/35 [==============================] - 20s 575ms/step - loss: 0.0324 - r2_inet_coefficient_loss: 0.0186 - r2_inet_lambda_fv_loss: -0.9281 - mae_inet_coefficient_loss: 1.8336 - mae_inet_lambda_fv_loss: 0.0324 - val_loss: 0.0303 - val_r2_inet_coefficient_loss: 0.0155 - val_r2_inet_lambda_fv_loss: -0.9171 - val_mae_inet_coefficient_loss: 1.9475 - val_mae_inet_lambda_fv_loss: 0.0305\n",
      "Epoch 81/500\n",
      "35/35 [==============================] - 20s 565ms/step - loss: 0.0266 - r2_inet_coefficient_loss: 0.0185 - r2_inet_lambda_fv_loss: -0.9299 - mae_inet_coefficient_loss: 1.8341 - mae_inet_lambda_fv_loss: 0.0266 - val_loss: 0.0276 - val_r2_inet_coefficient_loss: 0.0163 - val_r2_inet_lambda_fv_loss: -0.9129 - val_mae_inet_coefficient_loss: 1.9548 - val_mae_inet_lambda_fv_loss: 0.0277\n",
      "Epoch 82/500\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0250 - r2_inet_coefficient_loss: 0.0187 - r2_inet_lambda_fv_loss: -0.9354 - mae_inet_coefficient_loss: 1.8369 - mae_inet_lambda_fv_loss: 0.0250 - val_loss: 0.0303 - val_r2_inet_coefficient_loss: 0.0168 - val_r2_inet_lambda_fv_loss: -0.9124 - val_mae_inet_coefficient_loss: 1.9605 - val_mae_inet_lambda_fv_loss: 0.0304\n",
      "Epoch 83/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0262 - r2_inet_coefficient_loss: 0.0186 - r2_inet_lambda_fv_loss: -0.9340 - mae_inet_coefficient_loss: 1.8357 - mae_inet_lambda_fv_loss: 0.0262 - val_loss: 0.0305 - val_r2_inet_coefficient_loss: 0.0159 - val_r2_inet_lambda_fv_loss: -0.9088 - val_mae_inet_coefficient_loss: 1.9530 - val_mae_inet_lambda_fv_loss: 0.0307\n",
      "Epoch 84/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0294 - r2_inet_coefficient_loss: 0.0183 - r2_inet_lambda_fv_loss: -0.9281 - mae_inet_coefficient_loss: 1.8327 - mae_inet_lambda_fv_loss: 0.0294 - val_loss: 0.0294 - val_r2_inet_coefficient_loss: 0.0161 - val_r2_inet_lambda_fv_loss: -0.9096 - val_mae_inet_coefficient_loss: 1.9539 - val_mae_inet_lambda_fv_loss: 0.0295\n",
      "Epoch 85/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0251 - r2_inet_coefficient_loss: 0.0186 - r2_inet_lambda_fv_loss: -0.9355 - mae_inet_coefficient_loss: 1.8372 - mae_inet_lambda_fv_loss: 0.0251 - val_loss: 0.0283 - val_r2_inet_coefficient_loss: 0.0162 - val_r2_inet_lambda_fv_loss: -0.9092 - val_mae_inet_coefficient_loss: 1.9556 - val_mae_inet_lambda_fv_loss: 0.0284\n",
      "Epoch 86/500\n",
      "35/35 [==============================] - 20s 565ms/step - loss: 0.0262 - r2_inet_coefficient_loss: 0.0187 - r2_inet_lambda_fv_loss: -0.9318 - mae_inet_coefficient_loss: 1.8382 - mae_inet_lambda_fv_loss: 0.0262 - val_loss: 0.0368 - val_r2_inet_coefficient_loss: 0.0156 - val_r2_inet_lambda_fv_loss: -0.9105 - val_mae_inet_coefficient_loss: 1.9501 - val_mae_inet_lambda_fv_loss: 0.0369\n",
      "Epoch 87/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0338 - r2_inet_coefficient_loss: 0.0180 - r2_inet_lambda_fv_loss: -0.9276 - mae_inet_coefficient_loss: 1.8304 - mae_inet_lambda_fv_loss: 0.0338 - val_loss: 0.0317 - val_r2_inet_coefficient_loss: 0.0156 - val_r2_inet_lambda_fv_loss: -0.9171 - val_mae_inet_coefficient_loss: 1.9496 - val_mae_inet_lambda_fv_loss: 0.0318\n",
      "Epoch 88/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0271 - r2_inet_coefficient_loss: 0.0183 - r2_inet_lambda_fv_loss: -0.9317 - mae_inet_coefficient_loss: 1.8361 - mae_inet_lambda_fv_loss: 0.0271 - val_loss: 0.0268 - val_r2_inet_coefficient_loss: 0.0162 - val_r2_inet_lambda_fv_loss: -0.9185 - val_mae_inet_coefficient_loss: 1.9584 - val_mae_inet_lambda_fv_loss: 0.0269\n",
      "Epoch 89/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0243 - r2_inet_coefficient_loss: 0.0184 - r2_inet_lambda_fv_loss: -0.9358 - mae_inet_coefficient_loss: 1.8368 - mae_inet_lambda_fv_loss: 0.0243 - val_loss: 0.0264 - val_r2_inet_coefficient_loss: 0.0166 - val_r2_inet_lambda_fv_loss: -0.9176 - val_mae_inet_coefficient_loss: 1.9612 - val_mae_inet_lambda_fv_loss: 0.0265\n",
      "Epoch 90/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0242 - r2_inet_coefficient_loss: 0.0183 - r2_inet_lambda_fv_loss: -0.9367 - mae_inet_coefficient_loss: 1.8368 - mae_inet_lambda_fv_loss: 0.0242 - val_loss: 0.0269 - val_r2_inet_coefficient_loss: 0.0166 - val_r2_inet_lambda_fv_loss: -0.9192 - val_mae_inet_coefficient_loss: 1.9627 - val_mae_inet_lambda_fv_loss: 0.0270\n",
      "Epoch 91/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0242 - r2_inet_coefficient_loss: 0.0185 - r2_inet_lambda_fv_loss: -0.9384 - mae_inet_coefficient_loss: 1.8387 - mae_inet_lambda_fv_loss: 0.0242 - val_loss: 0.0275 - val_r2_inet_coefficient_loss: 0.0158 - val_r2_inet_lambda_fv_loss: -0.9152 - val_mae_inet_coefficient_loss: 1.9559 - val_mae_inet_lambda_fv_loss: 0.0277\n",
      "Epoch 92/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0260 - r2_inet_coefficient_loss: 0.0180 - r2_inet_lambda_fv_loss: -0.9374 - mae_inet_coefficient_loss: 1.8359 - mae_inet_lambda_fv_loss: 0.0260 - val_loss: 0.0341 - val_r2_inet_coefficient_loss: 0.0157 - val_r2_inet_lambda_fv_loss: -0.9127 - val_mae_inet_coefficient_loss: 1.9538 - val_mae_inet_lambda_fv_loss: 0.0342\n",
      "Epoch 93/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0282 - r2_inet_coefficient_loss: 0.0181 - r2_inet_lambda_fv_loss: -0.9330 - mae_inet_coefficient_loss: 1.8358 - mae_inet_lambda_fv_loss: 0.0282 - val_loss: 0.0274 - val_r2_inet_coefficient_loss: 0.0160 - val_r2_inet_lambda_fv_loss: -0.9123 - val_mae_inet_coefficient_loss: 1.9552 - val_mae_inet_lambda_fv_loss: 0.0275\n",
      "Epoch 94/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0241 - r2_inet_coefficient_loss: 0.0182 - r2_inet_lambda_fv_loss: -0.9390 - mae_inet_coefficient_loss: 1.8366 - mae_inet_lambda_fv_loss: 0.0241 - val_loss: 0.0267 - val_r2_inet_coefficient_loss: 0.0156 - val_r2_inet_lambda_fv_loss: -0.9126 - val_mae_inet_coefficient_loss: 1.9536 - val_mae_inet_lambda_fv_loss: 0.0268\n",
      "Epoch 95/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0248 - r2_inet_coefficient_loss: 0.0181 - r2_inet_lambda_fv_loss: -0.9336 - mae_inet_coefficient_loss: 1.8375 - mae_inet_lambda_fv_loss: 0.0248 - val_loss: 0.0282 - val_r2_inet_coefficient_loss: 0.0152 - val_r2_inet_lambda_fv_loss: -0.9119 - val_mae_inet_coefficient_loss: 1.9497 - val_mae_inet_lambda_fv_loss: 0.0283\n",
      "Epoch 96/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0251 - r2_inet_coefficient_loss: 0.0179 - r2_inet_lambda_fv_loss: -0.9381 - mae_inet_coefficient_loss: 1.8357 - mae_inet_lambda_fv_loss: 0.0251 - val_loss: 0.0288 - val_r2_inet_coefficient_loss: 0.0161 - val_r2_inet_lambda_fv_loss: -0.9181 - val_mae_inet_coefficient_loss: 1.9596 - val_mae_inet_lambda_fv_loss: 0.0289\n",
      "Epoch 97/500\n",
      "35/35 [==============================] - 19s 544ms/step - loss: 0.0248 - r2_inet_coefficient_loss: 0.0179 - r2_inet_lambda_fv_loss: -0.9385 - mae_inet_coefficient_loss: 1.8372 - mae_inet_lambda_fv_loss: 0.0248 - val_loss: 0.0286 - val_r2_inet_coefficient_loss: 0.0153 - val_r2_inet_lambda_fv_loss: -0.9162 - val_mae_inet_coefficient_loss: 1.9530 - val_mae_inet_lambda_fv_loss: 0.0287\n",
      "Epoch 98/500\n",
      "35/35 [==============================] - 20s 569ms/step - loss: 0.0265 - r2_inet_coefficient_loss: 0.0177 - r2_inet_lambda_fv_loss: -0.9340 - mae_inet_coefficient_loss: 1.8353 - mae_inet_lambda_fv_loss: 0.0265 - val_loss: 0.0280 - val_r2_inet_coefficient_loss: 0.0151 - val_r2_inet_lambda_fv_loss: -0.9199 - val_mae_inet_coefficient_loss: 1.9521 - val_mae_inet_lambda_fv_loss: 0.0281\n",
      "Epoch 99/500\n",
      "35/35 [==============================] - 20s 572ms/step - loss: 0.0249 - r2_inet_coefficient_loss: 0.0177 - r2_inet_lambda_fv_loss: -0.9373 - mae_inet_coefficient_loss: 1.8358 - mae_inet_lambda_fv_loss: 0.0249 - val_loss: 0.0260 - val_r2_inet_coefficient_loss: 0.0159 - val_r2_inet_lambda_fv_loss: -0.9225 - val_mae_inet_coefficient_loss: 1.9608 - val_mae_inet_lambda_fv_loss: 0.0261\n",
      "Epoch 100/500\n",
      "35/35 [==============================] - 20s 569ms/step - loss: 0.0239 - r2_inet_coefficient_loss: 0.0179 - r2_inet_lambda_fv_loss: -0.9394 - mae_inet_coefficient_loss: 1.8380 - mae_inet_lambda_fv_loss: 0.0239 - val_loss: 0.0263 - val_r2_inet_coefficient_loss: 0.0157 - val_r2_inet_lambda_fv_loss: -0.9205 - val_mae_inet_coefficient_loss: 1.9601 - val_mae_inet_lambda_fv_loss: 0.0264\n",
      "Epoch 101/500\n",
      "35/35 [==============================] - 20s 564ms/step - loss: 0.0235 - r2_inet_coefficient_loss: 0.0177 - r2_inet_lambda_fv_loss: -0.9394 - mae_inet_coefficient_loss: 1.8376 - mae_inet_lambda_fv_loss: 0.0235 - val_loss: 0.0262 - val_r2_inet_coefficient_loss: 0.0156 - val_r2_inet_lambda_fv_loss: -0.9185 - val_mae_inet_coefficient_loss: 1.9569 - val_mae_inet_lambda_fv_loss: 0.0263\n",
      "Epoch 102/500\n",
      "35/35 [==============================] - 20s 564ms/step - loss: 0.0239 - r2_inet_coefficient_loss: 0.0177 - r2_inet_lambda_fv_loss: -0.9377 - mae_inet_coefficient_loss: 1.8378 - mae_inet_lambda_fv_loss: 0.0239 - val_loss: 0.0263 - val_r2_inet_coefficient_loss: 0.0158 - val_r2_inet_lambda_fv_loss: -0.9187 - val_mae_inet_coefficient_loss: 1.9589 - val_mae_inet_lambda_fv_loss: 0.0264\n",
      "Epoch 103/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0264 - r2_inet_coefficient_loss: 0.0176 - r2_inet_lambda_fv_loss: -0.9358 - mae_inet_coefficient_loss: 1.8363 - mae_inet_lambda_fv_loss: 0.0264 - val_loss: 0.0271 - val_r2_inet_coefficient_loss: 0.0155 - val_r2_inet_lambda_fv_loss: -0.9135 - val_mae_inet_coefficient_loss: 1.9575 - val_mae_inet_lambda_fv_loss: 0.0273\n",
      "Epoch 104/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0236 - r2_inet_coefficient_loss: 0.0176 - r2_inet_lambda_fv_loss: -0.9392 - mae_inet_coefficient_loss: 1.8376 - mae_inet_lambda_fv_loss: 0.0236 - val_loss: 0.0262 - val_r2_inet_coefficient_loss: 0.0153 - val_r2_inet_lambda_fv_loss: -0.9178 - val_mae_inet_coefficient_loss: 1.9568 - val_mae_inet_lambda_fv_loss: 0.0263\n",
      "Epoch 105/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0233 - r2_inet_coefficient_loss: 0.0174 - r2_inet_lambda_fv_loss: -0.9414 - mae_inet_coefficient_loss: 1.8374 - mae_inet_lambda_fv_loss: 0.0233 - val_loss: 0.0254 - val_r2_inet_coefficient_loss: 0.0153 - val_r2_inet_lambda_fv_loss: -0.9226 - val_mae_inet_coefficient_loss: 1.9574 - val_mae_inet_lambda_fv_loss: 0.0255\n",
      "Epoch 106/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0245 - r2_inet_coefficient_loss: 0.0173 - r2_inet_lambda_fv_loss: -0.9402 - mae_inet_coefficient_loss: 1.8366 - mae_inet_lambda_fv_loss: 0.0245 - val_loss: 0.0304 - val_r2_inet_coefficient_loss: 0.0146 - val_r2_inet_lambda_fv_loss: -0.9139 - val_mae_inet_coefficient_loss: 1.9502 - val_mae_inet_lambda_fv_loss: 0.0305\n",
      "Epoch 107/500\n",
      "35/35 [==============================] - 20s 558ms/step - loss: 0.0270 - r2_inet_coefficient_loss: 0.0168 - r2_inet_lambda_fv_loss: -0.9346 - mae_inet_coefficient_loss: 1.8317 - mae_inet_lambda_fv_loss: 0.0270 - val_loss: 0.0278 - val_r2_inet_coefficient_loss: 0.0148 - val_r2_inet_lambda_fv_loss: -0.9179 - val_mae_inet_coefficient_loss: 1.9503 - val_mae_inet_lambda_fv_loss: 0.0279\n",
      "Epoch 108/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0240 - r2_inet_coefficient_loss: 0.0172 - r2_inet_lambda_fv_loss: -0.9411 - mae_inet_coefficient_loss: 1.8366 - mae_inet_lambda_fv_loss: 0.0240 - val_loss: 0.0255 - val_r2_inet_coefficient_loss: 0.0151 - val_r2_inet_lambda_fv_loss: -0.9199 - val_mae_inet_coefficient_loss: 1.9547 - val_mae_inet_lambda_fv_loss: 0.0257\n",
      "Epoch 109/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0235 - r2_inet_coefficient_loss: 0.0171 - r2_inet_lambda_fv_loss: -0.9422 - mae_inet_coefficient_loss: 1.8362 - mae_inet_lambda_fv_loss: 0.0235 - val_loss: 0.0271 - val_r2_inet_coefficient_loss: 0.0149 - val_r2_inet_lambda_fv_loss: -0.9215 - val_mae_inet_coefficient_loss: 1.9542 - val_mae_inet_lambda_fv_loss: 0.0272\n",
      "Epoch 110/500\n",
      "35/35 [==============================] - 20s 573ms/step - loss: 0.0263 - r2_inet_coefficient_loss: 0.0170 - r2_inet_lambda_fv_loss: -0.9378 - mae_inet_coefficient_loss: 1.8345 - mae_inet_lambda_fv_loss: 0.0263 - val_loss: 0.0291 - val_r2_inet_coefficient_loss: 0.0148 - val_r2_inet_lambda_fv_loss: -0.9106 - val_mae_inet_coefficient_loss: 1.9505 - val_mae_inet_lambda_fv_loss: 0.0292\n",
      "Epoch 111/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0238 - r2_inet_coefficient_loss: 0.0173 - r2_inet_lambda_fv_loss: -0.9418 - mae_inet_coefficient_loss: 1.8360 - mae_inet_lambda_fv_loss: 0.0238 - val_loss: 0.0273 - val_r2_inet_coefficient_loss: 0.0154 - val_r2_inet_lambda_fv_loss: -0.9166 - val_mae_inet_coefficient_loss: 1.9585 - val_mae_inet_lambda_fv_loss: 0.0274\n",
      "Epoch 112/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0232 - r2_inet_coefficient_loss: 0.0171 - r2_inet_lambda_fv_loss: -0.9410 - mae_inet_coefficient_loss: 1.8358 - mae_inet_lambda_fv_loss: 0.0232 - val_loss: 0.0260 - val_r2_inet_coefficient_loss: 0.0151 - val_r2_inet_lambda_fv_loss: -0.9184 - val_mae_inet_coefficient_loss: 1.9568 - val_mae_inet_lambda_fv_loss: 0.0261\n",
      "Epoch 113/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0224 - r2_inet_coefficient_loss: 0.0170 - r2_inet_lambda_fv_loss: -0.9428 - mae_inet_coefficient_loss: 1.8367 - mae_inet_lambda_fv_loss: 0.0224 - val_loss: 0.0268 - val_r2_inet_coefficient_loss: 0.0148 - val_r2_inet_lambda_fv_loss: -0.9150 - val_mae_inet_coefficient_loss: 1.9544 - val_mae_inet_lambda_fv_loss: 0.0269\n",
      "Epoch 114/500\n",
      "35/35 [==============================] - 20s 568ms/step - loss: 0.0240 - r2_inet_coefficient_loss: 0.0169 - r2_inet_lambda_fv_loss: -0.9399 - mae_inet_coefficient_loss: 1.8354 - mae_inet_lambda_fv_loss: 0.0240 - val_loss: 0.0269 - val_r2_inet_coefficient_loss: 0.0151 - val_r2_inet_lambda_fv_loss: -0.9191 - val_mae_inet_coefficient_loss: 1.9579 - val_mae_inet_lambda_fv_loss: 0.0270\n",
      "Epoch 115/500\n",
      "35/35 [==============================] - 20s 577ms/step - loss: 0.0244 - r2_inet_coefficient_loss: 0.0169 - r2_inet_lambda_fv_loss: -0.9378 - mae_inet_coefficient_loss: 1.8366 - mae_inet_lambda_fv_loss: 0.0244 - val_loss: 0.0280 - val_r2_inet_coefficient_loss: 0.0147 - val_r2_inet_lambda_fv_loss: -0.9168 - val_mae_inet_coefficient_loss: 1.9517 - val_mae_inet_lambda_fv_loss: 0.0281\n",
      "Epoch 116/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0240 - r2_inet_coefficient_loss: 0.0167 - r2_inet_lambda_fv_loss: -0.9415 - mae_inet_coefficient_loss: 1.8343 - mae_inet_lambda_fv_loss: 0.0240 - val_loss: 0.0283 - val_r2_inet_coefficient_loss: 0.0149 - val_r2_inet_lambda_fv_loss: -0.9171 - val_mae_inet_coefficient_loss: 1.9564 - val_mae_inet_lambda_fv_loss: 0.0285\n",
      "Epoch 117/500\n",
      "35/35 [==============================] - 20s 565ms/step - loss: 0.0233 - r2_inet_coefficient_loss: 0.0168 - r2_inet_lambda_fv_loss: -0.9425 - mae_inet_coefficient_loss: 1.8347 - mae_inet_lambda_fv_loss: 0.0233 - val_loss: 0.0277 - val_r2_inet_coefficient_loss: 0.0154 - val_r2_inet_lambda_fv_loss: -0.9173 - val_mae_inet_coefficient_loss: 1.9610 - val_mae_inet_lambda_fv_loss: 0.0278\n",
      "Epoch 118/500\n",
      "35/35 [==============================] - 20s 570ms/step - loss: 0.0236 - r2_inet_coefficient_loss: 0.0167 - r2_inet_lambda_fv_loss: -0.9429 - mae_inet_coefficient_loss: 1.8357 - mae_inet_lambda_fv_loss: 0.0236 - val_loss: 0.0274 - val_r2_inet_coefficient_loss: 0.0151 - val_r2_inet_lambda_fv_loss: -0.9151 - val_mae_inet_coefficient_loss: 1.9602 - val_mae_inet_lambda_fv_loss: 0.0275\n",
      "Epoch 119/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0239 - r2_inet_coefficient_loss: 0.0167 - r2_inet_lambda_fv_loss: -0.9424 - mae_inet_coefficient_loss: 1.8351 - mae_inet_lambda_fv_loss: 0.0239 - val_loss: 0.0254 - val_r2_inet_coefficient_loss: 0.0145 - val_r2_inet_lambda_fv_loss: -0.9239 - val_mae_inet_coefficient_loss: 1.9537 - val_mae_inet_lambda_fv_loss: 0.0255\n",
      "Epoch 120/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0225 - r2_inet_coefficient_loss: 0.0167 - r2_inet_lambda_fv_loss: -0.9404 - mae_inet_coefficient_loss: 1.8362 - mae_inet_lambda_fv_loss: 0.0225 - val_loss: 0.0273 - val_r2_inet_coefficient_loss: 0.0146 - val_r2_inet_lambda_fv_loss: -0.9061 - val_mae_inet_coefficient_loss: 1.9548 - val_mae_inet_lambda_fv_loss: 0.0274\n",
      "Epoch 121/500\n",
      "35/35 [==============================] - 20s 565ms/step - loss: 0.0235 - r2_inet_coefficient_loss: 0.0165 - r2_inet_lambda_fv_loss: -0.9401 - mae_inet_coefficient_loss: 1.8348 - mae_inet_lambda_fv_loss: 0.0235 - val_loss: 0.0294 - val_r2_inet_coefficient_loss: 0.0140 - val_r2_inet_lambda_fv_loss: -0.9160 - val_mae_inet_coefficient_loss: 1.9513 - val_mae_inet_lambda_fv_loss: 0.0295\n",
      "Epoch 122/500\n",
      "35/35 [==============================] - 20s 565ms/step - loss: 0.0243 - r2_inet_coefficient_loss: 0.0164 - r2_inet_lambda_fv_loss: -0.9384 - mae_inet_coefficient_loss: 1.8351 - mae_inet_lambda_fv_loss: 0.0243 - val_loss: 0.0283 - val_r2_inet_coefficient_loss: 0.0142 - val_r2_inet_lambda_fv_loss: -0.9164 - val_mae_inet_coefficient_loss: 1.9534 - val_mae_inet_lambda_fv_loss: 0.0284\n",
      "Epoch 123/500\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0253 - r2_inet_coefficient_loss: 0.0165 - r2_inet_lambda_fv_loss: -0.9408 - mae_inet_coefficient_loss: 1.8346 - mae_inet_lambda_fv_loss: 0.0253 - val_loss: 0.0277 - val_r2_inet_coefficient_loss: 0.0141 - val_r2_inet_lambda_fv_loss: -0.9238 - val_mae_inet_coefficient_loss: 1.9517 - val_mae_inet_lambda_fv_loss: 0.0278\n",
      "Epoch 124/500\n",
      "35/35 [==============================] - 20s 569ms/step - loss: 0.0236 - r2_inet_coefficient_loss: 0.0162 - r2_inet_lambda_fv_loss: -0.9438 - mae_inet_coefficient_loss: 1.8334 - mae_inet_lambda_fv_loss: 0.0236 - val_loss: 0.0271 - val_r2_inet_coefficient_loss: 0.0145 - val_r2_inet_lambda_fv_loss: -0.9167 - val_mae_inet_coefficient_loss: 1.9567 - val_mae_inet_lambda_fv_loss: 0.0272\n",
      "Epoch 125/500\n",
      "35/35 [==============================] - 20s 564ms/step - loss: 0.0223 - r2_inet_coefficient_loss: 0.0163 - r2_inet_lambda_fv_loss: -0.9443 - mae_inet_coefficient_loss: 1.8350 - mae_inet_lambda_fv_loss: 0.0223 - val_loss: 0.0263 - val_r2_inet_coefficient_loss: 0.0145 - val_r2_inet_lambda_fv_loss: -0.9188 - val_mae_inet_coefficient_loss: 1.9560 - val_mae_inet_lambda_fv_loss: 0.0264\n",
      "Epoch 126/500\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0222 - r2_inet_coefficient_loss: 0.0162 - r2_inet_lambda_fv_loss: -0.9448 - mae_inet_coefficient_loss: 1.8348 - mae_inet_lambda_fv_loss: 0.0222 - val_loss: 0.0257 - val_r2_inet_coefficient_loss: 0.0146 - val_r2_inet_lambda_fv_loss: -0.9193 - val_mae_inet_coefficient_loss: 1.9567 - val_mae_inet_lambda_fv_loss: 0.0258\n",
      "Epoch 127/500\n",
      "35/35 [==============================] - 20s 572ms/step - loss: 0.0232 - r2_inet_coefficient_loss: 0.0160 - r2_inet_lambda_fv_loss: -0.9437 - mae_inet_coefficient_loss: 1.8337 - mae_inet_lambda_fv_loss: 0.0232 - val_loss: 0.0260 - val_r2_inet_coefficient_loss: 0.0142 - val_r2_inet_lambda_fv_loss: -0.9185 - val_mae_inet_coefficient_loss: 1.9524 - val_mae_inet_lambda_fv_loss: 0.0261\n",
      "Epoch 128/500\n",
      "35/35 [==============================] - 20s 575ms/step - loss: 0.0251 - r2_inet_coefficient_loss: 0.0161 - r2_inet_lambda_fv_loss: -0.9396 - mae_inet_coefficient_loss: 1.8330 - mae_inet_lambda_fv_loss: 0.0251 - val_loss: 0.0267 - val_r2_inet_coefficient_loss: 0.0146 - val_r2_inet_lambda_fv_loss: -0.9195 - val_mae_inet_coefficient_loss: 1.9556 - val_mae_inet_lambda_fv_loss: 0.0269\n",
      "Epoch 129/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0234 - r2_inet_coefficient_loss: 0.0161 - r2_inet_lambda_fv_loss: -0.9413 - mae_inet_coefficient_loss: 1.8345 - mae_inet_lambda_fv_loss: 0.0234 - val_loss: 0.0258 - val_r2_inet_coefficient_loss: 0.0144 - val_r2_inet_lambda_fv_loss: -0.9231 - val_mae_inet_coefficient_loss: 1.9564 - val_mae_inet_lambda_fv_loss: 0.0259\n",
      "Epoch 130/500\n",
      "35/35 [==============================] - 20s 564ms/step - loss: 0.0232 - r2_inet_coefficient_loss: 0.0161 - r2_inet_lambda_fv_loss: -0.9401 - mae_inet_coefficient_loss: 1.8349 - mae_inet_lambda_fv_loss: 0.0232 - val_loss: 0.0258 - val_r2_inet_coefficient_loss: 0.0144 - val_r2_inet_lambda_fv_loss: -0.9192 - val_mae_inet_coefficient_loss: 1.9568 - val_mae_inet_lambda_fv_loss: 0.0259\n",
      "Epoch 131/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0224 - r2_inet_coefficient_loss: 0.0160 - r2_inet_lambda_fv_loss: -0.9422 - mae_inet_coefficient_loss: 1.8353 - mae_inet_lambda_fv_loss: 0.0224 - val_loss: 0.0263 - val_r2_inet_coefficient_loss: 0.0138 - val_r2_inet_lambda_fv_loss: -0.9197 - val_mae_inet_coefficient_loss: 1.9527 - val_mae_inet_lambda_fv_loss: 0.0264\n",
      "Epoch 132/500\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 0.0235 - r2_inet_coefficient_loss: 0.0160 - r2_inet_lambda_fv_loss: -0.9436 - mae_inet_coefficient_loss: 1.8338 - mae_inet_lambda_fv_loss: 0.0235 - val_loss: 0.0316 - val_r2_inet_coefficient_loss: 0.0141 - val_r2_inet_lambda_fv_loss: -0.9149 - val_mae_inet_coefficient_loss: 1.9521 - val_mae_inet_lambda_fv_loss: 0.0318\n",
      "Epoch 133/500\n",
      "35/35 [==============================] - 20s 571ms/step - loss: 0.0263 - r2_inet_coefficient_loss: 0.0157 - r2_inet_lambda_fv_loss: -0.9388 - mae_inet_coefficient_loss: 1.8310 - mae_inet_lambda_fv_loss: 0.0263 - val_loss: 0.0257 - val_r2_inet_coefficient_loss: 0.0145 - val_r2_inet_lambda_fv_loss: -0.9237 - val_mae_inet_coefficient_loss: 1.9567 - val_mae_inet_lambda_fv_loss: 0.0258\n",
      "Epoch 134/500\n",
      "28/35 [=======================>......] - ETA: 3s - loss: 0.0249 - r2_inet_coefficient_loss: 0.0162 - r2_inet_lambda_fv_loss: -0.9424 - mae_inet_coefficient_loss: 1.8164 - mae_inet_lambda_fv_loss: 0.0249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0974 - r2_inet_coefficient_loss: -0.0046 - r2_inet_lambda_fv_loss: -0.7441 - mae_inet_coefficient_loss: 1.7373 - mae_inet_lambda_fv_loss: 0.0974 - val_loss: 0.0695 - val_r2_inet_coefficient_loss: -0.0043 - val_r2_inet_lambda_fv_loss: -0.8317 - val_mae_inet_coefficient_loss: 1.8580 - val_mae_inet_lambda_fv_loss: 0.0697\n",
      "Epoch 28/500\n",
      "35/35 [==============================] - 20s 565ms/step - loss: 0.0955 - r2_inet_coefficient_loss: -0.0047 - r2_inet_lambda_fv_loss: -0.7507 - mae_inet_coefficient_loss: 1.7361 - mae_inet_lambda_fv_loss: 0.0955 - val_loss: 0.0702 - val_r2_inet_coefficient_loss: -0.0037 - val_r2_inet_lambda_fv_loss: -0.8329 - val_mae_inet_coefficient_loss: 1.8592 - val_mae_inet_lambda_fv_loss: 0.0703\n",
      "Epoch 29/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0964 - r2_inet_coefficient_loss: -0.0044 - r2_inet_lambda_fv_loss: -0.7484 - mae_inet_coefficient_loss: 1.7360 - mae_inet_lambda_fv_loss: 0.0964 - val_loss: 0.0705 - val_r2_inet_coefficient_loss: -0.0041 - val_r2_inet_lambda_fv_loss: -0.8252 - val_mae_inet_coefficient_loss: 1.8551 - val_mae_inet_lambda_fv_loss: 0.0707\n",
      "Epoch 30/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0958 - r2_inet_coefficient_loss: -0.0048 - r2_inet_lambda_fv_loss: -0.7441 - mae_inet_coefficient_loss: 1.7351 - mae_inet_lambda_fv_loss: 0.0958 - val_loss: 0.0678 - val_r2_inet_coefficient_loss: -0.0038 - val_r2_inet_lambda_fv_loss: -0.8422 - val_mae_inet_coefficient_loss: 1.8574 - val_mae_inet_lambda_fv_loss: 0.0680\n",
      "Epoch 31/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0955 - r2_inet_coefficient_loss: -0.0046 - r2_inet_lambda_fv_loss: -0.7481 - mae_inet_coefficient_loss: 1.7357 - mae_inet_lambda_fv_loss: 0.0955 - val_loss: 0.0665 - val_r2_inet_coefficient_loss: -0.0041 - val_r2_inet_lambda_fv_loss: -0.8397 - val_mae_inet_coefficient_loss: 1.8596 - val_mae_inet_lambda_fv_loss: 0.0667\n",
      "Epoch 32/500\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0948 - r2_inet_coefficient_loss: -0.0045 - r2_inet_lambda_fv_loss: -0.7524 - mae_inet_coefficient_loss: 1.7366 - mae_inet_lambda_fv_loss: 0.0948 - val_loss: 0.0686 - val_r2_inet_coefficient_loss: -0.0038 - val_r2_inet_lambda_fv_loss: -0.8284 - val_mae_inet_coefficient_loss: 1.8553 - val_mae_inet_lambda_fv_loss: 0.0687\n",
      "Epoch 33/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0949 - r2_inet_coefficient_loss: -0.0047 - r2_inet_lambda_fv_loss: -0.7504 - mae_inet_coefficient_loss: 1.7348 - mae_inet_lambda_fv_loss: 0.0949 - val_loss: 0.0696 - val_r2_inet_coefficient_loss: -0.0041 - val_r2_inet_lambda_fv_loss: -0.8281 - val_mae_inet_coefficient_loss: 1.8560 - val_mae_inet_lambda_fv_loss: 0.0697\n",
      "Epoch 34/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0945 - r2_inet_coefficient_loss: -0.0049 - r2_inet_lambda_fv_loss: -0.7539 - mae_inet_coefficient_loss: 1.7342 - mae_inet_lambda_fv_loss: 0.0945 - val_loss: 0.0672 - val_r2_inet_coefficient_loss: -0.0041 - val_r2_inet_lambda_fv_loss: -0.8347 - val_mae_inet_coefficient_loss: 1.8574 - val_mae_inet_lambda_fv_loss: 0.0674\n",
      "Epoch 35/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0938 - r2_inet_coefficient_loss: -0.0045 - r2_inet_lambda_fv_loss: -0.7574 - mae_inet_coefficient_loss: 1.7339 - mae_inet_lambda_fv_loss: 0.0938 - val_loss: 0.0655 - val_r2_inet_coefficient_loss: -0.0037 - val_r2_inet_lambda_fv_loss: -0.8437 - val_mae_inet_coefficient_loss: 1.8579 - val_mae_inet_lambda_fv_loss: 0.0657\n",
      "Epoch 36/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0934 - r2_inet_coefficient_loss: -0.0044 - r2_inet_lambda_fv_loss: -0.7618 - mae_inet_coefficient_loss: 1.7327 - mae_inet_lambda_fv_loss: 0.0934 - val_loss: 0.0628 - val_r2_inet_coefficient_loss: -0.0039 - val_r2_inet_lambda_fv_loss: -0.8434 - val_mae_inet_coefficient_loss: 1.8593 - val_mae_inet_lambda_fv_loss: 0.0630\n",
      "Epoch 37/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0923 - r2_inet_coefficient_loss: -0.0044 - r2_inet_lambda_fv_loss: -0.7593 - mae_inet_coefficient_loss: 1.7347 - mae_inet_lambda_fv_loss: 0.0923 - val_loss: 0.0617 - val_r2_inet_coefficient_loss: -0.0036 - val_r2_inet_lambda_fv_loss: -0.8532 - val_mae_inet_coefficient_loss: 1.8587 - val_mae_inet_lambda_fv_loss: 0.0619\n",
      "Epoch 38/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0911 - r2_inet_coefficient_loss: -0.0042 - r2_inet_lambda_fv_loss: -0.7630 - mae_inet_coefficient_loss: 1.7338 - mae_inet_lambda_fv_loss: 0.0911 - val_loss: 0.0672 - val_r2_inet_coefficient_loss: -0.0035 - val_r2_inet_lambda_fv_loss: -0.8430 - val_mae_inet_coefficient_loss: 1.8538 - val_mae_inet_lambda_fv_loss: 0.0674\n",
      "Epoch 39/500\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0926 - r2_inet_coefficient_loss: -0.0041 - r2_inet_lambda_fv_loss: -0.7573 - mae_inet_coefficient_loss: 1.7333 - mae_inet_lambda_fv_loss: 0.0926 - val_loss: 0.0664 - val_r2_inet_coefficient_loss: -0.0037 - val_r2_inet_lambda_fv_loss: -0.8465 - val_mae_inet_coefficient_loss: 1.8516 - val_mae_inet_lambda_fv_loss: 0.0665\n",
      "Epoch 40/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0918 - r2_inet_coefficient_loss: -0.0040 - r2_inet_lambda_fv_loss: -0.7649 - mae_inet_coefficient_loss: 1.7312 - mae_inet_lambda_fv_loss: 0.0918 - val_loss: 0.0636 - val_r2_inet_coefficient_loss: -0.0032 - val_r2_inet_lambda_fv_loss: -0.8516 - val_mae_inet_coefficient_loss: 1.8534 - val_mae_inet_lambda_fv_loss: 0.0638\n",
      "Epoch 41/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0913 - r2_inet_coefficient_loss: -0.0038 - r2_inet_lambda_fv_loss: -0.7651 - mae_inet_coefficient_loss: 1.7329 - mae_inet_lambda_fv_loss: 0.0913 - val_loss: 0.0673 - val_r2_inet_coefficient_loss: -0.0030 - val_r2_inet_lambda_fv_loss: -0.8463 - val_mae_inet_coefficient_loss: 1.8544 - val_mae_inet_lambda_fv_loss: 0.0675\n",
      "Epoch 42/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0900 - r2_inet_coefficient_loss: -0.0037 - r2_inet_lambda_fv_loss: -0.7715 - mae_inet_coefficient_loss: 1.7324 - mae_inet_lambda_fv_loss: 0.0900 - val_loss: 0.0620 - val_r2_inet_coefficient_loss: -0.0029 - val_r2_inet_lambda_fv_loss: -0.8514 - val_mae_inet_coefficient_loss: 1.8543 - val_mae_inet_lambda_fv_loss: 0.0622\n",
      "Epoch 43/500\n",
      "35/35 [==============================] - 19s 537ms/step - loss: 0.0899 - r2_inet_coefficient_loss: -0.0033 - r2_inet_lambda_fv_loss: -0.7624 - mae_inet_coefficient_loss: 1.7330 - mae_inet_lambda_fv_loss: 0.0899 - val_loss: 0.0650 - val_r2_inet_coefficient_loss: -0.0032 - val_r2_inet_lambda_fv_loss: -0.8515 - val_mae_inet_coefficient_loss: 1.8513 - val_mae_inet_lambda_fv_loss: 0.0652\n",
      "Epoch 44/500\n",
      "35/35 [==============================] - 15s 431ms/step - loss: 0.0904 - r2_inet_coefficient_loss: -0.0037 - r2_inet_lambda_fv_loss: -0.7644 - mae_inet_coefficient_loss: 1.7299 - mae_inet_lambda_fv_loss: 0.0904 - val_loss: 0.0604 - val_r2_inet_coefficient_loss: -0.0032 - val_r2_inet_lambda_fv_loss: -0.8534 - val_mae_inet_coefficient_loss: 1.8541 - val_mae_inet_lambda_fv_loss: 0.0606\n",
      "Epoch 45/500\n",
      "35/35 [==============================] - 18s 513ms/step - loss: 0.0899 - r2_inet_coefficient_loss: -0.0036 - r2_inet_lambda_fv_loss: -0.7678 - mae_inet_coefficient_loss: 1.7305 - mae_inet_lambda_fv_loss: 0.0899 - val_loss: 0.0645 - val_r2_inet_coefficient_loss: -0.0032 - val_r2_inet_lambda_fv_loss: -0.8427 - val_mae_inet_coefficient_loss: 1.8544 - val_mae_inet_lambda_fv_loss: 0.0647\n",
      "Epoch 46/500\n",
      "35/35 [==============================] - 19s 550ms/step - loss: 0.0886 - r2_inet_coefficient_loss: -0.0038 - r2_inet_lambda_fv_loss: -0.7694 - mae_inet_coefficient_loss: 1.7308 - mae_inet_lambda_fv_loss: 0.0886 - val_loss: 0.0634 - val_r2_inet_coefficient_loss: -0.0028 - val_r2_inet_lambda_fv_loss: -0.8540 - val_mae_inet_coefficient_loss: 1.8557 - val_mae_inet_lambda_fv_loss: 0.0636\n",
      "Epoch 47/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0878 - r2_inet_coefficient_loss: -0.0039 - r2_inet_lambda_fv_loss: -0.7799 - mae_inet_coefficient_loss: 1.7306 - mae_inet_lambda_fv_loss: 0.0879 - val_loss: 0.0609 - val_r2_inet_coefficient_loss: -0.0037 - val_r2_inet_lambda_fv_loss: -0.8553 - val_mae_inet_coefficient_loss: 1.8522 - val_mae_inet_lambda_fv_loss: 0.0611\n",
      "Epoch 48/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0880 - r2_inet_coefficient_loss: -0.0041 - r2_inet_lambda_fv_loss: -0.7788 - mae_inet_coefficient_loss: 1.7298 - mae_inet_lambda_fv_loss: 0.0880 - val_loss: 0.0629 - val_r2_inet_coefficient_loss: -0.0036 - val_r2_inet_lambda_fv_loss: -0.8548 - val_mae_inet_coefficient_loss: 1.8491 - val_mae_inet_lambda_fv_loss: 0.0631\n",
      "Epoch 49/500\n",
      "35/35 [==============================] - 19s 549ms/step - loss: 0.0886 - r2_inet_coefficient_loss: -0.0040 - r2_inet_lambda_fv_loss: -0.7721 - mae_inet_coefficient_loss: 1.7290 - mae_inet_lambda_fv_loss: 0.0886 - val_loss: 0.0625 - val_r2_inet_coefficient_loss: -0.0035 - val_r2_inet_lambda_fv_loss: -0.8567 - val_mae_inet_coefficient_loss: 1.8507 - val_mae_inet_lambda_fv_loss: 0.0626\n",
      "Epoch 50/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0879 - r2_inet_coefficient_loss: -0.0040 - r2_inet_lambda_fv_loss: -0.7740 - mae_inet_coefficient_loss: 1.7291 - mae_inet_lambda_fv_loss: 0.0879 - val_loss: 0.0638 - val_r2_inet_coefficient_loss: -0.0037 - val_r2_inet_lambda_fv_loss: -0.8500 - val_mae_inet_coefficient_loss: 1.8494 - val_mae_inet_lambda_fv_loss: 0.0639\n",
      "Epoch 51/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0871 - r2_inet_coefficient_loss: -0.0043 - r2_inet_lambda_fv_loss: -0.7784 - mae_inet_coefficient_loss: 1.7278 - mae_inet_lambda_fv_loss: 0.0871 - val_loss: 0.0621 - val_r2_inet_coefficient_loss: -0.0033 - val_r2_inet_lambda_fv_loss: -0.8580 - val_mae_inet_coefficient_loss: 1.8526 - val_mae_inet_lambda_fv_loss: 0.0622\n",
      "Epoch 52/500\n",
      "35/35 [==============================] - 19s 559ms/step - loss: 0.0869 - r2_inet_coefficient_loss: -0.0037 - r2_inet_lambda_fv_loss: -0.7768 - mae_inet_coefficient_loss: 1.7296 - mae_inet_lambda_fv_loss: 0.0869 - val_loss: 0.0628 - val_r2_inet_coefficient_loss: -0.0033 - val_r2_inet_lambda_fv_loss: -0.8573 - val_mae_inet_coefficient_loss: 1.8491 - val_mae_inet_lambda_fv_loss: 0.0629\n",
      "Epoch 53/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0873 - r2_inet_coefficient_loss: -0.0039 - r2_inet_lambda_fv_loss: -0.7849 - mae_inet_coefficient_loss: 1.7269 - mae_inet_lambda_fv_loss: 0.0873 - val_loss: 0.0623 - val_r2_inet_coefficient_loss: -0.0033 - val_r2_inet_lambda_fv_loss: -0.8509 - val_mae_inet_coefficient_loss: 1.8526 - val_mae_inet_lambda_fv_loss: 0.0625\n",
      "Epoch 54/500\n",
      "35/35 [==============================] - 19s 548ms/step - loss: 0.0869 - r2_inet_coefficient_loss: -0.0041 - r2_inet_lambda_fv_loss: -0.7810 - mae_inet_coefficient_loss: 1.7271 - mae_inet_lambda_fv_loss: 0.0869 - val_loss: 0.0607 - val_r2_inet_coefficient_loss: -0.0035 - val_r2_inet_lambda_fv_loss: -0.8567 - val_mae_inet_coefficient_loss: 1.8497 - val_mae_inet_lambda_fv_loss: 0.0609\n",
      "Epoch 55/500\n",
      "35/35 [==============================] - 19s 551ms/step - loss: 0.0865 - r2_inet_coefficient_loss: -0.0039 - r2_inet_lambda_fv_loss: -0.7846 - mae_inet_coefficient_loss: 1.7274 - mae_inet_lambda_fv_loss: 0.0865 - val_loss: 0.0569 - val_r2_inet_coefficient_loss: -0.0032 - val_r2_inet_lambda_fv_loss: -0.8632 - val_mae_inet_coefficient_loss: 1.8503 - val_mae_inet_lambda_fv_loss: 0.0570\n",
      "Epoch 56/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0848 - r2_inet_coefficient_loss: -0.0041 - r2_inet_lambda_fv_loss: -0.7872 - mae_inet_coefficient_loss: 1.7269 - mae_inet_lambda_fv_loss: 0.0848 - val_loss: 0.0643 - val_r2_inet_coefficient_loss: -0.0031 - val_r2_inet_lambda_fv_loss: -0.8528 - val_mae_inet_coefficient_loss: 1.8498 - val_mae_inet_lambda_fv_loss: 0.0645\n",
      "Epoch 57/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0868 - r2_inet_coefficient_loss: -0.0034 - r2_inet_lambda_fv_loss: -0.7819 - mae_inet_coefficient_loss: 1.7264 - mae_inet_lambda_fv_loss: 0.0868 - val_loss: 0.0607 - val_r2_inet_coefficient_loss: -0.0033 - val_r2_inet_lambda_fv_loss: -0.8611 - val_mae_inet_coefficient_loss: 1.8474 - val_mae_inet_lambda_fv_loss: 0.0609\n",
      "Epoch 58/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0858 - r2_inet_coefficient_loss: -0.0038 - r2_inet_lambda_fv_loss: -0.7851 - mae_inet_coefficient_loss: 1.7253 - mae_inet_lambda_fv_loss: 0.0858 - val_loss: 0.0574 - val_r2_inet_coefficient_loss: -0.0036 - val_r2_inet_lambda_fv_loss: -0.8633 - val_mae_inet_coefficient_loss: 1.8503 - val_mae_inet_lambda_fv_loss: 0.0576\n",
      "Epoch 59/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0852 - r2_inet_coefficient_loss: -0.0043 - r2_inet_lambda_fv_loss: -0.7820 - mae_inet_coefficient_loss: 1.7263 - mae_inet_lambda_fv_loss: 0.0852 - val_loss: 0.0606 - val_r2_inet_coefficient_loss: -0.0030 - val_r2_inet_lambda_fv_loss: -0.8473 - val_mae_inet_coefficient_loss: 1.8514 - val_mae_inet_lambda_fv_loss: 0.0607\n",
      "Epoch 60/500\n",
      "35/35 [==============================] - 20s 567ms/step - loss: 0.0855 - r2_inet_coefficient_loss: -0.0034 - r2_inet_lambda_fv_loss: -0.7932 - mae_inet_coefficient_loss: 1.7269 - mae_inet_lambda_fv_loss: 0.0855 - val_loss: 0.0581 - val_r2_inet_coefficient_loss: -0.0036 - val_r2_inet_lambda_fv_loss: -0.8497 - val_mae_inet_coefficient_loss: 1.8493 - val_mae_inet_lambda_fv_loss: 0.0583\n",
      "Epoch 61/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0845 - r2_inet_coefficient_loss: -0.0043 - r2_inet_lambda_fv_loss: -0.7880 - mae_inet_coefficient_loss: 1.7253 - mae_inet_lambda_fv_loss: 0.0845 - val_loss: 0.0653 - val_r2_inet_coefficient_loss: -0.0035 - val_r2_inet_lambda_fv_loss: -0.8459 - val_mae_inet_coefficient_loss: 1.8465 - val_mae_inet_lambda_fv_loss: 0.0655\n",
      "Epoch 62/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0842 - r2_inet_coefficient_loss: -0.0040 - r2_inet_lambda_fv_loss: -0.7920 - mae_inet_coefficient_loss: 1.7250 - mae_inet_lambda_fv_loss: 0.0842 - val_loss: 0.0631 - val_r2_inet_coefficient_loss: -0.0033 - val_r2_inet_lambda_fv_loss: -0.8477 - val_mae_inet_coefficient_loss: 1.8440 - val_mae_inet_lambda_fv_loss: 0.0632\n",
      "Epoch 63/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0847 - r2_inet_coefficient_loss: -0.0041 - r2_inet_lambda_fv_loss: -0.7902 - mae_inet_coefficient_loss: 1.7240 - mae_inet_lambda_fv_loss: 0.0847 - val_loss: 0.0605 - val_r2_inet_coefficient_loss: -0.0034 - val_r2_inet_lambda_fv_loss: -0.8448 - val_mae_inet_coefficient_loss: 1.8476 - val_mae_inet_lambda_fv_loss: 0.0607\n",
      "Epoch 64/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0831 - r2_inet_coefficient_loss: -0.0039 - r2_inet_lambda_fv_loss: -0.7934 - mae_inet_coefficient_loss: 1.7255 - mae_inet_lambda_fv_loss: 0.0831 - val_loss: 0.0576 - val_r2_inet_coefficient_loss: -0.0030 - val_r2_inet_lambda_fv_loss: -0.8608 - val_mae_inet_coefficient_loss: 1.8488 - val_mae_inet_lambda_fv_loss: 0.0578\n",
      "Epoch 65/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0829 - r2_inet_coefficient_loss: -0.0034 - r2_inet_lambda_fv_loss: -0.7934 - mae_inet_coefficient_loss: 1.7253 - mae_inet_lambda_fv_loss: 0.0829 - val_loss: 0.0584 - val_r2_inet_coefficient_loss: -0.0028 - val_r2_inet_lambda_fv_loss: -0.8605 - val_mae_inet_coefficient_loss: 1.8472 - val_mae_inet_lambda_fv_loss: 0.0586\n",
      "Epoch 66/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0834 - r2_inet_coefficient_loss: -0.0032 - r2_inet_lambda_fv_loss: -0.7878 - mae_inet_coefficient_loss: 1.7257 - mae_inet_lambda_fv_loss: 0.0834 - val_loss: 0.0593 - val_r2_inet_coefficient_loss: -0.0027 - val_r2_inet_lambda_fv_loss: -0.8647 - val_mae_inet_coefficient_loss: 1.8476 - val_mae_inet_lambda_fv_loss: 0.0594\n",
      "Epoch 67/500\n",
      "35/35 [==============================] - 19s 543ms/step - loss: 0.0830 - r2_inet_coefficient_loss: -0.0033 - r2_inet_lambda_fv_loss: -0.7925 - mae_inet_coefficient_loss: 1.7260 - mae_inet_lambda_fv_loss: 0.0830 - val_loss: 0.0570 - val_r2_inet_coefficient_loss: -0.0027 - val_r2_inet_lambda_fv_loss: -0.8568 - val_mae_inet_coefficient_loss: 1.8495 - val_mae_inet_lambda_fv_loss: 0.0572\n",
      "Epoch 68/500\n",
      "35/35 [==============================] - 19s 549ms/step - loss: 0.0827 - r2_inet_coefficient_loss: -0.0033 - r2_inet_lambda_fv_loss: -0.7931 - mae_inet_coefficient_loss: 1.7265 - mae_inet_lambda_fv_loss: 0.0827 - val_loss: 0.0573 - val_r2_inet_coefficient_loss: -0.0024 - val_r2_inet_lambda_fv_loss: -0.8634 - val_mae_inet_coefficient_loss: 1.8485 - val_mae_inet_lambda_fv_loss: 0.0574\n",
      "Epoch 69/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0832 - r2_inet_coefficient_loss: -0.0031 - r2_inet_lambda_fv_loss: -0.7936 - mae_inet_coefficient_loss: 1.7270 - mae_inet_lambda_fv_loss: 0.0832 - val_loss: 0.0584 - val_r2_inet_coefficient_loss: -0.0022 - val_r2_inet_lambda_fv_loss: -0.8610 - val_mae_inet_coefficient_loss: 1.8461 - val_mae_inet_lambda_fv_loss: 0.0586\n",
      "Epoch 70/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0820 - r2_inet_coefficient_loss: -0.0030 - r2_inet_lambda_fv_loss: -0.7977 - mae_inet_coefficient_loss: 1.7280 - mae_inet_lambda_fv_loss: 0.0820 - val_loss: 0.0589 - val_r2_inet_coefficient_loss: -0.0022 - val_r2_inet_lambda_fv_loss: -0.8507 - val_mae_inet_coefficient_loss: 1.8483 - val_mae_inet_lambda_fv_loss: 0.0590\n",
      "Epoch 71/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0827 - r2_inet_coefficient_loss: -0.0025 - r2_inet_lambda_fv_loss: -0.7969 - mae_inet_coefficient_loss: 1.7282 - mae_inet_lambda_fv_loss: 0.0827 - val_loss: 0.0570 - val_r2_inet_coefficient_loss: -0.0026 - val_r2_inet_lambda_fv_loss: -0.8545 - val_mae_inet_coefficient_loss: 1.8480 - val_mae_inet_lambda_fv_loss: 0.0572\n",
      "Epoch 72/500\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0822 - r2_inet_coefficient_loss: -0.0032 - r2_inet_lambda_fv_loss: -0.7960 - mae_inet_coefficient_loss: 1.7280 - mae_inet_lambda_fv_loss: 0.0822 - val_loss: 0.0561 - val_r2_inet_coefficient_loss: -0.0025 - val_r2_inet_lambda_fv_loss: -0.8695 - val_mae_inet_coefficient_loss: 1.8472 - val_mae_inet_lambda_fv_loss: 0.0562\n",
      "Epoch 73/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0818 - r2_inet_coefficient_loss: -0.0029 - r2_inet_lambda_fv_loss: -0.7959 - mae_inet_coefficient_loss: 1.7301 - mae_inet_lambda_fv_loss: 0.0818 - val_loss: 0.0560 - val_r2_inet_coefficient_loss: -0.0021 - val_r2_inet_lambda_fv_loss: -0.8668 - val_mae_inet_coefficient_loss: 1.8489 - val_mae_inet_lambda_fv_loss: 0.0562\n",
      "Epoch 74/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0818 - r2_inet_coefficient_loss: -0.0024 - r2_inet_lambda_fv_loss: -0.8017 - mae_inet_coefficient_loss: 1.7311 - mae_inet_lambda_fv_loss: 0.0818 - val_loss: 0.0551 - val_r2_inet_coefficient_loss: -0.0023 - val_r2_inet_lambda_fv_loss: -0.8632 - val_mae_inet_coefficient_loss: 1.8507 - val_mae_inet_lambda_fv_loss: 0.0553\n",
      "Epoch 75/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0803 - r2_inet_coefficient_loss: -0.0022 - r2_inet_lambda_fv_loss: -0.8072 - mae_inet_coefficient_loss: 1.7332 - mae_inet_lambda_fv_loss: 0.0803 - val_loss: 0.0579 - val_r2_inet_coefficient_loss: -0.0024 - val_r2_inet_lambda_fv_loss: -0.8602 - val_mae_inet_coefficient_loss: 1.8507 - val_mae_inet_lambda_fv_loss: 0.0581\n",
      "Epoch 76/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0804 - r2_inet_coefficient_loss: -0.0025 - r2_inet_lambda_fv_loss: -0.8002 - mae_inet_coefficient_loss: 1.7340 - mae_inet_lambda_fv_loss: 0.0804 - val_loss: 0.0537 - val_r2_inet_coefficient_loss: -0.0021 - val_r2_inet_lambda_fv_loss: -0.8695 - val_mae_inet_coefficient_loss: 1.8517 - val_mae_inet_lambda_fv_loss: 0.0539\n",
      "Epoch 77/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0812 - r2_inet_coefficient_loss: -0.0016 - r2_inet_lambda_fv_loss: -0.7997 - mae_inet_coefficient_loss: 1.7362 - mae_inet_lambda_fv_loss: 0.0812 - val_loss: 0.0539 - val_r2_inet_coefficient_loss: -0.0016 - val_r2_inet_lambda_fv_loss: -0.8646 - val_mae_inet_coefficient_loss: 1.8550 - val_mae_inet_lambda_fv_loss: 0.0541\n",
      "Epoch 78/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0796 - r2_inet_coefficient_loss: -0.0020 - r2_inet_lambda_fv_loss: -0.8100 - mae_inet_coefficient_loss: 1.7385 - mae_inet_lambda_fv_loss: 0.0796 - val_loss: 0.0577 - val_r2_inet_coefficient_loss: -0.0018 - val_r2_inet_lambda_fv_loss: -0.8603 - val_mae_inet_coefficient_loss: 1.8533 - val_mae_inet_lambda_fv_loss: 0.0578\n",
      "Epoch 79/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0806 - r2_inet_coefficient_loss: -0.0019 - r2_inet_lambda_fv_loss: -0.8033 - mae_inet_coefficient_loss: 1.7371 - mae_inet_lambda_fv_loss: 0.0806 - val_loss: 0.0570 - val_r2_inet_coefficient_loss: -0.0017 - val_r2_inet_lambda_fv_loss: -0.8621 - val_mae_inet_coefficient_loss: 1.8529 - val_mae_inet_lambda_fv_loss: 0.0571\n",
      "Epoch 80/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0812 - r2_inet_coefficient_loss: -0.0013 - r2_inet_lambda_fv_loss: -0.8016 - mae_inet_coefficient_loss: 1.7392 - mae_inet_lambda_fv_loss: 0.0812 - val_loss: 0.0574 - val_r2_inet_coefficient_loss: -0.0013 - val_r2_inet_lambda_fv_loss: -0.8657 - val_mae_inet_coefficient_loss: 1.8575 - val_mae_inet_lambda_fv_loss: 0.0576\n",
      "Epoch 81/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0807 - r2_inet_coefficient_loss: -0.0016 - r2_inet_lambda_fv_loss: -0.8026 - mae_inet_coefficient_loss: 1.7408 - mae_inet_lambda_fv_loss: 0.0807 - val_loss: 0.0538 - val_r2_inet_coefficient_loss: -0.0015 - val_r2_inet_lambda_fv_loss: -0.8678 - val_mae_inet_coefficient_loss: 1.8585 - val_mae_inet_lambda_fv_loss: 0.0540\n",
      "Epoch 82/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0789 - r2_inet_coefficient_loss: -0.0014 - r2_inet_lambda_fv_loss: -0.8075 - mae_inet_coefficient_loss: 1.7428 - mae_inet_lambda_fv_loss: 0.0789 - val_loss: 0.0541 - val_r2_inet_coefficient_loss: -0.0014 - val_r2_inet_lambda_fv_loss: -0.8781 - val_mae_inet_coefficient_loss: 1.8610 - val_mae_inet_lambda_fv_loss: 0.0543\n",
      "Epoch 83/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0788 - r2_inet_coefficient_loss: -0.0015 - r2_inet_lambda_fv_loss: -0.8110 - mae_inet_coefficient_loss: 1.7454 - mae_inet_lambda_fv_loss: 0.0788 - val_loss: 0.0570 - val_r2_inet_coefficient_loss: -0.0016 - val_r2_inet_lambda_fv_loss: -0.8726 - val_mae_inet_coefficient_loss: 1.8595 - val_mae_inet_lambda_fv_loss: 0.0571\n",
      "Epoch 84/500\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0790 - r2_inet_coefficient_loss: -0.0013 - r2_inet_lambda_fv_loss: -0.8102 - mae_inet_coefficient_loss: 1.7462 - mae_inet_lambda_fv_loss: 0.0790 - val_loss: 0.0543 - val_r2_inet_coefficient_loss: -0.0014 - val_r2_inet_lambda_fv_loss: -0.8746 - val_mae_inet_coefficient_loss: 1.8625 - val_mae_inet_lambda_fv_loss: 0.0545\n",
      "Epoch 85/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0795 - r2_inet_coefficient_loss: -0.0016 - r2_inet_lambda_fv_loss: -0.8089 - mae_inet_coefficient_loss: 1.7459 - mae_inet_lambda_fv_loss: 0.0795 - val_loss: 0.0520 - val_r2_inet_coefficient_loss: -0.0010 - val_r2_inet_lambda_fv_loss: -0.8417 - val_mae_inet_coefficient_loss: 1.8649 - val_mae_inet_lambda_fv_loss: 0.0521\n",
      "Epoch 86/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0802 - r2_inet_coefficient_loss: -3.1942e-04 - r2_inet_lambda_fv_loss: -0.7973 - mae_inet_coefficient_loss: 1.7492 - mae_inet_lambda_fv_loss: 0.0802 - val_loss: 0.0557 - val_r2_inet_coefficient_loss: -0.0012 - val_r2_inet_lambda_fv_loss: -0.8698 - val_mae_inet_coefficient_loss: 1.8639 - val_mae_inet_lambda_fv_loss: 0.0558\n",
      "Epoch 87/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0781 - r2_inet_coefficient_loss: -6.5025e-04 - r2_inet_lambda_fv_loss: -0.8130 - mae_inet_coefficient_loss: 1.7515 - mae_inet_lambda_fv_loss: 0.0781 - val_loss: 0.0544 - val_r2_inet_coefficient_loss: -0.0010 - val_r2_inet_lambda_fv_loss: -0.8682 - val_mae_inet_coefficient_loss: 1.8647 - val_mae_inet_lambda_fv_loss: 0.0546\n",
      "Epoch 88/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0786 - r2_inet_coefficient_loss: -7.7573e-04 - r2_inet_lambda_fv_loss: -0.8059 - mae_inet_coefficient_loss: 1.7534 - mae_inet_lambda_fv_loss: 0.0786 - val_loss: 0.0582 - val_r2_inet_coefficient_loss: -2.7783e-04 - val_r2_inet_lambda_fv_loss: -0.8687 - val_mae_inet_coefficient_loss: 1.8686 - val_mae_inet_lambda_fv_loss: 0.0583\n",
      "Epoch 89/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0795 - r2_inet_coefficient_loss: 2.2387e-04 - r2_inet_lambda_fv_loss: -0.8065 - mae_inet_coefficient_loss: 1.7567 - mae_inet_lambda_fv_loss: 0.0795 - val_loss: 0.0540 - val_r2_inet_coefficient_loss: -2.5560e-04 - val_r2_inet_lambda_fv_loss: -0.8763 - val_mae_inet_coefficient_loss: 1.8682 - val_mae_inet_lambda_fv_loss: 0.0541\n",
      "Epoch 90/500\n",
      "35/35 [==============================] - 19s 559ms/step - loss: 0.0784 - r2_inet_coefficient_loss: 5.5920e-04 - r2_inet_lambda_fv_loss: -0.8152 - mae_inet_coefficient_loss: 1.7590 - mae_inet_lambda_fv_loss: 0.0784 - val_loss: 0.0522 - val_r2_inet_coefficient_loss: -6.2719e-04 - val_r2_inet_lambda_fv_loss: -0.8765 - val_mae_inet_coefficient_loss: 1.8682 - val_mae_inet_lambda_fv_loss: 0.0524\n",
      "Epoch 91/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0786 - r2_inet_coefficient_loss: 1.9036e-04 - r2_inet_lambda_fv_loss: -0.8141 - mae_inet_coefficient_loss: 1.7578 - mae_inet_lambda_fv_loss: 0.0786 - val_loss: 0.0516 - val_r2_inet_coefficient_loss: 5.3644e-07 - val_r2_inet_lambda_fv_loss: -0.8810 - val_mae_inet_coefficient_loss: 1.8773 - val_mae_inet_lambda_fv_loss: 0.0517\n",
      "Epoch 92/500\n",
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0778 - r2_inet_coefficient_loss: 6.9884e-04 - r2_inet_lambda_fv_loss: -0.8143 - mae_inet_coefficient_loss: 1.7637 - mae_inet_lambda_fv_loss: 0.0778 - val_loss: 0.0522 - val_r2_inet_coefficient_loss: -5.4379e-04 - val_r2_inet_lambda_fv_loss: -0.8693 - val_mae_inet_coefficient_loss: 1.8721 - val_mae_inet_lambda_fv_loss: 0.0523\n",
      "Epoch 93/500\n",
      "35/35 [==============================] - 19s 559ms/step - loss: 0.0768 - r2_inet_coefficient_loss: 2.8256e-04 - r2_inet_lambda_fv_loss: -0.8169 - mae_inet_coefficient_loss: 1.7624 - mae_inet_lambda_fv_loss: 0.0768 - val_loss: 0.0514 - val_r2_inet_coefficient_loss: -1.2071e-04 - val_r2_inet_lambda_fv_loss: -0.8771 - val_mae_inet_coefficient_loss: 1.8759 - val_mae_inet_lambda_fv_loss: 0.0516\n",
      "Epoch 94/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0777 - r2_inet_coefficient_loss: 8.7908e-04 - r2_inet_lambda_fv_loss: -0.8135 - mae_inet_coefficient_loss: 1.7665 - mae_inet_lambda_fv_loss: 0.0777 - val_loss: 0.0539 - val_r2_inet_coefficient_loss: -4.3082e-04 - val_r2_inet_lambda_fv_loss: -0.8818 - val_mae_inet_coefficient_loss: 1.8738 - val_mae_inet_lambda_fv_loss: 0.0540\n",
      "Epoch 95/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0783 - r2_inet_coefficient_loss: 0.0012 - r2_inet_lambda_fv_loss: -0.8062 - mae_inet_coefficient_loss: 1.7690 - mae_inet_lambda_fv_loss: 0.0783 - val_loss: 0.0573 - val_r2_inet_coefficient_loss: -7.0105e-04 - val_r2_inet_lambda_fv_loss: -0.8727 - val_mae_inet_coefficient_loss: 1.8728 - val_mae_inet_lambda_fv_loss: 0.0574\n",
      "Epoch 96/500\n",
      "35/35 [==============================] - 19s 559ms/step - loss: 0.0766 - r2_inet_coefficient_loss: 2.5406e-04 - r2_inet_lambda_fv_loss: -0.8200 - mae_inet_coefficient_loss: 1.7674 - mae_inet_lambda_fv_loss: 0.0766 - val_loss: 0.0570 - val_r2_inet_coefficient_loss: -3.8566e-04 - val_r2_inet_lambda_fv_loss: -0.8773 - val_mae_inet_coefficient_loss: 1.8767 - val_mae_inet_lambda_fv_loss: 0.0571\n",
      "Epoch 97/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0778 - r2_inet_coefficient_loss: 8.9822e-04 - r2_inet_lambda_fv_loss: -0.8100 - mae_inet_coefficient_loss: 1.7698 - mae_inet_lambda_fv_loss: 0.0778 - val_loss: 0.0523 - val_r2_inet_coefficient_loss: -6.6023e-04 - val_r2_inet_lambda_fv_loss: -0.8795 - val_mae_inet_coefficient_loss: 1.8778 - val_mae_inet_lambda_fv_loss: 0.0525\n",
      "Epoch 98/500\n",
      "35/35 [==============================] - 20s 564ms/step - loss: 0.0773 - r2_inet_coefficient_loss: 4.7112e-04 - r2_inet_lambda_fv_loss: -0.8151 - mae_inet_coefficient_loss: 1.7682 - mae_inet_lambda_fv_loss: 0.0773 - val_loss: 0.0519 - val_r2_inet_coefficient_loss: -3.4009e-04 - val_r2_inet_lambda_fv_loss: -0.8763 - val_mae_inet_coefficient_loss: 1.8760 - val_mae_inet_lambda_fv_loss: 0.0521\n",
      "Epoch 99/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0774 - r2_inet_coefficient_loss: 0.0014 - r2_inet_lambda_fv_loss: -0.8138 - mae_inet_coefficient_loss: 1.7729 - mae_inet_lambda_fv_loss: 0.0774 - val_loss: 0.0548 - val_r2_inet_coefficient_loss: -1.0359e-04 - val_r2_inet_lambda_fv_loss: -0.8657 - val_mae_inet_coefficient_loss: 1.8785 - val_mae_inet_lambda_fv_loss: 0.0549\n",
      "Epoch 100/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0784 - r2_inet_coefficient_loss: 8.6425e-04 - r2_inet_lambda_fv_loss: -0.8084 - mae_inet_coefficient_loss: 1.7717 - mae_inet_lambda_fv_loss: 0.0784 - val_loss: 0.0537 - val_r2_inet_coefficient_loss: -6.1812e-04 - val_r2_inet_lambda_fv_loss: -0.8846 - val_mae_inet_coefficient_loss: 1.8759 - val_mae_inet_lambda_fv_loss: 0.0538\n",
      "Epoch 101/500\n",
      " 9/35 [======>.......................] - ETA: 13s - loss: 0.0766 - r2_inet_coefficient_loss: 5.9803e-04 - r2_inet_lambda_fv_loss: -0.8281 - mae_inet_coefficient_loss: 1.6134 - mae_inet_lambda_fv_loss: 0.0766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 19s 555ms/step - loss: 0.0563 - r2_inet_coefficient_loss: 0.0072 - r2_inet_lambda_fv_loss: -0.8470 - mae_inet_coefficient_loss: 1.7505 - mae_inet_lambda_fv_loss: 0.0563 - val_loss: 0.0529 - val_r2_inet_coefficient_loss: 0.0065 - val_r2_inet_lambda_fv_loss: -0.8609 - val_mae_inet_coefficient_loss: 1.8781 - val_mae_inet_lambda_fv_loss: 0.0531\n",
      "Epoch 13/500\n",
      "35/35 [==============================] - 20s 558ms/step - loss: 0.0548 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8511 - mae_inet_coefficient_loss: 1.7499 - mae_inet_lambda_fv_loss: 0.0548 - val_loss: 0.0539 - val_r2_inet_coefficient_loss: 0.0063 - val_r2_inet_lambda_fv_loss: -0.8646 - val_mae_inet_coefficient_loss: 1.8783 - val_mae_inet_lambda_fv_loss: 0.0540\n",
      "Epoch 14/500\n",
      "35/35 [==============================] - 19s 548ms/step - loss: 0.0564 - r2_inet_coefficient_loss: 0.0072 - r2_inet_lambda_fv_loss: -0.8471 - mae_inet_coefficient_loss: 1.7494 - mae_inet_lambda_fv_loss: 0.0564 - val_loss: 0.0535 - val_r2_inet_coefficient_loss: 0.0064 - val_r2_inet_lambda_fv_loss: -0.8591 - val_mae_inet_coefficient_loss: 1.8773 - val_mae_inet_lambda_fv_loss: 0.0537\n",
      "Epoch 15/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0541 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8550 - mae_inet_coefficient_loss: 1.7481 - mae_inet_lambda_fv_loss: 0.0541 - val_loss: 0.0516 - val_r2_inet_coefficient_loss: 0.0066 - val_r2_inet_lambda_fv_loss: -0.8632 - val_mae_inet_coefficient_loss: 1.8769 - val_mae_inet_lambda_fv_loss: 0.0518\n",
      "Epoch 16/500\n",
      "35/35 [==============================] - 20s 564ms/step - loss: 0.0534 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8578 - mae_inet_coefficient_loss: 1.7472 - mae_inet_lambda_fv_loss: 0.0534 - val_loss: 0.0515 - val_r2_inet_coefficient_loss: 0.0067 - val_r2_inet_lambda_fv_loss: -0.8648 - val_mae_inet_coefficient_loss: 1.8767 - val_mae_inet_lambda_fv_loss: 0.0516\n",
      "Epoch 17/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0538 - r2_inet_coefficient_loss: 0.0075 - r2_inet_lambda_fv_loss: -0.8553 - mae_inet_coefficient_loss: 1.7465 - mae_inet_lambda_fv_loss: 0.0538 - val_loss: 0.0527 - val_r2_inet_coefficient_loss: 0.0065 - val_r2_inet_lambda_fv_loss: -0.8690 - val_mae_inet_coefficient_loss: 1.8746 - val_mae_inet_lambda_fv_loss: 0.0529\n",
      "Epoch 18/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0551 - r2_inet_coefficient_loss: 0.0072 - r2_inet_lambda_fv_loss: -0.8518 - mae_inet_coefficient_loss: 1.7448 - mae_inet_lambda_fv_loss: 0.0551 - val_loss: 0.0515 - val_r2_inet_coefficient_loss: 0.0060 - val_r2_inet_lambda_fv_loss: -0.8676 - val_mae_inet_coefficient_loss: 1.8702 - val_mae_inet_lambda_fv_loss: 0.0516\n",
      "Epoch 19/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0535 - r2_inet_coefficient_loss: 0.0072 - r2_inet_lambda_fv_loss: -0.8570 - mae_inet_coefficient_loss: 1.7439 - mae_inet_lambda_fv_loss: 0.0535 - val_loss: 0.0558 - val_r2_inet_coefficient_loss: 0.0067 - val_r2_inet_lambda_fv_loss: -0.8372 - val_mae_inet_coefficient_loss: 1.8742 - val_mae_inet_lambda_fv_loss: 0.0559\n",
      "Epoch 20/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0554 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8470 - mae_inet_coefficient_loss: 1.7434 - mae_inet_lambda_fv_loss: 0.0554 - val_loss: 0.0500 - val_r2_inet_coefficient_loss: 0.0062 - val_r2_inet_lambda_fv_loss: -0.8727 - val_mae_inet_coefficient_loss: 1.8709 - val_mae_inet_lambda_fv_loss: 0.0501\n",
      "Epoch 21/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0534 - r2_inet_coefficient_loss: 0.0072 - r2_inet_lambda_fv_loss: -0.8594 - mae_inet_coefficient_loss: 1.7421 - mae_inet_lambda_fv_loss: 0.0534 - val_loss: 0.0549 - val_r2_inet_coefficient_loss: 0.0064 - val_r2_inet_lambda_fv_loss: -0.8618 - val_mae_inet_coefficient_loss: 1.8678 - val_mae_inet_lambda_fv_loss: 0.0550\n",
      "Epoch 22/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0528 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8629 - mae_inet_coefficient_loss: 1.7416 - mae_inet_lambda_fv_loss: 0.0528 - val_loss: 0.0494 - val_r2_inet_coefficient_loss: 0.0064 - val_r2_inet_lambda_fv_loss: -0.8715 - val_mae_inet_coefficient_loss: 1.8702 - val_mae_inet_lambda_fv_loss: 0.0496\n",
      "Epoch 23/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0516 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8661 - mae_inet_coefficient_loss: 1.7408 - mae_inet_lambda_fv_loss: 0.0516 - val_loss: 0.0499 - val_r2_inet_coefficient_loss: 0.0064 - val_r2_inet_lambda_fv_loss: -0.8671 - val_mae_inet_coefficient_loss: 1.8698 - val_mae_inet_lambda_fv_loss: 0.0500\n",
      "Epoch 24/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0502 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8679 - mae_inet_coefficient_loss: 1.7393 - mae_inet_lambda_fv_loss: 0.0502 - val_loss: 0.0489 - val_r2_inet_coefficient_loss: 0.0062 - val_r2_inet_lambda_fv_loss: -0.8702 - val_mae_inet_coefficient_loss: 1.8671 - val_mae_inet_lambda_fv_loss: 0.0491\n",
      "Epoch 25/500\n",
      "35/35 [==============================] - 19s 552ms/step - loss: 0.0490 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8720 - mae_inet_coefficient_loss: 1.7383 - mae_inet_lambda_fv_loss: 0.0490 - val_loss: 0.0483 - val_r2_inet_coefficient_loss: 0.0065 - val_r2_inet_lambda_fv_loss: -0.8742 - val_mae_inet_coefficient_loss: 1.8669 - val_mae_inet_lambda_fv_loss: 0.0484\n",
      "Epoch 26/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0489 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8706 - mae_inet_coefficient_loss: 1.7374 - mae_inet_lambda_fv_loss: 0.0489 - val_loss: 0.0488 - val_r2_inet_coefficient_loss: 0.0064 - val_r2_inet_lambda_fv_loss: -0.8714 - val_mae_inet_coefficient_loss: 1.8632 - val_mae_inet_lambda_fv_loss: 0.0490\n",
      "Epoch 27/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0487 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8728 - mae_inet_coefficient_loss: 1.7358 - mae_inet_lambda_fv_loss: 0.0487 - val_loss: 0.0472 - val_r2_inet_coefficient_loss: 0.0066 - val_r2_inet_lambda_fv_loss: -0.8769 - val_mae_inet_coefficient_loss: 1.8643 - val_mae_inet_lambda_fv_loss: 0.0474\n",
      "Epoch 28/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0479 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8750 - mae_inet_coefficient_loss: 1.7350 - mae_inet_lambda_fv_loss: 0.0479 - val_loss: 0.0510 - val_r2_inet_coefficient_loss: 0.0057 - val_r2_inet_lambda_fv_loss: -0.8711 - val_mae_inet_coefficient_loss: 1.8619 - val_mae_inet_lambda_fv_loss: 0.0511\n",
      "Epoch 29/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0492 - r2_inet_coefficient_loss: 0.0070 - r2_inet_lambda_fv_loss: -0.8725 - mae_inet_coefficient_loss: 1.7344 - mae_inet_lambda_fv_loss: 0.0492 - val_loss: 0.0481 - val_r2_inet_coefficient_loss: 0.0065 - val_r2_inet_lambda_fv_loss: -0.8738 - val_mae_inet_coefficient_loss: 1.8658 - val_mae_inet_lambda_fv_loss: 0.0483\n",
      "Epoch 30/500\n",
      "35/35 [==============================] - 19s 558ms/step - loss: 0.0483 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8744 - mae_inet_coefficient_loss: 1.7351 - mae_inet_lambda_fv_loss: 0.0483 - val_loss: 0.0469 - val_r2_inet_coefficient_loss: 0.0065 - val_r2_inet_lambda_fv_loss: -0.8721 - val_mae_inet_coefficient_loss: 1.8643 - val_mae_inet_lambda_fv_loss: 0.0471\n",
      "Epoch 31/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0486 - r2_inet_coefficient_loss: 0.0076 - r2_inet_lambda_fv_loss: -0.8728 - mae_inet_coefficient_loss: 1.7355 - mae_inet_lambda_fv_loss: 0.0486 - val_loss: 0.0464 - val_r2_inet_coefficient_loss: 0.0066 - val_r2_inet_lambda_fv_loss: -0.8761 - val_mae_inet_coefficient_loss: 1.8653 - val_mae_inet_lambda_fv_loss: 0.0465\n",
      "Epoch 32/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0471 - r2_inet_coefficient_loss: 0.0077 - r2_inet_lambda_fv_loss: -0.8785 - mae_inet_coefficient_loss: 1.7362 - mae_inet_lambda_fv_loss: 0.0471 - val_loss: 0.0447 - val_r2_inet_coefficient_loss: 0.0072 - val_r2_inet_lambda_fv_loss: -0.8803 - val_mae_inet_coefficient_loss: 1.8690 - val_mae_inet_lambda_fv_loss: 0.0448\n",
      "Epoch 33/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0447 - r2_inet_coefficient_loss: 0.0078 - r2_inet_lambda_fv_loss: -0.8829 - mae_inet_coefficient_loss: 1.7388 - mae_inet_lambda_fv_loss: 0.0447 - val_loss: 0.0465 - val_r2_inet_coefficient_loss: 0.0075 - val_r2_inet_lambda_fv_loss: -0.8718 - val_mae_inet_coefficient_loss: 1.8738 - val_mae_inet_lambda_fv_loss: 0.0466\n",
      "Epoch 34/500\n",
      "35/35 [==============================] - 19s 553ms/step - loss: 0.0456 - r2_inet_coefficient_loss: 0.0081 - r2_inet_lambda_fv_loss: -0.8776 - mae_inet_coefficient_loss: 1.7406 - mae_inet_lambda_fv_loss: 0.0456 - val_loss: 0.0439 - val_r2_inet_coefficient_loss: 0.0070 - val_r2_inet_lambda_fv_loss: -0.8815 - val_mae_inet_coefficient_loss: 1.8726 - val_mae_inet_lambda_fv_loss: 0.0441\n",
      "Epoch 35/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0450 - r2_inet_coefficient_loss: 0.0079 - r2_inet_lambda_fv_loss: -0.8842 - mae_inet_coefficient_loss: 1.7410 - mae_inet_lambda_fv_loss: 0.0450 - val_loss: 0.0457 - val_r2_inet_coefficient_loss: 0.0069 - val_r2_inet_lambda_fv_loss: -0.8831 - val_mae_inet_coefficient_loss: 1.8723 - val_mae_inet_lambda_fv_loss: 0.0458\n",
      "Epoch 36/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0489 - r2_inet_coefficient_loss: 0.0079 - r2_inet_lambda_fv_loss: -0.8770 - mae_inet_coefficient_loss: 1.7415 - mae_inet_lambda_fv_loss: 0.0489 - val_loss: 0.0473 - val_r2_inet_coefficient_loss: 0.0069 - val_r2_inet_lambda_fv_loss: -0.8626 - val_mae_inet_coefficient_loss: 1.8752 - val_mae_inet_lambda_fv_loss: 0.0474\n",
      "Epoch 37/500\n",
      "35/35 [==============================] - 19s 549ms/step - loss: 0.0457 - r2_inet_coefficient_loss: 0.0079 - r2_inet_lambda_fv_loss: -0.8787 - mae_inet_coefficient_loss: 1.7427 - mae_inet_lambda_fv_loss: 0.0457 - val_loss: 0.0435 - val_r2_inet_coefficient_loss: 0.0068 - val_r2_inet_lambda_fv_loss: -0.8805 - val_mae_inet_coefficient_loss: 1.8737 - val_mae_inet_lambda_fv_loss: 0.0436\n",
      "Epoch 38/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0459 - r2_inet_coefficient_loss: 0.0077 - r2_inet_lambda_fv_loss: -0.8849 - mae_inet_coefficient_loss: 1.7417 - mae_inet_lambda_fv_loss: 0.0459 - val_loss: 0.0449 - val_r2_inet_coefficient_loss: 0.0066 - val_r2_inet_lambda_fv_loss: -0.8811 - val_mae_inet_coefficient_loss: 1.8724 - val_mae_inet_lambda_fv_loss: 0.0450\n",
      "Epoch 39/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0449 - r2_inet_coefficient_loss: 0.0075 - r2_inet_lambda_fv_loss: -0.8850 - mae_inet_coefficient_loss: 1.7424 - mae_inet_lambda_fv_loss: 0.0449 - val_loss: 0.0436 - val_r2_inet_coefficient_loss: 0.0064 - val_r2_inet_lambda_fv_loss: -0.8821 - val_mae_inet_coefficient_loss: 1.8717 - val_mae_inet_lambda_fv_loss: 0.0438\n",
      "Epoch 40/500\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0425 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8875 - mae_inet_coefficient_loss: 1.7428 - mae_inet_lambda_fv_loss: 0.0425 - val_loss: 0.0408 - val_r2_inet_coefficient_loss: 0.0066 - val_r2_inet_lambda_fv_loss: -0.8868 - val_mae_inet_coefficient_loss: 1.8756 - val_mae_inet_lambda_fv_loss: 0.0410\n",
      "Epoch 41/500\n",
      "35/35 [==============================] - 19s 557ms/step - loss: 0.0408 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8942 - mae_inet_coefficient_loss: 1.7443 - mae_inet_lambda_fv_loss: 0.0408 - val_loss: 0.0439 - val_r2_inet_coefficient_loss: 0.0065 - val_r2_inet_lambda_fv_loss: -0.8831 - val_mae_inet_coefficient_loss: 1.8798 - val_mae_inet_lambda_fv_loss: 0.0440\n",
      "Epoch 42/500\n",
      "35/35 [==============================] - 19s 559ms/step - loss: 0.0422 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8919 - mae_inet_coefficient_loss: 1.7469 - mae_inet_lambda_fv_loss: 0.0422 - val_loss: 0.0422 - val_r2_inet_coefficient_loss: 0.0063 - val_r2_inet_lambda_fv_loss: -0.8815 - val_mae_inet_coefficient_loss: 1.8793 - val_mae_inet_lambda_fv_loss: 0.0424\n",
      "Epoch 43/500\n",
      "35/35 [==============================] - 20s 560ms/step - loss: 0.0405 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8936 - mae_inet_coefficient_loss: 1.7476 - mae_inet_lambda_fv_loss: 0.0405 - val_loss: 0.0433 - val_r2_inet_coefficient_loss: 0.0064 - val_r2_inet_lambda_fv_loss: -0.8797 - val_mae_inet_coefficient_loss: 1.8817 - val_mae_inet_lambda_fv_loss: 0.0434\n",
      "Epoch 44/500\n",
      "35/35 [==============================] - 19s 554ms/step - loss: 0.0420 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8898 - mae_inet_coefficient_loss: 1.7502 - mae_inet_lambda_fv_loss: 0.0420 - val_loss: 0.0404 - val_r2_inet_coefficient_loss: 0.0063 - val_r2_inet_lambda_fv_loss: -0.8892 - val_mae_inet_coefficient_loss: 1.8836 - val_mae_inet_lambda_fv_loss: 0.0405\n",
      "Epoch 45/500\n",
      "35/35 [==============================] - 20s 561ms/step - loss: 0.0387 - r2_inet_coefficient_loss: 0.0074 - r2_inet_lambda_fv_loss: -0.8960 - mae_inet_coefficient_loss: 1.7542 - mae_inet_lambda_fv_loss: 0.0387 - val_loss: 0.0396 - val_r2_inet_coefficient_loss: 0.0062 - val_r2_inet_lambda_fv_loss: -0.8918 - val_mae_inet_coefficient_loss: 1.8861 - val_mae_inet_lambda_fv_loss: 0.0398\n",
      "Epoch 46/500\n",
      "35/35 [==============================] - 19s 556ms/step - loss: 0.0410 - r2_inet_coefficient_loss: 0.0073 - r2_inet_lambda_fv_loss: -0.8983 - mae_inet_coefficient_loss: 1.7555 - mae_inet_lambda_fv_loss: 0.0410 - val_loss: 0.0407 - val_r2_inet_coefficient_loss: 0.0056 - val_r2_inet_lambda_fv_loss: -0.8912 - val_mae_inet_coefficient_loss: 1.8854 - val_mae_inet_lambda_fv_loss: 0.0408\n",
      "Epoch 47/500\n",
      "35/35 [==============================] - 20s 562ms/step - loss: 0.0401 - r2_inet_coefficient_loss: 0.0069 - r2_inet_lambda_fv_loss: -0.8986 - mae_inet_coefficient_loss: 1.7580 - mae_inet_lambda_fv_loss: 0.0401 - val_loss: 0.0416 - val_r2_inet_coefficient_loss: 0.0061 - val_r2_inet_lambda_fv_loss: -0.8935 - val_mae_inet_coefficient_loss: 1.8945 - val_mae_inet_lambda_fv_loss: 0.0417\n",
      "Epoch 48/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0387 - r2_inet_coefficient_loss: 0.0070 - r2_inet_lambda_fv_loss: -0.9015 - mae_inet_coefficient_loss: 1.7632 - mae_inet_lambda_fv_loss: 0.0387 - val_loss: 0.0378 - val_r2_inet_coefficient_loss: 0.0058 - val_r2_inet_lambda_fv_loss: -0.8977 - val_mae_inet_coefficient_loss: 1.8975 - val_mae_inet_lambda_fv_loss: 0.0379\n",
      "Epoch 49/500\n",
      "35/35 [==============================] - 20s 563ms/step - loss: 0.0382 - r2_inet_coefficient_loss: 0.0067 - r2_inet_lambda_fv_loss: -0.9049 - mae_inet_coefficient_loss: 1.7672 - mae_inet_lambda_fv_loss: 0.0382 - val_loss: 0.0368 - val_r2_inet_coefficient_loss: 0.0060 - val_r2_inet_lambda_fv_loss: -0.8914 - val_mae_inet_coefficient_loss: 1.8976 - val_mae_inet_lambda_fv_loss: 0.0369\n",
      "Epoch 50/500\n",
      "35/35 [==============================] - 20s 559ms/step - loss: 0.0353 - r2_inet_coefficient_loss: 0.0067 - r2_inet_lambda_fv_loss: -0.9075 - mae_inet_coefficient_loss: 1.7703 - mae_inet_lambda_fv_loss: 0.0353 - val_loss: 0.0360 - val_r2_inet_coefficient_loss: 0.0056 - val_r2_inet_lambda_fv_loss: -0.9041 - val_mae_inet_coefficient_loss: 1.9009 - val_mae_inet_lambda_fv_loss: 0.0362\n",
      "Epoch 51/500\n",
      "33/35 [===========================>..] - ETA: 1s - loss: 0.0357 - r2_inet_coefficient_loss: 0.0068 - r2_inet_lambda_fv_loss: -0.9095 - mae_inet_coefficient_loss: 1.7695 - mae_inet_lambda_fv_loss: 0.0357"
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "(history_list, \n",
    "\n",
    "#scores_valid_list,\n",
    "scores_test_list, \n",
    "\n",
    "#function_values_valid_list, \n",
    "function_values_test_list, \n",
    "\n",
    "#polynomial_dict_valid_list,\n",
    "polynomial_dict_test_list,\n",
    "\n",
    "#distrib_dict_valid_list,\n",
    "distrib_dict_test_list,\n",
    "\n",
    "model_list) = calculate_interpretation_net_results(lambda_net_train_dataset_list, \n",
    "                                                   lambda_net_valid_dataset_list, \n",
    "                                                   lambda_net_test_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize = tf.constant([float(i) for i in range(interpretation_net_output_shape)])\n",
    "\n",
    "if interpretation_net_output_monomials != None:\n",
    "    poly_optimize_coeffs = poly_optimize[:interpretation_net_output_monomials]\n",
    "\n",
    "    poly_optimize_identifiers_list = []\n",
    "    for i in range(interpretation_net_output_monomials):\n",
    "        poly_optimize_identifiers = tf.math.softmax(poly_optimize[sparsity*i+interpretation_net_output_monomials:sparsity*(i+1)+interpretation_net_output_monomials])\n",
    "        poly_optimize_identifiers_list.append(poly_optimize_identifiers)\n",
    "    poly_optimize_identifiers_list = tf.keras.backend.flatten(poly_optimize_identifiers_list)\n",
    "    poly_optimize = tf.concat([poly_optimize_coeffs, poly_optimize_identifiers_list], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.196958Z",
     "start_time": "2021-01-07T20:33:18.177611Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "polynomial_inet = polynomial_dict_test_list[-1]['inet_polynomials'][index_min]\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][index_min])\n",
    "\n",
    "print_polynomial_from_coefficients(polynomial_inet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lstsq_lambda_pred_polynomials_VS_inet_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.820457Z",
     "start_time": "2021-01-07T20:33:18.813628Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "polynomial_target = lambda_net_test_dataset.target_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_target, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:22.149541Z",
     "start_time": "2021-01-07T20:33:22.141264Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "polynomial_inet = polynomial_dict_test_list[-1]['inet_polynomials'][index_max]\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][index_max])\n",
    "\n",
    "print_polynomial_from_coefficients(polynomial_inet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:22.526765Z",
     "start_time": "2021-01-07T20:33:22.518702Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][index_max])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_max]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:23.155159Z",
     "start_time": "2021-01-07T20:33:23.146225Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_target_polynomials']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "polynomial_target = lambda_net_test_dataset.target_polynomial_list[index_max]\n",
    "print_polynomial_from_coefficients(polynomial_target, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['target_polynomials_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'] > -50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.417509Z",
     "start_time": "2021-01-07T15:49:44.181928Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'] > -50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.935810Z",
     "start_time": "2021-01-07T15:49:44.419772Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'] > -50], binwidth=0.2)\n",
    "p.set(xlim=(-30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:45.795559Z",
     "start_time": "2021-01-07T15:49:44.938329Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_inet_polynomials'] > -50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:47.784878Z",
     "start_time": "2021-01-07T15:49:45.797362Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'] > -50], binwidth=0.2)\n",
    "p.set(xlim=(-10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.252121Z",
     "start_time": "2021-01-07T15:49:47.786575Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['R2'].loc['target_polynomials_VS_lstsq_lambda_pred_polynomials'] > -50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == train_features_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isinf(features_autoMPG_model.values).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[42], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    x = tf.linspace(0.0, 1, 250)#tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, normalized=False, normalization_parameter_dict=None)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function:')\n",
    "    #display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    #ax.set_ylim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=500,\n",
    "         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = keras.models.clone_model(model)\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/1000\n",
    "#model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.set_weights(model_2_normalized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.predict([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_metric_list)\n",
    "    if length_plt >= 2:\n",
    "        fig, ax = plt.subplots(length_plt//2, 2, figsize=(30,20))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "\n",
    "    for index, metric in enumerate(plot_metric_list):\n",
    "        \n",
    "        plot_scores_dict = {}\n",
    "        for key in evaluation_key_list:\n",
    "            try:\n",
    "                scores_list[-1][metric].loc[key]\n",
    "                plot_scores_dict[key] = []\n",
    "            except:\n",
    "                #print(key + 'not in scores_list')\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        for scores in scores_list:\n",
    "            for key in evaluation_key_list:\n",
    "                try:\n",
    "                    plot_scores_dict[key].append(scores[metric].loc[key])\n",
    "                except:\n",
    "                    #print(key + 'not in scores_list')\n",
    "                    continue\n",
    "                                        \n",
    "            \n",
    "        plot_df = pd.DataFrame(data=np.vstack(plot_scores_dict.values()).T, \n",
    "                               index=x_axis_steps,\n",
    "                               columns=plot_scores_dict.keys())\n",
    "\n",
    "        if length_plt >= 2:\n",
    "            ax[index//2, index%2].set_title(metric)\n",
    "            sns.set(font_scale = 1.25)\n",
    "            p = sns.lineplot(data=plot_df, ax=ax[index//2, index%2])\n",
    "        else:\n",
    "            ax.set_title(metric)\n",
    "            sns.set(font_scale = 1.25)\n",
    "            p = sns.lineplot(data=plot_df, ax=ax)\n",
    "\n",
    "        if ylim != None:\n",
    "            p.set(ylim=ylim)\n",
    "\n",
    "        p.set_yticklabels(np.round(p.get_yticks(), 2), size = 20)\n",
    "        p.set_xticklabels(p.get_xticks(), size = 20)     \n",
    "        \n",
    "        #p.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        p.legend(loc='upper center', bbox_to_anchor=(0.47, -0.1),\n",
    "          fancybox=False, shadow=False, ncol=2, fontsize=12)   \n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
