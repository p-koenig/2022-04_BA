{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:07.684459Z",
     "start_time": "2020-12-02T13:39:07.676264Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:08.039619Z",
     "start_time": "2020-12-02T13:39:07.688088Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3  \n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "n_jobs = 21\n",
    "\n",
    "\n",
    "data_size = 10000 #for loading lambda models\n",
    "\n",
    "#specify interpretation net structure\n",
    "optimizer = 'adam'\n",
    "dropout = 0\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "interpretation_network_layers = [2048]\n",
    "\n",
    "random_evaluation_dataset_size = 500\n",
    "\n",
    "#lambda net specifications for loading (need to be set according to lambda net training to load correct weights)\n",
    "epochs_lambda = 200\n",
    "batch_lambda = 64\n",
    "lambda_network_layers = [5*sparsity]\n",
    "optimizer_lambda = '_' + 'SGD'\n",
    "\n",
    "\n",
    "lambda_dataset_size = 1000\n",
    "\n",
    "#set if multi_epoch_analysis should be performed\n",
    "multi_epoch_analysis = True\n",
    "each_epochs_save_lambda = 10 #None if no checkpointing (otherwise set according to lambda-net training)\n",
    "epoch_start = 0 #use to skip first epochs in multi_epoch_analysis\n",
    "\n",
    "#set if samples analysis should be performed\n",
    "samples_list = None#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "\n",
    "evaluate_with_real_function = False\n",
    "consider_labels_training = False\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "fixed_seed_lambda_training = False\n",
    "initialize_network_zero = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:08.059949Z",
     "start_time": "2020-12-02T13:39:08.043966Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n] \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else None\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_SeedMethod'\n",
    "elif not fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_NoSeedMethod'\n",
    "    \n",
    "    \n",
    "if initialize_network_zero:\n",
    "    seed_shuffle_string = seed_shuffle_string + '_zero_initialize'\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "structure = '_' + layers_str + str(epochs_lambda) + 'e' + str(batch_lambda) + 'b' + optimizer_lambda\n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure\n",
    "\n",
    "interpretation_network_string = 'drop' + str(dropout) + 'e' + str(epochs) + 'b' + str(batch_size) + '_' + str(interpretation_network_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:46.776501Z",
     "start_time": "2020-12-02T13:39:08.063785Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('./data/plotting/' + interpretation_network_string + filename + '/')\n",
    "    os.mkdir('./data/results/' + interpretation_network_string + filename + '/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:46.790776Z",
     "start_time": "2020-12-02T13:39:46.781354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:46.824141Z",
     "start_time": "2020-12-02T13:39:46.793920Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "\n",
    "def return_float_tensor_representation(some_representation, dtype=tf.float32):\n",
    "    if tf.is_tensor(some_representation):\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "    else:\n",
    "        some_representation = tf.convert_to_tensor(some_representation)\n",
    "        some_representation = tf.dtypes.cast(some_representation, dtype) \n",
    "        \n",
    "    if not tf.is_tensor(some_representation):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(dtype) + ':' + str(some_representation))\n",
    "     \n",
    "    return some_representation\n",
    "\n",
    "\n",
    "def return_numpy_representation(some_representation):\n",
    "    if isinstance(some_representation, pd.DataFrame):\n",
    "        some_representation = some_representation.values\n",
    "        \n",
    "    if isinstance(some_representation, list):\n",
    "        some_representation = np.array(some_representation)\n",
    "    \n",
    "    if not isinstance(some_representation, np.ndarray):\n",
    "        raise SystemExit('Given variable is no instance of ' + str(np.ndarray) + ':' + str(some_representation))\n",
    "    \n",
    "    return some_representation\n",
    "\n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict\n",
    "\n",
    "def return_callbacks_from_string(callback_string_list):\n",
    "    callbacks = [] if len(callback_string_list) > 0 else None\n",
    "    #if 'plot_losses_callback' in callback_string_list:\n",
    "        #callbacks.append(PlotLossesCallback())\n",
    "    if 'reduce_lr_loss' in callback_string_list:\n",
    "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=epochs/10, verbose=0, min_delta=0, mode='min') #epsilon\n",
    "        callbacks.append(reduce_lr_loss)\n",
    "    if 'early_stopping' in callback_string_list:\n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0, verbose=0, mode='min')\n",
    "        callbacks.append(earlyStopping)\n",
    "        \n",
    "    #if not multi_epoch_analysis and samples_list == None: \n",
    "        #callbacks.append(TQDMNotebookCallback())\n",
    "        \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:46.919023Z",
     "start_time": "2020-12-02T13:39:46.827431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae5d75ccbd24001b666943e4294bb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296fb2753b3f486ba189b0b262537c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:46.937471Z",
     "start_time": "2020-12-02T13:39:46.921679Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with polynomials on function value basis\n",
    "\n",
    "def calculate_mae_fv(polynomial_true_pred):\n",
    "    polynomial_true = polynomial_true_pred[0]\n",
    "    polynomial_pred = polynomial_true_pred[1]\n",
    "    \n",
    "    global lambda_train_input_train_split \n",
    "    lambda_input = lambda_train_input_train_split[0]\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "            \n",
    "        polynomial_true_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_true))\n",
    "        polynomial_true_fv = tf.reduce_sum(polynomial_true_value_per_term)\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "        \n",
    "        if index == 0:   \n",
    "            result = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "        else:           \n",
    "            current_valiue = tf.math.abs(tf.math.subtract(polynomial_true_fv, polynomial_pred_fv))\n",
    "            result = tf.math.add(result, current_valiue)    \n",
    "            \n",
    "    return  tf.math.divide(result, lambda_input.shape[0]) #tf.random.uniform(shape=[1], minval=0.1, maxval=10.0)\n",
    "\n",
    "def mean_absolute_error_tf_fv(y_true, y_pred):\n",
    "        \n",
    "    y_true = return_float_tensor_representation(y_true)\n",
    "    y_pred = return_float_tensor_representation(y_pred)\n",
    "    \n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv, (y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:46.971298Z",
     "start_time": "2020-12-02T13:39:46.940088Z"
    }
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with lambda-net prediction based (predictions made in loss function)\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "\n",
    "def mean_absolute_error_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers, base_model):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers)    \n",
    "    \n",
    "    model_lambda_placeholder = keras.models.clone_model(base_model)  \n",
    "    \n",
    "    weights_structure = base_model.get_weights()\n",
    "    dims = [np_arrays.shape for np_arrays in weights_structure]\n",
    "    \n",
    "    def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        network_parameters = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "        polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "        network_parameters = return_float_tensor_representation(network_parameters)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred)\n",
    "        return tf.math.reduce_mean(tf.map_fn(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder), (polynomial_pred, network_parameters), fn_output_signature=tf.float32))\n",
    "    return mean_absolute_error_tf_fv_lambda_extended\n",
    "\n",
    "\n",
    "#CHANGES NEEDED\n",
    "def calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers, dims, model_lambda_placeholder):\n",
    "\n",
    "    def calculate_mae_fv_lambda(input_list):\n",
    "\n",
    "        #single polynomials\n",
    "        #polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[0]\n",
    "        network_parameters = input_list[1]\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        #CALCULATE LAMBDA FV HERE FOR EVALUATION DATASET\n",
    "        # build models\n",
    "        start = 0\n",
    "        layers = []\n",
    "        for i in range(len(dims)//2):\n",
    "            \n",
    "            # set weights of layer\n",
    "            index = i*2\n",
    "            size = np.product(dims[index])\n",
    "            weights_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[0].assign(weights_tf_true)\n",
    "            start += size\n",
    "            \n",
    "            # set biases of layer\n",
    "            index += 1\n",
    "            size = np.product(dims[index])\n",
    "            biases_tf_true = tf.reshape(network_parameters[start:start+size], dims[index])\n",
    "            model_lambda_placeholder.layers[i].weights[1].assign(biases_tf_true)\n",
    "            start += size\n",
    "\n",
    "        \n",
    "        lambda_fv = tf.keras.backend.flatten(model_lambda_placeholder(evaluation_dataset))\n",
    "        \n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_lambda\n",
    "\n",
    "#nothing to change here (just fv calculation for evaluation entry for single polynomial)\n",
    "def calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred):\n",
    "\n",
    "\n",
    "    def calculate_fv_from_data(evaluation_entry):\n",
    "\n",
    "\n",
    "        value_without_coefficient = tf.vectorized_map(calculate_value_without_coefficient_wrapper(evaluation_entry), (list_of_monomial_identifiers))\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        \n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)     \n",
    "        \n",
    "        return polynomial_pred_fv\n",
    "    return calculate_fv_from_data\n",
    "\n",
    "\n",
    "#calculate intermediate term (without coefficient multiplication)\n",
    "def calculate_value_without_coefficient_wrapper(evaluation_entry):\n",
    "    def calculate_value_without_coefficient(coefficient_multiplier_term):      \n",
    "   \n",
    "        return tf.math.reduce_prod(tf.vectorized_map(lambda x: x[0]**x[1], (evaluation_entry, coefficient_multiplier_term)))\n",
    "    return calculate_value_without_coefficient\n",
    "\n",
    "#calculate MAE at the end ---> general:REPLACE FUNCTION WITH LOSS CALL OR LAMBDA\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:46.991428Z",
     "start_time": "2020-12-02T13:39:46.974242Z"
    }
   },
   "outputs": [],
   "source": [
    "#Manual TF Loss function for comparison with pre-saved lambda-net predictions\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "def calculate_mae_single_input_preds_appended(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def calculate_mae_fv_lambda_preds_appended(input_list):\n",
    "    \n",
    "    polynomial_true = input_list[0]\n",
    "    polynomial_pred = input_list[1]\n",
    "    lambda_fv = input_list[2]\n",
    "\n",
    "    global lambda_train_input_train_split \n",
    "    lambda_input = lambda_train_input_train_split[0] #[0] correct here?\n",
    "    \n",
    "    for index, lambda_input_entry in enumerate(lambda_input):\n",
    "        value_without_coefficient = np.array([reduce(lambda x, y: x*y, [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multiplier_term, lambda_input_entry)]) for coefficient_multiplier_term in list_of_monomial_identifiers], dtype='float32')\n",
    "\n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)\n",
    "\n",
    "        if index == 0:\n",
    "            polynomial_pred_fv_list = tf.convert_to_tensor([polynomial_pred_fv])\n",
    "        else:\n",
    "            polynomial_pred_fv_list = tf.concat([polynomial_pred_fv_list, tf.convert_to_tensor([polynomial_pred_fv])], 0)\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input_preds_appended, (lambda_fv, polynomial_pred_fv_list)))\n",
    "  \n",
    "    \n",
    "def mean_absolute_error_tf_fv_lambda_preds_appended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "    lambda_fv = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "    return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda_preds_appended, (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:47.038908Z",
     "start_time": "2020-12-02T13:39:46.994550Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error #DEPRECATED, MAYBE NOT WORKING\n",
    "\n",
    "#Manual TF Loss function for comparison with pre-saved lambda-net predictions\n",
    "\n",
    "#extended means that the lambda-net predictions are appended to y_true \n",
    "#in order to get them into the loss function without loosing the allocation\n",
    "\n",
    "\n",
    "def calculate_mae_single_input(input_list):\n",
    "    true_fv = input_list[0]\n",
    "    pred_fv = input_list[1]\n",
    "\n",
    "    return tf.math.abs(tf.math.subtract(true_fv, pred_fv))\n",
    "\n",
    "\n",
    "def calculate_value_without_coefficient_wrapper(evaluation_entry):\n",
    "    def calculate_value_without_coefficient(coefficient_multiplier_term):      \n",
    "        #return tf.math.reduce_prod(tf.vectorized_map(calculate_single_value_without_coefficient, (tf.strings.to_number(tf.strings.bytes_split(coefficient_multiplier_term), tf.float64), evaluation_entry)))\n",
    "        \n",
    "\n",
    "        \n",
    "        #coefficient_multiplier_term_integers = tf.reshape(tf.strings.to_number(tf.strings.bytes_split(coefficient_multiplier_term), tf.float64), [n,])\n",
    "        \n",
    "        print('calculate_value_without_coefficient_wrapper')\n",
    "        print(coefficient_multiplier_term.get_shape())\n",
    "        print(evaluation_entry.get_shape())\n",
    "        \n",
    "        return tf.math.reduce_prod(tf.vectorized_map(lambda x: x[0]**x[1], (evaluation_entry, coefficient_multiplier_term)))\n",
    "    return calculate_value_without_coefficient\n",
    "\n",
    "\n",
    "def calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred):\n",
    "    \n",
    "    #print('calculate_fv_from_data_wrapper')\n",
    "    #print(list_of_monomial_identifiers)\n",
    "    #print(list_of_monomial_identifiers.get_shape())\n",
    "    #print(polynomial_pred)\n",
    "    #print(polynomial_pred.get_shape())\n",
    "\n",
    "    def calculate_fv_from_data(evaluation_entry):\n",
    "        \n",
    "        #print('calculate_fv_from_data')\n",
    "        #print(evaluation_entry)\n",
    "        #print(evaluation_entry.get_shape())\n",
    "\n",
    "        value_without_coefficient = tf.vectorized_map(calculate_value_without_coefficient_wrapper(evaluation_entry), (list_of_monomial_identifiers))\n",
    "        \n",
    "        #print(value_without_coefficient)\n",
    "        #print(value_without_coefficient.get_shape())\n",
    "        \n",
    "        polynomial_pred_value_per_term = tf.vectorized_map(lambda x: x[0]*x[1], (value_without_coefficient, polynomial_pred))\n",
    "        \n",
    "        #print(polynomial_pred_value_per_term)\n",
    "        #print(polynomial_pred_value_per_term.get_shape().ndims)\n",
    "        \n",
    "        polynomial_pred_fv = tf.reduce_sum(polynomial_pred_value_per_term)     \n",
    "        \n",
    "        return polynomial_pred_fv\n",
    "    return calculate_fv_from_data\n",
    "\n",
    "def calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "\n",
    "    def calculate_mae_fv_lambda(input_list):\n",
    "\n",
    "        polynomial_true = input_list[0]\n",
    "        polynomial_pred = input_list[1]\n",
    "        lambda_fv = input_list[2]\n",
    "        #evaluation_dataset = input_list[3]\n",
    "        #list_of_monomial_identifiers = input_list[4]\n",
    "\n",
    "        #print('calculate_mae_fv_lambda')\n",
    "        #print(list_of_monomial_identifiers)\n",
    "        #print(list_of_monomial_identifiers.get_shape())\n",
    "        #print(polynomial_pred)\n",
    "        #print(polynomial_pred.get_shape())\n",
    "        #print(evaluation_dataset)\n",
    "        #print(evaluation_dataset.get_shape())\n",
    "\n",
    "        \n",
    "        polynomial_pred_fv_list = tf.vectorized_map(calculate_fv_from_data_wrapper(list_of_monomial_identifiers, polynomial_pred), (evaluation_dataset))\n",
    "\n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_single_input, (lambda_fv, polynomial_pred_fv_list)))\n",
    "    \n",
    "    return calculate_mae_fv_lambda\n",
    "    \n",
    "def mean_absolute_error_tf_fv_lambda_extended_wrapper(evaluation_dataset, list_of_monomial_identifiers):\n",
    "    \n",
    "    evaluation_dataset = return_float_tensor_representation(evaluation_dataset, tf.float64)\n",
    "    list_of_monomial_identifiers = return_float_tensor_representation(list_of_monomial_identifiers, tf.float64)    \n",
    "    \n",
    "    def mean_absolute_error_tf_fv_lambda_extended(polynomial_true_with_lambda_fv, polynomial_pred):\n",
    "\n",
    "        lambda_fv = polynomial_true_with_lambda_fv[:,sparsity:]\n",
    "        polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "\n",
    "        lambda_fv = return_float_tensor_representation(lambda_fv, tf.float64)\n",
    "        polynomial_true = return_float_tensor_representation(polynomial_true, tf.float64)\n",
    "        polynomial_pred = return_float_tensor_representation(polynomial_pred, tf.float64)\n",
    "        #return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers), (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "        return tf.math.reduce_mean(tf.vectorized_map(calculate_mae_fv_lambda_wrapper(evaluation_dataset, list_of_monomial_identifiers), (polynomial_true, polynomial_pred, lambda_fv)))\n",
    "    return mean_absolute_error_tf_fv_lambda_extended\n",
    "\n",
    "def mean_absolute_error_extended(polynomial_true_with_lambda_fv, polynomial_pred): \n",
    "    polynomial_true = polynomial_true_with_lambda_fv[:,:sparsity]\n",
    "    return tf.keras.losses.MAE(polynomial_true, polynomial_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:47.068313Z",
     "start_time": "2020-12-02T13:39:47.043961Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Basic Keras/TF Loss functions\n",
    "def root_mean_squared_error(y_true, y_pred):   \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "        \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)           \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))      \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred) \n",
    "            \n",
    "    n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "    y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "    y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    y_true =  return_float_tensor_representation(y_true)\n",
    "    y_pred =  return_float_tensor_representation(y_pred)        \n",
    "    epsilon = return_float_tensor_representation(epsilon)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T13:39:47.087164Z",
     "start_time": "2020-12-02T13:39:47.074844Z"
    }
   },
   "outputs": [],
   "source": [
    "#Manual calculations for comparison of polynomials based on function values (no TF!)\n",
    "\n",
    "def calcualate_function_value(coefficient_list, lambda_input_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "    \n",
    "    result = 0   \n",
    "        \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        value_without_coefficient = [lambda_input_value**int(coefficient_multiplier) for coefficient_multiplier, lambda_input_value in zip(coefficient_multipliers, lambda_input_entry)]\n",
    "\n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, value_without_coefficient)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calculate_function_values_from_polynomial(polynomial, lambda_input_data):\n",
    "    polynomial = return_numpy_representation(polynomial)\n",
    "    \n",
    "    function_value_list = []\n",
    "        \n",
    "    for lambda_input_entry in lambda_input_data:\n",
    "        function_value = calcualate_function_value(polynomial, lambda_input_entry)\n",
    "        function_value_list.append(function_value)\n",
    "\n",
    "    return np.array(function_value_list)\n",
    "\n",
    "\n",
    "def parallel_fv_calculation_from_polynomial(polynomial_list, lambda_input_list):\n",
    "    parallel = Parallel(n_jobs=10, verbose=0, backend='threading')\n",
    "    polynomial_true_fv = parallel(delayed(calculate_function_values_from_polynomial)(polynomial, lambda_inputs) for polynomial, lambda_inputs in zip(polynomial_list, lambda_input_list))  \n",
    "    del parallel   \n",
    "    \n",
    "\n",
    "    return np.array(polynomial_true_fv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.526Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Standard Metrics (no TF!)\n",
    "\n",
    "def mean_absolute_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)      \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(true_values-pred_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))  \n",
    "\n",
    "def root_mean_squared_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)         \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sqrt(np.mean((true_values-pred_values)**2)))\n",
    "    \n",
    "    return np.mean(np.array(result_list)) \n",
    "\n",
    "def mean_absolute_percentage_error_function_values(y_true, y_pred, epsilon=10e-3):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred) \n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.mean(np.abs(((true_values-pred_values)/(true_values+epsilon)))))\n",
    "\n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def r2_score_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(r2_score(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_absolute_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.sum(np.abs(true_values-pred_values))/(true_values.shape[0]*np.std(true_values)))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def relative_maximum_average_error_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(np.max(true_values-pred_values)/np.std(true_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_area_between_two_curves_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "      \n",
    "    assert(number_of_variables==1)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(area_between_two_curves(true_values, pred_values))\n",
    " \n",
    "    return np.mean(np.array(result_list))\n",
    "\n",
    "def mean_dtw_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "\n",
    "    result_list_single = []\n",
    "    result_list_array = []\n",
    "    \n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_single_value, result_single_array = dtw(true_values, pred_values)\n",
    "        result_list_single.append(result_single_value)\n",
    "        result_list_array.append(result_single_array)\n",
    "    \n",
    "    return np.mean(np.array(result_list_single)), np.mean(np.array(result_list_array), axis=1)\n",
    "\n",
    "def mean_frechet_dist_function_values(y_true, y_pred):\n",
    "    y_true = return_numpy_representation(y_true)\n",
    "    y_pred = return_numpy_representation(y_pred)\n",
    "    \n",
    "    result_list = []\n",
    "    for true_values, pred_values in zip(y_true, y_pred):\n",
    "        result_list.append(frechet_dist(true_values, pred_values))\n",
    "    \n",
    "    return np.mean(np.array(result_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.528Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_interpretation_net(y_data_real, \n",
    "                                y_data_pred, \n",
    "                                polynomial_true_fv, \n",
    "                                polynomial_pred_inet_fv):\n",
    "    \n",
    "    mae_coeff = np.round(mean_absolute_error(y_data_real, y_data_pred), 4)\n",
    "    rmse_coeff = np.round(root_mean_squared_error(y_data_real, y_data_pred), 4)\n",
    "    mape_coeff = np.round(mean_absolute_percentage_error_keras(y_data_real, y_data_pred), 4)\n",
    "    accuracy_coeff = np.round(accuracy_single(y_data_real, y_data_pred), 4)\n",
    "    accuracy_multi_coeff = np.round(accuracy_multilabel(y_data_real, y_data_pred), 4)\n",
    "    \n",
    "    print(polynomial_true_fv.shape)\n",
    "    print(polynomial_pred_inet_fv.shape)\n",
    "    \n",
    "    mae_fv = np.round(mean_absolute_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmse_fv = np.round(root_mean_squared_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    mape_fv = np.round(mean_absolute_percentage_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    r2_fv = np.round(r2_score_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    raae_fv = np.round(relative_absolute_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4)\n",
    "    rmae_fv = np.round(relative_maximum_average_error_function_values(polynomial_true_fv, polynomial_pred_inet_fv), 4) \n",
    "\n",
    "    std_fv = np.std(polynomial_pred_inet_fv)\n",
    "    mean_fv = np.mean(polynomial_pred_inet_fv)\n",
    "\n",
    "    return {\n",
    "             'MAE': mae_coeff,\n",
    "             'RMSE': rmse_coeff, \n",
    "             'MAPE': mape_coeff,\n",
    "             'Accuracy': accuracy_coeff, \n",
    "             'Accuracy Multilabel': accuracy_multi_coeff, \n",
    "\n",
    "             'MAE FV': mae_fv,\n",
    "             'RMSE FV': rmse_fv,\n",
    "             'MAPE FV': mape_fv,\n",
    "             'R2 FV': r2_fv,\n",
    "             'RAAE FV': raae_fv,\n",
    "             'RMAE FV': rmae_fv,         \n",
    "             'STD FV PRED': std_fv,   \n",
    "             'MEAN FV PRED': mean_fv\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.531Z"
    }
   },
   "outputs": [],
   "source": [
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def generate_random_x_values(size, x_max, x_min, x_step, numnber_of_variables):\n",
    "    x_values_list = []\n",
    "    \n",
    "    for j in range(size):\n",
    "        values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))\n",
    "        while arreq_in_list(values, x_values_list):\n",
    "                values = np.round(np.array(random_product(np.arange(x_min, x_max, x_step), repeat=numnber_of_variables)), int(-np.log10(x_step)))         \n",
    "        x_values_list.append(values)\n",
    "    \n",
    "    return np.array(x_values_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.534Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    return weight_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=21)]: Using backend LokyBackend with 21 concurrent workers.\n",
      "[Parallel(n_jobs=21)]: Done   4 out of  21 | elapsed:  1.1min remaining:  4.6min\n",
      "[Parallel(n_jobs=21)]: Done  12 out of  21 | elapsed:  1.1min remaining:   49.9s\n",
      "[Parallel(n_jobs=21)]: Done  21 out of  21 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if multi_epoch_analysis:  \n",
    "    weight_data_list = []\n",
    "    \n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    weight_data_list = parallel(delayed(load_data)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    weight_data = weight_data_list[-1]\n",
    "else:\n",
    "\n",
    "    foldername = 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "                \n",
    "    path = './data/weights/' + foldername + 'weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3) + filename + '.txt'\n",
    "\n",
    "    weight_data = pd.read_csv(path, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=[i for i in range(nCr(n+d, d))]).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8795d01f78e49f59959923fa2f1af41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if multi_epoch_analysis == False:\n",
    "    path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(epochs_lambda).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    lambda_train_fv_with_lambda_input = pd.read_csv(path_lambda_train_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_valid_fv_with_lambda_input = pd.read_csv(path_lambda_valid_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    lambda_test_fv_with_lambda_input = pd.read_csv(path_lambda_test_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_train_fv = lambda_train_fv_with_lambda_input[lambda_train_fv_with_lambda_input.columns[n::n+1]]\n",
    "    lambda_valid_fv = lambda_valid_fv_with_lambda_input[lambda_valid_fv_with_lambda_input.columns[n::n+1]]\n",
    "    lambda_test_fv = lambda_test_fv_with_lambda_input[lambda_test_fv_with_lambda_input.columns[n::n+1]]\n",
    "    \n",
    "    lambda_train_input = lambda_train_fv_with_lambda_input.drop(lambda_train_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input.shape[0], int((lambda_train_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    lambda_valid_input = lambda_valid_fv_with_lambda_input.drop(lambda_valid_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input.shape[0], int((lambda_valid_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    lambda_test_input = lambda_test_fv_with_lambda_input.drop(lambda_test_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input.shape[0], int((lambda_test_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    \n",
    "    lambda_train_input = lambda_train_fv_with_lambda_input.drop(lambda_train_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input.shape[0], int((lambda_train_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    lambda_valid_input = lambda_valid_fv_with_lambda_input.drop(lambda_valid_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input.shape[0], int((lambda_valid_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    lambda_test_input = lambda_test_fv_with_lambda_input.drop(lambda_test_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input.shape[0], int((lambda_test_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "    \n",
    "else:\n",
    "    lambda_train_fv_list = []\n",
    "    lambda_valid_fv_list = []\n",
    "    lambda_test_fv_list = []\n",
    "\n",
    "    for i in tqdm(epochs_save_range_lambda):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "        path_lambda_train_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "        path_lambda_valid_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "        path_lambda_test_fv = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        lambda_train_fv_with_lambda_input = pd.read_csv(path_lambda_train_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv_with_lambda_input = pd.read_csv(path_lambda_valid_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv_with_lambda_input = pd.read_csv(path_lambda_test_fv, sep=',').sort_values(by=list_of_monomial_identifiers).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True).drop(list_of_monomial_identifiers, axis=1).sample(n=data_size, random_state=RANDOM_SEED)\n",
    "\n",
    "        lambda_train_fv = lambda_train_fv_with_lambda_input[lambda_train_fv_with_lambda_input.columns[n::n+1]]\n",
    "        lambda_valid_fv = lambda_valid_fv_with_lambda_input[lambda_valid_fv_with_lambda_input.columns[n::n+1]]\n",
    "        lambda_test_fv = lambda_test_fv_with_lambda_input[lambda_test_fv_with_lambda_input.columns[n::n+1]]\n",
    "\n",
    "        if i == 0:\n",
    "            lambda_train_input = lambda_train_fv_with_lambda_input.drop(lambda_train_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_train_fv_with_lambda_input.shape[0], int((lambda_train_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "            lambda_valid_input = lambda_valid_fv_with_lambda_input.drop(lambda_valid_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_valid_fv_with_lambda_input.shape[0], int((lambda_valid_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "            lambda_test_input = lambda_test_fv_with_lambda_input.drop(lambda_test_fv_with_lambda_input.columns[n::n+1], axis=1).values.reshape(lambda_test_fv_with_lambda_input.shape[0], int((lambda_test_fv_with_lambda_input.shape[1]*(n/(n+1)))/n), n)\n",
    "\n",
    "        lambda_train_fv_list.append(lambda_train_fv)\n",
    "        lambda_valid_fv_list.append(lambda_valid_fv)\n",
    "        lambda_test_fv_list.append(lambda_test_fv)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FV_1</th>\n",
       "      <th>FV_2</th>\n",
       "      <th>FV_3</th>\n",
       "      <th>FV_4</th>\n",
       "      <th>FV_5</th>\n",
       "      <th>FV_6</th>\n",
       "      <th>FV_7</th>\n",
       "      <th>FV_8</th>\n",
       "      <th>FV_9</th>\n",
       "      <th>FV_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FV_553</th>\n",
       "      <th>FV_554</th>\n",
       "      <th>FV_555</th>\n",
       "      <th>FV_556</th>\n",
       "      <th>FV_557</th>\n",
       "      <th>FV_558</th>\n",
       "      <th>FV_559</th>\n",
       "      <th>FV_560</th>\n",
       "      <th>FV_561</th>\n",
       "      <th>FV_562</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>-8.376</td>\n",
       "      <td>0.481</td>\n",
       "      <td>1.181</td>\n",
       "      <td>7.134</td>\n",
       "      <td>-6.378</td>\n",
       "      <td>8.535</td>\n",
       "      <td>1.699</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-11.953</td>\n",
       "      <td>...</td>\n",
       "      <td>9.329</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-8.804</td>\n",
       "      <td>-8.538</td>\n",
       "      <td>-13.301</td>\n",
       "      <td>-5.477</td>\n",
       "      <td>-13.544</td>\n",
       "      <td>3.861</td>\n",
       "      <td>1.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>15.875</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-5.516</td>\n",
       "      <td>9.213</td>\n",
       "      <td>5.313</td>\n",
       "      <td>4.785</td>\n",
       "      <td>-5.161</td>\n",
       "      <td>7.183</td>\n",
       "      <td>7.866</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805</td>\n",
       "      <td>8.189</td>\n",
       "      <td>12.335</td>\n",
       "      <td>12.033</td>\n",
       "      <td>2.620</td>\n",
       "      <td>10.395</td>\n",
       "      <td>18.565</td>\n",
       "      <td>4.913</td>\n",
       "      <td>19.110</td>\n",
       "      <td>13.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>12.215</td>\n",
       "      <td>9.506</td>\n",
       "      <td>14.546</td>\n",
       "      <td>1.233</td>\n",
       "      <td>2.914</td>\n",
       "      <td>3.203</td>\n",
       "      <td>-4.161</td>\n",
       "      <td>15.305</td>\n",
       "      <td>-15.853</td>\n",
       "      <td>-19.728</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.859</td>\n",
       "      <td>0.897</td>\n",
       "      <td>-7.735</td>\n",
       "      <td>5.579</td>\n",
       "      <td>-18.800</td>\n",
       "      <td>10.070</td>\n",
       "      <td>8.709</td>\n",
       "      <td>-4.208</td>\n",
       "      <td>-8.135</td>\n",
       "      <td>16.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>-14.189</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-14.304</td>\n",
       "      <td>-10.212</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>0.895</td>\n",
       "      <td>-11.645</td>\n",
       "      <td>5.988</td>\n",
       "      <td>-10.996</td>\n",
       "      <td>-9.104</td>\n",
       "      <td>...</td>\n",
       "      <td>14.486</td>\n",
       "      <td>-6.381</td>\n",
       "      <td>-7.845</td>\n",
       "      <td>0.547</td>\n",
       "      <td>-12.430</td>\n",
       "      <td>-6.627</td>\n",
       "      <td>-4.632</td>\n",
       "      <td>5.288</td>\n",
       "      <td>6.521</td>\n",
       "      <td>-14.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>-5.803</td>\n",
       "      <td>-6.582</td>\n",
       "      <td>-9.763</td>\n",
       "      <td>-7.522</td>\n",
       "      <td>-3.198</td>\n",
       "      <td>-3.375</td>\n",
       "      <td>-6.033</td>\n",
       "      <td>-10.934</td>\n",
       "      <td>-5.524</td>\n",
       "      <td>-12.646</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.341</td>\n",
       "      <td>-4.914</td>\n",
       "      <td>-6.495</td>\n",
       "      <td>-8.176</td>\n",
       "      <td>-10.031</td>\n",
       "      <td>0.632</td>\n",
       "      <td>-2.645</td>\n",
       "      <td>-7.370</td>\n",
       "      <td>-4.776</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FV_1   FV_2    FV_3    FV_4   FV_5   FV_6    FV_7    FV_8    FV_9  \\\n",
       "6252  -8.376  0.481   1.181   7.134 -6.378  8.535   1.699  -0.701   0.161   \n",
       "4684  15.875  3.365   0.555  -5.516  9.213  5.313   4.785  -5.161   7.183   \n",
       "1731  12.215  9.506  14.546   1.233  2.914  3.203  -4.161  15.305 -15.853   \n",
       "4742 -14.189 -1.099 -14.304 -10.212 -1.038  0.895 -11.645   5.988 -10.996   \n",
       "4521  -5.803 -6.582  -9.763  -7.522 -3.198 -3.375  -6.033 -10.934  -5.524   \n",
       "\n",
       "       FV_10  ...  FV_553  FV_554  FV_555  FV_556  FV_557  FV_558  FV_559  \\\n",
       "6252 -11.953  ...   9.329  -0.941  -0.026  -8.804  -8.538 -13.301  -5.477   \n",
       "4684   7.866  ...   1.805   8.189  12.335  12.033   2.620  10.395  18.565   \n",
       "1731 -19.728  ... -23.859   0.897  -7.735   5.579 -18.800  10.070   8.709   \n",
       "4742  -9.104  ...  14.486  -6.381  -7.845   0.547 -12.430  -6.627  -4.632   \n",
       "4521 -12.646  ...  -1.341  -4.914  -6.495  -8.176 -10.031   0.632  -2.645   \n",
       "\n",
       "      FV_560  FV_561  FV_562  \n",
       "6252 -13.544   3.861   1.543  \n",
       "4684   4.913  19.110  13.627  \n",
       "1731  -4.208  -8.135  16.215  \n",
       "4742   5.288   6.521 -14.292  \n",
       "4521  -7.370  -4.776   0.908  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_train_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.62, -0.49,  0.73, -0.32],\n",
       "        [ 0.13, -0.92, -0.07,  0.  ],\n",
       "        [ 0.11, -0.67,  0.21, -0.55],\n",
       "        ...,\n",
       "        [-0.57,  0.82,  0.79,  0.49],\n",
       "        [-0.2 , -0.17, -0.92,  0.58],\n",
       "        [-0.09, -0.77,  0.15,  0.44]],\n",
       "\n",
       "       [[ 0.65, -0.31,  0.65, -0.61],\n",
       "        [-0.38,  0.87,  0.48,  0.14],\n",
       "        [-0.11,  0.53, -0.84, -0.2 ],\n",
       "        ...,\n",
       "        [ 0.8 ,  0.21, -0.24, -0.5 ],\n",
       "        [ 0.85, -0.32,  0.8 ,  0.66],\n",
       "        [ 0.62, -0.51, -0.39, -0.94]],\n",
       "\n",
       "       [[-0.75,  0.23,  0.98, -0.76],\n",
       "        [-0.34,  0.46,  0.29,  0.56],\n",
       "        [-0.17, -0.54,  0.5 ,  0.95],\n",
       "        ...,\n",
       "        [ 0.71,  0.28, -0.36, -0.08],\n",
       "        [ 0.85, -0.26, -0.36, -0.14],\n",
       "        [ 0.34,  0.12,  0.96,  0.67]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.05, -0.76, -0.7 ,  0.97],\n",
       "        [-0.86,  0.05,  0.35,  0.56],\n",
       "        [ 0.92, -0.85, -0.38,  0.02],\n",
       "        ...,\n",
       "        [-0.68, -0.33,  0.68,  0.72],\n",
       "        [ 0.54, -0.37,  0.12,  0.85],\n",
       "        [ 0.04,  0.02, -0.99,  0.19]],\n",
       "\n",
       "       [[-0.67,  0.27, -0.79, -0.83],\n",
       "        [-0.79, -0.94, -0.98, -0.99],\n",
       "        [ 0.45, -0.91,  0.36,  0.36],\n",
       "        ...,\n",
       "        [ 0.94, -0.01,  0.87,  0.6 ],\n",
       "        [ 0.25,  0.94, -0.66, -0.99],\n",
       "        [ 0.21,  0.16, -0.61, -0.01]],\n",
       "\n",
       "       [[ 0.2 ,  0.56, -0.32,  0.45],\n",
       "        [ 0.15, -0.1 , -0.61, -0.3 ],\n",
       "        [-1.  , -0.94,  0.92,  0.21],\n",
       "        ...,\n",
       "        [-0.54, -0.92, -0.33, -0.24],\n",
       "        [ 0.71, -0.2 ,  0.17,  0.55],\n",
       "        [-0.11, -0.23,  0.14, -0.29]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1146</th>\n",
       "      <th>1147</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-4.900</td>\n",
       "      <td>8.700</td>\n",
       "      <td>3.200</td>\n",
       "      <td>4.100</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>-7.000</td>\n",
       "      <td>-6.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.684</td>\n",
       "      <td>0.632</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-1.214</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.500</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>5.300</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>7.100</td>\n",
       "      <td>5.500</td>\n",
       "      <td>2.200</td>\n",
       "      <td>-7.600</td>\n",
       "      <td>2.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.500</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-3.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>8.600</td>\n",
       "      <td>1.400</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>9.100</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>-0.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.400</td>\n",
       "      <td>-3.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-8.200</td>\n",
       "      <td>3.900</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>5.100</td>\n",
       "      <td>7.100</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>3.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.102</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.685</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.900</td>\n",
       "      <td>4.300</td>\n",
       "      <td>-3.600</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.200</td>\n",
       "      <td>8.500</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>6.400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.552</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0      1      2      3     4      5      6      7      8      9     ...  \\\n",
       "0  2.600  0.600 -4.900  8.700 3.200  4.100 -7.700 -9.900 -7.000 -6.900  ...   \n",
       "1 -0.500 -2.600  5.300  7.000 6.000  7.100  5.500  2.200 -7.600  2.800  ...   \n",
       "2 -6.500  9.300 -3.300 -8.800 8.600  1.400 -5.700  9.100 -9.200 -0.900  ...   \n",
       "3 -0.400 -3.900 -4.200 -8.200 3.900 -9.900  5.100  7.100 -4.400  3.000  ...   \n",
       "4 -0.900  4.300 -3.600  7.000 1.000  6.000  5.200  8.500 -3.800  6.400  ...   \n",
       "\n",
       "    1146   1147   1148   1149   1150   1151   1152   1153   1154   1155  \n",
       "0 -0.600  0.262 -0.559  0.086 -0.684  0.632 -0.135 -1.214  0.003 -0.216  \n",
       "1 -0.273  0.341 -0.002 -0.112 -0.494 -0.154  0.067 -0.094 -0.904  0.321  \n",
       "2 -0.178  0.142  0.314  0.432 -0.883  0.048  0.100  0.321  0.150 -0.715  \n",
       "3 -0.227 -0.233  0.102 -0.794 -0.181  0.685 -0.345 -0.164  0.428  0.153  \n",
       "4  0.419 -0.258 -0.552 -0.314  0.124 -0.770  0.047 -0.241 -0.523 -0.186  \n",
       "\n",
       "[5 rows x 1156 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FV_1</th>\n",
       "      <th>FV_2</th>\n",
       "      <th>FV_3</th>\n",
       "      <th>FV_4</th>\n",
       "      <th>FV_5</th>\n",
       "      <th>FV_6</th>\n",
       "      <th>FV_7</th>\n",
       "      <th>FV_8</th>\n",
       "      <th>FV_9</th>\n",
       "      <th>FV_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FV_241</th>\n",
       "      <th>FV_242</th>\n",
       "      <th>FV_243</th>\n",
       "      <th>FV_244</th>\n",
       "      <th>FV_245</th>\n",
       "      <th>FV_246</th>\n",
       "      <th>FV_247</th>\n",
       "      <th>FV_248</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>11.575</td>\n",
       "      <td>-7.324</td>\n",
       "      <td>-11.452</td>\n",
       "      <td>-7.535</td>\n",
       "      <td>7.073</td>\n",
       "      <td>1.730</td>\n",
       "      <td>4.360</td>\n",
       "      <td>-4.314</td>\n",
       "      <td>-9.215</td>\n",
       "      <td>-2.166</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.724</td>\n",
       "      <td>-12.701</td>\n",
       "      <td>-7.872</td>\n",
       "      <td>-5.637</td>\n",
       "      <td>-11.101</td>\n",
       "      <td>9.065</td>\n",
       "      <td>-13.088</td>\n",
       "      <td>-5.559</td>\n",
       "      <td>-4.782</td>\n",
       "      <td>-10.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>8.888</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-6.814</td>\n",
       "      <td>5.895</td>\n",
       "      <td>7.501</td>\n",
       "      <td>11.344</td>\n",
       "      <td>14.329</td>\n",
       "      <td>3.771</td>\n",
       "      <td>2.333</td>\n",
       "      <td>13.227</td>\n",
       "      <td>...</td>\n",
       "      <td>5.929</td>\n",
       "      <td>0.916</td>\n",
       "      <td>7.720</td>\n",
       "      <td>3.073</td>\n",
       "      <td>11.813</td>\n",
       "      <td>2.253</td>\n",
       "      <td>4.276</td>\n",
       "      <td>15.370</td>\n",
       "      <td>15.443</td>\n",
       "      <td>4.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>11.233</td>\n",
       "      <td>2.547</td>\n",
       "      <td>3.819</td>\n",
       "      <td>-15.820</td>\n",
       "      <td>-3.917</td>\n",
       "      <td>2.705</td>\n",
       "      <td>13.699</td>\n",
       "      <td>-3.435</td>\n",
       "      <td>-9.346</td>\n",
       "      <td>12.214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663</td>\n",
       "      <td>-7.992</td>\n",
       "      <td>-21.455</td>\n",
       "      <td>-2.580</td>\n",
       "      <td>5.905</td>\n",
       "      <td>13.991</td>\n",
       "      <td>1.856</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>-4.643</td>\n",
       "      <td>1.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>-3.805</td>\n",
       "      <td>-15.281</td>\n",
       "      <td>-18.529</td>\n",
       "      <td>-14.033</td>\n",
       "      <td>1.407</td>\n",
       "      <td>3.918</td>\n",
       "      <td>-7.135</td>\n",
       "      <td>9.835</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>18.301</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.576</td>\n",
       "      <td>-22.264</td>\n",
       "      <td>-6.687</td>\n",
       "      <td>-6.782</td>\n",
       "      <td>-8.591</td>\n",
       "      <td>0.357</td>\n",
       "      <td>1.491</td>\n",
       "      <td>-12.793</td>\n",
       "      <td>3.379</td>\n",
       "      <td>-17.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>0.548</td>\n",
       "      <td>-4.696</td>\n",
       "      <td>-5.691</td>\n",
       "      <td>-5.956</td>\n",
       "      <td>-10.502</td>\n",
       "      <td>-8.707</td>\n",
       "      <td>-4.809</td>\n",
       "      <td>-5.161</td>\n",
       "      <td>-5.735</td>\n",
       "      <td>-13.999</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.310</td>\n",
       "      <td>-7.371</td>\n",
       "      <td>-4.334</td>\n",
       "      <td>-7.364</td>\n",
       "      <td>-7.343</td>\n",
       "      <td>-2.215</td>\n",
       "      <td>-12.756</td>\n",
       "      <td>-3.952</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-7.790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FV_1    FV_2    FV_3    FV_4    FV_5   FV_6   FV_7   FV_8   FV_9  \\\n",
       "6252 11.575  -7.324 -11.452  -7.535   7.073  1.730  4.360 -4.314 -9.215   \n",
       "4684  8.888   0.271  -6.814   5.895   7.501 11.344 14.329  3.771  2.333   \n",
       "1731 11.233   2.547   3.819 -15.820  -3.917  2.705 13.699 -3.435 -9.346   \n",
       "4742 -3.805 -15.281 -18.529 -14.033   1.407  3.918 -7.135  9.835 -1.875   \n",
       "4521  0.548  -4.696  -5.691  -5.956 -10.502 -8.707 -4.809 -5.161 -5.735   \n",
       "\n",
       "       FV_10  ...  FV_241  FV_242  FV_243  FV_244  FV_245  FV_246  FV_247  \\\n",
       "6252  -2.166  ...  -6.724 -12.701  -7.872  -5.637 -11.101   9.065 -13.088   \n",
       "4684  13.227  ...   5.929   0.916   7.720   3.073  11.813   2.253   4.276   \n",
       "1731  12.214  ...   0.663  -7.992 -21.455  -2.580   5.905  13.991   1.856   \n",
       "4742  18.301  ... -11.576 -22.264  -6.687  -6.782  -8.591   0.357   1.491   \n",
       "4521 -13.999  ...  -5.310  -7.371  -4.334  -7.364  -7.343  -2.215 -12.756   \n",
       "\n",
       "      FV_248  FV_249  FV_250  \n",
       "6252  -5.559  -4.782 -10.570  \n",
       "4684  15.370  15.443   4.802  \n",
       "1731  -0.610  -4.643   1.771  \n",
       "4742 -12.793   3.379 -17.589  \n",
       "4521  -3.952   0.268  -7.790  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_test_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FV_1</th>\n",
       "      <th>FV_2</th>\n",
       "      <th>FV_3</th>\n",
       "      <th>FV_4</th>\n",
       "      <th>FV_5</th>\n",
       "      <th>FV_6</th>\n",
       "      <th>FV_7</th>\n",
       "      <th>FV_8</th>\n",
       "      <th>FV_9</th>\n",
       "      <th>FV_10</th>\n",
       "      <th>...</th>\n",
       "      <th>FV_179</th>\n",
       "      <th>FV_180</th>\n",
       "      <th>FV_181</th>\n",
       "      <th>FV_182</th>\n",
       "      <th>FV_183</th>\n",
       "      <th>FV_184</th>\n",
       "      <th>FV_185</th>\n",
       "      <th>FV_186</th>\n",
       "      <th>FV_187</th>\n",
       "      <th>FV_188</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>-1.059</td>\n",
       "      <td>-6.771</td>\n",
       "      <td>9.000</td>\n",
       "      <td>3.336</td>\n",
       "      <td>-9.672</td>\n",
       "      <td>-8.842</td>\n",
       "      <td>-10.638</td>\n",
       "      <td>-7.302</td>\n",
       "      <td>3.111</td>\n",
       "      <td>5.541</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.445</td>\n",
       "      <td>3.732</td>\n",
       "      <td>4.911</td>\n",
       "      <td>7.510</td>\n",
       "      <td>2.072</td>\n",
       "      <td>-4.195</td>\n",
       "      <td>4.502</td>\n",
       "      <td>2.901</td>\n",
       "      <td>-4.775</td>\n",
       "      <td>-5.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>3.099</td>\n",
       "      <td>8.462</td>\n",
       "      <td>0.428</td>\n",
       "      <td>7.409</td>\n",
       "      <td>4.578</td>\n",
       "      <td>1.150</td>\n",
       "      <td>13.327</td>\n",
       "      <td>14.388</td>\n",
       "      <td>5.045</td>\n",
       "      <td>-3.601</td>\n",
       "      <td>...</td>\n",
       "      <td>13.097</td>\n",
       "      <td>-4.292</td>\n",
       "      <td>21.278</td>\n",
       "      <td>11.754</td>\n",
       "      <td>11.621</td>\n",
       "      <td>-5.248</td>\n",
       "      <td>14.055</td>\n",
       "      <td>15.124</td>\n",
       "      <td>2.626</td>\n",
       "      <td>3.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>6.396</td>\n",
       "      <td>7.234</td>\n",
       "      <td>13.277</td>\n",
       "      <td>-4.553</td>\n",
       "      <td>-7.387</td>\n",
       "      <td>-7.069</td>\n",
       "      <td>-0.985</td>\n",
       "      <td>10.461</td>\n",
       "      <td>4.503</td>\n",
       "      <td>-13.571</td>\n",
       "      <td>...</td>\n",
       "      <td>14.705</td>\n",
       "      <td>9.385</td>\n",
       "      <td>-7.935</td>\n",
       "      <td>7.082</td>\n",
       "      <td>6.465</td>\n",
       "      <td>7.691</td>\n",
       "      <td>16.010</td>\n",
       "      <td>6.754</td>\n",
       "      <td>-5.158</td>\n",
       "      <td>8.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>0.018</td>\n",
       "      <td>8.080</td>\n",
       "      <td>4.565</td>\n",
       "      <td>7.073</td>\n",
       "      <td>19.341</td>\n",
       "      <td>15.650</td>\n",
       "      <td>-7.191</td>\n",
       "      <td>11.707</td>\n",
       "      <td>-13.701</td>\n",
       "      <td>-8.247</td>\n",
       "      <td>...</td>\n",
       "      <td>7.544</td>\n",
       "      <td>-13.380</td>\n",
       "      <td>-11.675</td>\n",
       "      <td>-8.898</td>\n",
       "      <td>8.530</td>\n",
       "      <td>9.940</td>\n",
       "      <td>5.014</td>\n",
       "      <td>13.329</td>\n",
       "      <td>14.448</td>\n",
       "      <td>17.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>-6.170</td>\n",
       "      <td>-5.526</td>\n",
       "      <td>-5.445</td>\n",
       "      <td>-3.788</td>\n",
       "      <td>-5.266</td>\n",
       "      <td>-2.181</td>\n",
       "      <td>-4.295</td>\n",
       "      <td>-4.942</td>\n",
       "      <td>-9.595</td>\n",
       "      <td>-9.132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-12.554</td>\n",
       "      <td>0.624</td>\n",
       "      <td>-5.838</td>\n",
       "      <td>-3.569</td>\n",
       "      <td>-3.298</td>\n",
       "      <td>-11.234</td>\n",
       "      <td>-8.272</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-10.556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FV_1   FV_2   FV_3   FV_4   FV_5   FV_6    FV_7   FV_8    FV_9   FV_10  \\\n",
       "6252 -1.059 -6.771  9.000  3.336 -9.672 -8.842 -10.638 -7.302   3.111   5.541   \n",
       "4684  3.099  8.462  0.428  7.409  4.578  1.150  13.327 14.388   5.045  -3.601   \n",
       "1731  6.396  7.234 13.277 -4.553 -7.387 -7.069  -0.985 10.461   4.503 -13.571   \n",
       "4742  0.018  8.080  4.565  7.073 19.341 15.650  -7.191 11.707 -13.701  -8.247   \n",
       "4521 -6.170 -5.526 -5.445 -3.788 -5.266 -2.181  -4.295 -4.942  -9.595  -9.132   \n",
       "\n",
       "      ...  FV_179  FV_180  FV_181  FV_182  FV_183  FV_184  FV_185  FV_186  \\\n",
       "6252  ...  -3.445   3.732   4.911   7.510   2.072  -4.195   4.502   2.901   \n",
       "4684  ...  13.097  -4.292  21.278  11.754  11.621  -5.248  14.055  15.124   \n",
       "1731  ...  14.705   9.385  -7.935   7.082   6.465   7.691  16.010   6.754   \n",
       "4742  ...   7.544 -13.380 -11.675  -8.898   8.530   9.940   5.014  13.329   \n",
       "4521  ...  -0.001 -12.554   0.624  -5.838  -3.569  -3.298 -11.234  -8.272   \n",
       "\n",
       "      FV_187  FV_188  \n",
       "6252  -4.775  -5.301  \n",
       "4684   2.626   3.754  \n",
       "1731  -5.158   8.757  \n",
       "4742  14.448  17.507  \n",
       "4521  -0.239 -10.556  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_valid_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1146</th>\n",
       "      <th>1147</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.753</td>\n",
       "      <td>5.723</td>\n",
       "      <td>5.832</td>\n",
       "      <td>5.809</td>\n",
       "      <td>5.787</td>\n",
       "      <td>5.782</td>\n",
       "      <td>5.760</td>\n",
       "      <td>5.761</td>\n",
       "      <td>5.811</td>\n",
       "      <td>5.758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.538</td>\n",
       "      <td>-1.447</td>\n",
       "      <td>-1.513</td>\n",
       "      <td>-1.670</td>\n",
       "      <td>-1.532</td>\n",
       "      <td>-1.439</td>\n",
       "      <td>-1.468</td>\n",
       "      <td>-1.403</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>-2.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.900</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>-5.100</td>\n",
       "      <td>-5.200</td>\n",
       "      <td>-4.900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000</td>\n",
       "      <td>4.800</td>\n",
       "      <td>5.100</td>\n",
       "      <td>4.800</td>\n",
       "      <td>4.900</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5.100</td>\n",
       "      <td>4.900</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.900</td>\n",
       "      <td>...</td>\n",
       "      <td>1.493</td>\n",
       "      <td>1.571</td>\n",
       "      <td>1.580</td>\n",
       "      <td>1.570</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.521</td>\n",
       "      <td>1.596</td>\n",
       "      <td>1.662</td>\n",
       "      <td>1.391</td>\n",
       "      <td>2.198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 1156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000   \n",
       "mean      0.058    -0.075    -0.071    -0.213    -0.094    -0.057     0.040   \n",
       "std       5.753     5.723     5.832     5.809     5.787     5.782     5.760   \n",
       "min     -10.000   -10.000   -10.000   -10.000   -10.000   -10.000   -10.000   \n",
       "25%      -4.900    -5.000    -5.200    -5.300    -5.200    -5.100    -5.000   \n",
       "50%       0.100    -0.100    -0.100    -0.400    -0.100     0.000     0.100   \n",
       "75%       5.000     4.800     5.100     4.800     4.900     4.900     5.100   \n",
       "max       9.900     9.900     9.900     9.900     9.900     9.900     9.900   \n",
       "\n",
       "           7         8         9     ...      1146      1147      1148  \\\n",
       "count 10000.000 10000.000 10000.000  ... 10000.000 10000.000 10000.000   \n",
       "mean     -0.119    -0.036     0.034  ...    -0.003     0.008    -0.004   \n",
       "std       5.761     5.811     5.758  ...     0.432     0.427     0.429   \n",
       "min     -10.000   -10.000   -10.000  ...    -1.538    -1.447    -1.513   \n",
       "25%      -5.100    -5.200    -4.900  ...    -0.295    -0.281    -0.291   \n",
       "50%      -0.100     0.100     0.100  ...    -0.005     0.010    -0.005   \n",
       "75%       4.900     5.000     5.100  ...     0.288     0.294     0.289   \n",
       "max       9.900     9.900     9.900  ...     1.493     1.571     1.580   \n",
       "\n",
       "           1149      1150      1151      1152      1153      1154      1155  \n",
       "count 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000 10000.000  \n",
       "mean      0.002     0.001    -0.005    -0.007     0.001     0.003    -0.004  \n",
       "std       0.433     0.434     0.432     0.424     0.431     0.429     0.861  \n",
       "min      -1.670    -1.532    -1.439    -1.468    -1.403    -1.509    -2.233  \n",
       "25%      -0.291    -0.301    -0.307    -0.301    -0.295    -0.290    -0.635  \n",
       "50%       0.004    -0.000    -0.003    -0.007    -0.003     0.005     0.005  \n",
       "75%       0.297     0.289     0.288     0.275     0.296     0.290     0.631  \n",
       "max       1.570     1.560     1.521     1.596     1.662     1.391     2.198  \n",
       "\n",
       "[8 rows x 1156 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.557Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d73d30b03fa42bc9ae47cee17746fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#generate train, test and validation data for training\n",
    "if multi_epoch_analysis:    \n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    \n",
    "    X_valid_list = []\n",
    "    y_valid_list = []\n",
    "    \n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    \n",
    "    lambda_test_fv_valid_split_list = []\n",
    "    lambda_test_fv_test_split_list = []\n",
    "    lambda_test_fv_train_split_list = []\n",
    "    \n",
    "    lambda_valid_fv_valid_split_list = []\n",
    "    lambda_valid_fv_test_split_list = []\n",
    "    lambda_valid_fv_train_split_list = []\n",
    "    \n",
    "    lambda_train_fv_valid_split_list = []\n",
    "    lambda_train_fv_test_split_list = []\n",
    "    lambda_train_fv_train_split_list = []\n",
    "    \n",
    "    lambda_test_input_valid_split_list = []\n",
    "    lambda_test_input_test_split_list = []\n",
    "    lambda_test_input_train_split_list = []\n",
    "    \n",
    "    lambda_valid_input_valid_split_list = []\n",
    "    lambda_valid_input_test_split_list = []\n",
    "    lambda_valid_input_train_split_list = []\n",
    "    \n",
    "    lambda_train_input_valid_split_list = []\n",
    "    lambda_train_input_test_split_list = []\n",
    "    lambda_train_input_train_split_list = []\n",
    "    \n",
    "    for weight_data, lambda_train_fv, lambda_valid_fv, lambda_test_fv in tqdm(zip(weight_data_list, lambda_train_fv_list, lambda_valid_fv_list, lambda_test_fv_list), total=len(weight_data_list)): \n",
    "        \n",
    "        if psutil.virtual_memory().percent > 80:\n",
    "            raise SystemExit(\"Out of RAM!\")\n",
    "        \n",
    "        X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3)], axis=1)\n",
    "        y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3)]].astype(float)\n",
    "        \n",
    "        y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "        y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "        y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)      \n",
    "        \n",
    "        #y_data_polynomial = y_data_polynomial_true\n",
    "        #y_data_polynomial_pred_lstsq = y_data_polynomial_lstsq_pred\n",
    "        #y_data_polynomial_true_lstsq = y_data_polynomial_lstsq_true\n",
    "        \n",
    "        if evaluate_with_real_function:\n",
    "            y_data = y_data_polynomial_true\n",
    "        else:\n",
    "            y_data = y_data_polynomial_lstsq_pred  \n",
    "                                         \n",
    "        X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    \n",
    "        X_train_list.append(X_train)\n",
    "        y_train_list.append(y_train)\n",
    "\n",
    "        X_valid_list.append(X_valid)\n",
    "        y_valid_list.append(y_valid)\n",
    "\n",
    "        X_test_list.append(X_test)\n",
    "        y_test_list.append(y_test)   \n",
    "        \n",
    "\n",
    "        lambda_train_fv_with_valid_split, lambda_train_fv_test_split = train_test_split(lambda_train_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_train_fv_train_split, lambda_train_fv_valid_split = train_test_split(lambda_train_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_train_fv_valid_split_list.append(lambda_train_fv_valid_split)\n",
    "        lambda_train_fv_test_split_list.append(lambda_train_fv_test_split)\n",
    "        lambda_train_fv_train_split_list.append(lambda_train_fv_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_valid_fv_with_valid_split, lambda_valid_fv_test_split = train_test_split(lambda_valid_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_valid_fv_train_split, lambda_valid_fv_valid_split = train_test_split(lambda_valid_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_valid_fv_valid_split_list.append(lambda_valid_fv_valid_split)\n",
    "        lambda_valid_fv_test_split_list.append(lambda_valid_fv_test_split)\n",
    "        lambda_valid_fv_train_split_list.append(lambda_valid_fv_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_test_fv_with_valid_split, lambda_test_fv_test_split = train_test_split(lambda_test_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_test_fv_train_split, lambda_test_fv_valid_split = train_test_split(lambda_test_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_test_fv_valid_split_list.append(lambda_test_fv_valid_split)\n",
    "        lambda_test_fv_test_split_list.append(lambda_test_fv_test_split)\n",
    "        lambda_test_fv_train_split_list.append(lambda_test_fv_train_split)   \n",
    "           \n",
    "            \n",
    "            \n",
    "        lambda_train_input_with_valid_split, lambda_train_input_test_split = train_test_split(lambda_train_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_train_input_train_split, lambda_train_input_valid_split = train_test_split(lambda_train_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_train_input_valid_split_list.append(lambda_train_input_valid_split)\n",
    "        lambda_train_input_test_split_list.append(lambda_train_input_test_split)\n",
    "        lambda_train_input_train_split_list.append(lambda_train_input_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_valid_input_with_valid_split, lambda_valid_input_test_split = train_test_split(lambda_valid_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_valid_input_train_split, lambda_valid_input_valid_split = train_test_split(lambda_valid_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_valid_input_valid_split_list.append(lambda_valid_input_valid_split)\n",
    "        lambda_valid_input_test_split_list.append(lambda_valid_input_test_split)\n",
    "        lambda_valid_input_train_split_list.append(lambda_valid_input_train_split)\n",
    "        \n",
    "        \n",
    "        lambda_test_input_with_valid_split, lambda_test_input_test_split = train_test_split(lambda_test_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "        lambda_test_input_train_split, lambda_test_input_valid_split = train_test_split(lambda_test_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "        \n",
    "        lambda_test_input_valid_split_list.append(lambda_test_input_valid_split)\n",
    "        lambda_test_input_test_split_list.append(lambda_test_input_test_split)\n",
    "        lambda_test_input_train_split_list.append(lambda_test_input_train_split)   \n",
    "        \n",
    "else:        \n",
    "    X_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED).drop([i for i in range(nCr(n+d, d)*3)], axis=1)\n",
    "    y_data = weight_data.sample(n=data_size, random_state=RANDOM_SEED)[[i for i in range(nCr(n+d, d)*3)]].astype(float)\n",
    "    \n",
    "    y_data_polynomial_true = y_data[[i for i in range(nCr(n+d, d))]]\n",
    "    y_data_polynomial_lstsq_pred = y_data[[i for i in range(nCr(n+d, d), nCr(n+d, d)*2)]]\n",
    "    y_data_polynomial_lstsq_true = y_data.drop([i for i in range(nCr(n+d, d)*2)], axis=1)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        y_data = y_data_polynomial_true\n",
    "    else:\n",
    "        y_data = y_data_polynomial_lstsq_pred         \n",
    "    \n",
    "    X_train_with_valid, X_test, y_train_with_valid, y_test = train_test_split(X_data, y_data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_with_valid, y_train_with_valid, test_size=0.25, random_state=RANDOM_SEED)           \n",
    "    \n",
    "    lambda_train_fv_with_valid_split, lambda_train_fv_test_split = train_test_split(lambda_train_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    lambda_train_fv_train_split, lambda_train_fv_valid_split = train_test_split(lambda_train_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "    lambda_valid_fv_with_valid_split, lambda_valid_fv_test_split = train_test_split(lambda_valid_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    lambda_valid_fv_train_split, lambda_valid_fv_valid_split = train_test_split(lambda_valid_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "    lambda_test_fv_with_valid_split, lambda_test_fv_test_split = train_test_split(lambda_test_fv, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    lambda_test_fv_train_split, lambda_test_fv_valid_split = train_test_split(lambda_test_fv_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "            \n",
    "    lambda_train_input_with_valid_split, lambda_train_input_test_split = train_test_split(lambda_train_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    lambda_train_input_train_split, lambda_train_input_valid_split = train_test_split(lambda_train_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "    lambda_valid_input_with_valid_split, lambda_valid_input_test_split = train_test_split(lambda_valid_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    lambda_valid_input_train_split, lambda_valid_input_valid_split = train_test_split(lambda_valid_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "    lambda_test_input_with_valid_split, lambda_test_input_test_split = train_test_split(lambda_test_input, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    lambda_test_input_train_split, lambda_test_input_valid_split = train_test_split(lambda_test_input_with_valid_split, test_size=0.25, random_state=RANDOM_SEED)               \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 35)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 35)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 35)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_with_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.568Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>...</th>\n",
       "      <th>1146</th>\n",
       "      <th>1147</th>\n",
       "      <th>1148</th>\n",
       "      <th>1149</th>\n",
       "      <th>1150</th>\n",
       "      <th>1151</th>\n",
       "      <th>1152</th>\n",
       "      <th>1153</th>\n",
       "      <th>1154</th>\n",
       "      <th>1155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>-0.261</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>1.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.289</td>\n",
       "      <td>1.238</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>0.482</td>\n",
       "      <td>-0.786</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>0.066</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.214</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.346</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-1.060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1051 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       105    106    107    108    109   110    111    112    113    114   \\\n",
       "6374 -0.261  0.079  0.090  0.041 -0.412 0.011  0.265 -0.041  0.035 -0.024   \n",
       "2657  0.126 -0.294 -0.079 -0.043  0.183 0.001 -0.048  0.206 -0.094  0.193   \n",
       "3076 -0.141 -0.021  0.084  0.453  0.055 0.134 -0.125 -0.725  0.263  0.191   \n",
       "8178 -0.165 -0.003  0.391 -0.228  0.394 0.131 -0.248  0.177  0.030  0.049   \n",
       "7311  0.066  0.543  0.076  0.214 -0.114 0.346 -0.231 -0.098 -0.366  0.503   \n",
       "\n",
       "      ...   1146   1147   1148   1149   1150   1151   1152   1153   1154  \\\n",
       "6374  ...  0.135 -0.540 -0.354 -0.803 -0.682 -0.282  0.134 -0.080  0.695   \n",
       "2657  ... -0.174 -0.178  0.033  0.287  0.006 -0.299  0.017 -0.097 -0.358   \n",
       "3076  ... -0.025  0.115  0.326  0.563  0.417  0.705  0.030  0.309 -0.025   \n",
       "8178  ...  0.308  0.256 -0.190  0.289  1.238 -0.590 -0.834  0.482 -0.786   \n",
       "7311  ... -0.709  0.052 -0.500 -0.189  0.118 -0.070 -0.540  0.132 -0.538   \n",
       "\n",
       "       1155  \n",
       "6374  0.099  \n",
       "2657 -0.603  \n",
       "3076  1.637  \n",
       "8178  0.685  \n",
       "7311 -1.060  \n",
       "\n",
       "[5 rows x 1051 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = X_train_list[-1].head()\n",
    "else:\n",
    "    print_head = X_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>1.662</td>\n",
       "      <td>-7.927</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.650</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.326</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>0.402</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>-4.114</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>-3.232</td>\n",
       "      <td>0.461</td>\n",
       "      <td>1.682</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>2.788</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-3.967</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.139</td>\n",
       "      <td>1.614</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>9.152</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.674</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>8.183</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.142</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>-7.170</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.352</td>\n",
       "      <td>5.014</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>4.148</td>\n",
       "      <td>-4.225</td>\n",
       "      <td>-0.583</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>1.250</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>7.052</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-1.815</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-1.095</td>\n",
       "      <td>-0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>-9.934</td>\n",
       "      <td>-3.906</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>0.203</td>\n",
       "      <td>7.308</td>\n",
       "      <td>0.656</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554</td>\n",
       "      <td>-4.759</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>1.088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         35     36     37     38     39     40     41     42     43     44  \\\n",
       "6374  1.662 -7.927 -1.595  0.650  1.680  2.326 -0.086 -0.461  0.402 -0.043   \n",
       "2657 -3.232  0.461  1.682  0.010 -1.526  2.788 -0.311  0.081  0.156 -0.069   \n",
       "3076  9.152 -0.099  0.674 -0.051  8.183 -0.012 -0.097  1.313  0.142 -0.258   \n",
       "8178  4.148 -4.225 -0.583  0.038  0.060  0.112 -0.603  1.250 -0.032  0.476   \n",
       "7311 -9.934 -3.906 -0.481  0.203  7.308  0.656 -0.336 -0.473  0.151 -0.079   \n",
       "\n",
       "      ...     60     61     62     63     64     65     66     67     68  \\\n",
       "6374  ... -0.151  0.067  0.132 -0.191 -0.442 -4.114  0.158 -0.545  0.068   \n",
       "2657  ...  0.077 -3.967 -0.027  0.292  0.139  1.614 -0.105 -0.012  0.444   \n",
       "3076  ... -0.389 -7.170  0.122  0.815  0.352  5.014 -0.398 -0.808  0.161   \n",
       "8178  ... -0.389  7.052  0.334  0.049  0.438 -1.815 -0.349  0.511 -1.095   \n",
       "7311  ...  0.554 -4.759 -0.237  1.053  0.313 -0.441  0.643 -0.526 -1.030   \n",
       "\n",
       "         69  \n",
       "6374 -0.805  \n",
       "2657  0.267  \n",
       "3076 -0.502  \n",
       "8178 -0.283  \n",
       "7311  1.088  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if multi_epoch_analysis:\n",
    "    print_head = y_train_list[-1].head()\n",
    "else:\n",
    "    print_head = y_train.head()\n",
    "print_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.574Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_base_model():\n",
    "    base_model = Sequential()\n",
    "\n",
    "    base_model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=lambda_train_input_train_split[0].shape[1])) #1024\n",
    "\n",
    "    if dropout > 0:\n",
    "        base_model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        base_model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            base_model.add(Dropout(dropout))   \n",
    "\n",
    "    base_model.add(Dense(1))\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.576Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn_and_pred(X_train, \n",
    "                      X_valid, \n",
    "                      X_test, \n",
    "                      y_train, \n",
    "                      y_valid, \n",
    "                      y_test,\n",
    "                      lambda_train_fv_valid_split, \n",
    "                      lambda_train_fv_test_split, \n",
    "                      lambda_train_fv_train_split, \n",
    "                      lambda_valid_fv_valid_split, \n",
    "                      lambda_valid_fv_test_split, \n",
    "                      lambda_valid_fv_train_split, \n",
    "                      lambda_test_fv_valid_split, \n",
    "                      lambda_test_fv_test_split, \n",
    "                      lambda_test_fv_train_split, \n",
    "                      callback_names=[], \n",
    "                      return_model=False):       \n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(interpretation_network_layers[0], activation='relu', input_dim=X_train.shape[1])) #1024\n",
    "    \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in interpretation_network_layers[1:]:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    model.add(Dense(nCr(n+d, d))) \n",
    "    \n",
    "    #decide whether to use lambda preds for evaluation or polynomial from lstsq lambda preds\n",
    "    if not consider_labels_training and not evaluate_with_real_function:\n",
    "        if True: #implementation with direct lambda net prediction\n",
    "            base_model = generate_base_model()\n",
    "            random_evaluation_dataset = generate_random_x_values(random_evaluation_dataset_size, x_max, x_min, x_step, n)\n",
    "            #random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "            list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "            loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "            metrics = [loss_function, mean_absolute_error_extended]\n",
    "            valid_data = None\n",
    "            y_train_model = np.hstack((y_train, X_train))\n",
    "        else: #old implementation with preloaded lambda preds\n",
    "            loss_function = mean_absolute_error_tf_fv_lambda_extended_preds_appended    \n",
    "            metrics = [mean_absolute_error_tf_fv_lambda_extended_preds_appended, mean_absolute_error_extended]\n",
    "            valid_data = None\n",
    "            y_train_model = np.hstack((y_train, lambda_train_fv_train_split))\n",
    "            \n",
    "    else:\n",
    "        loss_function = mean_absolute_error_tf_fv\n",
    "        metrics = ['mean_absolute_error']\n",
    "        valid_data = (X_valid, y_valid)\n",
    "        y_train_model = y_train\n",
    "     \n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=metrics\n",
    "                 )\n",
    "\n",
    "    #Callbacks\n",
    "    callbacks = return_callbacks_from_string(callback_names)\n",
    "        \n",
    "    history = model.fit(X_train,\n",
    "              y_train_model,\n",
    "              epochs=epochs, \n",
    "              batch_size=batch_size, \n",
    "              validation_data=valid_data,\n",
    "              callbacks=callbacks,\n",
    "              verbose=10)\n",
    "\n",
    "    print('I-NET TRAINING FINISHED')\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    \n",
    "    polynomial_true_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_valid_input_valid_split) #USE SPLIT HERE CORRECT?\n",
    "    polynomial_pred_inet_valid_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_valid_input_valid_split)\n",
    "    \n",
    "    polynomial_true_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_valid_input_test_split)\n",
    "    polynomial_pred_inet_valid_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_valid_input_test_split)\n",
    "\n",
    "    \n",
    "    polynomial_true_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid, lambda_test_input_valid_split)\n",
    "    polynomial_pred_inet_test_fv_valid_split = parallel_fv_calculation_from_polynomial(y_valid_pred, lambda_test_input_valid_split)\n",
    "    \n",
    "    polynomial_true_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test, lambda_test_input_test_split)\n",
    "    polynomial_pred_inet_test_fv_test_split = parallel_fv_calculation_from_polynomial(y_test_pred, lambda_test_input_test_split)\n",
    "    \n",
    "    \n",
    "    polynomial_test_fv = [polynomial_true_test_fv_valid_split, \n",
    "                            polynomial_pred_inet_test_fv_valid_split, \n",
    "                            polynomial_true_test_fv_test_split, \n",
    "                            polynomial_pred_inet_test_fv_test_split]\n",
    "    \n",
    "    polynomial_valid_fv = [polynomial_true_valid_fv_valid_split, \n",
    "                             polynomial_pred_inet_valid_fv_valid_split, \n",
    "                             polynomial_true_valid_fv_test_split, \n",
    "                             polynomial_pred_inet_valid_fv_test_split]\n",
    "    \n",
    "    polynomial_fv = [polynomial_valid_fv, polynomial_test_fv]\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_test_fv_valid_split, \n",
    "                                polynomial_pred_inet_test_fv_valid_split)\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_test_fv_test_split, \n",
    "                                polynomial_pred_inet_test_fv_test_split)\n",
    "    \n",
    "    \n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                y_valid_pred, \n",
    "                                polynomial_true_valid_fv_valid_split, \n",
    "                                polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "    scores_truePoly_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                y_test_pred, \n",
    "                                polynomial_true_valid_fv_test_split, \n",
    "                                polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "    \n",
    "    scores_truePoly_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv_valid_split, scores_truePoly_VS_inetPoly_test_fv_test_split)\n",
    "    scores_truePoly_VS_inetPoly_valid_fv = mergeDict(scores_truePoly_VS_inetPoly_valid_fv_valid_split, scores_truePoly_VS_inetPoly_valid_fv_test_split)\n",
    "    \n",
    "    if evaluate_with_real_function:\n",
    "        scores_dict = [scores_truePoly_VS_inetPoly_test_fv, scores_truePoly_VS_inetPoly_valid_fv]\n",
    "    else:   \n",
    "        scores_predLambda_VS_inetPoly_test_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_test_fv_valid_split, \n",
    "                                    polynomial_pred_inet_test_fv_valid_split)\n",
    "        \n",
    "        scores_predLambda_VS_inetPoly_test_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_test_fv_test_split, \n",
    "                                    polynomial_pred_inet_test_fv_test_split)\n",
    "\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_valid_split = evaluate_interpretation_net(y_valid, \n",
    "                                    y_valid_pred, \n",
    "                                    lambda_valid_fv_valid_split, \n",
    "                                    polynomial_pred_inet_valid_fv_valid_split)\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_valid_fv_test_split = evaluate_interpretation_net(y_test, \n",
    "                                    y_test_pred, \n",
    "                                    lambda_valid_fv_test_split, \n",
    "                                    polynomial_pred_inet_valid_fv_test_split)\n",
    "\n",
    "        eval_metrics = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'STD FV PRED', 'MEAN FV PRED']\n",
    "\n",
    "        scores_predLambda_VS_inetPoly_test_fv = mergeDict(scores_predLambda_VS_inetPoly_test_fv_valid_split, scores_predLambda_VS_inetPoly_test_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_test_fv = {key: scores_predLambda_VS_inetPoly_test_fv[key] for key in eval_metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_test_fv = mergeDict(scores_truePoly_VS_inetPoly_test_fv, scores_predLambda_VS_inetPoly_test_fv)\n",
    "        \n",
    "        scores_predLambda_VS_inetPoly_valid_fv = mergeDict(scores_predLambda_VS_inetPoly_valid_fv_valid_split, scores_predLambda_VS_inetPoly_valid_fv_test_split)\n",
    "        scores_predLambda_VS_inetPoly_valid_fv = {key: scores_predLambda_VS_inetPoly_valid_fv[key] for key in eval_metrics}\n",
    "        scores_truePoly_and_predLambda_VS_inetPoly_valid_fv =mergeDict(scores_truePoly_VS_inetPoly_valid_fv, scores_predLambda_VS_inetPoly_valid_fv)\n",
    "\n",
    "        scores_dict = [scores_truePoly_and_predLambda_VS_inetPoly_test_fv, scores_truePoly_and_predLambda_VS_inetPoly_valid_fv]\n",
    "\n",
    "    if return_model:\n",
    "        return history.history, scores_dict, polynomial_fv, model         \n",
    "    else: \n",
    "        return history.history, scores_dict, polynomial_fv       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.578Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=21)]: Using backend LokyBackend with 21 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results = train_nn_and_pred(X_train.values, \n",
    "                                X_valid.values, \n",
    "                                X_test.values, \n",
    "                                y_train.values, \n",
    "                                y_valid.values, \n",
    "                                y_test.values, \n",
    "                                lambda_train_fv_valid_split.values, \n",
    "                                lambda_train_fv_test_split.values, \n",
    "                                lambda_train_fv_train_split.values, \n",
    "                                lambda_valid_fv_valid_split.values, \n",
    "                                lambda_valid_fv_test_split.values, \n",
    "                                lambda_valid_fv_train_split.values, \n",
    "                                lambda_test_fv_valid_split.values, \n",
    "                                lambda_test_fv_test_split.values, \n",
    "                                lambda_test_fv_train_split.values, \n",
    "                                callback_names=['plot_losses_callback', 'early_stopping'], \n",
    "                                return_model=True)\n",
    "    \n",
    "    history = results[0]\n",
    "    \n",
    "    scores_complete = results[1]\n",
    "    scores_with_valid_fv = scores_complete[0]\n",
    "    scores_with_test_fv = scores_complete[1]\n",
    "    \n",
    "    polynomial_fv_complete = results[2]\n",
    "    polynomial_valid_fv = polynomial_fv_complete[0]\n",
    "    polynomial_test_fv = polynomial_fv_complete[1]\n",
    "    \n",
    "    model = results[3]\n",
    "    \n",
    "    x = PrettyTable()\n",
    "\n",
    "    x.field_names = [\"Error Name\", \"Valid Error Int\", \"Test Error Int\"]\n",
    "\n",
    "    for error, value in scores_with_test_fv.items():\n",
    "\n",
    "        x.add_row([error, value[0], value[1]])\n",
    "\n",
    "    print(x)    \n",
    "    \n",
    "elif multi_epoch_analysis and samples_list == None: \n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, \n",
    "                            verbose=11, \n",
    "                            backend='loky')(delayed(train_nn_and_pred)(X_train.values, \n",
    "                                                                      X_valid.values, \n",
    "                                                                      X_test.values, \n",
    "                                                                      y_train.values, \n",
    "                                                                      y_valid.values, \n",
    "                                                                      y_test.values, \n",
    "                                                                      lambda_train_fv_valid_split.values, \n",
    "                                                                      lambda_train_fv_test_split.values, \n",
    "                                                                      lambda_train_fv_train_split.values, \n",
    "                                                                      lambda_valid_fv_valid_split.values, \n",
    "                                                                      lambda_valid_fv_test_split.values, \n",
    "                                                                      lambda_valid_fv_train_split.values, \n",
    "                                                                      lambda_test_fv_valid_split.values, \n",
    "                                                                      lambda_test_fv_test_split.values, \n",
    "                                                                      lambda_test_fv_train_split.values, \n",
    "                                                                      callback_names=['early_stopping']) for X_train, \n",
    "                                                                                                               X_valid, \n",
    "                                                                                                               X_test, \n",
    "                                                                                                               y_train, \n",
    "                                                                                                               y_valid, \n",
    "                                                                                                               y_test, \n",
    "                                                                                                               lambda_train_fv_valid_split, \n",
    "                                                                                                               lambda_train_fv_test_split, \n",
    "                                                                                                               lambda_train_fv_train_split,                                            \n",
    "                                                                                                               lambda_valid_fv_valid_split, \n",
    "                                                                                                               lambda_valid_fv_test_split, \n",
    "                                                                                                               lambda_valid_fv_train_split, \n",
    "                                                                                                               lambda_test_fv_valid_split, \n",
    "                                                                                                               lambda_test_fv_test_split, \n",
    "                                                                                                               lambda_test_fv_train_split in zip(X_train_list, \n",
    "                                                                                                                                                 X_valid_list, \n",
    "                                                                                                                                                 X_test_list, \n",
    "                                                                                                                                                 y_train_list, \n",
    "                                                                                                                                                 y_valid_list, \n",
    "                                                                                                                                                 y_test_list, \n",
    "                                                                                                                                                 lambda_train_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_train_fv_train_split_list,                                                                                                                                                  \n",
    "                                                                                                                                                 lambda_valid_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_valid_fv_train_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_valid_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_test_split_list, \n",
    "                                                                                                                                                 lambda_test_fv_train_split_list))      \n",
    "\n",
    "    history_list = [result[0] for result in results_list]\n",
    "    \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "\n",
    "    for i, history in enumerate(history_list):  \n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "        \n",
    "        plt.plot(history[list(history.keys())[1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(index).zfill(3) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)   \n",
    "        \n",
    "elif not multi_epoch_analysis and  samples_list != None:\n",
    "    \n",
    "    results_list = Parallel(n_jobs=n_jobs, verbose=11, backend='loky')(delayed(train_nn_and_pred)(X_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  X_valid.values, \n",
    "                                                                                                  X_test.values, \n",
    "                                                                                                  y_train.sample(n=samples, random_state=RANDOM_SEED).values, \n",
    "                                                                                                  y_valid.values, \n",
    "                                                                                                  y_test.values, \n",
    "                                                                                                  lambda_train_fv_valid_split.values, \n",
    "                                                                                                  lambda_train_fv_test_split.values, \n",
    "                                                                                                  lambda_train_fv_train_split.values, \n",
    "                                                                                                  lambda_valid_fv_valid_split.values, \n",
    "                                                                                                  lambda_valid_fv_test_split.values, \n",
    "                                                                                                  lambda_valid_fv_train_split.values, \n",
    "                                                                                                  lambda_test_fv_valid_split.values, \n",
    "                                                                                                  lambda_test_fv_test_split.values, \n",
    "                                                                                                  lambda_test_fv_train_split.values, \n",
    "                                                                                                  callback_names=['early_stopping']) for samples in samples_list)     \n",
    "    \n",
    "    history_list = [result[0] for result in results_list]\n",
    "     \n",
    "    scores_complete_list = [result[1] for result in results_list]\n",
    "    scores_with_valid_fv_list = [scores[0] for scores in scores_complete_list]\n",
    "    scores_with_test_fv_list = [scores[1] for scores in scores_complete_list]\n",
    "    \n",
    "    polynomial_fv_complete_list = [result[2] for result in results_list]\n",
    "    polynomial_valid_fv_list = [polynomial[0] for polynomial in polynomial_fv_complete_list]\n",
    "    polynomial_test_fv_list = [polynomial[1] for polynomial in polynomial_fv_complete_list]\n",
    "\n",
    "    for i, history in enumerate(history_list):       \n",
    "        \n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history[list(history.keys())[1]])\n",
    "        plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "        plt.ylabel('metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/' + list(history.keys())[len(history.keys())//2+1] +  '_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.plot(history['loss'])\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plt.plot(history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'valid'], loc='upper left')\n",
    "        plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(samples_list[i]).zfill(5) + '.png')    \n",
    "        if i < len(history_list)-1:\n",
    "            plt.clf()\n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(history_list, f, protocol=2)   \n",
    "        \n",
    "    path = './data/results/' + interpretation_network_string + filename + '/history_' + interpretation_network_string + filename + '.pkl'\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scores_with_test_fv_list, f, protocol=2)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.581Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_valid_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_valid_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.583Z"
    }
   },
   "outputs": [],
   "source": [
    "printer = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    printer = scores_with_test_fv_list[-1]\n",
    "else:\n",
    "    printer = scores_with_test_fv\n",
    "printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.585Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history[list(history.keys())[len(history.keys())//2+1]])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "plt.title('model ' + list(history.keys())[len(history.keys())//2+1])\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.588Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "if consider_labels_training or evaluate_with_real_function:\n",
    "    plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.590Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss = []\n",
    "    plot_history_metric = []\n",
    "    plot_history_val_loss = []\n",
    "    plot_history_val_metric = []\n",
    "        \n",
    "    for history in history_list:\n",
    "        plot_history_loss.append(history['loss'][-1])\n",
    "        plot_history_metric.append(history[list(history.keys())[1]][-1])\n",
    "\n",
    "        if consider_labels_training or evaluate_with_real_function:\n",
    "            plot_history_val_loss.append(history['val_loss'][-1])\n",
    "            plot_history_val_metric.append(history[list(history.keys())[len(history.keys())//2+1]][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.592Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_loss_df = pd.DataFrame(data=plot_history_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_loss))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_loss_df = pd.DataFrame(data=plot_history_val_loss, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_loss))])\n",
    "    \n",
    "    plt.plot(plot_history_loss_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_loss_df)\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/loss_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.594Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and samples_list == None: \n",
    "    plot_history_metric_df = pd.DataFrame(data=plot_history_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_metric))])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plot_history_val_metric_df = pd.DataFrame(data=plot_history_val_metric, index=[(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in range(len(plot_history_val_metric))])\n",
    "    \n",
    "    plt.plot(plot_history_metric_df)\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(plot_history_val_metric_df)\n",
    "    plt.title('Metric')\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + interpretation_network_string + filename + '/metric_' + interpretation_network_string + filename + '_total.eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure Interpretation-Net Socres for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.596Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics_coeff_keys = ['MAE', 'RMSE', 'MAPE', 'Accuracy', 'Accuracy Multilabel']\n",
    "metrics_fv_keys = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "new_row_identifiers_coeff = ['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT']\n",
    "new_row_identifiers_fv = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "\n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "  \n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)\n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "\n",
    "\n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for i, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = (i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1\n",
    "\n",
    "        if i == 0:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST E' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['E' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['E' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'means_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)  \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)  \n",
    "    \n",
    "elif not multi_epoch_analysis and samples_list != None and evaluate_with_real_function:\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "\n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "        \n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([scores_int[score][0] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([scores_int[score][1] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED'])\n",
    "            \n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED'])\n",
    "\n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1) \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)     \n",
    "            \n",
    "elif not multi_epoch_analysis and  samples_list != None and not evaluate_with_real_function:\n",
    "\n",
    "    scores_coeff_valid = []\n",
    "    scores_coeff_test = []\n",
    "\n",
    "\n",
    "    scores_valid_list = []\n",
    "    scores_test_list = []\n",
    "    stds_list = []    \n",
    "    means_list = []    \n",
    "    for index, scores_int in enumerate(scores_with_test_fv_list):\n",
    "        index = samples_list[index]\n",
    "\n",
    "        if index == samples_list[0]:\n",
    "            scores_coeff_valid = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            scores_coeff_test = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=new_row_identifiers_coeff)\n",
    "            \n",
    "            scores_valid_list = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            \n",
    "            stds_list = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            means_list = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "\n",
    "        else:\n",
    "            scores_coeff_valid_new = pd.DataFrame([scores_int[score][0] for score in metrics_coeff_keys], columns=['VALID S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            scores_coeff_test_new = pd.DataFrame([scores_int[score][1] for score in metrics_coeff_keys], columns=['TEST S' + str(index)], index=['MAE', 'RMSE', 'MAPE', 'ACC', 'ACC MULT'])\n",
    "            \n",
    "            scores_valid_list_new = pd.DataFrame([[scores_int[score][2], scores_int[score][0]] for score in metrics_fv_keys], columns=['VALID PRED S' + str(index), 'VALID POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "            scores_test_list_new = pd.DataFrame([[scores_int[score][3], scores_int[score][1]] for score in metrics_fv_keys], columns=['TEST PRED S' + str(index), 'TEST POLY S' + str(index)], index=new_row_identifiers_fv)\n",
    "                       \n",
    "            stds_list_new = pd.DataFrame(scores_int['STD FV PRED'], columns=['S' + str(index)], index=['STD FUNC VALID PRED', 'STD FUNC TEST PRED', 'std_function_valid_pred_lambda_fv', 'std_function_test_pred_lambda_fv'])\n",
    "\n",
    "            means_list_new = pd.DataFrame(scores_int['MEAN FV PRED'], columns=['S' + str(index)], index=['MEAN FUNC VALID PRED', 'MEAN FUNC TEST PRED', 'mean_function_valid_pred_lambda_fv', 'mean_function_test_pred_lambda_fv'])\n",
    "            \n",
    "            \n",
    "            scores_coeff_valid = pd.concat([scores_coeff_valid, scores_coeff_valid_new],axis=1)  \n",
    "            scores_coeff_test = pd.concat([scores_coeff_test, scores_coeff_test_new],axis=1)  \n",
    "            \n",
    "            scores_valid_list = pd.concat([scores_valid_list, scores_valid_list_new],axis=1)  \n",
    "            scores_test_list = pd.concat([scores_test_list, scores_test_list_new],axis=1)  \n",
    "\n",
    "            stds_list = pd.concat([stds_list, stds_list_new],axis=1)      \n",
    "            \n",
    "            means_list = pd.concat([means_list, means_list_new],axis=1)    \n",
    "            \n",
    "if multi_epoch_analysis:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_test_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_multiepoch_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')  \n",
    "elif samples_list != None:            \n",
    "    path_scores_valid_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_valid_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_coef_int = './data/results/' + interpretation_network_string + filename + '/scores_test_samples_coef_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_scores_valid_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_valid_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "    path_scores_test_int = './data/results/' + interpretation_network_string + filename + '/scores_samples_test_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_stds_int = './data/results/' + interpretation_network_string + filename + '/stds_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "    path_means_int = './data/results/' + interpretation_network_string + filename + '/means_samples_' + interpretation_network_string + filename + '_epoch_' + str(epochs_lambda).zfill(3) + '.txt'\n",
    "\n",
    "\n",
    "    scores_coeff_valid.to_csv(path_scores_valid_coef_int, sep=',')\n",
    "    scores_coeff_test.to_csv(path_scores_test_coef_int, sep=',') \n",
    "\n",
    "    scores_valid_list.to_csv(path_scores_valid_int, sep=',')\n",
    "    scores_test_list.to_csv(path_scores_test_int, sep=',')\n",
    "\n",
    "    stds_list.to_csv(path_stds_int, sep=',')  \n",
    "    means_list.to_csv(path_means_int, sep=',')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Lambda Scores for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.599Z"
    }
   },
   "outputs": [],
   "source": [
    "path_scores_valid_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_scores_test_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_stds_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "path_means_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "df_mean_scores_valid_lambda = pd.read_csv(path_scores_valid_lambda, sep=',', index_col=0)\n",
    "df_mean_scores_test_lambda = pd.read_csv(path_scores_test_lambda, sep=',', index_col=0)\n",
    "df_stds_lambda = pd.read_csv(path_stds_lambda, sep=',', index_col=0)\n",
    "df_means_lambda = pd.read_csv(path_means_lambda, sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.601Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mean_scores_test_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.604Z"
    }
   },
   "outputs": [],
   "source": [
    "df_stds_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.607Z"
    }
   },
   "outputs": [],
   "source": [
    "df_means_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns to Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.609Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:   \n",
    "    select_columns = []\n",
    "\n",
    "    for column in df_mean_scores_test_lambda.columns:\n",
    "        if int(column.split(' ')[-1][1:]) in [(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda]:\n",
    "            select_columns.append(column)\n",
    "    \n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "    scores_lambda = df_mean_scores_test_lambda[select_columns].loc[plot_cols]\n",
    "    scores_int = scores_test_list.loc[plot_cols]    \n",
    "elif samples_list != None:\n",
    "    'Reduce the dfs to equal keys for plotting comparison'\n",
    "    plot_cols = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "    scores_int = scores_test_list.loc[plot_cols] \n",
    "    scores_lambda = df_mean_scores_test_lambda.loc[plot_cols].iloc[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.612Z"
    }
   },
   "outputs": [],
   "source": [
    "print_head = None\n",
    "if multi_epoch_analysis or samples_list != None:\n",
    "    print_head = scores_int\n",
    "print_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.615Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_test_fv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.618Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_valid_fv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.620Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate plot TEST PRED\n",
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "        vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "            \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "    #Plot Polynom, lamdba net, and Interpration net\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_lambda.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, epochs_lambda])\n",
    "        \n",
    "        \n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "                    \n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_real = scores_int.loc[index].values\n",
    "        vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "        vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "\n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "        ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    length_plt = len(plot_cols)\n",
    "    subplot_number = 1\n",
    "    plt.figure(figsize=(12*2, 7*length_plt/2))\n",
    "\n",
    "    #For plotting of the int net, only second value of the tuple can be used for the comparison. Thus, always\n",
    "    #extract the second value from the scores_int df\n",
    "    for index in scores_int.index:\n",
    "\n",
    "        vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "        vals_int_preds = scores_int.loc[index].values[::2]\n",
    "        vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "        \n",
    "        ax = plt.subplot(length_plt//2+1, 2, subplot_number)\n",
    "        ax.set_title(index, fontsize=20)\n",
    "        ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "        ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "        ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "        ax.legend(loc=\"bottom right\", fontsize=14)\n",
    "        ax.set_xlim([0, samples_list[-1]])\n",
    "\n",
    "        for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            label.set_fontsize(14)   \n",
    "\n",
    "        #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "        subplot_number += 1\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file    \n",
    "    \n",
    "    plt.savefig(path, format='eps')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.622Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = scores_lambda.loc[index].values[::4]\n",
    "    vals_lambda_lstsq = scores_lambda.loc[index].values[3::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    #ax.set_xticks(np.arange(0, epochs, step=1))\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file \n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "    \n",
    "elif multi_epoch_analysis and not evaluate_with_real_function:\n",
    "       \n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = scores_lambda.loc[index].values[2::4]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    #ax.set_title('Accuracy Evaluation')\n",
    "\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot([(i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1 for i in epochs_save_range_lambda], vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "\n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Epochs', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, epochs_lambda])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'multi_epoch_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "    vals_int_real = scores_int.loc[index].values\n",
    "    vals_lambda_real = np.concatenate([scores_lambda.loc[index].values[::4] for i in samples_list], axis=None)\n",
    "    vals_lambda_lstsq = np.concatenate([scores_lambda.loc[index].values[3::4] for i in samples_list], axis=None)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    ax.plot(samples_list, vals_int_real, label='Error I-Net Poly vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_real, label='Error Lambda Model Preds vs. Real Poly')\n",
    "    ax.plot(samples_list, vals_lambda_lstsq, label='Error LSTSQ Preds vs. Real Poly')\n",
    "        \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_REAL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "    \n",
    "    fig.savefig(path, format='eps')\n",
    "\n",
    "elif samples_list != None and not evaluate_with_real_function:\n",
    "    index = 'MAE FV'\n",
    "\n",
    "\n",
    "    vals_int_poly = scores_int.loc[index].values[1::2]\n",
    "    vals_int_preds = scores_int.loc[index].values[::2]\n",
    "    vals_lambda_poly_pred = np.concatenate([scores_lambda.loc[index].values[2::4] for i in samples_list], axis=None)\n",
    "\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    ax.plot(samples_list, vals_int_poly, label='Error I-Net Poly vs Lambda Poly')\n",
    "    ax.plot(samples_list, vals_int_preds, label='Error I-Net Poly vs Lambda Model Preds')\n",
    "    ax.plot(samples_list, vals_lambda_poly_pred, label='Error Lambda Poly vs Lambda Model Preds')\n",
    "    \n",
    "    ax.set_ylabel(index, fontsize=20)\n",
    "    ax.set_xlabel('Training Set Size I-Net', fontsize=20)\n",
    "\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(15)   \n",
    "    \n",
    "    ax.legend(loc=\"bottom right\", fontsize=15)\n",
    "    ax.set_xlim([0, samples_list[-1]])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file = 'sample_list_' + '-'.join([str(samples_list[0]), str(samples_list[-1])]) +'_MODEL_' + index + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "\n",
    "    path = location + folder + file\n",
    "        \n",
    "    fig.savefig(path, format='eps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.625Z"
    }
   },
   "outputs": [],
   "source": [
    "if multi_epoch_analysis:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split_list[-1]\n",
    "elif samples_list != None:\n",
    "    plot_preds = polynomial_test_fv_list[-1]\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "else:\n",
    "    plot_preds = polynomial_test_fv\n",
    "    plot_eval = lambda_test_fv_test_split\n",
    "\n",
    "x_vars = ['x' + str(i) for i in range(1, n+1)]\n",
    "\n",
    "columns = x_vars.copy()\n",
    "columns.append('FVs')\n",
    "\n",
    "columns_single = x_vars.copy()\n",
    "columns_single.extend(['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'])\n",
    "\n",
    "eval_size_plot = plot_preds[2].shape[1]\n",
    "rand_index = 2#42#random.randint(0, plot_preds[2].shape[0]-1)\n",
    "vars_plot = np.column_stack([lambda_test_input_test_split[rand_index][::,i] for i in range(n)])\n",
    "plot_data_single = pd.DataFrame(data=np.column_stack([vars_plot, plot_preds[2][rand_index], plot_preds[3][rand_index], plot_eval.values[rand_index]]), columns=columns_single)\n",
    "\n",
    "vars_plot_all_preds = np.append(np.append(vars_plot, vars_plot, axis=0), vars_plot, axis=0)\n",
    "preds_plot_all = np.append(np.append(plot_preds[2][rand_index], plot_preds[3][rand_index], axis=0), plot_eval.values[rand_index], axis=0)\n",
    "\n",
    "if evaluate_with_real_function:\n",
    "    real_str = np.array(['Real Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds LSTSQ\n",
    "else:\n",
    "    real_str = np.array(['Lambda Poly FVs (Target)' for i in range(eval_size_plot)])\n",
    "    int_str = np.array(['Int Pred Poly FVs' for i in range(eval_size_plot)])\n",
    "    lambda_str = np.array(['Lambda Preds' for i in range(eval_size_plot)])\n",
    "    #Add Lambda Poly Preds\n",
    "    \n",
    "identifier = np.concatenate([real_str, int_str, lambda_str])\n",
    "\n",
    "plot_data = pd.DataFrame(data=np.column_stack([vars_plot_all_preds, preds_plot_all]), columns=columns)\n",
    "plot_data['Identifier'] = identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.627Z"
    }
   },
   "outputs": [],
   "source": [
    "pp1 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  y_vars=['FVs'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.628Z"
    }
   },
   "outputs": [],
   "source": [
    "pp2 = sns.pairplot(data=plot_data,\n",
    "                  #kind='reg',\n",
    "                  hue='Identifier',\n",
    "                  #y_vars=['FVs'],\n",
    "                  #x_vars=x_vars\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.630Z"
    }
   },
   "outputs": [],
   "source": [
    "pp3 = sns.pairplot(data=plot_data_single,\n",
    "                  #kind='reg',\n",
    "                  y_vars=['Real Poly FVs (Target)', 'Int Pred Poly FVs', 'Lambda Preds'],\n",
    "                  x_vars=x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.632Z"
    }
   },
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_REAL_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_REAL_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')\n",
    "else:\n",
    "    location = './data/plotting/'\n",
    "    folder = interpretation_network_string + filename + '/'\n",
    "    file1 = 'pp3in1_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file2 = 'pp3in1_extended_PRED_' + str(rand_index) + '_' + interpretation_network_string +  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    file3 = 'pp1_PRED_' + str(rand_index) + '_' + interpretation_network_string+  '_lambda_' + filename + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '.eps'\n",
    "    \n",
    "    path1 = location + folder + file1\n",
    "    path2 = location + folder + file2\n",
    "    path3 = location + folder + file3\n",
    "    \n",
    "    pp1.savefig(path1, format='eps')\n",
    "    pp2.savefig(path2, format='eps')\n",
    "    pp3.savefig(path3, format='eps')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.634Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n)+ '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "    path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs_lambda).zfill(3)  + filename + '.txt'\n",
    "\n",
    "\n",
    "    loss_df_lambda = pd.read_csv(path_loss, sep=',')\n",
    "    metric_df_lambda = pd.read_csv(path_metric, sep=',')\n",
    "    val_loss_df_lambda = pd.read_csv(path_val_loss, sep=',')\n",
    "    val_metric_df_lambda = pd.read_csv(path_val_metric, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.636Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "\n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_metric = 0\n",
    "\n",
    "    metric_df_adjusted = metric_df_lambda.copy(deep=True)\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "\n",
    "    val_metric_df_adjusted = val_metric_df_lambda.copy(deep=True)\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_metric'])\n",
    "    plt.title('model metric')\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.638Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    #%%script false --no-raise-error\n",
    "    adjustment_threshold_loss = 1000\n",
    "\n",
    "    loss_df_adjusted = loss_df_lambda.copy(deep=True)\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "\n",
    "    val_loss_df_adjusted = val_loss_df_lambda.copy(deep=True)\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "\n",
    "    plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "    plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.640Z"
    }
   },
   "outputs": [],
   "source": [
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    preds = model.predict(X_test)\n",
    "    preds_rounded = np.round(preds, 1)\n",
    "    #preds_true = pd.DataFrame(data=[np.round(preds, 1), y_test.values])\n",
    "    for pred, y in tqdm(zip(preds_rounded, y_test.values)):\n",
    "        if (pred == y).all():\n",
    "            print(pred)\n",
    "    \n",
    "    #print(preds_rounded)\n",
    "    #print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.642Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#summarize history for loss\n",
    "if not multi_epoch_analysis and samples_list == None: \n",
    "    plt.plot(history['loss'])\n",
    "    try:\n",
    "        plt.plot(history['val_loss'])\n",
    "    except KeyError:\n",
    "        print('no val_loss in keys')\n",
    "    #plt.plot(random_network[2].history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/loss_' + interpretation_network_string + filename + '.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.644Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    random_polynomial = list(random_product([i*a_step for i in range(int(a_min*10**int(-np.log10(a_step))), int(a_max*10**int(-np.log10(a_step))))], repeat=nCr(n+d, d)))\n",
    "    list_of_random_polynomials.append(random_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.646Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(y_test.values, lambda_test_input_test_split)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_test_input_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.648Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(y_test, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.650Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.652Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(y_train.values, lambda_train_input_train_split)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-02T13:39:07.654Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
