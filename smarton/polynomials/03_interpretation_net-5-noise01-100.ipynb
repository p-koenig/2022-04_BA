{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 2, #degree\n",
    "        'n': 5, #number of variables\n",
    "        'monomial_vars': None, #int or None\n",
    "        'laurent': False, #use Laurent polynomials (negative degree with up to -d)  \n",
    "        'neg_d': 0,#int or None\n",
    "        'neg_d_prob': 0,\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': 5,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 100,\n",
    "        'a_min': -100,\n",
    "        'lambda_nets_total': 1000,\n",
    "        'noise': 0.1,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'border_min': 0.2, #needs to be between 0 and (x_max-x_min)/2\n",
    "        'border_max': 0.4,\n",
    "        'lower_degree_prob': 0.5,\n",
    "        'a_zero_prob': 0.25,\n",
    "        'a_random_prob': 0.1,      \n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sample_sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0.25,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 500, \n",
    "        'early_stopping': True,\n",
    "        'batch_size': 256,\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 10000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': 5, #(None, int) #CONSTANT IS NOT INCLUDED\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        'test_size': 100, #Float for fraction, Int for number 0\n",
    "        \n",
    "        'normalize_inet_data': False,\n",
    "        'inet_training_without_noise': True, #dataset size without noise hardcoded to 50k in generate_paths\n",
    "        \n",
    "\n",
    "        'evaluate_with_real_function': False,\n",
    "        'consider_labels_training': False,\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': False,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 100,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 100,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "        \n",
    "        'symbolic_metamodeling_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_evaluation': False,\n",
    "        'symbolic_metamodeling_function_evaluation': False,\n",
    "        'symbolic_metamodeling_poly_function_evaluation': False,\n",
    "        \n",
    "        'symbolic_regression_evaluation': True,\n",
    "        'per_network_evaluation': False,\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 10,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold, ParameterGrid, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 243\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 1, 2], [0, 0, 0, 2, 0], [0, 0, 0, 2, 1], [0, 0, 0, 2, 2], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 0, 2], [0, 0, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 2], [0, 0, 1, 2, 0], [0, 0, 1, 2, 1], [0, 0, 1, 2, 2], [0, 0, 2, 0, 0], [0, 0, 2, 0, 1], [0, 0, 2, 0, 2], [0, 0, 2, 1, 0], [0, 0, 2, 1, 1], [0, 0, 2, 1, 2], [0, 0, 2, 2, 0], [0, 0, 2, 2, 1], [0, 0, 2, 2, 2], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 0, 2], [0, 1, 0, 1, 0], [0, 1, 0, 1, 1], [0, 1, 0, 1, 2], [0, 1, 0, 2, 0], [0, 1, 0, 2, 1], [0, 1, 0, 2, 2], [0, 1, 1, 0, 0], [0, 1, 1, 0, 1], [0, 1, 1, 0, 2], [0, 1, 1, 1, 0], [0, 1, 1, 1, 1], [0, 1, 1, 1, 2], [0, 1, 1, 2, 0], [0, 1, 1, 2, 1], [0, 1, 1, 2, 2], [0, 1, 2, 0, 0], [0, 1, 2, 0, 1], [0, 1, 2, 0, 2], [0, 1, 2, 1, 0], [0, 1, 2, 1, 1], [0, 1, 2, 1, 2], [0, 1, 2, 2, 0], [0, 1, 2, 2, 1], [0, 1, 2, 2, 2], [0, 2, 0, 0, 0], [0, 2, 0, 0, 1], [0, 2, 0, 0, 2], [0, 2, 0, 1, 0], [0, 2, 0, 1, 1], [0, 2, 0, 1, 2], [0, 2, 0, 2, 0], [0, 2, 0, 2, 1], [0, 2, 0, 2, 2], [0, 2, 1, 0, 0], [0, 2, 1, 0, 1], [0, 2, 1, 0, 2], [0, 2, 1, 1, 0], [0, 2, 1, 1, 1], [0, 2, 1, 1, 2], [0, 2, 1, 2, 0], [0, 2, 1, 2, 1], [0, 2, 1, 2, 2], [0, 2, 2, 0, 0], [0, 2, 2, 0, 1], [0, 2, 2, 0, 2], [0, 2, 2, 1, 0], [0, 2, 2, 1, 1], [0, 2, 2, 1, 2], [0, 2, 2, 2, 0], [0, 2, 2, 2, 1], [0, 2, 2, 2, 2], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 0, 2], [1, 0, 0, 1, 0], [1, 0, 0, 1, 1], [1, 0, 0, 1, 2], [1, 0, 0, 2, 0], [1, 0, 0, 2, 1], [1, 0, 0, 2, 2], [1, 0, 1, 0, 0], [1, 0, 1, 0, 1], [1, 0, 1, 0, 2], [1, 0, 1, 1, 0], [1, 0, 1, 1, 1], [1, 0, 1, 1, 2], [1, 0, 1, 2, 0], [1, 0, 1, 2, 1], [1, 0, 1, 2, 2], [1, 0, 2, 0, 0], [1, 0, 2, 0, 1], [1, 0, 2, 0, 2], [1, 0, 2, 1, 0], [1, 0, 2, 1, 1], [1, 0, 2, 1, 2], [1, 0, 2, 2, 0], [1, 0, 2, 2, 1], [1, 0, 2, 2, 2], [1, 1, 0, 0, 0], [1, 1, 0, 0, 1], [1, 1, 0, 0, 2], [1, 1, 0, 1, 0], [1, 1, 0, 1, 1], [1, 1, 0, 1, 2], [1, 1, 0, 2, 0], [1, 1, 0, 2, 1], [1, 1, 0, 2, 2], [1, 1, 1, 0, 0], [1, 1, 1, 0, 1], [1, 1, 1, 0, 2], [1, 1, 1, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 2], [1, 1, 1, 2, 0], [1, 1, 1, 2, 1], [1, 1, 1, 2, 2], [1, 1, 2, 0, 0], [1, 1, 2, 0, 1], [1, 1, 2, 0, 2], [1, 1, 2, 1, 0], [1, 1, 2, 1, 1], [1, 1, 2, 1, 2], [1, 1, 2, 2, 0], [1, 1, 2, 2, 1], [1, 1, 2, 2, 2], [1, 2, 0, 0, 0], [1, 2, 0, 0, 1], [1, 2, 0, 0, 2], [1, 2, 0, 1, 0], [1, 2, 0, 1, 1], [1, 2, 0, 1, 2], [1, 2, 0, 2, 0], [1, 2, 0, 2, 1], [1, 2, 0, 2, 2], [1, 2, 1, 0, 0], [1, 2, 1, 0, 1], [1, 2, 1, 0, 2], [1, 2, 1, 1, 0], [1, 2, 1, 1, 1], [1, 2, 1, 1, 2], [1, 2, 1, 2, 0], [1, 2, 1, 2, 1], [1, 2, 1, 2, 2], [1, 2, 2, 0, 0], [1, 2, 2, 0, 1], [1, 2, 2, 0, 2], [1, 2, 2, 1, 0], [1, 2, 2, 1, 1], [1, 2, 2, 1, 2], [1, 2, 2, 2, 0], [1, 2, 2, 2, 1], [1, 2, 2, 2, 2], [2, 0, 0, 0, 0], [2, 0, 0, 0, 1], [2, 0, 0, 0, 2], [2, 0, 0, 1, 0], [2, 0, 0, 1, 1], [2, 0, 0, 1, 2], [2, 0, 0, 2, 0], [2, 0, 0, 2, 1], [2, 0, 0, 2, 2], [2, 0, 1, 0, 0], [2, 0, 1, 0, 1], [2, 0, 1, 0, 2], [2, 0, 1, 1, 0], [2, 0, 1, 1, 1], [2, 0, 1, 1, 2], [2, 0, 1, 2, 0], [2, 0, 1, 2, 1], [2, 0, 1, 2, 2], [2, 0, 2, 0, 0], [2, 0, 2, 0, 1], [2, 0, 2, 0, 2], [2, 0, 2, 1, 0], [2, 0, 2, 1, 1], [2, 0, 2, 1, 2], [2, 0, 2, 2, 0], [2, 0, 2, 2, 1], [2, 0, 2, 2, 2], [2, 1, 0, 0, 0], [2, 1, 0, 0, 1], [2, 1, 0, 0, 2], [2, 1, 0, 1, 0], [2, 1, 0, 1, 1], [2, 1, 0, 1, 2], [2, 1, 0, 2, 0], [2, 1, 0, 2, 1], [2, 1, 0, 2, 2], [2, 1, 1, 0, 0], [2, 1, 1, 0, 1], [2, 1, 1, 0, 2], [2, 1, 1, 1, 0], [2, 1, 1, 1, 1], [2, 1, 1, 1, 2], [2, 1, 1, 2, 0], [2, 1, 1, 2, 1], [2, 1, 1, 2, 2], [2, 1, 2, 0, 0], [2, 1, 2, 0, 1], [2, 1, 2, 0, 2], [2, 1, 2, 1, 0], [2, 1, 2, 1, 1], [2, 1, 2, 1, 2], [2, 1, 2, 2, 0], [2, 1, 2, 2, 1], [2, 1, 2, 2, 2], [2, 2, 0, 0, 0], [2, 2, 0, 0, 1], [2, 2, 0, 0, 2], [2, 2, 0, 1, 0], [2, 2, 0, 1, 1], [2, 2, 0, 1, 2], [2, 2, 0, 2, 0], [2, 2, 0, 2, 1], [2, 2, 0, 2, 2], [2, 2, 1, 0, 0], [2, 2, 1, 0, 1], [2, 2, 1, 0, 2], [2, 2, 1, 1, 0], [2, 2, 1, 1, 1], [2, 2, 1, 1, 2], [2, 2, 1, 2, 0], [2, 2, 1, 2, 1], [2, 2, 1, 2, 2], [2, 2, 2, 0, 0], [2, 2, 2, 0, 1], [2, 2, 2, 0, 2], [2, 2, 2, 1, 0], [2, 2, 2, 1, 1], [2, 2, 2, 1, 2], [2, 2, 2, 2, 0], [2, 2, 2, 2, 1], [2, 2, 2, 2, 2]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23de3d7adc75475c917f311ef81536ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 21\n",
      "[[0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 2], [0, 0, 0, 1, 0], [0, 0, 0, 1, 1], [0, 0, 0, 2, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 1], [0, 0, 1, 1, 0], [0, 0, 2, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1], [0, 1, 0, 1, 0], [0, 1, 1, 0, 0], [0, 2, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 1], [1, 0, 0, 1, 0], [1, 0, 1, 0, 0], [1, 1, 0, 0, 0], [2, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from utilities.utility_functions import flatten, rec_gen\n",
    "\n",
    "list_of_monomial_identifiers_extended = []\n",
    "\n",
    "if laurent:\n",
    "    variable_sets = [list(flatten([[_d for _d in range(d+1)], [-_d for _d in range(1, neg_d+1)]])) for _ in range(n)]\n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)    \n",
    "        \n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity:' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)        \n",
    "else:\n",
    "    variable_sets = [[_d for _d in range(d+1)] for _ in range(n)]  \n",
    "    list_of_monomial_identifiers_extended = rec_gen(variable_sets)\n",
    "\n",
    "    print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "    #print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "    #print('Sparsity: ' + str(sparsity))\n",
    "    if len(list_of_monomial_identifiers_extended) < 500:\n",
    "        print(list_of_monomial_identifiers_extended)    \n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    if np.sum(monomial_identifier) <= d:\n",
    "        if monomial_vars == None or len(list(filter(lambda x: x != 0, monomial_identifier))) <= monomial_vars:\n",
    "            list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "#print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(sparsity))\n",
    "#print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d']) if not laurent else len(list_of_monomial_identifiers)\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']\n",
    "\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sample_sparsity')*config['data']['sample_sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense512-1024-output_110_drop0.25e500b256_custom/lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0.1bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n",
      "lnets_1000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0.1bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index, no_noise=False):\n",
    "        \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    path_identifier_lambda_net_data_loading = None \n",
    "                \n",
    "    if no_noise==True:\n",
    "        path_identifier_lambda_net_data_loading = generate_paths(path_type='interpretation_net_no_noise')['path_identifier_lambda_net_data']\n",
    "        print('interpretation_net_no_noise', path_identifier_lambda_net_data_loading)\n",
    "    else:\n",
    "        path_identifier_lambda_net_data_loading = path_identifier_lambda_net_data \n",
    "        print(path_identifier_lambda_net_data_loading)\n",
    "        \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data_loading + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == True:\n",
    "        weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == True:\n",
    "        lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    if no_noise == True:\n",
    "        lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpretation_net_no_noise lnets_10000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend MultiprocessingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:   50.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnets_1000_25-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_5_d_2_negd_0_prob_0_spars_5_amin_-100_amax_100_xdist_uniform_noise_normal_0.1bmin0.2bmax0.4lowd0.5azero0.25arand0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend MultiprocessingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 out of   1 | elapsed:    7.3s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "if inet_training_without_noise:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list_without_noise = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1, no_noise=True) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "else:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "    del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.792</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-88.373</td>\n",
       "      <td>85.823</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.935</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>70.591</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-21.692</td>\n",
       "      <td>-36.859</td>\n",
       "      <td>0.031</td>\n",
       "      <td>11.123</td>\n",
       "      <td>1.747</td>\n",
       "      <td>-90.079</td>\n",
       "      <td>140.346</td>\n",
       "      <td>0.708</td>\n",
       "      <td>87.090</td>\n",
       "      <td>-34.835</td>\n",
       "      <td>1.636</td>\n",
       "      <td>-2.111</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>-1.694</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>35.268</td>\n",
       "      <td>-1.264</td>\n",
       "      <td>-6.964</td>\n",
       "      <td>43.343</td>\n",
       "      <td>2.492</td>\n",
       "      <td>-18.880</td>\n",
       "      <td>1.921</td>\n",
       "      <td>-39.809</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-3.451</td>\n",
       "      <td>2.172</td>\n",
       "      <td>-87.013</td>\n",
       "      <td>84.307</td>\n",
       "      <td>0.846</td>\n",
       "      <td>102.337</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-1.036</td>\n",
       "      <td>2.540</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>-0.745</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.549</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.735</td>\n",
       "      <td>70.790</td>\n",
       "      <td>1.224</td>\n",
       "      <td>-1.197</td>\n",
       "      <td>0.651</td>\n",
       "      <td>1.099</td>\n",
       "      <td>0.622</td>\n",
       "      <td>2.118</td>\n",
       "      <td>2.372</td>\n",
       "      <td>2.276</td>\n",
       "      <td>2.357</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.490</td>\n",
       "      <td>1.958</td>\n",
       "      <td>1.289</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.503</td>\n",
       "      <td>2.315</td>\n",
       "      <td>2.411</td>\n",
       "      <td>2.147</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.419</td>\n",
       "      <td>2.476</td>\n",
       "      <td>2.318</td>\n",
       "      <td>0.643</td>\n",
       "      <td>2.320</td>\n",
       "      <td>-3.446</td>\n",
       "      <td>0.105</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.221</td>\n",
       "      <td>1.247</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.603</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.272</td>\n",
       "      <td>1.161</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>0.546</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>1.057</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.583</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.999</td>\n",
       "      <td>-2.913</td>\n",
       "      <td>-2.405</td>\n",
       "      <td>-3.210</td>\n",
       "      <td>-3.442</td>\n",
       "      <td>3.010</td>\n",
       "      <td>3.763</td>\n",
       "      <td>3.911</td>\n",
       "      <td>-2.986</td>\n",
       "      <td>-2.461</td>\n",
       "      <td>-3.457</td>\n",
       "      <td>1.316</td>\n",
       "      <td>0.209</td>\n",
       "      <td>2.478</td>\n",
       "      <td>1.672</td>\n",
       "      <td>3.681</td>\n",
       "      <td>-3.350</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.863</td>\n",
       "      <td>3.380</td>\n",
       "      <td>3.162</td>\n",
       "      <td>3.897</td>\n",
       "      <td>-3.084</td>\n",
       "      <td>3.547</td>\n",
       "      <td>2.731</td>\n",
       "      <td>-2.196</td>\n",
       "      <td>2.336</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.203</td>\n",
       "      <td>3.378</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>2.399</td>\n",
       "      <td>1.987</td>\n",
       "      <td>3.188</td>\n",
       "      <td>-4.636</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-5.106</td>\n",
       "      <td>-5.702</td>\n",
       "      <td>0.365</td>\n",
       "      <td>2.999</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>2.407</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.563</td>\n",
       "      <td>1.651</td>\n",
       "      <td>1.423</td>\n",
       "      <td>0.629</td>\n",
       "      <td>1.825</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>1.775</td>\n",
       "      <td>1.394</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.313</td>\n",
       "      <td>1.706</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>1.762</td>\n",
       "      <td>1.454</td>\n",
       "      <td>0.574</td>\n",
       "      <td>1.803</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>1.372</td>\n",
       "      <td>1.312</td>\n",
       "      <td>1.223</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.287</td>\n",
       "      <td>0.479</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>1.641</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.351</td>\n",
       "      <td>-1.819</td>\n",
       "      <td>1.367</td>\n",
       "      <td>1.406</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.391</td>\n",
       "      <td>1.430</td>\n",
       "      <td>1.519</td>\n",
       "      <td>1.465</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.386</td>\n",
       "      <td>-3.925</td>\n",
       "      <td>-3.374</td>\n",
       "      <td>-3.942</td>\n",
       "      <td>-5.659</td>\n",
       "      <td>2.646</td>\n",
       "      <td>2.840</td>\n",
       "      <td>2.764</td>\n",
       "      <td>-3.930</td>\n",
       "      <td>-3.436</td>\n",
       "      <td>-3.756</td>\n",
       "      <td>-5.870</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-6.642</td>\n",
       "      <td>-6.342</td>\n",
       "      <td>2.706</td>\n",
       "      <td>-6.592</td>\n",
       "      <td>3.012</td>\n",
       "      <td>2.491</td>\n",
       "      <td>2.883</td>\n",
       "      <td>2.593</td>\n",
       "      <td>2.676</td>\n",
       "      <td>-3.860</td>\n",
       "      <td>2.642</td>\n",
       "      <td>-5.325</td>\n",
       "      <td>-3.352</td>\n",
       "      <td>1.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>9.300</td>\n",
       "      <td>90.715</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-57.684</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-18.513</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.709</td>\n",
       "      <td>13.069</td>\n",
       "      <td>87.111</td>\n",
       "      <td>0.505</td>\n",
       "      <td>-6.054</td>\n",
       "      <td>-52.583</td>\n",
       "      <td>2.421</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.665</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-1.857</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>1.719</td>\n",
       "      <td>-0.864</td>\n",
       "      <td>1.127</td>\n",
       "      <td>-5.929</td>\n",
       "      <td>-16.022</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.065</td>\n",
       "      <td>0.819</td>\n",
       "      <td>-90.624</td>\n",
       "      <td>9.325</td>\n",
       "      <td>89.433</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-57.533</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>1.165</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.389</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.499</td>\n",
       "      <td>-19.058</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>0.402</td>\n",
       "      <td>-95.428</td>\n",
       "      <td>1.233</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>4.912</td>\n",
       "      <td>1.347</td>\n",
       "      <td>-0.933</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>1.887</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>3.743</td>\n",
       "      <td>1.726</td>\n",
       "      <td>1.790</td>\n",
       "      <td>1.190</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>4.723</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>1.648</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>2.410</td>\n",
       "      <td>-1.243</td>\n",
       "      <td>1.364</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>0.661</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.906</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.432</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.812</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0.671</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.347</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.427</td>\n",
       "      <td>1.115</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.814</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.212</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>1.077</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.350</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.804</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-4.110</td>\n",
       "      <td>2.375</td>\n",
       "      <td>-2.036</td>\n",
       "      <td>0.840</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>2.483</td>\n",
       "      <td>2.100</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-0.945</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.576</td>\n",
       "      <td>2.498</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>2.328</td>\n",
       "      <td>2.709</td>\n",
       "      <td>2.323</td>\n",
       "      <td>2.434</td>\n",
       "      <td>4.058</td>\n",
       "      <td>2.689</td>\n",
       "      <td>2.886</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-2.431</td>\n",
       "      <td>0.420</td>\n",
       "      <td>1.437</td>\n",
       "      <td>1.378</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-2.036</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.438</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.806</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.408</td>\n",
       "      <td>1.453</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.494</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-3.760</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-2.106</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-9.760</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.995</td>\n",
       "      <td>-1.502</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-7.473</td>\n",
       "      <td>-1.654</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-2.157</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-6.867</td>\n",
       "      <td>2.002</td>\n",
       "      <td>-1.781</td>\n",
       "      <td>2.513</td>\n",
       "      <td>1.806</td>\n",
       "      <td>2.015</td>\n",
       "      <td>1.935</td>\n",
       "      <td>5.147</td>\n",
       "      <td>-9.377</td>\n",
       "      <td>2.255</td>\n",
       "      <td>-1.669</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-40.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>48.856</td>\n",
       "      <td>49.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61.743</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.947</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-19.794</td>\n",
       "      <td>-14.986</td>\n",
       "      <td>-14.649</td>\n",
       "      <td>72.039</td>\n",
       "      <td>29.846</td>\n",
       "      <td>-13.490</td>\n",
       "      <td>60.752</td>\n",
       "      <td>1.258</td>\n",
       "      <td>-1.583</td>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-1.978</td>\n",
       "      <td>2.250</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>61.373</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>1.410</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.811</td>\n",
       "      <td>-36.884</td>\n",
       "      <td>-1.309</td>\n",
       "      <td>48.240</td>\n",
       "      <td>49.353</td>\n",
       "      <td>2.635</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-1.822</td>\n",
       "      <td>63.023</td>\n",
       "      <td>2.473</td>\n",
       "      <td>-2.045</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-3.298</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>58.702</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>1.722</td>\n",
       "      <td>0.684</td>\n",
       "      <td>1.827</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>2.165</td>\n",
       "      <td>2.064</td>\n",
       "      <td>2.245</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>2.442</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>0.169</td>\n",
       "      <td>2.117</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>1.885</td>\n",
       "      <td>2.288</td>\n",
       "      <td>2.215</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.107</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>2.104</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-1.279</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.786</td>\n",
       "      <td>1.563</td>\n",
       "      <td>1.311</td>\n",
       "      <td>0.804</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.742</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.189</td>\n",
       "      <td>0.541</td>\n",
       "      <td>1.145</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.216</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.643</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>1.804</td>\n",
       "      <td>2.521</td>\n",
       "      <td>2.644</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>2.105</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2.448</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.738</td>\n",
       "      <td>2.601</td>\n",
       "      <td>2.159</td>\n",
       "      <td>1.933</td>\n",
       "      <td>2.623</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>2.265</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>1.524</td>\n",
       "      <td>1.623</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>1.947</td>\n",
       "      <td>2.822</td>\n",
       "      <td>2.251</td>\n",
       "      <td>2.675</td>\n",
       "      <td>-1.490</td>\n",
       "      <td>-1.772</td>\n",
       "      <td>2.862</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-2.872</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>2.649</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.932</td>\n",
       "      <td>2.257</td>\n",
       "      <td>2.416</td>\n",
       "      <td>2.320</td>\n",
       "      <td>2.115</td>\n",
       "      <td>-1.321</td>\n",
       "      <td>2.177</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-2.725</td>\n",
       "      <td>-1.663</td>\n",
       "      <td>-1.707</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-2.019</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>2.106</td>\n",
       "      <td>1.626</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.524</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>2.252</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>0.517</td>\n",
       "      <td>-1.093</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.517</td>\n",
       "      <td>1.701</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>2.704</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.300</td>\n",
       "      <td>2.099</td>\n",
       "      <td>1.833</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>1.664</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.636</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.715</td>\n",
       "      <td>1.782</td>\n",
       "      <td>0.694</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>2.468</td>\n",
       "      <td>-3.278</td>\n",
       "      <td>-3.374</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-3.701</td>\n",
       "      <td>2.097</td>\n",
       "      <td>2.041</td>\n",
       "      <td>1.968</td>\n",
       "      <td>-3.125</td>\n",
       "      <td>-2.889</td>\n",
       "      <td>1.970</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-3.326</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>1.986</td>\n",
       "      <td>-1.758</td>\n",
       "      <td>2.231</td>\n",
       "      <td>1.716</td>\n",
       "      <td>2.138</td>\n",
       "      <td>1.896</td>\n",
       "      <td>1.818</td>\n",
       "      <td>-2.641</td>\n",
       "      <td>1.793</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-3.974</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-38.064</td>\n",
       "      <td>-40.577</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-38.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.878</td>\n",
       "      <td>-29.078</td>\n",
       "      <td>-52.980</td>\n",
       "      <td>6.571</td>\n",
       "      <td>-2.401</td>\n",
       "      <td>2.326</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>55.385</td>\n",
       "      <td>1.399</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-13.896</td>\n",
       "      <td>-30.862</td>\n",
       "      <td>1.907</td>\n",
       "      <td>6.109</td>\n",
       "      <td>3.852</td>\n",
       "      <td>-4.437</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>3.965</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-37.145</td>\n",
       "      <td>-39.728</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>-2.009</td>\n",
       "      <td>0.228</td>\n",
       "      <td>1.028</td>\n",
       "      <td>59.601</td>\n",
       "      <td>-1.326</td>\n",
       "      <td>0.988</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-1.681</td>\n",
       "      <td>-37.934</td>\n",
       "      <td>0.765</td>\n",
       "      <td>1.646</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.286</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.823</td>\n",
       "      <td>-3.393</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.559</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.663</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.596</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.847</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.373</td>\n",
       "      <td>1.065</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.139</td>\n",
       "      <td>2.341</td>\n",
       "      <td>0.352</td>\n",
       "      <td>3.097</td>\n",
       "      <td>1.982</td>\n",
       "      <td>0.959</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-1.783</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>2.393</td>\n",
       "      <td>-0.928</td>\n",
       "      <td>-1.818</td>\n",
       "      <td>-1.440</td>\n",
       "      <td>-1.635</td>\n",
       "      <td>-1.160</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>-1.256</td>\n",
       "      <td>2.650</td>\n",
       "      <td>-1.579</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>2.352</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>-1.943</td>\n",
       "      <td>-1.479</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-1.307</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.570</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.120</td>\n",
       "      <td>1.080</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.894</td>\n",
       "      <td>1.008</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>0.616</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>1.045</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.515</td>\n",
       "      <td>0.905</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-2.049</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.679</td>\n",
       "      <td>1.267</td>\n",
       "      <td>1.152</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.315</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.293</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>-3.134</td>\n",
       "      <td>1.825</td>\n",
       "      <td>1.707</td>\n",
       "      <td>1.663</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.972</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.076</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.232</td>\n",
       "      <td>1.137</td>\n",
       "      <td>1.023</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.991</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.801</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.878</td>\n",
       "      <td>1.165</td>\n",
       "      <td>0.979</td>\n",
       "      <td>1.088</td>\n",
       "      <td>-1.737</td>\n",
       "      <td>-1.625</td>\n",
       "      <td>-2.115</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>0.122</td>\n",
       "      <td>2.322</td>\n",
       "      <td>-1.093</td>\n",
       "      <td>-2.154</td>\n",
       "      <td>-1.747</td>\n",
       "      <td>-1.504</td>\n",
       "      <td>-1.503</td>\n",
       "      <td>-1.468</td>\n",
       "      <td>-1.746</td>\n",
       "      <td>-1.545</td>\n",
       "      <td>2.135</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-2.661</td>\n",
       "      <td>2.306</td>\n",
       "      <td>5.600</td>\n",
       "      <td>-1.982</td>\n",
       "      <td>-2.012</td>\n",
       "      <td>-1.543</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>-2.085</td>\n",
       "      <td>-0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61.796</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>65.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.238</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-81.773</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-60.609</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.480</td>\n",
       "      <td>2.618</td>\n",
       "      <td>-4.274</td>\n",
       "      <td>5.856</td>\n",
       "      <td>59.859</td>\n",
       "      <td>-7.139</td>\n",
       "      <td>11.461</td>\n",
       "      <td>1.478</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>54.381</td>\n",
       "      <td>4.088</td>\n",
       "      <td>0.798</td>\n",
       "      <td>2.439</td>\n",
       "      <td>5.551</td>\n",
       "      <td>-7.221</td>\n",
       "      <td>1.877</td>\n",
       "      <td>-80.060</td>\n",
       "      <td>1.346</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-56.111</td>\n",
       "      <td>-5.406</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>62.154</td>\n",
       "      <td>0.564</td>\n",
       "      <td>1.446</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.165</td>\n",
       "      <td>64.104</td>\n",
       "      <td>-1.233</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.054</td>\n",
       "      <td>7.092</td>\n",
       "      <td>1.859</td>\n",
       "      <td>0.453</td>\n",
       "      <td>-82.342</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.192</td>\n",
       "      <td>-61.484</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>2.131</td>\n",
       "      <td>-2.220</td>\n",
       "      <td>2.400</td>\n",
       "      <td>2.378</td>\n",
       "      <td>-1.788</td>\n",
       "      <td>-2.409</td>\n",
       "      <td>2.753</td>\n",
       "      <td>1.816</td>\n",
       "      <td>2.320</td>\n",
       "      <td>2.631</td>\n",
       "      <td>2.441</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>1.830</td>\n",
       "      <td>2.435</td>\n",
       "      <td>-2.990</td>\n",
       "      <td>2.518</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-1.837</td>\n",
       "      <td>-2.548</td>\n",
       "      <td>3.254</td>\n",
       "      <td>-3.602</td>\n",
       "      <td>2.136</td>\n",
       "      <td>2.570</td>\n",
       "      <td>3.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-1.069</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.367</td>\n",
       "      <td>-1.482</td>\n",
       "      <td>0.615</td>\n",
       "      <td>1.506</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.534</td>\n",
       "      <td>-1.062</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.035</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-1.614</td>\n",
       "      <td>1.607</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.927</td>\n",
       "      <td>2.084</td>\n",
       "      <td>-1.331</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>-0.932</td>\n",
       "      <td>1.526</td>\n",
       "      <td>2.349</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-1.355</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-1.236</td>\n",
       "      <td>5.300</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>2.991</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>2.522</td>\n",
       "      <td>2.402</td>\n",
       "      <td>-3.056</td>\n",
       "      <td>3.038</td>\n",
       "      <td>-1.335</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>2.175</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>2.825</td>\n",
       "      <td>1.671</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>-1.287</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-1.185</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-1.318</td>\n",
       "      <td>1.525</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>1.799</td>\n",
       "      <td>1.819</td>\n",
       "      <td>-0.827</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-1.556</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-2.280</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-2.860</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>2.148</td>\n",
       "      <td>1.641</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.269</td>\n",
       "      <td>1.970</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-2.610</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>1.708</td>\n",
       "      <td>1.736</td>\n",
       "      <td>2.060</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>2.259</td>\n",
       "      <td>1.859</td>\n",
       "      <td>-1.966</td>\n",
       "      <td>1.857</td>\n",
       "      <td>0.498</td>\n",
       "      <td>2.246</td>\n",
       "      <td>0.460</td>\n",
       "      <td>1.909</td>\n",
       "      <td>1.089</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.090</td>\n",
       "      <td>1.184</td>\n",
       "      <td>-1.878</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-2.676</td>\n",
       "      <td>1.018</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>1.237</td>\n",
       "      <td>-1.433</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-2.308</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.928</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.130</td>\n",
       "      <td>2.757</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.945</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>-2.305</td>\n",
       "      <td>0.742</td>\n",
       "      <td>-2.241</td>\n",
       "      <td>-6.784</td>\n",
       "      <td>-2.313</td>\n",
       "      <td>-2.363</td>\n",
       "      <td>4.599</td>\n",
       "      <td>2.685</td>\n",
       "      <td>-6.205</td>\n",
       "      <td>-2.120</td>\n",
       "      <td>-3.311</td>\n",
       "      <td>-2.020</td>\n",
       "      <td>9.978</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-7.770</td>\n",
       "      <td>-2.611</td>\n",
       "      <td>3.347</td>\n",
       "      <td>-2.122</td>\n",
       "      <td>0.391</td>\n",
       "      <td>2.431</td>\n",
       "      <td>3.119</td>\n",
       "      <td>9.298</td>\n",
       "      <td>3.715</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>-3.142</td>\n",
       "      <td>-7.131</td>\n",
       "      <td>-5.277</td>\n",
       "      <td>0.561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "521  1373158606         0.000       -37.792         0.000         0.000   \n",
       "737  1373158606         9.300        90.715         0.000         0.000   \n",
       "740  1373158606         0.000       -40.002         0.000        48.856   \n",
       "660  1373158606       -38.064       -40.577         0.000         0.000   \n",
       "411  1373158606         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "521         0.000       -88.373        85.823         0.000        99.935   \n",
       "737       -57.684         0.000         0.000         0.000         0.000   \n",
       "740        49.219         0.000         0.000         0.000         0.000   \n",
       "660         0.000         0.000        59.750         0.000         0.000   \n",
       "411        61.796         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "521         0.000         0.000         0.000         0.000         0.000   \n",
       "737         0.000         0.000         0.000         0.000         0.000   \n",
       "740        61.743         0.000         0.000         0.000         0.000   \n",
       "660         0.000         0.000       -38.149         0.000         0.000   \n",
       "411        65.432         0.000         0.000         0.000         7.238   \n",
       "\n",
       "     02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "521         0.000         0.000         0.000         0.000        70.591   \n",
       "737         0.000         0.000       -18.513         0.000         0.000   \n",
       "740         0.000        60.947         0.000         0.000         0.000   \n",
       "660         0.000         0.000         0.000         0.000         0.000   \n",
       "411         0.000         0.000       -81.773         0.000         0.000   \n",
       "\n",
       "     11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "521         0.000         0.000             -21.692             -36.859   \n",
       "737         0.000       -94.709              13.069              87.111   \n",
       "740         0.000         0.000             -19.794             -14.986   \n",
       "660         0.000        -2.878             -29.078             -52.980   \n",
       "411       -60.609         0.000              -3.480               2.618   \n",
       "\n",
       "     00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "521               0.031              11.123               1.747   \n",
       "737               0.505              -6.054             -52.583   \n",
       "740             -14.649              72.039              29.846   \n",
       "660               6.571              -2.401               2.326   \n",
       "411              -4.274               5.856              59.859   \n",
       "\n",
       "     00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "521             -90.079             140.346               0.708   \n",
       "737               2.421              -0.372              -0.078   \n",
       "740             -13.490              60.752               1.258   \n",
       "660              -0.062              55.385               1.399   \n",
       "411              -7.139              11.461               1.478   \n",
       "\n",
       "     00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "521              87.090             -34.835               1.636   \n",
       "737               0.665              -0.511              -1.857   \n",
       "740              -1.583               0.336              -0.523   \n",
       "660               0.284               0.045             -13.896   \n",
       "411              -0.316              54.381               4.088   \n",
       "\n",
       "     01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "521              -2.111              -0.799              -1.694   \n",
       "737              -0.061               1.719              -0.864   \n",
       "740              -1.978               2.250              -0.185   \n",
       "660             -30.862               1.907               6.109   \n",
       "411               0.798               2.439               5.551   \n",
       "\n",
       "     02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "521              -0.223              35.268              -1.264   \n",
       "737               1.127              -5.929             -16.022   \n",
       "740              -0.931              61.373              -0.241   \n",
       "660               3.852              -4.437               0.693   \n",
       "411              -7.221               1.877             -80.060   \n",
       "\n",
       "     10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "521              -6.964              43.343               2.492   \n",
       "737               0.200               1.065               0.819   \n",
       "740              -0.006              -0.901               1.410   \n",
       "660              -0.405              -0.127               3.965   \n",
       "411               1.346              -0.322             -56.111   \n",
       "\n",
       "     20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "521             -18.880               1.921             -39.809   \n",
       "737             -90.624               9.325              89.433   \n",
       "740              -0.513              -0.811             -36.884   \n",
       "660              -0.408             -37.145             -39.728   \n",
       "411              -5.406               0.108              -0.047   \n",
       "\n",
       "     00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "521              -0.451              -3.451               2.172   \n",
       "737               0.972               0.351             -57.533   \n",
       "740              -1.309              48.240              49.353   \n",
       "660              -0.489              -2.009               0.228   \n",
       "411               0.210              -1.230              62.154   \n",
       "\n",
       "     00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "521             -87.013              84.307               0.846   \n",
       "737              -0.245              -0.442               1.165   \n",
       "740               2.635              -0.269              -0.969   \n",
       "660               1.028              59.601              -1.326   \n",
       "411               0.564               1.446              -0.395   \n",
       "\n",
       "     00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "521             102.337              -0.160              -1.036   \n",
       "737              -0.034               0.323              -0.389   \n",
       "740              -1.822              63.023               2.473   \n",
       "660               0.988              -0.783              -1.681   \n",
       "411               0.165              64.104              -1.233   \n",
       "\n",
       "     01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "521               2.540              -0.953              -0.745   \n",
       "737               0.075              -0.584              -0.626   \n",
       "740              -2.045              -0.231              -3.298   \n",
       "660             -37.934               0.765               1.646   \n",
       "411              -0.308               0.054               7.092   \n",
       "\n",
       "     02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "521               0.107               0.549              -0.898   \n",
       "737               0.730               1.499             -19.058   \n",
       "740              -0.455              58.702              -0.886   \n",
       "660              -0.287               0.286              -0.018   \n",
       "411               1.859               0.453             -82.342   \n",
       "\n",
       "     10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "521               0.735              70.790               1.224   \n",
       "737              -0.165              -0.699               0.402   \n",
       "740              -0.327               1.722               0.684   \n",
       "660              -0.763               0.464               0.823   \n",
       "411               0.616               0.192             -61.484   \n",
       "\n",
       "     20000-lstsq_target   wb_0   wb_1  wb_2   wb_3   wb_4   wb_5  wb_6   wb_7  \\\n",
       "521              -1.197  0.651  1.099 0.622  2.118  2.372  2.276 2.357  0.543   \n",
       "737             -95.428  1.233 -0.121 4.912  1.347 -0.933 -0.885 1.887 -0.423   \n",
       "740               1.827 -0.170 -0.263 0.205 -0.248  2.165  2.064 2.245 -0.901   \n",
       "660              -3.393  0.321  0.034 0.670  0.503  0.061  0.379 0.559 -0.267   \n",
       "411              -0.023  2.131 -2.220 2.400  2.378 -1.788 -2.409 2.753  1.816   \n",
       "\n",
       "      wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "521  0.490 1.958  1.289 -0.213  0.093  0.503  2.315  2.411  2.147  2.500   \n",
       "737  3.743 1.726  1.790  1.190 -0.044  4.723 -0.941  1.648 -0.962 -0.875   \n",
       "740 -0.621 2.442  0.264 -0.215 -0.833  0.169  2.117 -0.052  1.885  2.288   \n",
       "660  0.398 0.689  0.783  0.261  0.274  0.603  0.935  0.663 -0.041 -0.957   \n",
       "411  2.320 2.631  2.441 -0.218  1.830  2.435 -2.990  2.518 -0.042 -1.837   \n",
       "\n",
       "     wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "521  2.419  2.476  2.318  0.643  2.320 -3.446  0.105  1.074  0.221  1.247   \n",
       "737 -0.812 -0.923 -0.609  2.410 -1.243  1.364 -0.498  0.661 -0.050 -0.132   \n",
       "740  2.215  2.270  2.107 -0.465  2.104 -0.226 -1.279  0.062 -0.016 -0.337   \n",
       "660  1.002  0.096 -0.232  0.308  0.358  0.245 -0.086  0.719  0.818  0.621   \n",
       "411 -2.548  3.254 -3.602  2.136  2.570  3.000 -0.069  0.361 -1.069  0.351   \n",
       "\n",
       "     wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "521  0.086  0.603  1.014  1.034  1.272  1.161 -0.145  0.176 -0.416 -0.170   \n",
       "737  0.645  0.399  0.870  0.852  0.064  0.203  0.733  0.630  0.630 -0.125   \n",
       "740 -0.046  0.532  0.973  0.786  1.563  1.311  0.804 -0.444 -0.416  0.742   \n",
       "660  0.596 -0.350  0.847  1.178  1.373  1.065  0.854  0.527  0.481  0.731   \n",
       "411  0.367 -1.482  0.615  1.506  0.602  0.996  0.534 -1.062 -0.423  0.759   \n",
       "\n",
       "     wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "521 -0.295  0.546 -0.231  1.057  1.250  0.583  1.200  1.034  1.205  1.325   \n",
       "737 -0.070  0.388  0.779  0.906  1.075  0.432  1.056 -0.271  0.007  0.469   \n",
       "740 -0.294  0.491  0.175  1.018  1.189  0.541  1.145  0.974  0.980  1.216   \n",
       "660  0.601  0.254  0.928  0.139  2.341  0.352  3.097  1.982  0.959  1.292   \n",
       "411  0.612  0.181  0.550  0.143  1.035 -0.154 -1.614  1.607  0.581  0.927   \n",
       "\n",
       "     wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "521  0.107  0.999 -2.913 -2.405 -3.210 -3.442  3.010  3.763  3.911 -2.986   \n",
       "737  0.812 -0.494  0.671 -0.004 -0.001  0.673  0.347  1.090  0.838 -0.521   \n",
       "740  0.100  1.643 -0.054 -0.041 -0.458 -0.051  1.804  2.521  2.644 -0.968   \n",
       "660  1.092  0.469 -1.378 -1.192 -1.783 -1.405 -0.440  2.393 -0.928 -1.818   \n",
       "411  2.084 -1.331 -0.920  0.233 -0.990 -0.932  1.526  2.349 -0.833 -1.355   \n",
       "\n",
       "     wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "521 -2.461 -3.457  1.316  0.209  2.478  1.672  3.681 -3.350  3.000  3.863   \n",
       "737 -0.195  0.685  0.872  0.847  0.173  0.209  0.982  0.696  0.427  1.115   \n",
       "740 -0.716  2.105  0.014  0.180 -0.225  0.029  2.448  0.001  1.738  2.601   \n",
       "660 -1.440 -1.635 -1.160 -0.858 -1.026 -1.256  2.650 -1.579 -0.373 -0.803   \n",
       "411 -0.477 -1.236  5.300  0.210 -0.377 -0.438  2.991 -0.974 -0.374  2.522   \n",
       "\n",
       "     wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "521  3.380  3.162  3.897 -3.084  3.547  2.731 -2.196  2.336  2.475  2.203   \n",
       "737  0.743  0.488  0.217 -0.085 -0.007  0.814 -0.045  0.942  0.212 -0.895   \n",
       "740  2.159  1.933  2.623 -0.874  2.265  0.109 -0.814  1.524  1.623 -0.461   \n",
       "660  2.352  0.033 -1.570 -1.943 -1.479 -0.904 -1.307  0.507  0.602  0.020   \n",
       "411  2.402 -3.056  3.038 -1.335 -0.440 -0.353  0.169 -0.257  2.175 -0.696   \n",
       "\n",
       "     wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "521  3.378  0.589  0.001 -0.048  2.399  1.987  3.188 -4.636  0.101 -5.106   \n",
       "737  1.077  0.555  0.068  0.123  0.038  0.452  1.027  0.350  1.038 -0.481   \n",
       "740  1.947  2.822  2.251  2.675 -1.490 -1.772  2.862 -0.272  0.098 -2.872   \n",
       "660  0.655  0.220  0.792  0.414  0.520  0.138  0.559  0.147  0.570 -0.169   \n",
       "411 -0.166  2.825  1.671 -0.255 -0.435 -1.287 -0.366 -1.185  0.113 -1.192   \n",
       "\n",
       "     wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "521 -5.702  0.365  2.999 -0.227 -0.050  0.176  0.055 -0.218  2.407 -0.227   \n",
       "737 -0.879  0.402  0.804 -0.130 -0.020  0.175  0.011 -4.110  2.375 -2.036   \n",
       "740 -0.212  2.649  0.967  1.932  2.257  2.416  2.320  2.115 -1.321  2.177   \n",
       "660  0.120  1.080  0.382 -0.466 -0.894  1.008 -0.187 -0.606  0.381 -0.120   \n",
       "411 -1.318  1.525 -0.503 -0.455  1.799  1.819 -0.827  0.228 -0.566 -1.556   \n",
       "\n",
       "     wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "521  0.563  1.651   1.423   0.629   1.825   0.235   0.260  -0.046  -0.534   \n",
       "737  0.840 -0.351  -0.845  -0.062  -0.531  -0.919   2.483   2.100  -0.622   \n",
       "740  0.153 -2.725  -1.663  -1.707  -0.022  -2.019   0.408   0.144  -0.419   \n",
       "660  0.616 -0.001   1.045   1.330   1.515   0.905  -0.055  -2.049   1.046   \n",
       "411 -0.201 -2.280  -0.154  -2.860   0.275  -0.206   2.148   1.641   0.323   \n",
       "\n",
       "     wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "521   1.775   1.394   0.484   0.055  -0.519   0.234   0.176   0.339  -0.312   \n",
       "737   0.184   0.253  -0.957  -0.868  -0.945   0.015  -0.576   2.498  -0.825   \n",
       "740   2.106   1.626  -0.017  -0.524  -0.530   2.252  -0.159   0.517  -1.093   \n",
       "660   1.679   1.267   1.152   0.983   0.825   1.424   1.315  -0.812   1.004   \n",
       "411   0.269   1.970  -0.041  -2.610  -0.525   1.708   1.736   2.060   0.056   \n",
       "\n",
       "     wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "521   0.241   0.516   0.108   0.175   0.313   1.706   0.238  -0.008   1.762   \n",
       "737   2.328   2.709   2.323   2.434   4.058   2.689   2.886  -0.570   0.286   \n",
       "740   0.437   0.708   0.287   0.352   0.517   1.701   0.434  -0.356   2.704   \n",
       "660   0.002   2.293  -0.872  -3.134   1.825   1.707   1.663   1.049   1.810   \n",
       "411  -0.002   2.259   1.859  -1.966   1.857   0.498   2.246   0.460   1.909   \n",
       "\n",
       "     wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "521   1.454   0.574   1.803  -0.799   1.372   1.312   1.223   1.578   1.287   \n",
       "737   0.381  -0.175  -2.431   0.420   1.437   1.378  -0.251  -0.182  -2.036   \n",
       "740   0.148   0.240  -0.080   0.332   0.644   0.617   0.300   2.099   1.833   \n",
       "660   1.082   0.972   1.071   1.076  -0.173  -0.180   1.047   0.712   0.997   \n",
       "411   1.089   1.942   1.090   1.184  -1.878   0.484  -2.676   1.018  -0.711   \n",
       "\n",
       "     wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "521   0.479   1.309  -0.175   1.641   0.636   1.351  -1.819   1.367   1.406   \n",
       "737   0.392   0.212   0.438  -0.133  -0.806   1.395   0.265   1.408   1.453   \n",
       "740   0.770  -0.140  -0.185   1.664  -0.110   0.636  -0.035   0.690   0.687   \n",
       "660   1.232   1.137   1.023   1.010   1.096   0.121   0.991  -0.085  -0.801   \n",
       "411   1.237  -1.433  -0.170  -2.308  -0.204  -0.536   0.928  -0.087   0.960   \n",
       "\n",
       "     wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "521   1.316   1.391   1.430   1.519   1.465   0.023   1.386  -3.925  -3.374   \n",
       "737   1.424   1.494   0.304  -3.760   0.732   0.071  -0.134  -2.106  -0.275   \n",
       "740   0.619   0.679   0.715   1.782   0.694  -0.261   2.468  -3.278  -3.374   \n",
       "660   0.194  -0.052   0.214   0.878   1.165   0.979   1.088  -1.737  -1.625   \n",
       "411   0.130   2.757  -0.311   0.945  -0.702  -2.305   0.742  -2.241  -6.784   \n",
       "\n",
       "     wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "521  -3.942  -5.659   2.646   2.840   2.764  -3.930  -3.436  -3.756  -5.870   \n",
       "737  -9.760  -2.028   2.076   1.995  -1.502  -0.331  -7.473  -1.654  -1.695   \n",
       "740  -0.262  -3.701   2.097   2.041   1.968  -3.125  -2.889   1.970  -0.061   \n",
       "660  -2.115  -1.719   0.122   2.322  -1.093  -2.154  -1.747  -1.504  -1.503   \n",
       "411  -2.313  -2.363   4.599   2.685  -6.205  -2.120  -3.311  -2.020   9.978   \n",
       "\n",
       "     wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "521  -0.265  -6.642  -6.342   2.706  -6.592   3.012   2.491   2.883   2.593   \n",
       "737  -2.157  -0.312  -6.867   2.002  -1.781   2.513   1.806   2.015   1.935   \n",
       "740  -0.262  -3.326  -0.091   1.986  -1.758   2.231   1.716   2.138   1.896   \n",
       "660  -1.468  -1.746  -1.545   2.135  -1.624   0.391  -2.661   2.306   5.600   \n",
       "411  -0.276  -7.770  -2.611   3.347  -2.122   0.391   2.431   3.119   9.298   \n",
       "\n",
       "     wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "521   2.676  -3.860   2.642  -5.325  -3.352   1.033  \n",
       "737   5.147  -9.377   2.255  -1.669  -0.332   0.931  \n",
       "740   1.818  -2.641   1.793  -0.163  -3.974   0.531  \n",
       "660  -1.982  -2.012  -1.543  -1.234  -2.085  -0.759  \n",
       "411   3.715  -2.100  -3.142  -7.131  -5.277   0.561  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.097</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>1.204</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.924</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.648</td>\n",
       "      <td>-2.696</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>1.271</td>\n",
       "      <td>-3.995</td>\n",
       "      <td>7.096</td>\n",
       "      <td>-5.420</td>\n",
       "      <td>6.125</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-5.675</td>\n",
       "      <td>7.692</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.824</td>\n",
       "      <td>-5.631</td>\n",
       "      <td>5.371</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-5.004</td>\n",
       "      <td>5.781</td>\n",
       "      <td>0.318</td>\n",
       "      <td>-2.257</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>-4.737</td>\n",
       "      <td>1.106</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>1.155</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>1.314</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.951</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-2.649</td>\n",
       "      <td>-0.935</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>1.283</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.627</td>\n",
       "      <td>1.025</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.612</td>\n",
       "      <td>1.449</td>\n",
       "      <td>1.313</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.656</td>\n",
       "      <td>1.077</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1.267</td>\n",
       "      <td>1.181</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.914</td>\n",
       "      <td>1.142</td>\n",
       "      <td>0.763</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.706</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.593</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.233</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1.355</td>\n",
       "      <td>1.011</td>\n",
       "      <td>0.257</td>\n",
       "      <td>1.472</td>\n",
       "      <td>0.764</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>1.338</td>\n",
       "      <td>1.267</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.706</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1.097</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1.194</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.407</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.828</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>1.126</td>\n",
       "      <td>1.397</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.367</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.795</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>1.033</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.789</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.490</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.967</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.427</td>\n",
       "      <td>1.066</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>1.179</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.581</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-2.161</td>\n",
       "      <td>-2.675</td>\n",
       "      <td>-2.043</td>\n",
       "      <td>-2.629</td>\n",
       "      <td>2.867</td>\n",
       "      <td>2.824</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.574</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.976</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>-2.511</td>\n",
       "      <td>-2.214</td>\n",
       "      <td>2.476</td>\n",
       "      <td>-2.523</td>\n",
       "      <td>2.478</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>2.892</td>\n",
       "      <td>2.143</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-2.796</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-2.296</td>\n",
       "      <td>-2.549</td>\n",
       "      <td>0.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>26.624</td>\n",
       "      <td>26.772</td>\n",
       "      <td>29.241</td>\n",
       "      <td>28.648</td>\n",
       "      <td>28.618</td>\n",
       "      <td>27.238</td>\n",
       "      <td>28.292</td>\n",
       "      <td>25.811</td>\n",
       "      <td>29.951</td>\n",
       "      <td>27.897</td>\n",
       "      <td>26.075</td>\n",
       "      <td>27.995</td>\n",
       "      <td>27.721</td>\n",
       "      <td>27.461</td>\n",
       "      <td>29.148</td>\n",
       "      <td>26.615</td>\n",
       "      <td>28.066</td>\n",
       "      <td>30.084</td>\n",
       "      <td>27.029</td>\n",
       "      <td>28.314</td>\n",
       "      <td>30.021</td>\n",
       "      <td>27.721</td>\n",
       "      <td>36.664</td>\n",
       "      <td>21.535</td>\n",
       "      <td>38.502</td>\n",
       "      <td>23.671</td>\n",
       "      <td>21.091</td>\n",
       "      <td>40.097</td>\n",
       "      <td>20.417</td>\n",
       "      <td>23.656</td>\n",
       "      <td>20.627</td>\n",
       "      <td>36.492</td>\n",
       "      <td>22.149</td>\n",
       "      <td>22.467</td>\n",
       "      <td>21.710</td>\n",
       "      <td>23.355</td>\n",
       "      <td>35.595</td>\n",
       "      <td>23.067</td>\n",
       "      <td>24.464</td>\n",
       "      <td>21.641</td>\n",
       "      <td>22.862</td>\n",
       "      <td>24.027</td>\n",
       "      <td>26.676</td>\n",
       "      <td>26.751</td>\n",
       "      <td>29.303</td>\n",
       "      <td>28.696</td>\n",
       "      <td>28.608</td>\n",
       "      <td>27.320</td>\n",
       "      <td>28.395</td>\n",
       "      <td>25.818</td>\n",
       "      <td>29.988</td>\n",
       "      <td>27.922</td>\n",
       "      <td>26.253</td>\n",
       "      <td>28.020</td>\n",
       "      <td>27.702</td>\n",
       "      <td>27.521</td>\n",
       "      <td>29.170</td>\n",
       "      <td>26.687</td>\n",
       "      <td>28.083</td>\n",
       "      <td>30.064</td>\n",
       "      <td>27.098</td>\n",
       "      <td>28.383</td>\n",
       "      <td>30.081</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.194</td>\n",
       "      <td>1.015</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.272</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.281</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.179</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.109</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.372</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.261</td>\n",
       "      <td>1.251</td>\n",
       "      <td>1.215</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.126</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.152</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.119</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.237</td>\n",
       "      <td>1.551</td>\n",
       "      <td>1.255</td>\n",
       "      <td>1.251</td>\n",
       "      <td>1.076</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.254</td>\n",
       "      <td>1.147</td>\n",
       "      <td>1.147</td>\n",
       "      <td>1.154</td>\n",
       "      <td>1.480</td>\n",
       "      <td>1.149</td>\n",
       "      <td>1.106</td>\n",
       "      <td>1.260</td>\n",
       "      <td>1.399</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.306</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.139</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.135</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.497</td>\n",
       "      <td>1.189</td>\n",
       "      <td>1.249</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.092</td>\n",
       "      <td>1.175</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.977</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.169</td>\n",
       "      <td>1.179</td>\n",
       "      <td>1.240</td>\n",
       "      <td>1.278</td>\n",
       "      <td>1.609</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.228</td>\n",
       "      <td>1.135</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1.160</td>\n",
       "      <td>0.938</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.256</td>\n",
       "      <td>1.531</td>\n",
       "      <td>1.329</td>\n",
       "      <td>1.092</td>\n",
       "      <td>1.172</td>\n",
       "      <td>1.111</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.201</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.239</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.277</td>\n",
       "      <td>1.334</td>\n",
       "      <td>1.497</td>\n",
       "      <td>1.727</td>\n",
       "      <td>1.418</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.233</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.177</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.966</td>\n",
       "      <td>1.146</td>\n",
       "      <td>1.489</td>\n",
       "      <td>1.551</td>\n",
       "      <td>1.281</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.249</td>\n",
       "      <td>1.313</td>\n",
       "      <td>1.083</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.089</td>\n",
       "      <td>1.268</td>\n",
       "      <td>1.220</td>\n",
       "      <td>1.168</td>\n",
       "      <td>1.321</td>\n",
       "      <td>1.502</td>\n",
       "      <td>1.552</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.053</td>\n",
       "      <td>1.130</td>\n",
       "      <td>1.215</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.038</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.137</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.108</td>\n",
       "      <td>1.361</td>\n",
       "      <td>1.146</td>\n",
       "      <td>1.067</td>\n",
       "      <td>1.091</td>\n",
       "      <td>1.185</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.057</td>\n",
       "      <td>1.059</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.147</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.719</td>\n",
       "      <td>1.106</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.554</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.166</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.079</td>\n",
       "      <td>1.496</td>\n",
       "      <td>1.975</td>\n",
       "      <td>1.645</td>\n",
       "      <td>1.939</td>\n",
       "      <td>2.088</td>\n",
       "      <td>2.081</td>\n",
       "      <td>3.150</td>\n",
       "      <td>2.088</td>\n",
       "      <td>1.825</td>\n",
       "      <td>2.252</td>\n",
       "      <td>2.908</td>\n",
       "      <td>1.798</td>\n",
       "      <td>1.812</td>\n",
       "      <td>1.629</td>\n",
       "      <td>1.960</td>\n",
       "      <td>1.850</td>\n",
       "      <td>1.848</td>\n",
       "      <td>3.314</td>\n",
       "      <td>2.036</td>\n",
       "      <td>3.260</td>\n",
       "      <td>3.276</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.411</td>\n",
       "      <td>2.008</td>\n",
       "      <td>1.874</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-99.955</td>\n",
       "      <td>-99.450</td>\n",
       "      <td>-99.716</td>\n",
       "      <td>-98.507</td>\n",
       "      <td>-99.857</td>\n",
       "      <td>-98.289</td>\n",
       "      <td>-99.256</td>\n",
       "      <td>-97.010</td>\n",
       "      <td>-98.528</td>\n",
       "      <td>-98.896</td>\n",
       "      <td>-98.870</td>\n",
       "      <td>-99.125</td>\n",
       "      <td>-98.199</td>\n",
       "      <td>-99.324</td>\n",
       "      <td>-99.869</td>\n",
       "      <td>-96.368</td>\n",
       "      <td>-99.214</td>\n",
       "      <td>-99.800</td>\n",
       "      <td>-98.054</td>\n",
       "      <td>-99.496</td>\n",
       "      <td>-99.934</td>\n",
       "      <td>-90.085</td>\n",
       "      <td>-151.293</td>\n",
       "      <td>-98.536</td>\n",
       "      <td>-205.632</td>\n",
       "      <td>-98.519</td>\n",
       "      <td>-95.061</td>\n",
       "      <td>-193.402</td>\n",
       "      <td>-91.084</td>\n",
       "      <td>-94.382</td>\n",
       "      <td>-98.406</td>\n",
       "      <td>-161.362</td>\n",
       "      <td>-93.453</td>\n",
       "      <td>-94.613</td>\n",
       "      <td>-92.680</td>\n",
       "      <td>-100.801</td>\n",
       "      <td>-124.367</td>\n",
       "      <td>-96.039</td>\n",
       "      <td>-96.617</td>\n",
       "      <td>-92.367</td>\n",
       "      <td>-98.935</td>\n",
       "      <td>-96.976</td>\n",
       "      <td>-100.872</td>\n",
       "      <td>-99.148</td>\n",
       "      <td>-100.358</td>\n",
       "      <td>-100.360</td>\n",
       "      <td>-100.833</td>\n",
       "      <td>-99.837</td>\n",
       "      <td>-101.597</td>\n",
       "      <td>-96.530</td>\n",
       "      <td>-98.500</td>\n",
       "      <td>-98.499</td>\n",
       "      <td>-100.235</td>\n",
       "      <td>-99.597</td>\n",
       "      <td>-98.919</td>\n",
       "      <td>-101.169</td>\n",
       "      <td>-98.949</td>\n",
       "      <td>-96.445</td>\n",
       "      <td>-101.673</td>\n",
       "      <td>-99.894</td>\n",
       "      <td>-98.755</td>\n",
       "      <td>-101.505</td>\n",
       "      <td>-100.899</td>\n",
       "      <td>-3.518</td>\n",
       "      <td>-6.504</td>\n",
       "      <td>-1.344</td>\n",
       "      <td>-2.637</td>\n",
       "      <td>-6.251</td>\n",
       "      <td>-4.832</td>\n",
       "      <td>-3.889</td>\n",
       "      <td>-7.198</td>\n",
       "      <td>-4.645</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-1.604</td>\n",
       "      <td>-3.326</td>\n",
       "      <td>-5.583</td>\n",
       "      <td>-1.587</td>\n",
       "      <td>-5.620</td>\n",
       "      <td>-1.719</td>\n",
       "      <td>-4.558</td>\n",
       "      <td>-3.692</td>\n",
       "      <td>-3.572</td>\n",
       "      <td>-3.227</td>\n",
       "      <td>-4.571</td>\n",
       "      <td>-3.727</td>\n",
       "      <td>-2.613</td>\n",
       "      <td>-5.765</td>\n",
       "      <td>-7.011</td>\n",
       "      <td>-4.438</td>\n",
       "      <td>-5.594</td>\n",
       "      <td>-4.530</td>\n",
       "      <td>-6.032</td>\n",
       "      <td>-8.732</td>\n",
       "      <td>-4.346</td>\n",
       "      <td>-3.659</td>\n",
       "      <td>-2.015</td>\n",
       "      <td>-2.425</td>\n",
       "      <td>-3.061</td>\n",
       "      <td>-8.273</td>\n",
       "      <td>-4.629</td>\n",
       "      <td>-7.606</td>\n",
       "      <td>-6.289</td>\n",
       "      <td>-8.170</td>\n",
       "      <td>-7.071</td>\n",
       "      <td>-1.924</td>\n",
       "      <td>-3.135</td>\n",
       "      <td>-6.620</td>\n",
       "      <td>-4.102</td>\n",
       "      <td>-3.389</td>\n",
       "      <td>-4.967</td>\n",
       "      <td>-2.468</td>\n",
       "      <td>-2.157</td>\n",
       "      <td>-6.449</td>\n",
       "      <td>-4.090</td>\n",
       "      <td>-5.335</td>\n",
       "      <td>-3.811</td>\n",
       "      <td>-4.929</td>\n",
       "      <td>-6.892</td>\n",
       "      <td>-1.767</td>\n",
       "      <td>-4.356</td>\n",
       "      <td>-5.874</td>\n",
       "      <td>-5.147</td>\n",
       "      <td>-3.907</td>\n",
       "      <td>-3.977</td>\n",
       "      <td>-1.844</td>\n",
       "      <td>-1.618</td>\n",
       "      <td>-4.475</td>\n",
       "      <td>-2.491</td>\n",
       "      <td>-6.305</td>\n",
       "      <td>-4.367</td>\n",
       "      <td>-3.736</td>\n",
       "      <td>-4.636</td>\n",
       "      <td>-8.757</td>\n",
       "      <td>-1.926</td>\n",
       "      <td>-5.908</td>\n",
       "      <td>-3.057</td>\n",
       "      <td>-3.279</td>\n",
       "      <td>-5.217</td>\n",
       "      <td>-1.630</td>\n",
       "      <td>-1.515</td>\n",
       "      <td>-4.183</td>\n",
       "      <td>-1.490</td>\n",
       "      <td>-2.715</td>\n",
       "      <td>-7.370</td>\n",
       "      <td>-5.103</td>\n",
       "      <td>-4.758</td>\n",
       "      <td>-5.700</td>\n",
       "      <td>-1.820</td>\n",
       "      <td>-7.564</td>\n",
       "      <td>-2.942</td>\n",
       "      <td>-6.684</td>\n",
       "      <td>-5.702</td>\n",
       "      <td>-4.429</td>\n",
       "      <td>-2.779</td>\n",
       "      <td>-8.219</td>\n",
       "      <td>-5.034</td>\n",
       "      <td>-8.422</td>\n",
       "      <td>-8.540</td>\n",
       "      <td>-5.490</td>\n",
       "      <td>-6.169</td>\n",
       "      <td>-3.852</td>\n",
       "      <td>-2.660</td>\n",
       "      <td>-6.217</td>\n",
       "      <td>-6.013</td>\n",
       "      <td>-4.083</td>\n",
       "      <td>-4.022</td>\n",
       "      <td>-6.943</td>\n",
       "      <td>-5.526</td>\n",
       "      <td>-8.086</td>\n",
       "      <td>-7.498</td>\n",
       "      <td>-2.078</td>\n",
       "      <td>-6.670</td>\n",
       "      <td>-4.543</td>\n",
       "      <td>-8.606</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>-3.898</td>\n",
       "      <td>-5.315</td>\n",
       "      <td>-3.230</td>\n",
       "      <td>-7.118</td>\n",
       "      <td>-3.805</td>\n",
       "      <td>-4.021</td>\n",
       "      <td>-8.677</td>\n",
       "      <td>-6.715</td>\n",
       "      <td>-4.590</td>\n",
       "      <td>-1.679</td>\n",
       "      <td>-2.354</td>\n",
       "      <td>-5.678</td>\n",
       "      <td>-1.564</td>\n",
       "      <td>-3.298</td>\n",
       "      <td>-3.912</td>\n",
       "      <td>-3.470</td>\n",
       "      <td>-4.317</td>\n",
       "      <td>-4.107</td>\n",
       "      <td>-3.547</td>\n",
       "      <td>-5.047</td>\n",
       "      <td>-4.945</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>-2.972</td>\n",
       "      <td>-4.096</td>\n",
       "      <td>-3.077</td>\n",
       "      <td>-3.669</td>\n",
       "      <td>-3.160</td>\n",
       "      <td>-4.317</td>\n",
       "      <td>-3.942</td>\n",
       "      <td>-3.593</td>\n",
       "      <td>-4.427</td>\n",
       "      <td>-4.634</td>\n",
       "      <td>-3.860</td>\n",
       "      <td>-4.064</td>\n",
       "      <td>-4.482</td>\n",
       "      <td>-4.501</td>\n",
       "      <td>-5.043</td>\n",
       "      <td>-3.343</td>\n",
       "      <td>-9.910</td>\n",
       "      <td>-11.745</td>\n",
       "      <td>-10.332</td>\n",
       "      <td>-10.529</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-9.564</td>\n",
       "      <td>-14.079</td>\n",
       "      <td>-10.240</td>\n",
       "      <td>-7.790</td>\n",
       "      <td>-10.066</td>\n",
       "      <td>-11.880</td>\n",
       "      <td>-11.218</td>\n",
       "      <td>-9.606</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-11.142</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-11.289</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-13.601</td>\n",
       "      <td>-11.802</td>\n",
       "      <td>-12.069</td>\n",
       "      <td>-8.985</td>\n",
       "      <td>-10.145</td>\n",
       "      <td>-11.525</td>\n",
       "      <td>-1.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-16.053</td>\n",
       "      <td>-3.847</td>\n",
       "      <td>-8.150</td>\n",
       "      <td>-6.066</td>\n",
       "      <td>-1.471</td>\n",
       "      <td>-8.217</td>\n",
       "      <td>-4.765</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-1.225</td>\n",
       "      <td>-6.243</td>\n",
       "      <td>-5.052</td>\n",
       "      <td>-1.517</td>\n",
       "      <td>-1.433</td>\n",
       "      <td>-1.231</td>\n",
       "      <td>-7.923</td>\n",
       "      <td>-5.479</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>-1.134</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>-7.062</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>-1.125</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>-1.194</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.904</td>\n",
       "      <td>-1.193</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.883</td>\n",
       "      <td>-1.239</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.848</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>-1.316</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.885</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.570</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.312</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.082</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.478</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.403</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.262</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-1.036</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.465</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.308</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.610</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.815</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.569</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-2.818</td>\n",
       "      <td>-3.494</td>\n",
       "      <td>-2.898</td>\n",
       "      <td>-3.261</td>\n",
       "      <td>1.843</td>\n",
       "      <td>1.771</td>\n",
       "      <td>-1.676</td>\n",
       "      <td>-3.692</td>\n",
       "      <td>-3.136</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>-3.066</td>\n",
       "      <td>-3.162</td>\n",
       "      <td>-3.250</td>\n",
       "      <td>-2.832</td>\n",
       "      <td>1.584</td>\n",
       "      <td>-3.109</td>\n",
       "      <td>1.471</td>\n",
       "      <td>-2.025</td>\n",
       "      <td>1.897</td>\n",
       "      <td>1.702</td>\n",
       "      <td>-2.015</td>\n",
       "      <td>-3.877</td>\n",
       "      <td>-1.772</td>\n",
       "      <td>-2.850</td>\n",
       "      <td>-3.350</td>\n",
       "      <td>-0.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.920</td>\n",
       "      <td>1.156</td>\n",
       "      <td>-0.832</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>1.128</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.698</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.895</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.477</td>\n",
       "      <td>1.408</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.260</td>\n",
       "      <td>1.088</td>\n",
       "      <td>1.146</td>\n",
       "      <td>1.115</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.859</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.745</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.882</td>\n",
       "      <td>1.229</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.435</td>\n",
       "      <td>0.527</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.216</td>\n",
       "      <td>1.103</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.446</td>\n",
       "      <td>1.003</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>1.189</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.217</td>\n",
       "      <td>1.319</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.874</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.331</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.599</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.583</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.871</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1.224</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.574</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.948</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-2.064</td>\n",
       "      <td>-2.082</td>\n",
       "      <td>-2.020</td>\n",
       "      <td>-2.216</td>\n",
       "      <td>2.277</td>\n",
       "      <td>2.172</td>\n",
       "      <td>-1.174</td>\n",
       "      <td>-2.268</td>\n",
       "      <td>-2.103</td>\n",
       "      <td>-1.126</td>\n",
       "      <td>-2.024</td>\n",
       "      <td>-2.156</td>\n",
       "      <td>-2.159</td>\n",
       "      <td>-1.947</td>\n",
       "      <td>2.042</td>\n",
       "      <td>-2.056</td>\n",
       "      <td>2.366</td>\n",
       "      <td>1.339</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.095</td>\n",
       "      <td>1.609</td>\n",
       "      <td>-2.264</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>-1.739</td>\n",
       "      <td>-2.295</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.227</td>\n",
       "      <td>16.956</td>\n",
       "      <td>0.344</td>\n",
       "      <td>16.612</td>\n",
       "      <td>2.005</td>\n",
       "      <td>0.404</td>\n",
       "      <td>15.085</td>\n",
       "      <td>1.777</td>\n",
       "      <td>1.833</td>\n",
       "      <td>0.359</td>\n",
       "      <td>12.132</td>\n",
       "      <td>1.829</td>\n",
       "      <td>1.657</td>\n",
       "      <td>1.648</td>\n",
       "      <td>0.323</td>\n",
       "      <td>13.614</td>\n",
       "      <td>1.551</td>\n",
       "      <td>1.631</td>\n",
       "      <td>1.384</td>\n",
       "      <td>1.562</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.755</td>\n",
       "      <td>1.279</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.366</td>\n",
       "      <td>0.908</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.455</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.339</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.806</td>\n",
       "      <td>1.070</td>\n",
       "      <td>1.239</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.084</td>\n",
       "      <td>1.006</td>\n",
       "      <td>0.494</td>\n",
       "      <td>1.572</td>\n",
       "      <td>1.370</td>\n",
       "      <td>1.674</td>\n",
       "      <td>1.486</td>\n",
       "      <td>1.801</td>\n",
       "      <td>0.529</td>\n",
       "      <td>1.254</td>\n",
       "      <td>2.115</td>\n",
       "      <td>1.944</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1.584</td>\n",
       "      <td>1.481</td>\n",
       "      <td>1.704</td>\n",
       "      <td>1.214</td>\n",
       "      <td>1.953</td>\n",
       "      <td>2.005</td>\n",
       "      <td>2.017</td>\n",
       "      <td>1.775</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.632</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.731</td>\n",
       "      <td>1.967</td>\n",
       "      <td>1.463</td>\n",
       "      <td>1.697</td>\n",
       "      <td>1.542</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.870</td>\n",
       "      <td>1.333</td>\n",
       "      <td>1.755</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.876</td>\n",
       "      <td>2.203</td>\n",
       "      <td>1.870</td>\n",
       "      <td>1.007</td>\n",
       "      <td>2.241</td>\n",
       "      <td>1.431</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.859</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.881</td>\n",
       "      <td>2.118</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.696</td>\n",
       "      <td>1.288</td>\n",
       "      <td>1.439</td>\n",
       "      <td>1.668</td>\n",
       "      <td>1.541</td>\n",
       "      <td>1.311</td>\n",
       "      <td>1.810</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.739</td>\n",
       "      <td>2.089</td>\n",
       "      <td>1.428</td>\n",
       "      <td>0.956</td>\n",
       "      <td>2.243</td>\n",
       "      <td>0.152</td>\n",
       "      <td>1.554</td>\n",
       "      <td>1.400</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.427</td>\n",
       "      <td>1.639</td>\n",
       "      <td>0.296</td>\n",
       "      <td>1.785</td>\n",
       "      <td>2.274</td>\n",
       "      <td>1.252</td>\n",
       "      <td>1.809</td>\n",
       "      <td>1.311</td>\n",
       "      <td>0.713</td>\n",
       "      <td>2.153</td>\n",
       "      <td>0.738</td>\n",
       "      <td>1.405</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.641</td>\n",
       "      <td>1.916</td>\n",
       "      <td>1.198</td>\n",
       "      <td>0.821</td>\n",
       "      <td>1.493</td>\n",
       "      <td>1.557</td>\n",
       "      <td>1.318</td>\n",
       "      <td>1.271</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.117</td>\n",
       "      <td>1.396</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.929</td>\n",
       "      <td>1.512</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.281</td>\n",
       "      <td>1.203</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.840</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.544</td>\n",
       "      <td>2.128</td>\n",
       "      <td>1.429</td>\n",
       "      <td>1.446</td>\n",
       "      <td>1.936</td>\n",
       "      <td>1.764</td>\n",
       "      <td>1.806</td>\n",
       "      <td>0.401</td>\n",
       "      <td>1.812</td>\n",
       "      <td>1.198</td>\n",
       "      <td>0.628</td>\n",
       "      <td>1.199</td>\n",
       "      <td>1.121</td>\n",
       "      <td>1.371</td>\n",
       "      <td>1.219</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.897</td>\n",
       "      <td>1.402</td>\n",
       "      <td>1.299</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.115</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.865</td>\n",
       "      <td>1.240</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.350</td>\n",
       "      <td>1.351</td>\n",
       "      <td>1.257</td>\n",
       "      <td>0.941</td>\n",
       "      <td>1.243</td>\n",
       "      <td>0.475</td>\n",
       "      <td>1.201</td>\n",
       "      <td>-1.436</td>\n",
       "      <td>-1.550</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-1.577</td>\n",
       "      <td>3.176</td>\n",
       "      <td>2.999</td>\n",
       "      <td>2.015</td>\n",
       "      <td>-1.539</td>\n",
       "      <td>-1.580</td>\n",
       "      <td>1.937</td>\n",
       "      <td>-1.438</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-1.556</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>2.650</td>\n",
       "      <td>-1.495</td>\n",
       "      <td>2.910</td>\n",
       "      <td>1.851</td>\n",
       "      <td>3.083</td>\n",
       "      <td>2.906</td>\n",
       "      <td>2.062</td>\n",
       "      <td>-1.591</td>\n",
       "      <td>1.920</td>\n",
       "      <td>-1.268</td>\n",
       "      <td>-1.503</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>98.901</td>\n",
       "      <td>99.248</td>\n",
       "      <td>99.640</td>\n",
       "      <td>99.430</td>\n",
       "      <td>98.990</td>\n",
       "      <td>97.283</td>\n",
       "      <td>99.944</td>\n",
       "      <td>99.327</td>\n",
       "      <td>99.935</td>\n",
       "      <td>99.213</td>\n",
       "      <td>99.305</td>\n",
       "      <td>99.415</td>\n",
       "      <td>98.835</td>\n",
       "      <td>99.870</td>\n",
       "      <td>99.765</td>\n",
       "      <td>99.985</td>\n",
       "      <td>99.433</td>\n",
       "      <td>99.841</td>\n",
       "      <td>98.545</td>\n",
       "      <td>97.649</td>\n",
       "      <td>98.828</td>\n",
       "      <td>108.485</td>\n",
       "      <td>160.442</td>\n",
       "      <td>94.234</td>\n",
       "      <td>169.855</td>\n",
       "      <td>92.112</td>\n",
       "      <td>83.513</td>\n",
       "      <td>157.733</td>\n",
       "      <td>90.666</td>\n",
       "      <td>88.020</td>\n",
       "      <td>93.263</td>\n",
       "      <td>155.769</td>\n",
       "      <td>92.073</td>\n",
       "      <td>88.639</td>\n",
       "      <td>98.132</td>\n",
       "      <td>94.445</td>\n",
       "      <td>161.145</td>\n",
       "      <td>101.597</td>\n",
       "      <td>97.681</td>\n",
       "      <td>96.403</td>\n",
       "      <td>89.916</td>\n",
       "      <td>90.011</td>\n",
       "      <td>100.111</td>\n",
       "      <td>102.660</td>\n",
       "      <td>100.108</td>\n",
       "      <td>102.320</td>\n",
       "      <td>98.498</td>\n",
       "      <td>96.806</td>\n",
       "      <td>101.610</td>\n",
       "      <td>98.994</td>\n",
       "      <td>102.337</td>\n",
       "      <td>101.463</td>\n",
       "      <td>100.711</td>\n",
       "      <td>99.737</td>\n",
       "      <td>98.788</td>\n",
       "      <td>100.825</td>\n",
       "      <td>101.198</td>\n",
       "      <td>100.504</td>\n",
       "      <td>100.060</td>\n",
       "      <td>99.434</td>\n",
       "      <td>99.716</td>\n",
       "      <td>98.498</td>\n",
       "      <td>99.510</td>\n",
       "      <td>3.741</td>\n",
       "      <td>3.179</td>\n",
       "      <td>5.405</td>\n",
       "      <td>4.926</td>\n",
       "      <td>6.026</td>\n",
       "      <td>5.896</td>\n",
       "      <td>7.248</td>\n",
       "      <td>2.937</td>\n",
       "      <td>4.218</td>\n",
       "      <td>6.132</td>\n",
       "      <td>6.978</td>\n",
       "      <td>3.124</td>\n",
       "      <td>3.539</td>\n",
       "      <td>5.278</td>\n",
       "      <td>6.077</td>\n",
       "      <td>5.629</td>\n",
       "      <td>6.086</td>\n",
       "      <td>5.684</td>\n",
       "      <td>6.129</td>\n",
       "      <td>5.889</td>\n",
       "      <td>6.705</td>\n",
       "      <td>4.006</td>\n",
       "      <td>3.740</td>\n",
       "      <td>4.933</td>\n",
       "      <td>2.795</td>\n",
       "      <td>5.019</td>\n",
       "      <td>4.579</td>\n",
       "      <td>2.952</td>\n",
       "      <td>4.679</td>\n",
       "      <td>3.324</td>\n",
       "      <td>6.365</td>\n",
       "      <td>8.398</td>\n",
       "      <td>5.652</td>\n",
       "      <td>4.979</td>\n",
       "      <td>3.520</td>\n",
       "      <td>4.130</td>\n",
       "      <td>2.766</td>\n",
       "      <td>4.144</td>\n",
       "      <td>2.820</td>\n",
       "      <td>3.244</td>\n",
       "      <td>4.534</td>\n",
       "      <td>5.943</td>\n",
       "      <td>6.635</td>\n",
       "      <td>6.813</td>\n",
       "      <td>5.829</td>\n",
       "      <td>6.385</td>\n",
       "      <td>4.161</td>\n",
       "      <td>6.842</td>\n",
       "      <td>5.577</td>\n",
       "      <td>3.103</td>\n",
       "      <td>3.694</td>\n",
       "      <td>4.787</td>\n",
       "      <td>3.679</td>\n",
       "      <td>3.255</td>\n",
       "      <td>3.749</td>\n",
       "      <td>6.949</td>\n",
       "      <td>7.721</td>\n",
       "      <td>2.810</td>\n",
       "      <td>3.260</td>\n",
       "      <td>3.777</td>\n",
       "      <td>6.866</td>\n",
       "      <td>4.701</td>\n",
       "      <td>4.284</td>\n",
       "      <td>4.408</td>\n",
       "      <td>6.631</td>\n",
       "      <td>3.239</td>\n",
       "      <td>3.449</td>\n",
       "      <td>6.575</td>\n",
       "      <td>4.419</td>\n",
       "      <td>3.822</td>\n",
       "      <td>5.781</td>\n",
       "      <td>2.799</td>\n",
       "      <td>6.288</td>\n",
       "      <td>5.042</td>\n",
       "      <td>5.636</td>\n",
       "      <td>4.704</td>\n",
       "      <td>5.406</td>\n",
       "      <td>2.751</td>\n",
       "      <td>5.645</td>\n",
       "      <td>8.277</td>\n",
       "      <td>6.490</td>\n",
       "      <td>7.939</td>\n",
       "      <td>7.207</td>\n",
       "      <td>3.657</td>\n",
       "      <td>5.820</td>\n",
       "      <td>3.719</td>\n",
       "      <td>7.002</td>\n",
       "      <td>2.569</td>\n",
       "      <td>3.384</td>\n",
       "      <td>5.891</td>\n",
       "      <td>4.870</td>\n",
       "      <td>3.983</td>\n",
       "      <td>6.427</td>\n",
       "      <td>6.093</td>\n",
       "      <td>4.170</td>\n",
       "      <td>5.005</td>\n",
       "      <td>5.086</td>\n",
       "      <td>3.594</td>\n",
       "      <td>5.169</td>\n",
       "      <td>4.566</td>\n",
       "      <td>3.191</td>\n",
       "      <td>4.105</td>\n",
       "      <td>3.707</td>\n",
       "      <td>2.710</td>\n",
       "      <td>6.433</td>\n",
       "      <td>3.844</td>\n",
       "      <td>4.542</td>\n",
       "      <td>5.403</td>\n",
       "      <td>3.312</td>\n",
       "      <td>3.439</td>\n",
       "      <td>2.472</td>\n",
       "      <td>2.432</td>\n",
       "      <td>4.093</td>\n",
       "      <td>3.117</td>\n",
       "      <td>5.314</td>\n",
       "      <td>2.937</td>\n",
       "      <td>6.383</td>\n",
       "      <td>6.724</td>\n",
       "      <td>5.807</td>\n",
       "      <td>6.392</td>\n",
       "      <td>6.183</td>\n",
       "      <td>5.469</td>\n",
       "      <td>5.414</td>\n",
       "      <td>4.770</td>\n",
       "      <td>5.497</td>\n",
       "      <td>2.917</td>\n",
       "      <td>4.036</td>\n",
       "      <td>3.425</td>\n",
       "      <td>3.267</td>\n",
       "      <td>4.083</td>\n",
       "      <td>3.480</td>\n",
       "      <td>3.914</td>\n",
       "      <td>3.097</td>\n",
       "      <td>3.383</td>\n",
       "      <td>3.802</td>\n",
       "      <td>4.104</td>\n",
       "      <td>2.985</td>\n",
       "      <td>3.348</td>\n",
       "      <td>2.892</td>\n",
       "      <td>4.382</td>\n",
       "      <td>2.774</td>\n",
       "      <td>3.459</td>\n",
       "      <td>3.609</td>\n",
       "      <td>4.248</td>\n",
       "      <td>4.739</td>\n",
       "      <td>3.682</td>\n",
       "      <td>2.996</td>\n",
       "      <td>3.767</td>\n",
       "      <td>2.665</td>\n",
       "      <td>3.411</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>12.905</td>\n",
       "      <td>13.283</td>\n",
       "      <td>12.120</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>10.186</td>\n",
       "      <td>14.363</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>12.133</td>\n",
       "      <td>1.373</td>\n",
       "      <td>11.898</td>\n",
       "      <td>7.479</td>\n",
       "      <td>14.619</td>\n",
       "      <td>12.258</td>\n",
       "      <td>7.407</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>12.688</td>\n",
       "      <td>10.621</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>2.813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "count       1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean  1373158606.000         1.078         1.097         0.452         0.637   \n",
       "std            0.000        26.624        26.772        29.241        28.648   \n",
       "min   1373158606.000       -99.955       -99.450       -99.716       -98.507   \n",
       "25%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "50%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "75%   1373158606.000         0.000         0.000         0.000         0.000   \n",
       "max   1373158606.000        98.901        99.248        99.640        99.430   \n",
       "\n",
       "       00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean          0.161        -0.184         1.204        -0.595         1.313   \n",
       "std          28.618        27.238        28.292        25.811        29.951   \n",
       "min         -99.857       -98.289       -99.256       -97.010       -98.528   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          98.990        97.283        99.944        99.327        99.935   \n",
       "\n",
       "       00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean          0.891         0.362         0.924        -0.107         0.396   \n",
       "std          27.897        26.075        27.995        27.721        27.461   \n",
       "min         -98.896       -98.870       -99.125       -98.199       -99.324   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          99.213        99.305        99.415        98.835        99.870   \n",
       "\n",
       "       02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean         -0.301        -0.186         0.648        -2.696        -0.966   \n",
       "std          29.148        26.615        28.066        30.084        27.029   \n",
       "min         -99.869       -96.368       -99.214       -99.800       -98.054   \n",
       "25%           0.000         0.000         0.000         0.000         0.000   \n",
       "50%           0.000         0.000         0.000         0.000         0.000   \n",
       "75%           0.000         0.000         0.000         0.000         0.000   \n",
       "max          99.765        99.985        99.433        99.841        98.545   \n",
       "\n",
       "       11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "count      1000.000      1000.000            1000.000            1000.000   \n",
       "mean         -0.763         1.271              -3.995               7.096   \n",
       "std          28.314        30.021              27.721              36.664   \n",
       "min         -99.496       -99.934             -90.085            -151.293   \n",
       "25%           0.000         0.000             -16.053              -3.847   \n",
       "50%           0.000         0.000              -1.920               1.156   \n",
       "75%           0.000         0.000               3.227              16.956   \n",
       "max          97.649        98.828             108.485             160.442   \n",
       "\n",
       "       00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -5.420               6.125               0.092   \n",
       "std                21.535              38.502              23.671   \n",
       "min               -98.536            -205.632             -98.519   \n",
       "25%                -8.150              -6.066              -1.471   \n",
       "50%                -0.832               0.311               0.144   \n",
       "75%                 0.344              16.612               2.005   \n",
       "max                94.234             169.855              92.112   \n",
       "\n",
       "       00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -5.675               7.692               0.016   \n",
       "std                21.091              40.097              20.417   \n",
       "min               -95.061            -193.402             -91.084   \n",
       "25%                -8.217              -4.765              -0.879   \n",
       "50%                -0.857               0.816               0.299   \n",
       "75%                 0.404              15.085               1.777   \n",
       "max                83.513             157.733              90.666   \n",
       "\n",
       "       00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.824              -5.631               5.371   \n",
       "std                23.656              20.627              36.492   \n",
       "min               -94.382             -98.406            -161.362   \n",
       "25%                -1.225              -6.243              -5.052   \n",
       "50%                 0.125              -0.739               1.128   \n",
       "75%                 1.833               0.359              12.132   \n",
       "max                88.020              93.263             155.769   \n",
       "\n",
       "       01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.506              -0.001               0.182   \n",
       "std                22.149              22.467              21.710   \n",
       "min               -93.453             -94.613             -92.680   \n",
       "25%                -1.517              -1.433              -1.231   \n",
       "50%                 0.033               0.072               0.061   \n",
       "75%                 1.829               1.657               1.648   \n",
       "max                92.073              88.639              98.132   \n",
       "\n",
       "       02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -5.004               5.781               0.318   \n",
       "std                23.355              35.595              23.067   \n",
       "min              -100.801            -124.367             -96.039   \n",
       "25%                -7.923              -5.479              -1.150   \n",
       "50%                -0.698               0.687               0.066   \n",
       "75%                 0.323              13.614               1.551   \n",
       "max                94.445             161.145             101.597   \n",
       "\n",
       "       10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -2.257              -0.833              -0.917   \n",
       "std                24.464              21.641              22.862   \n",
       "min               -96.617             -92.367             -98.935   \n",
       "25%                -1.134              -1.343              -1.420   \n",
       "50%                 0.209               0.004               0.110   \n",
       "75%                 1.631               1.384               1.562   \n",
       "max                97.681              96.403              89.916   \n",
       "\n",
       "       20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -4.737               1.106               1.024   \n",
       "std                24.027              26.676              26.751   \n",
       "min               -96.976            -100.872             -99.148   \n",
       "25%                -7.062              -0.709              -1.125   \n",
       "50%                -0.908               0.030               0.016   \n",
       "75%                 0.295               0.755               1.279   \n",
       "max                90.011             100.111             102.660   \n",
       "\n",
       "       00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.519               0.687               0.099   \n",
       "std                29.303              28.696              28.608   \n",
       "min              -100.358            -100.360            -100.833   \n",
       "25%                -0.962              -1.194              -0.862   \n",
       "50%                 0.049              -0.022              -0.057   \n",
       "75%                 1.016               1.366               0.908   \n",
       "max               100.108             102.320              98.498   \n",
       "\n",
       "       00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -0.217               1.155              -0.513   \n",
       "std                27.320              28.395              25.818   \n",
       "min               -99.837            -101.597             -96.530   \n",
       "25%                -0.904              -1.193              -0.899   \n",
       "50%                 0.037               0.015              -0.016   \n",
       "75%                 1.010               1.455               0.880   \n",
       "max                96.806             101.610              98.994   \n",
       "\n",
       "       00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                1.314               0.890               0.280   \n",
       "std                29.988              27.922              26.253   \n",
       "min               -98.500             -98.499            -100.235   \n",
       "25%                -0.823              -0.883              -1.239   \n",
       "50%                 0.052               0.098               0.001   \n",
       "75%                 0.980               1.020               1.339   \n",
       "max               102.337             101.463             100.711   \n",
       "\n",
       "       01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                0.951              -0.118               0.381   \n",
       "std                28.020              27.702              27.521   \n",
       "min               -99.597             -98.919            -101.169   \n",
       "25%                -0.901              -0.955              -0.848   \n",
       "50%                 0.067              -0.042              -0.018   \n",
       "75%                 0.908               0.834               0.806   \n",
       "max                99.737              98.788             100.825   \n",
       "\n",
       "       02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -0.208              -0.207               0.620   \n",
       "std                29.170              26.687              28.083   \n",
       "min               -98.949             -96.445            -101.673   \n",
       "25%                -0.838              -1.316              -0.832   \n",
       "50%                 0.080               0.040              -0.004   \n",
       "75%                 1.070               1.239               0.757   \n",
       "max               101.198             100.504             100.060   \n",
       "\n",
       "       10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               -2.649              -0.935              -0.799   \n",
       "std                30.064              27.098              28.383   \n",
       "min               -99.894             -98.755            -101.505   \n",
       "25%                -0.941              -0.885              -0.920   \n",
       "50%                 0.036               0.007              -0.070   \n",
       "75%                 0.830               0.933               0.730   \n",
       "max                99.434              99.716              98.498   \n",
       "\n",
       "       20000-lstsq_target     wb_0     wb_1     wb_2     wb_3     wb_4  \\\n",
       "count            1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean                1.283    0.388   -0.157    0.983    0.704    0.905   \n",
       "std                30.081    1.001    1.194    1.015    1.046    1.272   \n",
       "min              -100.899   -3.518   -6.504   -1.344   -2.637   -6.251   \n",
       "25%                -0.936   -0.080   -0.527    0.198    0.018    0.053   \n",
       "50%                 0.075    0.205   -0.153    0.661    0.508    0.909   \n",
       "75%                 1.084    1.006    0.494    1.572    1.370    1.674   \n",
       "max                99.510    3.741    3.179    5.405    4.926    6.026   \n",
       "\n",
       "          wb_5     wb_6     wb_7     wb_8     wb_9    wb_10    wb_11    wb_12  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.627    1.025   -0.205    0.612    1.449    1.313    0.174    0.269   \n",
       "std      1.396    1.281    1.224    1.046    1.032    1.179    1.054    1.109   \n",
       "min     -4.832   -3.889   -7.198   -4.645   -2.028   -1.604   -3.326   -5.583   \n",
       "25%     -0.027    0.206   -0.502    0.007    0.736    0.313   -0.223   -0.127   \n",
       "50%      0.689    0.895   -0.231    0.477    1.408    1.062    0.009    0.098   \n",
       "75%      1.486    1.801    0.529    1.254    2.115    1.944    0.817    0.923   \n",
       "max      5.896    7.248    2.937    4.218    6.132    6.978    3.124    3.539   \n",
       "\n",
       "         wb_13    wb_14    wb_15    wb_16    wb_17    wb_18    wb_19    wb_20  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.986    0.656    1.077    0.459    1.085    1.267    1.181    0.895   \n",
       "std      1.006    1.372    1.039    1.261    1.251    1.215    1.284    1.330   \n",
       "min     -1.587   -5.620   -1.719   -4.558   -3.692   -3.572   -3.227   -4.571   \n",
       "25%      0.168    0.001    0.218   -0.046    0.219    0.295    0.211    0.115   \n",
       "50%      0.721    0.696    0.846    0.260    1.088    1.146    1.115    0.904   \n",
       "75%      1.584    1.481    1.704    1.214    1.953    2.005    2.017    1.775   \n",
       "max      5.278    6.077    5.629    6.086    5.684    6.129    5.889    6.705   \n",
       "\n",
       "         wb_21    wb_22    wb_23    wb_24    wb_25    wb_26    wb_27    wb_28  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.441    0.896    0.112   -0.173    0.215    0.327   -0.039   -0.031   \n",
       "std      1.126    1.101    1.191    1.152    1.068    1.119    1.003    1.237   \n",
       "min     -3.727   -2.613   -5.765   -7.011   -4.438   -5.594   -4.530   -6.032   \n",
       "25%     -0.066    0.181   -0.254   -0.505   -0.154   -0.096   -0.360   -0.344   \n",
       "50%      0.296    0.914    0.028   -0.280    0.026    0.173   -0.336   -0.090   \n",
       "75%      1.097    1.632    0.822    0.548    0.911    0.986    0.515    0.680   \n",
       "max      4.006    3.740    4.933    2.795    5.019    4.579    2.952    4.679   \n",
       "\n",
       "         wb_29    wb_30    wb_31    wb_32    wb_33    wb_34    wb_35    wb_36  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.032    0.914    1.142    0.763    1.004    0.706   -0.175   -0.205   \n",
       "std      1.551    1.255    1.251    1.076    1.065    1.133    1.254    1.147   \n",
       "min     -8.732   -4.346   -3.659   -2.015   -2.425   -3.061   -8.273   -4.629   \n",
       "25%     -0.554    0.036    0.395    0.056    0.182    0.033   -0.570   -0.469   \n",
       "50%      0.130    0.859    1.036    0.463    0.763    0.745   -0.157   -0.258   \n",
       "75%      0.958    1.731    1.967    1.463    1.697    1.542    0.561    0.476   \n",
       "max      3.324    6.365    8.398    5.652    4.979    3.520    4.130    2.766   \n",
       "\n",
       "         wb_37    wb_38    wb_39    wb_40    wb_41    wb_42    wb_43    wb_44  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.206   -0.061   -0.056    0.593    1.051    1.233    0.032    1.355   \n",
       "std      1.147    1.154    1.480    1.149    1.106    1.260    1.399    1.287   \n",
       "min     -7.606   -6.289   -8.170   -7.071   -1.924   -3.135   -6.620   -4.102   \n",
       "25%     -0.139   -0.449   -0.539   -0.009    0.143    0.312   -0.484    0.399   \n",
       "50%      0.061   -0.145    0.050    0.407    0.882    1.229    0.134    1.294   \n",
       "75%      0.839    0.626    0.870    1.333    1.755    2.078    0.876    2.203   \n",
       "max      4.144    2.820    3.244    4.534    5.943    6.635    6.813    5.829   \n",
       "\n",
       "         wb_45    wb_46    wb_47    wb_48    wb_49    wb_50    wb_51    wb_52  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     1.011    0.257    1.472    0.764   -0.216    0.215    0.425   -0.235   \n",
       "std      1.306    1.144    1.139    1.037    1.135    1.069    1.085    1.025   \n",
       "min     -3.389   -4.967   -2.468   -2.157   -6.449   -4.090   -5.335   -3.811   \n",
       "25%      0.132   -0.141    0.716    0.082   -0.513   -0.107   -0.014   -0.462   \n",
       "50%      0.996    0.143    1.435    0.527   -0.347    0.074    0.301   -0.440   \n",
       "75%      1.870    1.007    2.241    1.431    0.468    0.859    1.002    0.273   \n",
       "max      6.385    4.161    6.842    5.577    3.103    3.694    4.787    3.679   \n",
       "\n",
       "         wb_53    wb_54    wb_55    wb_56    wb_57    wb_58    wb_59    wb_60  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.035   -0.015    1.338    1.267   -0.486    0.053    0.502    0.706   \n",
       "std      1.202    1.497    1.189    1.249    1.234    1.092    1.175    1.183   \n",
       "min     -4.929   -6.892   -1.767   -4.356   -5.874   -5.147   -3.907   -3.977   \n",
       "25%     -0.222   -0.451    0.357    0.478   -0.833   -0.296   -0.122    0.013   \n",
       "50%      0.016    0.143    1.216    1.103   -0.438    0.016    0.544    0.518   \n",
       "75%      0.737    0.881    2.118    2.078    0.169    0.696    1.288    1.439   \n",
       "max      3.255    3.749    6.949    7.721    2.810    3.260    3.777    6.866   \n",
       "\n",
       "         wb_61    wb_62    wb_63    wb_64    wb_65    wb_66    wb_67    wb_68  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     1.032    0.919    0.652    1.097   -0.084    0.059    1.194    0.634   \n",
       "std      0.996    0.977    1.040    1.158    1.169    1.179    1.240    1.278   \n",
       "min     -1.844   -1.618   -4.475   -2.491   -6.305   -4.367   -3.736   -4.636   \n",
       "25%      0.203    0.174    0.019    0.116   -0.403   -0.379    0.262   -0.044   \n",
       "50%      0.791    0.633    0.446    1.003   -0.064   -0.050    1.189    0.676   \n",
       "75%      1.668    1.541    1.311    1.810    0.546    0.739    2.089    1.428   \n",
       "max      4.701    4.284    4.408    6.631    3.239    3.449    6.575    4.419   \n",
       "\n",
       "         wb_69    wb_70    wb_71    wb_72    wb_73    wb_74    wb_75    wb_76  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.008    1.407   -0.546    0.800    0.850    0.439    0.828    1.000   \n",
       "std      1.609    1.212    1.228    1.135    0.981    1.160    0.938    1.001   \n",
       "min     -8.757   -1.926   -5.908   -3.057   -3.279   -5.217   -1.630   -1.515   \n",
       "25%     -0.612    0.526   -1.036    0.093    0.130   -0.074    0.162    0.210   \n",
       "50%      0.217    1.319   -0.428    0.768    0.648    0.251    0.599    0.770   \n",
       "75%      0.956    2.243    0.152    1.554    1.400    1.096    1.427    1.639   \n",
       "max      3.822    5.781    2.799    6.288    5.042    5.636    4.704    5.406   \n",
       "\n",
       "         wb_77    wb_78    wb_79    wb_80    wb_81    wb_82    wb_83    wb_84  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.223    1.126    1.397    0.277    0.945    0.682    0.034    1.367   \n",
       "std      1.022    1.125    1.256    1.531    1.329    1.092    1.172    1.111   \n",
       "min     -4.183   -1.490   -2.715   -7.370   -5.103   -4.758   -5.700   -1.820   \n",
       "25%     -0.465    0.236    0.308   -0.288    0.104    0.036   -0.345    0.610   \n",
       "50%     -0.443    0.874    1.280    0.330    0.777    0.473    0.017    1.331   \n",
       "75%      0.296    1.785    2.274    1.252    1.809    1.311    0.713    2.153   \n",
       "max      2.751    5.645    8.277    6.490    7.939    7.207    3.657    5.820   \n",
       "\n",
       "         wb_85    wb_86    wb_87    wb_88    wb_89    wb_90    wb_91    wb_92  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.058    0.795   -0.471   -0.026    1.033    0.554    0.023    0.577   \n",
       "std      1.316    1.029    1.201    1.150    1.239    1.027    1.277    1.334   \n",
       "min     -7.564   -2.942   -6.684   -5.702   -4.429   -2.779   -8.219   -5.034   \n",
       "25%     -0.469    0.109   -0.770   -0.353    0.045   -0.002   -0.462   -0.235   \n",
       "50%     -0.001    0.599   -0.453   -0.072    0.934    0.409   -0.123    0.598   \n",
       "75%      0.738    1.405    0.185    0.641    1.916    1.198    0.821    1.493   \n",
       "max      3.719    7.002    2.569    3.384    5.891    4.870    3.983    6.427   \n",
       "\n",
       "         wb_93    wb_94    wb_95    wb_96    wb_97    wb_98    wb_99   wb_100  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.581    0.168    0.275    0.414    0.277    0.789   -0.085   -0.145   \n",
       "std      1.497    1.727    1.418    1.199    1.233    1.001    1.177    1.006   \n",
       "min     -8.422   -8.540   -5.490   -6.169   -3.852   -2.660   -6.217   -6.013   \n",
       "25%     -0.064   -0.566   -0.561   -0.042   -0.411    0.113   -0.373   -0.452   \n",
       "50%      0.632    0.395    0.304    0.312    0.249    0.583   -0.221   -0.242   \n",
       "75%      1.557    1.318    1.271    1.099    1.117    1.396    0.597    0.422   \n",
       "max      6.093    4.170    5.005    5.086    3.594    5.169    4.566    3.191   \n",
       "\n",
       "        wb_101   wb_102   wb_103   wb_104   wb_105   wb_106   wb_107   wb_108  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.344    0.490   -0.430    0.701    0.132    0.105    0.884    0.123   \n",
       "std      1.120    0.966    1.146    1.489    1.551    1.281    1.058    1.133   \n",
       "min     -4.083   -4.022   -6.943   -5.526   -8.086   -7.498   -2.078   -6.670   \n",
       "25%     -0.137   -0.026   -0.815   -0.059   -0.308   -0.586    0.166   -0.292   \n",
       "50%      0.193    0.170   -0.410    0.722    0.344    0.112    0.591    0.079   \n",
       "75%      0.920    1.025    0.242    1.693    1.117    0.929    1.512    0.776   \n",
       "max      4.105    3.707    2.710    6.433    3.844    4.542    5.403    3.312   \n",
       "\n",
       "        wb_109   wb_110   wb_111   wb_112   wb_113   wb_114   wb_115   wb_116  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.103   -0.403   -0.339    0.607    0.087    0.967   -0.294    0.784   \n",
       "std      1.249    1.313    1.083    0.975    1.089    1.268    1.220    1.168   \n",
       "min     -4.543   -8.606   -6.000   -3.898   -5.315   -3.230   -7.118   -3.805   \n",
       "25%     -0.569   -0.872   -0.733    0.023   -0.232    0.044   -0.753   -0.003   \n",
       "50%      0.239   -0.326   -0.411    0.404    0.003    0.871   -0.235    0.661   \n",
       "75%      0.906    0.394    0.281    1.203    0.710    1.840    0.381    1.544   \n",
       "max      3.439    2.472    2.432    4.093    3.117    5.314    2.937    6.383   \n",
       "\n",
       "        wb_117   wb_118   wb_119   wb_120   wb_121   wb_122   wb_123   wb_124  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     1.245    0.420    0.427    1.066    1.074    0.982   -0.194    1.179   \n",
       "std      1.321    1.502    1.552    1.299    1.053    1.130    1.215    1.068   \n",
       "min     -4.021   -8.677   -6.715   -4.590   -1.679   -2.354   -5.678   -1.564   \n",
       "25%      0.363   -0.100   -0.201    0.230    0.243    0.283   -0.683    0.290   \n",
       "50%      1.224    0.524    0.574    1.049    0.830    0.948   -0.278    0.888   \n",
       "75%      2.128    1.429    1.446    1.936    1.764    1.806    0.401    1.812   \n",
       "max      6.724    5.807    6.392    6.183    5.469    5.414    4.770    5.497   \n",
       "\n",
       "        wb_125   wb_126   wb_127   wb_128   wb_129   wb_130   wb_131   wb_132  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.431    0.043    0.442    0.308    0.736    0.610    0.177    0.099   \n",
       "std      1.054    1.038    1.039    1.137    1.144    1.108    1.361    1.146   \n",
       "min     -3.298   -3.912   -3.470   -4.317   -4.107   -3.547   -5.047   -4.945   \n",
       "25%     -0.089   -0.290   -0.094   -0.165   -0.010   -0.026   -0.244   -0.202   \n",
       "50%      0.286    0.037    0.090    0.248    0.864    0.746    0.301    0.029   \n",
       "75%      1.198    0.628    1.199    1.121    1.371    1.219    0.934    0.821   \n",
       "max      2.917    4.036    3.425    3.267    4.083    3.480    3.914    3.097   \n",
       "\n",
       "        wb_133   wb_134   wb_135   wb_136   wb_137   wb_138   wb_139   wb_140  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.227    0.763    0.438    0.275    0.261    0.362    0.514    0.131   \n",
       "std      1.067    1.091    1.185    1.051    1.057    1.059    1.207    1.147   \n",
       "min     -3.484   -2.972   -4.096   -3.077   -3.669   -3.160   -4.317   -3.942   \n",
       "25%     -0.141    0.224   -0.156   -0.183   -0.137   -0.121   -0.166   -0.201   \n",
       "50%      0.165    0.849    0.373    0.192    0.123    0.220    0.681    0.132   \n",
       "75%      0.897    1.402    1.299    0.943    0.904    1.115    1.200    0.865   \n",
       "max      3.383    3.802    4.104    2.985    3.348    2.892    4.382    2.774   \n",
       "\n",
       "        wb_141   wb_142   wb_143   wb_144   wb_145   wb_146   wb_147   wb_148  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.654    0.104    0.754    0.700    0.337    0.169    0.581   -0.100   \n",
       "std      1.036    1.719    1.106    1.199    1.554    1.162    1.166    1.068   \n",
       "min     -3.593   -4.427   -4.634   -3.860   -4.064   -4.482   -4.501   -5.043   \n",
       "25%     -0.085   -0.923    0.142    0.141   -0.285   -0.194    0.066   -0.369   \n",
       "50%      0.712    0.646    0.826    0.848    0.744    0.149    0.638   -0.064   \n",
       "75%      1.240    1.190    1.350    1.351    1.257    0.941    1.243    0.475   \n",
       "max      3.459    3.609    4.248    4.739    3.682    2.996    3.767    2.665   \n",
       "\n",
       "        wb_149   wb_150   wb_151   wb_152   wb_153   wb_154   wb_155   wb_156  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.421   -2.161   -2.675   -2.043   -2.629    2.867    2.824    0.089   \n",
       "std      1.079    1.496    1.975    1.645    1.939    2.088    2.081    3.150   \n",
       "min     -3.343   -9.910  -11.745  -10.332  -10.529    0.096    0.118   -9.564   \n",
       "25%     -0.143   -2.818   -3.494   -2.898   -3.261    1.843    1.771   -1.676   \n",
       "50%      0.222   -2.064   -2.082   -2.020   -2.216    2.277    2.172   -1.174   \n",
       "75%      1.201   -1.436   -1.550   -0.268   -1.577    3.176    2.999    2.015   \n",
       "max      3.411   -0.185   -0.154   -0.224   -0.077   12.905   13.283   12.120   \n",
       "\n",
       "        wb_157   wb_158   wb_159   wb_160   wb_161   wb_162   wb_163   wb_164  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -2.767   -2.574   -0.081   -1.976   -2.476   -2.511   -2.214    2.476   \n",
       "std      2.088    1.825    2.252    2.908    1.798    1.812    1.629    1.960   \n",
       "min    -14.079  -10.240   -7.790  -10.066  -11.880  -11.218   -9.606    0.025   \n",
       "25%     -3.692   -3.136   -1.812   -3.066   -3.162   -3.250   -2.832    1.584   \n",
       "50%     -2.268   -2.103   -1.126   -2.024   -2.156   -2.159   -1.947    2.042   \n",
       "75%     -1.539   -1.580    1.937   -1.438   -1.526   -1.556   -1.400    2.650   \n",
       "max     -0.285   -0.256   10.186   14.363   -0.212   -0.230   -0.034   12.133   \n",
       "\n",
       "        wb_165   wb_166   wb_167   wb_168   wb_169   wb_170   wb_171   wb_172  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -2.523    2.478   -0.507    2.892    2.143   -0.130   -2.796   -0.114   \n",
       "std      1.850    1.848    3.314    2.036    3.260    3.276    1.943    2.411   \n",
       "min    -11.142    0.376  -11.289    0.183  -13.601  -11.802  -12.069   -8.985   \n",
       "25%     -3.109    1.471   -2.025    1.897    1.702   -2.015   -3.877   -1.772   \n",
       "50%     -2.056    2.366    1.339    2.316    2.095    1.609   -2.264   -1.129   \n",
       "75%     -1.495    2.910    1.851    3.083    2.906    2.062   -1.591    1.920   \n",
       "max      1.373   11.898    7.479   14.619   12.258    7.407   -0.237   12.688   \n",
       "\n",
       "        wb_173   wb_174   wb_175  \n",
       "count 1000.000 1000.000 1000.000  \n",
       "mean    -2.296   -2.549    0.319  \n",
       "std      2.008    1.874    0.745  \n",
       "min    -10.145  -11.525   -1.941  \n",
       "25%     -2.850   -3.350   -0.232  \n",
       "50%     -1.739   -2.295    0.424  \n",
       "75%     -1.268   -1.503    0.779  \n",
       "max     10.621   -0.254    2.813  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.41681430e-02, 3.72841132e-01, 6.39588164e-01, 4.31336366e-01,\n",
       "        8.56509560e-01],\n",
       "       [7.96713547e-01, 3.73376287e-01, 2.66019144e-01, 5.03038595e-01,\n",
       "        7.23443984e-01],\n",
       "       [5.71495801e-01, 2.77435866e-01, 8.10306934e-01, 4.42926452e-01,\n",
       "        6.03366085e-01],\n",
       "       [1.95610034e-01, 7.27693240e-01, 1.76130307e-01, 6.94681505e-01,\n",
       "        9.26936666e-01],\n",
       "       [7.98376944e-01, 8.60156528e-01, 2.02660185e-01, 4.99962330e-01,\n",
       "        8.25956287e-01],\n",
       "       [8.89907347e-01, 6.92556560e-01, 4.23117334e-01, 3.95357490e-01,\n",
       "        4.34007680e-01],\n",
       "       [7.99697868e-02, 1.41172526e-01, 2.86208851e-01, 7.98252916e-01,\n",
       "        1.44731288e-01],\n",
       "       [8.06024056e-02, 8.76677670e-01, 1.23288125e-01, 5.51061324e-01,\n",
       "        8.09642353e-01],\n",
       "       [9.47447594e-01, 3.43203002e-01, 6.69193460e-01, 4.65364961e-01,\n",
       "        8.87984698e-01],\n",
       "       [7.66968762e-01, 7.97805359e-01, 3.51139847e-01, 1.41215150e-04,\n",
       "        6.35638876e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 34.57374772],\n",
       "       [ -6.88011538],\n",
       "       [ 92.68280262],\n",
       "       [-51.39065904],\n",
       "       [-13.74431665],\n",
       "       [ 54.82789168],\n",
       "       [ -1.28638802],\n",
       "       [-32.82520575],\n",
       "       [ 82.58233726],\n",
       "       [ 31.68877602]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "\n",
    "if inet_training_without_noise:\n",
    "   \n",
    "    for lambda_net_dataset, lambda_net_dataset_without_noise in zip(lambda_net_dataset_list, lambda_net_dataset_list_without_noise):\n",
    "        if inet_holdout_seed_evaluation:\n",
    "            raise SystemExit('Holdout Evaluation not implemented with inet training without noise')\n",
    "            \n",
    "        else:\n",
    "            lambda_net_train_dataset = lambda_net_dataset_without_noise\n",
    "\n",
    "            lambda_net_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_dataset_list_without_noise\n",
    "        \n",
    "else:\n",
    "\n",
    "    for lambda_net_dataset in lambda_net_dataset_list:\n",
    "\n",
    "        if inet_holdout_seed_evaluation:\n",
    "\n",
    "            complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "\n",
    "            if isinstance(test_size, float):\n",
    "                test_size = int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-test_size)))\n",
    "\n",
    "            test_seeds = random.sample(complete_seed_list, test_size)\n",
    "            lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "\n",
    "            random.seed(RANDOM_SEED)\n",
    "            valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/(1-0.1))))\n",
    "            lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "            complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "            train_seeds = complete_seed_list\n",
    "            lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset\n",
    "        else:\n",
    "\n",
    "            lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=test_size)\n",
    "            lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "            lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "            lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "            lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "\n",
    "            del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "\n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 240)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 240)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 240)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.763</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>70.476</td>\n",
       "      <td>83.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-41.185</td>\n",
       "      <td>8.102</td>\n",
       "      <td>-2.361</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>0.420</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>70.312</td>\n",
       "      <td>4.155</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-7.685</td>\n",
       "      <td>45.664</td>\n",
       "      <td>1.059</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-22.699</td>\n",
       "      <td>-22.228</td>\n",
       "      <td>67.163</td>\n",
       "      <td>-4.917</td>\n",
       "      <td>0.747</td>\n",
       "      <td>31.916</td>\n",
       "      <td>57.046</td>\n",
       "      <td>-32.837</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.763</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>70.476</td>\n",
       "      <td>83.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.535</td>\n",
       "      <td>-1.360</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.032</td>\n",
       "      <td>2.249</td>\n",
       "      <td>2.175</td>\n",
       "      <td>2.428</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.545</td>\n",
       "      <td>2.848</td>\n",
       "      <td>-2.743</td>\n",
       "      <td>-2.918</td>\n",
       "      <td>2.354</td>\n",
       "      <td>2.211</td>\n",
       "      <td>2.566</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.412</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.388</td>\n",
       "      <td>2.234</td>\n",
       "      <td>-1.104</td>\n",
       "      <td>2.259</td>\n",
       "      <td>-3.106</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>1.359</td>\n",
       "      <td>1.806</td>\n",
       "      <td>2.103</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.176</td>\n",
       "      <td>1.731</td>\n",
       "      <td>-2.936</td>\n",
       "      <td>1.512</td>\n",
       "      <td>1.893</td>\n",
       "      <td>-2.204</td>\n",
       "      <td>1.344</td>\n",
       "      <td>-2.573</td>\n",
       "      <td>1.774</td>\n",
       "      <td>2.078</td>\n",
       "      <td>1.368</td>\n",
       "      <td>2.008</td>\n",
       "      <td>1.869</td>\n",
       "      <td>0.072</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.320</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-1.358</td>\n",
       "      <td>-0.793</td>\n",
       "      <td>-1.710</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>1.801</td>\n",
       "      <td>2.547</td>\n",
       "      <td>2.753</td>\n",
       "      <td>-1.258</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>2.124</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>2.089</td>\n",
       "      <td>1.331</td>\n",
       "      <td>-1.423</td>\n",
       "      <td>2.456</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>1.775</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.174</td>\n",
       "      <td>1.968</td>\n",
       "      <td>2.669</td>\n",
       "      <td>-1.414</td>\n",
       "      <td>2.343</td>\n",
       "      <td>1.604</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>1.134</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1.325</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.769</td>\n",
       "      <td>1.116</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>1.296</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.133</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.647</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.561</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.650</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-1.219</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>1.140</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.971</td>\n",
       "      <td>1.152</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.122</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>1.171</td>\n",
       "      <td>2.409</td>\n",
       "      <td>1.767</td>\n",
       "      <td>2.446</td>\n",
       "      <td>2.111</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>2.201</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-1.015</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2.063</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>2.312</td>\n",
       "      <td>-3.131</td>\n",
       "      <td>-2.432</td>\n",
       "      <td>-3.276</td>\n",
       "      <td>-2.749</td>\n",
       "      <td>1.911</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.990</td>\n",
       "      <td>-2.923</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>1.832</td>\n",
       "      <td>-4.610</td>\n",
       "      <td>-4.665</td>\n",
       "      <td>-5.314</td>\n",
       "      <td>-4.788</td>\n",
       "      <td>1.856</td>\n",
       "      <td>-4.437</td>\n",
       "      <td>2.250</td>\n",
       "      <td>1.728</td>\n",
       "      <td>2.036</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1.835</td>\n",
       "      <td>-2.751</td>\n",
       "      <td>1.862</td>\n",
       "      <td>-4.165</td>\n",
       "      <td>-2.982</td>\n",
       "      <td>0.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>21.501</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>27.649</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.613</td>\n",
       "      <td>-31.331</td>\n",
       "      <td>-86.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.778</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.989</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>-1.449</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>27.798</td>\n",
       "      <td>-1.441</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>-42.118</td>\n",
       "      <td>-30.896</td>\n",
       "      <td>-86.983</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.137</td>\n",
       "      <td>21.501</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>27.649</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-44.613</td>\n",
       "      <td>-31.331</td>\n",
       "      <td>-86.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.457</td>\n",
       "      <td>1.951</td>\n",
       "      <td>2.862</td>\n",
       "      <td>2.649</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.054</td>\n",
       "      <td>2.546</td>\n",
       "      <td>3.061</td>\n",
       "      <td>2.993</td>\n",
       "      <td>2.102</td>\n",
       "      <td>1.684</td>\n",
       "      <td>2.832</td>\n",
       "      <td>-2.667</td>\n",
       "      <td>2.894</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>2.774</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.689</td>\n",
       "      <td>2.505</td>\n",
       "      <td>2.417</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.429</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.658</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>3.252</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.086</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.639</td>\n",
       "      <td>1.217</td>\n",
       "      <td>0.709</td>\n",
       "      <td>-0.574</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.157</td>\n",
       "      <td>1.766</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>2.227</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.787</td>\n",
       "      <td>1.583</td>\n",
       "      <td>4.509</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>3.694</td>\n",
       "      <td>0.779</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.176</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>2.800</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-4.630</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.226</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.134</td>\n",
       "      <td>1.825</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.889</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.279</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-2.702</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>2.423</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>1.777</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-2.300</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-1.596</td>\n",
       "      <td>-2.369</td>\n",
       "      <td>-2.116</td>\n",
       "      <td>0.107</td>\n",
       "      <td>5.442</td>\n",
       "      <td>-7.049</td>\n",
       "      <td>-1.882</td>\n",
       "      <td>-1.946</td>\n",
       "      <td>-1.859</td>\n",
       "      <td>-2.118</td>\n",
       "      <td>-1.840</td>\n",
       "      <td>-1.574</td>\n",
       "      <td>-1.978</td>\n",
       "      <td>3.529</td>\n",
       "      <td>-2.011</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.208</td>\n",
       "      <td>5.516</td>\n",
       "      <td>-1.897</td>\n",
       "      <td>-2.041</td>\n",
       "      <td>-1.495</td>\n",
       "      <td>-6.235</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.972</td>\n",
       "      <td>47.932</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-35.026</td>\n",
       "      <td>79.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-64.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-6.966</td>\n",
       "      <td>46.836</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>49.512</td>\n",
       "      <td>-1.387</td>\n",
       "      <td>-3.075</td>\n",
       "      <td>4.746</td>\n",
       "      <td>0.526</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-15.663</td>\n",
       "      <td>0.802</td>\n",
       "      <td>4.168</td>\n",
       "      <td>1.668</td>\n",
       "      <td>-34.058</td>\n",
       "      <td>73.139</td>\n",
       "      <td>3.937</td>\n",
       "      <td>-0.805</td>\n",
       "      <td>2.379</td>\n",
       "      <td>-44.606</td>\n",
       "      <td>3.116</td>\n",
       "      <td>-16.918</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.972</td>\n",
       "      <td>47.932</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-35.026</td>\n",
       "      <td>79.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-64.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.361</td>\n",
       "      <td>2.719</td>\n",
       "      <td>1.734</td>\n",
       "      <td>3.569</td>\n",
       "      <td>2.739</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.104</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.748</td>\n",
       "      <td>1.138</td>\n",
       "      <td>7.690</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>1.144</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.219</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>1.630</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-2.118</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>2.537</td>\n",
       "      <td>1.898</td>\n",
       "      <td>3.675</td>\n",
       "      <td>2.422</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>0.247</td>\n",
       "      <td>2.441</td>\n",
       "      <td>1.840</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>2.478</td>\n",
       "      <td>-1.954</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-2.081</td>\n",
       "      <td>2.237</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.592</td>\n",
       "      <td>1.866</td>\n",
       "      <td>2.011</td>\n",
       "      <td>1.955</td>\n",
       "      <td>1.728</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>1.866</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>2.378</td>\n",
       "      <td>2.066</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>2.059</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-1.365</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>2.487</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>2.299</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.523</td>\n",
       "      <td>0.239</td>\n",
       "      <td>2.602</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>0.544</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.747</td>\n",
       "      <td>-3.771</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.483</td>\n",
       "      <td>-0.242</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>1.887</td>\n",
       "      <td>1.649</td>\n",
       "      <td>10.813</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>1.604</td>\n",
       "      <td>-2.985</td>\n",
       "      <td>-2.603</td>\n",
       "      <td>-9.552</td>\n",
       "      <td>-2.988</td>\n",
       "      <td>1.709</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>2.085</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.830</td>\n",
       "      <td>1.650</td>\n",
       "      <td>1.536</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>1.634</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>0.000</td>\n",
       "      <td>90.525</td>\n",
       "      <td>-36.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-17.307</td>\n",
       "      <td>-11.759</td>\n",
       "      <td>43.727</td>\n",
       "      <td>-21.109</td>\n",
       "      <td>25.278</td>\n",
       "      <td>69.439</td>\n",
       "      <td>-50.581</td>\n",
       "      <td>9.683</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>1.343</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-18.028</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>13.349</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>90.525</td>\n",
       "      <td>-36.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>8.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-17.307</td>\n",
       "      <td>0.747</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>1.222</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.087</td>\n",
       "      <td>-0.424</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.325</td>\n",
       "      <td>1.375</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.107</td>\n",
       "      <td>1.182</td>\n",
       "      <td>0.107</td>\n",
       "      <td>1.177</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.671</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.321</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.383</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.471</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.391</td>\n",
       "      <td>1.133</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.489</td>\n",
       "      <td>1.159</td>\n",
       "      <td>-0.536</td>\n",
       "      <td>0.797</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>1.830</td>\n",
       "      <td>3.048</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>2.560</td>\n",
       "      <td>1.977</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.403</td>\n",
       "      <td>2.740</td>\n",
       "      <td>-4.148</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1.777</td>\n",
       "      <td>0.814</td>\n",
       "      <td>1.147</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.565</td>\n",
       "      <td>1.461</td>\n",
       "      <td>1.235</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>1.323</td>\n",
       "      <td>2.914</td>\n",
       "      <td>-4.159</td>\n",
       "      <td>-1.798</td>\n",
       "      <td>-1.984</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>-1.841</td>\n",
       "      <td>1.921</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-1.593</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-1.283</td>\n",
       "      <td>-2.064</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>2.827</td>\n",
       "      <td>-1.584</td>\n",
       "      <td>2.039</td>\n",
       "      <td>-1.704</td>\n",
       "      <td>1.858</td>\n",
       "      <td>2.246</td>\n",
       "      <td>1.766</td>\n",
       "      <td>1.869</td>\n",
       "      <td>2.071</td>\n",
       "      <td>0.264</td>\n",
       "      <td>2.167</td>\n",
       "      <td>-2.012</td>\n",
       "      <td>2.306</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.955</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.279</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.534</td>\n",
       "      <td>1.151</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.649</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-2.380</td>\n",
       "      <td>-5.257</td>\n",
       "      <td>-2.280</td>\n",
       "      <td>-2.972</td>\n",
       "      <td>1.784</td>\n",
       "      <td>1.516</td>\n",
       "      <td>-1.550</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>-2.016</td>\n",
       "      <td>-1.814</td>\n",
       "      <td>-2.032</td>\n",
       "      <td>-3.822</td>\n",
       "      <td>-5.174</td>\n",
       "      <td>-2.085</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>1.995</td>\n",
       "      <td>1.346</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.536</td>\n",
       "      <td>1.436</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>1.571</td>\n",
       "      <td>-4.513</td>\n",
       "      <td>-5.060</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-52.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.785</td>\n",
       "      <td>5.567</td>\n",
       "      <td>-6.165</td>\n",
       "      <td>1.923</td>\n",
       "      <td>-10.618</td>\n",
       "      <td>-89.536</td>\n",
       "      <td>5.157</td>\n",
       "      <td>1.762</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>-31.265</td>\n",
       "      <td>1.290</td>\n",
       "      <td>1.220</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-6.202</td>\n",
       "      <td>2.913</td>\n",
       "      <td>-48.055</td>\n",
       "      <td>-2.042</td>\n",
       "      <td>2.466</td>\n",
       "      <td>-35.436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-28.089</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-52.851</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-37.785</td>\n",
       "      <td>1.367</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.693</td>\n",
       "      <td>1.547</td>\n",
       "      <td>-1.377</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>1.640</td>\n",
       "      <td>1.299</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.883</td>\n",
       "      <td>1.843</td>\n",
       "      <td>1.228</td>\n",
       "      <td>1.239</td>\n",
       "      <td>1.694</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.744</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>2.466</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>1.460</td>\n",
       "      <td>4.547</td>\n",
       "      <td>-0.582</td>\n",
       "      <td>1.507</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.176</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.047</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>1.524</td>\n",
       "      <td>1.304</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.061</td>\n",
       "      <td>-0.380</td>\n",
       "      <td>1.380</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.103</td>\n",
       "      <td>1.747</td>\n",
       "      <td>1.561</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.643</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.355</td>\n",
       "      <td>1.784</td>\n",
       "      <td>1.832</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.933</td>\n",
       "      <td>4.398</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>1.729</td>\n",
       "      <td>2.252</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.996</td>\n",
       "      <td>1.443</td>\n",
       "      <td>1.764</td>\n",
       "      <td>1.023</td>\n",
       "      <td>1.446</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.697</td>\n",
       "      <td>-1.865</td>\n",
       "      <td>3.340</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-4.390</td>\n",
       "      <td>0.982</td>\n",
       "      <td>2.697</td>\n",
       "      <td>1.266</td>\n",
       "      <td>1.857</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.296</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-3.556</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.087</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.144</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.073</td>\n",
       "      <td>2.234</td>\n",
       "      <td>0.037</td>\n",
       "      <td>3.779</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.944</td>\n",
       "      <td>1.409</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.584</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.052</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>2.015</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-1.541</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>2.421</td>\n",
       "      <td>-4.040</td>\n",
       "      <td>1.454</td>\n",
       "      <td>-1.494</td>\n",
       "      <td>-2.834</td>\n",
       "      <td>-2.611</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-1.609</td>\n",
       "      <td>-1.587</td>\n",
       "      <td>-1.689</td>\n",
       "      <td>-1.651</td>\n",
       "      <td>6.844</td>\n",
       "      <td>2.094</td>\n",
       "      <td>-1.265</td>\n",
       "      <td>-2.889</td>\n",
       "      <td>-1.571</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-1.459</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>-1.551</td>\n",
       "      <td>-1.421</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-1.524</td>\n",
       "      <td>2.835</td>\n",
       "      <td>-7.792</td>\n",
       "      <td>1.709</td>\n",
       "      <td>6.428</td>\n",
       "      <td>-7.317</td>\n",
       "      <td>-6.289</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>-1.675</td>\n",
       "      <td>-0.256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "6252  1373158606         0.000         0.000         0.000         0.000   \n",
       "4684  1373158606        21.501         0.000         0.000         0.000   \n",
       "1731  1373158606         0.000         0.000        47.972        47.932   \n",
       "4742  1373158606         0.000         0.000        13.349         0.000   \n",
       "4521  1373158606         0.000         0.261         0.000         0.000   \n",
       "\n",
       "      00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "6252         0.000         0.000         0.000        13.490         0.000   \n",
       "4684         0.000         0.000         0.000         0.000         0.000   \n",
       "1731         0.000         0.000         0.000         0.000         0.000   \n",
       "4742        90.525       -36.009         0.000         0.000         0.000   \n",
       "4521       -94.870         0.000         0.000         0.000         0.000   \n",
       "\n",
       "      00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "6252        27.763         0.000        -1.570         0.000         0.000   \n",
       "4684        27.649         0.000         0.000         0.000       -44.613   \n",
       "1731         0.000         0.000         0.000         0.000       -35.026   \n",
       "4742         8.893         0.000         0.000         0.000         0.000   \n",
       "4521         0.000       -28.089         0.000         0.000         0.000   \n",
       "\n",
       "      02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "6252         0.000         0.000         0.000         0.000        70.476   \n",
       "4684       -31.331       -86.712         0.000         0.000         0.000   \n",
       "1731        79.336         0.000         0.000         0.000       -64.234   \n",
       "4742         0.000         0.000         0.000         0.000         0.000   \n",
       "4521         0.000         0.000         0.000       -52.851         0.000   \n",
       "\n",
       "      11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "6252        83.810         0.000             -41.185               8.102   \n",
       "4684         0.000         0.000              21.778               0.372   \n",
       "1731         0.000         0.000              -6.966              46.836   \n",
       "4742         0.000       -17.307             -11.759              43.727   \n",
       "4521         0.000       -37.785               5.567              -6.165   \n",
       "\n",
       "      00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "6252              -2.361              -0.098               0.420   \n",
       "4684               0.327               0.989              -0.265   \n",
       "1731              -0.257              49.512              -1.387   \n",
       "4742             -21.109              25.278              69.439   \n",
       "4521               1.923             -10.618             -89.536   \n",
       "\n",
       "      00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "6252              -0.787              70.312               4.155   \n",
       "4684              -0.262              -1.449              -0.276   \n",
       "1731              -3.075               4.746               0.526   \n",
       "4742             -50.581               9.683               1.056   \n",
       "4521               5.157               1.762              -0.711   \n",
       "\n",
       "      00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "6252               0.154              -7.685              45.664   \n",
       "4684              -0.300              27.798              -1.441   \n",
       "1731              -0.141             -15.663               0.802   \n",
       "4742              -1.405              -0.290              -0.017   \n",
       "4521              -0.171              -0.511             -31.265   \n",
       "\n",
       "      01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "6252               1.059              -0.689             -22.699   \n",
       "4684              -0.186              -0.502             -42.118   \n",
       "1731               4.168               1.668             -34.058   \n",
       "4742               1.343              -0.966              -0.418   \n",
       "4521               1.290               1.220              -0.147   \n",
       "\n",
       "      02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "6252             -22.228              67.163              -4.917   \n",
       "4684             -30.896             -86.983              -0.677   \n",
       "1731              73.139               3.937              -0.805   \n",
       "4742              -0.220             -18.028              -0.205   \n",
       "4521               0.458              -6.202               2.913   \n",
       "\n",
       "      10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "6252               0.747              31.916              57.046   \n",
       "4684               0.339              -0.071               0.637   \n",
       "1731               2.379             -44.606               3.116   \n",
       "4742               0.507              -0.218              -0.296   \n",
       "4521             -48.055              -2.042               2.466   \n",
       "\n",
       "      20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "6252             -32.837              -0.000               0.000   \n",
       "4684               0.137              21.501              -0.000   \n",
       "1731             -16.918              -0.000               0.000   \n",
       "4742              -0.136              -0.000              -0.000   \n",
       "4521             -35.436               0.000               0.261   \n",
       "\n",
       "      00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "6252               0.000              -0.000              -0.000   \n",
       "4684               0.000              -0.000               0.000   \n",
       "1731              47.972              47.932              -0.000   \n",
       "4742              13.349              -0.000              90.525   \n",
       "4521              -0.000               0.000             -94.870   \n",
       "\n",
       "      00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "6252               0.000               0.000              13.490   \n",
       "4684               0.000              -0.000               0.000   \n",
       "1731              -0.000               0.000               0.000   \n",
       "4742             -36.009               0.000               0.000   \n",
       "4521               0.000              -0.000              -0.000   \n",
       "\n",
       "      00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "6252               0.000              27.763               0.000   \n",
       "4684              -0.000              27.649              -0.000   \n",
       "1731              -0.000              -0.000               0.000   \n",
       "4742              -0.000               8.893               0.000   \n",
       "4521              -0.000               0.000             -28.089   \n",
       "\n",
       "      01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "6252              -1.570              -0.000               0.000   \n",
       "4684              -0.000               0.000             -44.613   \n",
       "1731              -0.000               0.000             -35.026   \n",
       "4742               0.000              -0.000               0.000   \n",
       "4521              -0.000              -0.000               0.000   \n",
       "\n",
       "      02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "6252              -0.000              -0.000               0.000   \n",
       "4684             -31.331             -86.712               0.000   \n",
       "1731              79.336               0.000              -0.000   \n",
       "4742              -0.000               0.000              -0.000   \n",
       "4521               0.000              -0.000               0.000   \n",
       "\n",
       "      10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "6252              -0.000              70.476              83.810   \n",
       "4684               0.000               0.000              -0.000   \n",
       "1731              -0.000             -64.234               0.000   \n",
       "4742               0.000              -0.000              -0.000   \n",
       "4521             -52.851               0.000               0.000   \n",
       "\n",
       "      20000-lstsq_target   wb_0   wb_1   wb_2   wb_3   wb_4   wb_5  wb_6  \\\n",
       "6252               0.000 -1.535 -1.360 -0.996 -1.032  2.249  2.175 2.428   \n",
       "4684               0.000  2.457  1.951  2.862  2.649  0.011  0.006 0.002   \n",
       "1731              -0.000 -0.070 -0.342  0.204  0.040  0.226  0.179 0.208   \n",
       "4742             -17.307  0.747 -0.579  1.222  0.697  0.185  0.121 1.087   \n",
       "4521             -37.785  1.367  1.032  1.693  1.547 -1.377 -0.567 1.640   \n",
       "\n",
       "       wb_7  wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  \\\n",
       "6252 -1.586 0.007 2.545  2.848 -2.743 -2.918  2.354  2.211  2.566  2.000   \n",
       "4684  2.054 2.546 3.061  2.993  2.102  1.684  2.832 -2.667  2.894 -0.039   \n",
       "1731 -0.413 0.003 0.361  2.719  1.734  3.569  2.739  0.169  0.200  0.092   \n",
       "4742 -0.424 0.967 1.325  1.375  0.251  0.107  1.182  0.107  1.177  0.006   \n",
       "4521  1.299 1.434 1.883  1.843  1.228  1.239  1.694  0.003  1.744 -0.701   \n",
       "\n",
       "      wb_17  wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  \\\n",
       "6252  2.412  2.316  2.388  2.234 -1.104  2.259 -3.106 -1.624  0.023  0.119   \n",
       "4684  2.774  0.173  0.012  2.689  2.505  2.417 -0.027 -0.429  0.744  0.764   \n",
       "1731  0.289  0.332  0.292  0.104 -0.062 -0.083 -0.227 -0.497 -0.146 -0.045   \n",
       "4742  0.228  0.271  0.250  0.019 -0.066 -0.274 -0.052 -0.041  0.278 -0.158   \n",
       "4521  2.466 -0.008  1.460  4.547 -0.582  1.507  1.255  0.958  1.176  1.283   \n",
       "\n",
       "      wb_27  wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  \\\n",
       "6252 -0.229 -0.179  1.359  1.806  2.103  0.326  0.176  1.731 -2.936  1.512   \n",
       "4684  0.661  0.658 -0.350 -1.593  3.252  0.912  1.086  0.985  0.680  0.639   \n",
       "1731 -0.317 -0.301  0.748  1.138  7.690  0.043  0.170  1.122  0.041  0.321   \n",
       "4742  0.657  0.097  0.261  0.684  0.732  0.043  0.697  0.545  0.617 -0.247   \n",
       "4521  1.024  1.047 -0.086  0.075  1.649 -0.024  1.524  1.304  0.981  0.932   \n",
       "\n",
       "      wb_37  wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  \\\n",
       "6252  1.893 -2.204  1.344 -2.573  1.774  2.078  1.368  2.008  1.869  0.072   \n",
       "4684  1.217  0.709 -0.574  0.999  0.157  1.766 -0.274  2.227  1.570  0.787   \n",
       "1731  0.084  0.089  0.692 -0.020  1.144  1.429  0.710  1.392  1.219 -0.108   \n",
       "4742  0.081  0.660  0.205  0.547  0.717  0.890  0.263  0.850  0.671 -0.116   \n",
       "4521  1.182  1.061 -0.380  1.380  0.349 -0.086 -0.633 -0.046 -0.002  0.103   \n",
       "\n",
       "      wb_47  wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  \\\n",
       "6252  2.216  2.320  0.008 -1.358 -0.793 -1.710 -1.290  1.801  2.547  2.753   \n",
       "4684  1.583  4.509 -0.492 -0.153 -0.039 -0.357 -0.180 -0.442  3.694  0.779   \n",
       "1731  1.630  0.091 -0.497 -0.106 -0.009 -0.433 -0.208 -0.421  0.359 -2.118   \n",
       "4742  0.887  0.035 -0.204 -0.103 -0.035  0.321 -0.284  0.383  1.082  0.471   \n",
       "4521  1.747  1.561  0.883  0.210  0.365 -0.095  0.124 -0.100  1.424  0.643   \n",
       "\n",
       "      wb_57  wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  \\\n",
       "6252 -1.258 -0.147  2.124 -0.736  2.089  1.331 -1.423  2.456 -0.833  1.775   \n",
       "4684 -0.385 -0.143 -0.285 -0.027  0.229  0.176 -0.072  2.800 -0.213 -0.371   \n",
       "1731 -0.530 -0.149 -0.286  2.537  1.898  3.675  2.422  0.215 -0.231 -0.316   \n",
       "4742 -0.530  0.343  0.102  0.410 -0.099 -0.137  0.380  0.991  0.140  0.391   \n",
       "4521 -0.245  0.141  0.023  0.439  0.629  0.557  0.382  0.110  0.003  1.268   \n",
       "\n",
       "      wb_67  wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  \\\n",
       "6252  2.644  2.174  1.968  2.669 -1.414  2.343  1.604 -1.120  1.134  0.769   \n",
       "4684  0.358 -0.051 -4.630  0.380 -0.407  0.143  1.226 -0.086  0.375  0.427   \n",
       "1731  0.307 -0.001 -0.339  0.324 -0.539 -0.256  0.125 -0.053  0.161  0.207   \n",
       "4742  1.133  0.733  0.489  1.159 -0.536  0.797 -0.082 -0.225  1.830  3.048   \n",
       "4521  0.015  0.255 -0.100  0.059  0.017  0.254  0.617  0.355  1.784  1.832   \n",
       "\n",
       "      wb_77  wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  \\\n",
       "6252  0.940  0.952  1.325  0.768  0.769  1.116 -0.138  1.296 -0.089  0.016   \n",
       "4684  0.105  0.485  0.222  0.010 -0.004  0.416  0.181  0.419  0.116  0.307   \n",
       "1731 -0.550  0.247  2.441  1.840  0.063  0.033 -0.139  2.478 -1.954 -0.921   \n",
       "4742 -0.237  2.560  1.977  1.384  0.427  0.021  0.001  1.017  0.403  2.740   \n",
       "4521  1.200  1.933  4.398 -1.300  1.729  2.252  1.409  1.996  1.443  1.764   \n",
       "\n",
       "      wb_87  wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  \\\n",
       "6252 -0.286 -0.285  1.133 -0.136  0.532  0.734  0.934  0.817  0.589  0.911   \n",
       "4684 -0.204  0.134  1.825  0.350 -0.462 -0.032 -0.070  0.040 -0.087  0.371   \n",
       "1731  0.110 -2.081  2.237  0.007  1.592  1.866  2.011  1.955  1.728 -0.027   \n",
       "4742 -4.148  0.111  1.777  0.814  1.147  1.380  1.565  1.461  1.235 -0.041   \n",
       "4521  1.023  1.446  0.034  1.697 -1.865  3.340 -0.758 -4.390  0.982  2.697   \n",
       "\n",
       "      wb_97  wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  \\\n",
       "6252  0.598  0.058  0.850   0.919   0.647   1.081   0.561   1.018   0.749   \n",
       "4684 -0.127  0.032 -0.338   0.102   0.226   0.391   0.071  -0.053  -0.004   \n",
       "1731  1.866  0.122 -0.359  -0.383  -0.051  -0.057  -0.534   2.378   2.066   \n",
       "4742  1.323  2.914 -4.159  -1.798  -1.984  -1.352  -1.841   1.921   1.602   \n",
       "4521  1.266  1.857  1.260   0.885   1.190   1.296   0.757  -3.556  -0.196   \n",
       "\n",
       "      wb_106  wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  \\\n",
       "6252   0.422   1.180  -0.121   0.650  -0.302  -1.219  -0.264  -0.129   1.140   \n",
       "4684  -0.031   0.539   0.255   0.101   0.024  -0.119   0.177   0.231   1.889   \n",
       "1731   0.166   0.161  -0.121   2.059  -0.717  -1.365   0.050  -0.356   2.487   \n",
       "4742  -1.593   0.178  -1.283  -2.064  -1.916  -2.114   2.827  -1.584   2.039   \n",
       "4521   0.944   1.578   1.087   0.951   0.841   0.740   1.250   1.144   0.037   \n",
       "\n",
       "      wb_115  wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  \\\n",
       "6252  -0.347   0.988   1.340   0.892   0.971   1.152   1.040   1.122  -0.394   \n",
       "4684   0.128   0.001   0.339  -0.097  -0.001   0.208   0.593   0.232   0.013   \n",
       "1731  -0.397   2.299   2.707   2.216   2.346   2.523   0.239   2.602  -0.346   \n",
       "4742  -1.704   1.858   2.246   1.766   1.869   2.071   0.264   2.167  -2.012   \n",
       "4521   0.856   0.073   2.234   0.037   3.779   0.123   2.944   1.409   0.955   \n",
       "\n",
       "      wb_124  wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  \\\n",
       "6252   1.171   2.409   1.767   2.446   2.111   0.472   0.433  -0.051   2.201   \n",
       "4684   0.279  -0.096  -0.232   0.004  -0.073  -0.174  -0.144  -2.702  -0.081   \n",
       "1731   0.288  -0.086  -0.168  -0.069  -0.152   0.368   0.331  -2.834  -0.193   \n",
       "4742   2.306   0.673  -1.044   1.190   0.159   0.639   0.592   0.955  -0.196   \n",
       "4521   1.584  -0.004  -0.033   0.052  -0.053  -0.599   2.015  -0.051  -1.541   \n",
       "\n",
       "      wb_133  wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  \\\n",
       "6252  -0.128   0.566   0.741   0.281  -1.015  -0.183   0.453   0.338   0.514   \n",
       "4684  -0.135  -0.074  -0.087  -0.205  -0.359  -0.130   2.423  -0.116  -0.076   \n",
       "1731  -0.136   0.544   1.062   1.747  -3.771   0.522   0.364  -0.164   0.368   \n",
       "4742   0.958   1.136   1.279  -0.258   0.534   1.151   0.617   0.944   0.643   \n",
       "4521  -0.055  -0.047   0.033   0.029   0.044  -0.022  -0.166  -0.156   2.421   \n",
       "\n",
       "      wb_142  wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  \\\n",
       "6252   0.471   0.444   0.475   0.497   2.063   0.436  -0.347   2.312  -3.131   \n",
       "4684  -0.490  -0.139   1.777  -0.374  -0.078  -0.259  -2.300  -0.148  -2.000   \n",
       "1731   0.416   0.336   0.406   0.450  -0.170   0.483  -0.242  -0.132  -0.224   \n",
       "4742   0.649   0.604   0.646   0.673  -0.174   0.649  -0.898   0.156  -2.380   \n",
       "4521  -4.040   1.454  -1.494  -2.834  -2.611  -0.007  -0.038   0.107  -1.609   \n",
       "\n",
       "      wb_151  wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  \\\n",
       "6252  -2.432  -3.276  -2.749   1.911   2.000   1.990  -2.923  -0.265   1.832   \n",
       "4684  -1.596  -2.369  -2.116   0.107   5.442  -7.049  -1.882  -1.946  -1.859   \n",
       "1731  -0.295  -0.307  -0.213   1.887   1.649  10.813  -0.319  -0.263   1.604   \n",
       "4742  -5.257  -2.280  -2.972   1.784   1.516  -1.550  -0.324  -2.016  -1.814   \n",
       "4521  -1.587  -1.689  -1.651   6.844   2.094  -1.265  -2.889  -1.571  -1.370   \n",
       "\n",
       "      wb_160  wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  \\\n",
       "6252  -4.610  -4.665  -5.314  -4.788   1.856  -4.437   2.250   1.728   2.036   \n",
       "4684  -2.118  -1.840  -1.574  -1.978   3.529  -2.011   0.400  -1.595   0.208   \n",
       "1731  -2.985  -2.603  -9.552  -2.988   1.709  -0.148   2.085   1.450   1.830   \n",
       "4742  -2.032  -3.822  -5.174  -2.085   1.602  -2.028   1.995   1.346   1.723   \n",
       "4521  -1.459  -1.621  -1.551  -1.421   0.085  -1.524   2.835  -7.792   1.709   \n",
       "\n",
       "      wb_169  wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "6252   1.875   1.835  -2.751   1.862  -4.165  -2.982   0.453  \n",
       "4684   5.516  -1.897  -2.041  -1.495  -6.235  -0.319   0.627  \n",
       "1731   1.650   1.536  -0.275   1.634  -0.129  -0.338   0.079  \n",
       "4742   1.536   1.436  -0.277   1.571  -4.513  -5.060   0.472  \n",
       "4521   6.428  -7.317  -6.289  -1.280  -1.407  -1.675  -0.256  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-10.130</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.792</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.729</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-16.598</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>2.905</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-5.320</td>\n",
       "      <td>1.672</td>\n",
       "      <td>0.377</td>\n",
       "      <td>57.564</td>\n",
       "      <td>-4.537</td>\n",
       "      <td>22.004</td>\n",
       "      <td>-0.878</td>\n",
       "      <td>3.402</td>\n",
       "      <td>3.919</td>\n",
       "      <td>-14.992</td>\n",
       "      <td>15.565</td>\n",
       "      <td>0.156</td>\n",
       "      <td>4.543</td>\n",
       "      <td>4.081</td>\n",
       "      <td>34.964</td>\n",
       "      <td>-12.911</td>\n",
       "      <td>-10.278</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.434</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.248</td>\n",
       "      <td>60.293</td>\n",
       "      <td>0.433</td>\n",
       "      <td>4.728</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.071</td>\n",
       "      <td>49.306</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-1.016</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.604</td>\n",
       "      <td>1.153</td>\n",
       "      <td>1.058</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-3.263</td>\n",
       "      <td>1.436</td>\n",
       "      <td>1.987</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1.264</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.262</td>\n",
       "      <td>1.093</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>1.160</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>1.086</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>1.282</td>\n",
       "      <td>1.711</td>\n",
       "      <td>0.521</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>3.234</td>\n",
       "      <td>1.647</td>\n",
       "      <td>-2.061</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>1.246</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>1.729</td>\n",
       "      <td>1.955</td>\n",
       "      <td>1.279</td>\n",
       "      <td>1.907</td>\n",
       "      <td>1.753</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>2.141</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-2.559</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.960</td>\n",
       "      <td>2.763</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.563</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.863</td>\n",
       "      <td>-2.259</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.366</td>\n",
       "      <td>1.050</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.314</td>\n",
       "      <td>1.907</td>\n",
       "      <td>-0.825</td>\n",
       "      <td>2.865</td>\n",
       "      <td>1.868</td>\n",
       "      <td>1.296</td>\n",
       "      <td>2.905</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>1.932</td>\n",
       "      <td>-0.963</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-2.094</td>\n",
       "      <td>-0.999</td>\n",
       "      <td>1.679</td>\n",
       "      <td>2.172</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.297</td>\n",
       "      <td>1.462</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.166</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>1.335</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.766</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.646</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.529</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.744</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.951</td>\n",
       "      <td>1.235</td>\n",
       "      <td>0.015</td>\n",
       "      <td>1.678</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-3.390</td>\n",
       "      <td>1.178</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>0.777</td>\n",
       "      <td>1.487</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.253</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>1.664</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>-3.771</td>\n",
       "      <td>-2.447</td>\n",
       "      <td>-4.760</td>\n",
       "      <td>1.454</td>\n",
       "      <td>1.358</td>\n",
       "      <td>7.740</td>\n",
       "      <td>-1.447</td>\n",
       "      <td>-6.230</td>\n",
       "      <td>1.341</td>\n",
       "      <td>-3.143</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-2.901</td>\n",
       "      <td>-2.196</td>\n",
       "      <td>1.259</td>\n",
       "      <td>-3.767</td>\n",
       "      <td>1.713</td>\n",
       "      <td>1.111</td>\n",
       "      <td>1.430</td>\n",
       "      <td>1.347</td>\n",
       "      <td>1.178</td>\n",
       "      <td>-1.598</td>\n",
       "      <td>1.345</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-2.488</td>\n",
       "      <td>0.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-12.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-65.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.486</td>\n",
       "      <td>-49.433</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-94.376</td>\n",
       "      <td>1.223</td>\n",
       "      <td>3.852</td>\n",
       "      <td>-4.244</td>\n",
       "      <td>-14.104</td>\n",
       "      <td>-3.642</td>\n",
       "      <td>4.755</td>\n",
       "      <td>-4.158</td>\n",
       "      <td>-1.918</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>-58.916</td>\n",
       "      <td>8.944</td>\n",
       "      <td>40.482</td>\n",
       "      <td>-41.158</td>\n",
       "      <td>-2.119</td>\n",
       "      <td>-5.882</td>\n",
       "      <td>-1.297</td>\n",
       "      <td>0.797</td>\n",
       "      <td>2.075</td>\n",
       "      <td>1.354</td>\n",
       "      <td>-4.545</td>\n",
       "      <td>-92.167</td>\n",
       "      <td>-1.275</td>\n",
       "      <td>2.277</td>\n",
       "      <td>-0.365</td>\n",
       "      <td>0.882</td>\n",
       "      <td>-13.884</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>1.141</td>\n",
       "      <td>-1.586</td>\n",
       "      <td>-0.712</td>\n",
       "      <td>-65.149</td>\n",
       "      <td>0.999</td>\n",
       "      <td>44.998</td>\n",
       "      <td>-48.340</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>0.732</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.577</td>\n",
       "      <td>-1.044</td>\n",
       "      <td>-93.784</td>\n",
       "      <td>1.419</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.785</td>\n",
       "      <td>0.801</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>1.602</td>\n",
       "      <td>0.934</td>\n",
       "      <td>1.499</td>\n",
       "      <td>1.986</td>\n",
       "      <td>3.892</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1.336</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.381</td>\n",
       "      <td>-1.335</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.176</td>\n",
       "      <td>4.471</td>\n",
       "      <td>3.899</td>\n",
       "      <td>1.395</td>\n",
       "      <td>1.609</td>\n",
       "      <td>1.287</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.122</td>\n",
       "      <td>3.403</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-0.381</td>\n",
       "      <td>0.605</td>\n",
       "      <td>4.059</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-1.760</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.779</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.544</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.943</td>\n",
       "      <td>0.210</td>\n",
       "      <td>2.263</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.720</td>\n",
       "      <td>1.802</td>\n",
       "      <td>0.576</td>\n",
       "      <td>4.520</td>\n",
       "      <td>2.090</td>\n",
       "      <td>3.899</td>\n",
       "      <td>0.105</td>\n",
       "      <td>1.906</td>\n",
       "      <td>-1.080</td>\n",
       "      <td>1.155</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-2.736</td>\n",
       "      <td>1.610</td>\n",
       "      <td>1.306</td>\n",
       "      <td>1.966</td>\n",
       "      <td>2.179</td>\n",
       "      <td>1.882</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.251</td>\n",
       "      <td>1.877</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.162</td>\n",
       "      <td>-3.744</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-0.740</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.349</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-1.858</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.272</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>-1.304</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>3.019</td>\n",
       "      <td>0.103</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.645</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-2.122</td>\n",
       "      <td>1.190</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>-2.346</td>\n",
       "      <td>-2.258</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-2.306</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-2.575</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>-2.482</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-1.540</td>\n",
       "      <td>-1.478</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-6.391</td>\n",
       "      <td>1.280</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-1.393</td>\n",
       "      <td>-1.425</td>\n",
       "      <td>-1.504</td>\n",
       "      <td>-1.277</td>\n",
       "      <td>-5.694</td>\n",
       "      <td>-7.441</td>\n",
       "      <td>-1.723</td>\n",
       "      <td>-6.454</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-2.619</td>\n",
       "      <td>6.521</td>\n",
       "      <td>-6.453</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-9.493</td>\n",
       "      <td>-6.548</td>\n",
       "      <td>-1.444</td>\n",
       "      <td>-1.279</td>\n",
       "      <td>-1.342</td>\n",
       "      <td>-1.709</td>\n",
       "      <td>-0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-44.525</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.848</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>73.247</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-72.924</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.084</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-43.427</td>\n",
       "      <td>-4.326</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-23.838</td>\n",
       "      <td>-1.524</td>\n",
       "      <td>29.348</td>\n",
       "      <td>9.951</td>\n",
       "      <td>-2.899</td>\n",
       "      <td>42.752</td>\n",
       "      <td>16.132</td>\n",
       "      <td>10.014</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-8.462</td>\n",
       "      <td>0.441</td>\n",
       "      <td>1.065</td>\n",
       "      <td>-17.237</td>\n",
       "      <td>1.541</td>\n",
       "      <td>-42.061</td>\n",
       "      <td>-17.999</td>\n",
       "      <td>11.410</td>\n",
       "      <td>17.478</td>\n",
       "      <td>-46.923</td>\n",
       "      <td>2.608</td>\n",
       "      <td>-6.764</td>\n",
       "      <td>1.405</td>\n",
       "      <td>-0.633</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>1.950</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>73.261</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>1.019</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.662</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>1.062</td>\n",
       "      <td>3.463</td>\n",
       "      <td>-1.765</td>\n",
       "      <td>-72.319</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>23.348</td>\n",
       "      <td>-1.020</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.057</td>\n",
       "      <td>-1.051</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.627</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-2.888</td>\n",
       "      <td>0.912</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.184</td>\n",
       "      <td>2.713</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.069</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.583</td>\n",
       "      <td>-0.674</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>1.530</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.674</td>\n",
       "      <td>-0.602</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.666</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>2.960</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-0.626</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>2.766</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.323</td>\n",
       "      <td>-0.859</td>\n",
       "      <td>-1.057</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.344</td>\n",
       "      <td>1.095</td>\n",
       "      <td>0.210</td>\n",
       "      <td>2.958</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.564</td>\n",
       "      <td>1.013</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.562</td>\n",
       "      <td>4.027</td>\n",
       "      <td>0.841</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-1.934</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.345</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.689</td>\n",
       "      <td>1.560</td>\n",
       "      <td>1.384</td>\n",
       "      <td>1.658</td>\n",
       "      <td>1.560</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>-2.304</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.461</td>\n",
       "      <td>1.451</td>\n",
       "      <td>1.793</td>\n",
       "      <td>1.658</td>\n",
       "      <td>1.468</td>\n",
       "      <td>1.435</td>\n",
       "      <td>1.608</td>\n",
       "      <td>-1.836</td>\n",
       "      <td>1.526</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>1.743</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-1.318</td>\n",
       "      <td>1.843</td>\n",
       "      <td>1.495</td>\n",
       "      <td>1.672</td>\n",
       "      <td>1.382</td>\n",
       "      <td>1.569</td>\n",
       "      <td>-1.594</td>\n",
       "      <td>-1.385</td>\n",
       "      <td>-1.743</td>\n",
       "      <td>-1.676</td>\n",
       "      <td>0.118</td>\n",
       "      <td>6.348</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-1.635</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>-1.353</td>\n",
       "      <td>-1.472</td>\n",
       "      <td>-1.594</td>\n",
       "      <td>-1.393</td>\n",
       "      <td>-1.351</td>\n",
       "      <td>7.459</td>\n",
       "      <td>-1.388</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>0.212</td>\n",
       "      <td>5.343</td>\n",
       "      <td>-1.286</td>\n",
       "      <td>-1.572</td>\n",
       "      <td>-1.061</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>-1.730</td>\n",
       "      <td>-1.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-37.519</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-17.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.264</td>\n",
       "      <td>-59.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-45.018</td>\n",
       "      <td>-21.190</td>\n",
       "      <td>-1.454</td>\n",
       "      <td>0.434</td>\n",
       "      <td>2.524</td>\n",
       "      <td>0.902</td>\n",
       "      <td>3.286</td>\n",
       "      <td>-27.878</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-16.761</td>\n",
       "      <td>17.326</td>\n",
       "      <td>-20.737</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>18.292</td>\n",
       "      <td>-40.073</td>\n",
       "      <td>17.173</td>\n",
       "      <td>-45.604</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>1.157</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>1.106</td>\n",
       "      <td>-38.265</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>1.514</td>\n",
       "      <td>2.341</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-17.215</td>\n",
       "      <td>-1.127</td>\n",
       "      <td>1.247</td>\n",
       "      <td>-1.182</td>\n",
       "      <td>30.506</td>\n",
       "      <td>-61.799</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-44.217</td>\n",
       "      <td>1.588</td>\n",
       "      <td>1.242</td>\n",
       "      <td>1.938</td>\n",
       "      <td>1.762</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>1.889</td>\n",
       "      <td>1.170</td>\n",
       "      <td>1.673</td>\n",
       "      <td>2.082</td>\n",
       "      <td>2.110</td>\n",
       "      <td>1.477</td>\n",
       "      <td>1.522</td>\n",
       "      <td>1.943</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.973</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>1.992</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>1.838</td>\n",
       "      <td>1.586</td>\n",
       "      <td>1.756</td>\n",
       "      <td>1.489</td>\n",
       "      <td>1.197</td>\n",
       "      <td>1.218</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.093</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.680</td>\n",
       "      <td>1.503</td>\n",
       "      <td>1.571</td>\n",
       "      <td>1.327</td>\n",
       "      <td>1.011</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1.233</td>\n",
       "      <td>1.091</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>1.429</td>\n",
       "      <td>3.522</td>\n",
       "      <td>1.523</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>2.675</td>\n",
       "      <td>1.342</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.776</td>\n",
       "      <td>1.608</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.447</td>\n",
       "      <td>0.201</td>\n",
       "      <td>1.174</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1.064</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-3.419</td>\n",
       "      <td>1.164</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-2.794</td>\n",
       "      <td>1.208</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.814</td>\n",
       "      <td>1.067</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.565</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.464</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.307</td>\n",
       "      <td>1.926</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>1.320</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.974</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.806</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.774</td>\n",
       "      <td>-0.759</td>\n",
       "      <td>0.677</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.854</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.837</td>\n",
       "      <td>-1.429</td>\n",
       "      <td>-1.312</td>\n",
       "      <td>-1.601</td>\n",
       "      <td>-1.467</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-1.192</td>\n",
       "      <td>-1.408</td>\n",
       "      <td>-1.456</td>\n",
       "      <td>-1.225</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>-1.473</td>\n",
       "      <td>-1.513</td>\n",
       "      <td>-1.332</td>\n",
       "      <td>1.072</td>\n",
       "      <td>-1.420</td>\n",
       "      <td>6.674</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>0.211</td>\n",
       "      <td>5.756</td>\n",
       "      <td>-1.169</td>\n",
       "      <td>-1.383</td>\n",
       "      <td>-1.213</td>\n",
       "      <td>-1.249</td>\n",
       "      <td>-1.455</td>\n",
       "      <td>-0.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-99.955</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-75.927</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-13.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.479</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-53.454</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-78.585</td>\n",
       "      <td>2.016</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-39.311</td>\n",
       "      <td>3.536</td>\n",
       "      <td>-49.096</td>\n",
       "      <td>-17.951</td>\n",
       "      <td>-0.782</td>\n",
       "      <td>7.333</td>\n",
       "      <td>0.624</td>\n",
       "      <td>-28.568</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>5.793</td>\n",
       "      <td>3.391</td>\n",
       "      <td>14.855</td>\n",
       "      <td>-24.608</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>9.370</td>\n",
       "      <td>-2.766</td>\n",
       "      <td>-29.565</td>\n",
       "      <td>10.599</td>\n",
       "      <td>-100.057</td>\n",
       "      <td>-8.974</td>\n",
       "      <td>4.384</td>\n",
       "      <td>0.599</td>\n",
       "      <td>2.604</td>\n",
       "      <td>-76.409</td>\n",
       "      <td>1.207</td>\n",
       "      <td>6.786</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-14.788</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1.215</td>\n",
       "      <td>-4.216</td>\n",
       "      <td>1.064</td>\n",
       "      <td>2.725</td>\n",
       "      <td>5.981</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-9.456</td>\n",
       "      <td>-55.195</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.657</td>\n",
       "      <td>1.312</td>\n",
       "      <td>1.129</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>1.222</td>\n",
       "      <td>0.612</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.432</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.281</td>\n",
       "      <td>3.230</td>\n",
       "      <td>1.328</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>1.186</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.655</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.088</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-3.618</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.683</td>\n",
       "      <td>1.247</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.753</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.627</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.674</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.430</td>\n",
       "      <td>2.863</td>\n",
       "      <td>2.921</td>\n",
       "      <td>2.260</td>\n",
       "      <td>3.005</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>2.827</td>\n",
       "      <td>2.747</td>\n",
       "      <td>2.491</td>\n",
       "      <td>3.054</td>\n",
       "      <td>2.521</td>\n",
       "      <td>2.861</td>\n",
       "      <td>2.118</td>\n",
       "      <td>2.520</td>\n",
       "      <td>0.135</td>\n",
       "      <td>2.775</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>2.436</td>\n",
       "      <td>-0.652</td>\n",
       "      <td>-7.025</td>\n",
       "      <td>2.307</td>\n",
       "      <td>2.658</td>\n",
       "      <td>2.344</td>\n",
       "      <td>2.962</td>\n",
       "      <td>2.338</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.323</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.280</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.558</td>\n",
       "      <td>1.817</td>\n",
       "      <td>1.713</td>\n",
       "      <td>1.848</td>\n",
       "      <td>1.809</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.718</td>\n",
       "      <td>1.731</td>\n",
       "      <td>1.994</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1.774</td>\n",
       "      <td>1.719</td>\n",
       "      <td>1.845</td>\n",
       "      <td>0.220</td>\n",
       "      <td>1.786</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>1.819</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>1.731</td>\n",
       "      <td>1.862</td>\n",
       "      <td>1.727</td>\n",
       "      <td>1.909</td>\n",
       "      <td>1.758</td>\n",
       "      <td>1.797</td>\n",
       "      <td>-2.093</td>\n",
       "      <td>-2.004</td>\n",
       "      <td>-2.060</td>\n",
       "      <td>-2.174</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-1.627</td>\n",
       "      <td>-2.095</td>\n",
       "      <td>-1.898</td>\n",
       "      <td>-1.817</td>\n",
       "      <td>-1.923</td>\n",
       "      <td>-2.181</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>-1.814</td>\n",
       "      <td>7.466</td>\n",
       "      <td>-1.929</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-1.323</td>\n",
       "      <td>2.233</td>\n",
       "      <td>-8.277</td>\n",
       "      <td>-1.485</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-1.549</td>\n",
       "      <td>-1.807</td>\n",
       "      <td>-2.093</td>\n",
       "      <td>-1.264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "123  1373158606       -10.130         0.000         0.000         0.000   \n",
       "274  1373158606         0.000         0.000         0.000         0.000   \n",
       "600  1373158606       -44.525         0.000        -5.848         0.000   \n",
       "519  1373158606       -37.519         0.000         0.000         0.000   \n",
       "225  1373158606       -99.955         0.000         0.000         0.000   \n",
       "\n",
       "     00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "123         0.000         0.000         0.000         0.000        60.792   \n",
       "274       -12.447         0.000         0.000         0.000         0.000   \n",
       "600         0.000         0.000         0.000         0.000        73.247   \n",
       "519         0.000         0.000         0.000         0.000       -17.096   \n",
       "225         0.000       -75.927         0.000         0.000         0.000   \n",
       "\n",
       "     00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "123         0.000         4.034         0.000         0.000         0.000   \n",
       "274       -65.044         0.000        47.486       -49.433         0.000   \n",
       "600         0.000         0.000         0.000         0.000         0.000   \n",
       "519         0.000         0.000         0.000        30.264       -59.508   \n",
       "225       -13.893         0.000         2.479         0.000         0.000   \n",
       "\n",
       "     02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "123        -0.661         0.000         0.000         0.000         0.000   \n",
       "274         0.000         0.000         0.000         0.000         0.000   \n",
       "600         0.000         0.000         0.000       -72.924         0.000   \n",
       "519         0.000         0.000         0.000         0.000         0.000   \n",
       "225         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "123        49.729         0.000             -16.598               0.261   \n",
       "274         0.000       -94.376               1.223               3.852   \n",
       "600        26.084         0.000             -43.427              -4.326   \n",
       "519         0.000       -45.018             -21.190              -1.454   \n",
       "225       -53.454         0.000             -78.585               2.016   \n",
       "\n",
       "     00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "123              -0.154               2.905              -0.031   \n",
       "274              -4.244             -14.104              -3.642   \n",
       "600              -0.354             -23.838              -1.524   \n",
       "519               0.434               2.524               0.902   \n",
       "225              -0.855             -39.311               3.536   \n",
       "\n",
       "     00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "123              -5.320               1.672               0.377   \n",
       "274               4.755              -4.158              -1.918   \n",
       "600              29.348               9.951              -2.899   \n",
       "519               3.286             -27.878               0.388   \n",
       "225             -49.096             -17.951              -0.782   \n",
       "\n",
       "     00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "123              57.564              -4.537              22.004   \n",
       "274              -0.906             -58.916               8.944   \n",
       "600              42.752              16.132              10.014   \n",
       "519             -16.761              17.326             -20.737   \n",
       "225               7.333               0.624             -28.568   \n",
       "\n",
       "     01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "123              -0.878               3.402               3.919   \n",
       "274              40.482             -41.158              -2.119   \n",
       "600              -0.045              -8.462               0.441   \n",
       "519              -0.084              18.292             -40.073   \n",
       "225              -0.929               5.793               3.391   \n",
       "\n",
       "     02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "123             -14.992              15.565               0.156   \n",
       "274              -5.882              -1.297               0.797   \n",
       "600               1.065             -17.237               1.541   \n",
       "519              17.173             -45.604              -0.096   \n",
       "225              14.855             -24.608              -0.746   \n",
       "\n",
       "     10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "123               4.543               4.081              34.964   \n",
       "274               2.075               1.354              -4.545   \n",
       "600             -42.061             -17.999              11.410   \n",
       "519              -0.156               1.157              -0.716   \n",
       "225               9.370              -2.766             -29.565   \n",
       "\n",
       "     20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "123             -12.911             -10.278              -0.045   \n",
       "274             -92.167              -1.275               2.277   \n",
       "600              17.478             -46.923               2.608   \n",
       "519               1.106             -38.265               0.254   \n",
       "225              10.599            -100.057              -8.974   \n",
       "\n",
       "     00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "123               0.487               0.221              -0.434   \n",
       "274              -0.365               0.882             -13.884   \n",
       "600              -6.764               1.405              -0.633   \n",
       "519               0.010              -0.351              -0.266   \n",
       "225               4.384               0.599               2.604   \n",
       "\n",
       "     00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "123              -0.036               0.011               0.248   \n",
       "274              -0.344               1.141              -1.586   \n",
       "600              -0.789               1.950              -0.750   \n",
       "519               1.514               2.341              -0.952   \n",
       "225             -76.409               1.207               6.786   \n",
       "\n",
       "     00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "123              60.293               0.433               4.728   \n",
       "274              -0.712             -65.149               0.999   \n",
       "600              73.261              -0.714               1.019   \n",
       "519             -17.215              -1.127               1.247   \n",
       "225               0.436             -14.788               0.059   \n",
       "\n",
       "     01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "123              -0.437               0.195              -0.821   \n",
       "274              44.998             -48.340              -0.612   \n",
       "600              -0.481              -0.662              -0.972   \n",
       "519              -1.182              30.506             -61.799   \n",
       "225               1.215              -4.216               1.064   \n",
       "\n",
       "     02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "123              -0.774              -0.047              -0.288   \n",
       "274               0.732              -0.957               1.110   \n",
       "600               1.062               3.463              -1.765   \n",
       "519              -0.077              -0.222               0.759   \n",
       "225               2.725               5.981              -0.131   \n",
       "\n",
       "     10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "123               0.030               0.071              49.306   \n",
       "274               0.245               0.577              -1.044   \n",
       "600             -72.319              -0.890              23.348   \n",
       "519              -1.835               0.220              -0.070   \n",
       "225               0.468              -9.456             -55.195   \n",
       "\n",
       "     20000-lstsq_target   wb_0   wb_1  wb_2  wb_3   wb_4   wb_5  wb_6   wb_7  \\\n",
       "123               0.225 -0.059 -1.016 0.098 0.604  1.153  1.058 0.586 -0.454   \n",
       "274             -93.784  1.419  1.051 1.785 0.801 -0.467 -0.010 1.602  0.934   \n",
       "600              -1.020  0.543  0.224 0.903 0.720  0.057 -1.051 0.789  0.188   \n",
       "519             -44.217  1.588  1.242 1.938 1.762  0.041 -0.004 1.889  1.170   \n",
       "225               0.390  0.959  0.657 1.312 1.129  0.059 -0.014 1.222  0.612   \n",
       "\n",
       "      wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "123 -3.263 1.436  1.987 -0.223  0.011  0.195  1.092  0.512  0.943  1.264   \n",
       "274  1.499 1.986  3.892  0.276  1.336  0.128  0.005  1.381 -1.335  4.544   \n",
       "600  0.627 1.005  1.018  0.427  0.471  0.860 -2.888  0.912 -0.045  0.817   \n",
       "519  1.673 2.082  2.110  1.477  1.522  1.943 -0.414  1.973 -0.080  1.992   \n",
       "225  1.056 1.392  1.432  0.854  0.912  1.281  3.230  1.328 -0.044  1.186   \n",
       "\n",
       "     wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "123  1.209  1.262  1.093 -0.045  1.160 -0.245 -0.117 -0.696  1.086 -0.997   \n",
       "274  0.176  4.471  3.899  1.395  1.609  1.287  0.996  0.467  0.418  0.122   \n",
       "600  0.184  2.713  0.705  0.585  0.665  0.434  0.189 -0.415 -0.217 -0.625   \n",
       "519  0.173 -0.068  1.838  1.586  1.756  1.489  1.197  1.218  1.337  1.073   \n",
       "225  0.974  0.655  1.029  1.003  1.088  0.869  0.615  0.269  0.382  0.141   \n",
       "\n",
       "     wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "123 -0.441  1.282  1.711  0.521 -0.315  3.234  1.647 -2.061 -0.414 -0.092   \n",
       "274  3.403  0.137  0.036  0.730  0.616  0.623  0.432  0.959  0.040  0.303   \n",
       "600 -0.547 -0.349  0.399  0.069 -0.100 -0.027 -0.436 -0.667 -0.590 -0.330   \n",
       "519  1.093 -0.332  0.037  1.680  1.503  1.571  1.327  1.011  0.981  1.233   \n",
       "225  0.138 -0.351  0.043  0.759  0.542  0.630  0.401  0.088  0.031  0.305   \n",
       "\n",
       "     wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "123 -0.931  1.246 -0.774  1.729  1.955  1.279  1.907  1.753 -0.647  2.141   \n",
       "274 -0.746 -0.381  0.605  4.059 -0.441 -0.277 -0.098 -1.760  0.369  0.782   \n",
       "600 -0.583 -0.674 -0.215  0.134 -0.461 -0.275  1.530 -0.667 -0.352  0.070   \n",
       "519  1.091 -0.307  1.429  3.522  1.523 -0.274  2.675  1.342  1.283  1.776   \n",
       "225  0.172 -3.618  0.487  0.145  0.683  1.247  0.580  0.494  0.333  0.870   \n",
       "\n",
       "     wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "123  0.089 -0.758 -0.218 -1.875  0.075 -2.559  0.231  0.960  2.763 -0.441   \n",
       "274  0.623  0.029  1.779  1.884  1.544  0.341 -0.943  0.210  2.263  1.284   \n",
       "600  0.062 -0.674 -0.602 -0.346 -0.982 -0.666 -0.442  2.960 -0.124 -0.980   \n",
       "519  1.608  0.937  0.683  0.773  0.408  0.625 -0.447  0.201  1.174  0.201   \n",
       "225  0.664  0.001  0.294  0.419  0.031  0.231 -0.439  0.206  0.753 -0.130   \n",
       "\n",
       "     wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "123  0.161  0.534  0.596  0.222  1.563  0.427  0.863 -2.259  0.252  1.017   \n",
       "274  1.720  1.802  0.576  4.520  2.090  3.899  0.105  1.906 -1.080  1.155   \n",
       "600 -0.626 -0.896 -0.444 -0.110 -0.203 -0.488  2.766 -0.739 -0.376 -0.867   \n",
       "519  0.601  0.644  0.967  1.064  0.978  0.908  0.574  0.578 -3.419  1.164   \n",
       "225  0.230  0.198  0.545  0.683  0.621  0.496 -0.542  0.177 -0.379  0.627   \n",
       "\n",
       "     wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "123  0.597  0.366  1.050 -0.251  0.753  0.122  0.151 -0.314  1.907 -0.825   \n",
       "274 -0.047 -2.736  1.610  1.306  1.966  2.179  1.882  0.742  0.891  0.251   \n",
       "600 -0.040  0.323 -0.859 -1.057 -0.646 -0.073 -0.459  0.956  1.082  0.344   \n",
       "519 -0.043 -2.794  1.208  0.197  0.814  1.067  0.772  0.431  0.565 -0.150   \n",
       "225  1.270  0.571  0.674 -0.153  0.399  0.685  0.430  2.863  2.921  2.260   \n",
       "\n",
       "     wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "123  2.865  1.868  1.296  2.905 -0.117 -0.279  1.932 -0.963  0.114 -2.094   \n",
       "274  1.877  0.550 -0.250  0.351  0.726  0.456  0.859  0.796  0.420  0.120   \n",
       "600  1.095  0.210  2.958  0.875  0.901  0.615  0.995  0.564  1.013  0.293   \n",
       "519  0.574  0.226 -0.253  0.282  0.436  0.087  0.448 -0.000  0.464 -0.254   \n",
       "225  3.005  0.218 -0.246  2.827  2.747  2.491  3.054  2.521  2.861  2.118   \n",
       "\n",
       "     wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "123 -0.999  1.679  2.172  1.049  1.297  1.462  1.378  1.166 -0.341  1.335   \n",
       "274 -0.332  0.039  0.162 -3.744  0.317 -0.060 -0.281 -0.740  0.635  0.266   \n",
       "600  0.562  4.027  0.841 -0.459  0.170 -0.062 -1.934  0.078  0.778  0.345   \n",
       "519  0.006  0.878  0.307  1.926 -0.467 -0.061  1.320 -0.567  0.304 -0.216   \n",
       "225  2.520  0.135  2.775 -0.459  2.436 -0.652 -7.025  2.307  2.658  2.344   \n",
       "\n",
       "     wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "123  0.105 -0.931   0.233   0.225   0.766  -0.070   0.631   0.361   0.163   \n",
       "274  0.888  0.349  -0.497   0.227   0.254  -1.858   0.245  -0.272  -1.200   \n",
       "600  1.093  0.504  -0.066   0.288   0.345  -0.193  -0.059  -0.309  -0.018   \n",
       "519  0.529 -0.003  -0.181   0.183   0.236  -0.308  -0.041  -0.269  -0.190   \n",
       "225  2.962  2.338  -0.227   0.113   0.229  -0.361  -0.053  -0.270  -0.192   \n",
       "\n",
       "     wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "123   0.646  -0.003   0.203   0.009  -0.529   0.297   0.469   0.744  -0.007   \n",
       "274   0.561   0.038  -0.651  -1.304   0.174   0.281   0.187   0.037  -1.288   \n",
       "600   0.559   0.164  -0.093  -0.137  -0.166   0.368   0.163   0.196  -0.094   \n",
       "519   0.485   0.045  -0.244  -0.288  -0.292   0.238   0.018   0.974  -0.221   \n",
       "225   0.408   0.014  -0.252  -0.276  -0.339   0.233   0.027   0.108  -0.251   \n",
       "\n",
       "     wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "123   0.630   0.920   0.508   0.563   0.725   0.655   0.600  -0.355   0.951   \n",
       "274   3.019   0.103  -0.093   0.220   0.932   0.579   0.250  -0.327   0.645   \n",
       "600  -0.007   0.414  -0.098  -0.030   0.281   0.613   0.408   0.047   0.689   \n",
       "519   0.226   0.153  -0.098  -0.383   0.001   0.529   0.250  -0.096   0.585   \n",
       "225   0.005   0.323   1.460   0.283   0.170   0.472   0.280  -0.143   0.558   \n",
       "\n",
       "     wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "123   1.235   0.015   1.678   0.398   0.254   0.232  -3.390   1.178   0.039   \n",
       "274  -0.239   0.014   0.094  -2.122   1.190  -0.184  -0.881   0.092  -0.042   \n",
       "600   1.560   1.384   1.658   1.560  -0.180  -2.304   1.500   1.461   1.451   \n",
       "519   0.814   0.731   0.873   0.806  -0.178  -0.185   0.805   0.748   0.745   \n",
       "225   1.817   1.713   1.848   1.809  -0.176  -0.185   1.845   1.718   1.731   \n",
       "\n",
       "     wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "123   0.282   0.409  -0.155   0.777   1.487   0.234  -0.255   0.333   0.239   \n",
       "274  -0.205  -2.346  -2.258   0.006  -2.306  -0.170  -1.637  -0.148  -2.575   \n",
       "600   1.793   1.658   1.468   1.435   1.608  -1.836   1.526  -0.093   1.743   \n",
       "519   0.925   0.866   0.789   0.760   0.826   0.293   0.774  -0.759   0.677   \n",
       "225   1.994   1.882   1.774   1.719   1.845   0.220   1.786  -0.083   1.819   \n",
       "\n",
       "     wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "123   0.242   0.248   0.253   1.225   0.069  -0.250   1.664  -1.763  -3.771   \n",
       "274  -0.127  -0.870  -2.482   0.032   0.027  -0.171   0.143  -1.540  -1.478   \n",
       "600  -0.122  -1.318   1.843   1.495   1.672   1.382   1.569  -1.594  -1.385   \n",
       "519  -0.127  -0.854   0.739   0.750   0.871   0.751   0.837  -1.429  -1.312   \n",
       "225  -0.320   1.731   1.862   1.727   1.909   1.758   1.797  -2.093  -2.004   \n",
       "\n",
       "     wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "123  -2.447  -4.760   1.454   1.358   7.740  -1.447  -6.230   1.341  -3.143   \n",
       "274  -1.700  -6.391   1.280   0.158  -1.393  -1.425  -1.504  -1.277  -5.694   \n",
       "600  -1.743  -1.676   0.118   6.348  -0.950  -1.635  -1.352  -1.353  -1.472   \n",
       "519  -1.601  -1.467   0.113   0.158  -1.192  -1.408  -1.456  -1.225  -1.450   \n",
       "225  -2.060  -2.174   0.120   0.158  -1.627  -2.095  -1.898  -1.817  -1.923   \n",
       "\n",
       "     wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "123  -0.276  -2.901  -2.196   1.259  -3.767   1.713   1.111   1.430   1.347   \n",
       "274  -7.441  -1.723  -6.454   0.087  -2.619   6.521  -6.453   0.211  -9.493   \n",
       "600  -1.594  -1.393  -1.351   7.459  -1.388   0.392  -0.960   0.212   5.343   \n",
       "519  -1.473  -1.513  -1.332   1.072  -1.420   6.674  -1.024   0.211   5.756   \n",
       "225  -2.181  -1.907  -1.814   7.466  -1.929   0.395  -1.323   2.233  -8.277   \n",
       "\n",
       "     wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "123   1.178  -1.598   1.345  -0.132  -2.488   0.346  \n",
       "274  -6.548  -1.444  -1.279  -1.342  -1.709  -0.224  \n",
       "600  -1.286  -1.572  -1.061  -1.090  -1.730  -1.045  \n",
       "519  -1.169  -1.383  -1.213  -1.249  -1.455  -0.590  \n",
       "225  -1.485  -1.997  -1.549  -1.807  -2.093  -1.264  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>00000-target</th>\n",
       "      <th>00001-target</th>\n",
       "      <th>00002-target</th>\n",
       "      <th>00010-target</th>\n",
       "      <th>00011-target</th>\n",
       "      <th>00020-target</th>\n",
       "      <th>00100-target</th>\n",
       "      <th>00101-target</th>\n",
       "      <th>00110-target</th>\n",
       "      <th>00200-target</th>\n",
       "      <th>01000-target</th>\n",
       "      <th>01001-target</th>\n",
       "      <th>01010-target</th>\n",
       "      <th>01100-target</th>\n",
       "      <th>02000-target</th>\n",
       "      <th>10000-target</th>\n",
       "      <th>10001-target</th>\n",
       "      <th>10010-target</th>\n",
       "      <th>10100-target</th>\n",
       "      <th>11000-target</th>\n",
       "      <th>20000-target</th>\n",
       "      <th>00000-lstsq_lambda</th>\n",
       "      <th>00001-lstsq_lambda</th>\n",
       "      <th>00002-lstsq_lambda</th>\n",
       "      <th>00010-lstsq_lambda</th>\n",
       "      <th>00011-lstsq_lambda</th>\n",
       "      <th>00020-lstsq_lambda</th>\n",
       "      <th>00100-lstsq_lambda</th>\n",
       "      <th>00101-lstsq_lambda</th>\n",
       "      <th>00110-lstsq_lambda</th>\n",
       "      <th>00200-lstsq_lambda</th>\n",
       "      <th>01000-lstsq_lambda</th>\n",
       "      <th>01001-lstsq_lambda</th>\n",
       "      <th>01010-lstsq_lambda</th>\n",
       "      <th>01100-lstsq_lambda</th>\n",
       "      <th>02000-lstsq_lambda</th>\n",
       "      <th>10000-lstsq_lambda</th>\n",
       "      <th>10001-lstsq_lambda</th>\n",
       "      <th>10010-lstsq_lambda</th>\n",
       "      <th>10100-lstsq_lambda</th>\n",
       "      <th>11000-lstsq_lambda</th>\n",
       "      <th>20000-lstsq_lambda</th>\n",
       "      <th>00000-lstsq_target</th>\n",
       "      <th>00001-lstsq_target</th>\n",
       "      <th>00002-lstsq_target</th>\n",
       "      <th>00010-lstsq_target</th>\n",
       "      <th>00011-lstsq_target</th>\n",
       "      <th>00020-lstsq_target</th>\n",
       "      <th>00100-lstsq_target</th>\n",
       "      <th>00101-lstsq_target</th>\n",
       "      <th>00110-lstsq_target</th>\n",
       "      <th>00200-lstsq_target</th>\n",
       "      <th>01000-lstsq_target</th>\n",
       "      <th>01001-lstsq_target</th>\n",
       "      <th>01010-lstsq_target</th>\n",
       "      <th>01100-lstsq_target</th>\n",
       "      <th>02000-lstsq_target</th>\n",
       "      <th>10000-lstsq_target</th>\n",
       "      <th>10001-lstsq_target</th>\n",
       "      <th>10010-lstsq_target</th>\n",
       "      <th>10100-lstsq_target</th>\n",
       "      <th>11000-lstsq_target</th>\n",
       "      <th>20000-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "      <th>wb_91</th>\n",
       "      <th>wb_92</th>\n",
       "      <th>wb_93</th>\n",
       "      <th>wb_94</th>\n",
       "      <th>wb_95</th>\n",
       "      <th>wb_96</th>\n",
       "      <th>wb_97</th>\n",
       "      <th>wb_98</th>\n",
       "      <th>wb_99</th>\n",
       "      <th>wb_100</th>\n",
       "      <th>wb_101</th>\n",
       "      <th>wb_102</th>\n",
       "      <th>wb_103</th>\n",
       "      <th>wb_104</th>\n",
       "      <th>wb_105</th>\n",
       "      <th>wb_106</th>\n",
       "      <th>wb_107</th>\n",
       "      <th>wb_108</th>\n",
       "      <th>wb_109</th>\n",
       "      <th>wb_110</th>\n",
       "      <th>wb_111</th>\n",
       "      <th>wb_112</th>\n",
       "      <th>wb_113</th>\n",
       "      <th>wb_114</th>\n",
       "      <th>wb_115</th>\n",
       "      <th>wb_116</th>\n",
       "      <th>wb_117</th>\n",
       "      <th>wb_118</th>\n",
       "      <th>wb_119</th>\n",
       "      <th>wb_120</th>\n",
       "      <th>wb_121</th>\n",
       "      <th>wb_122</th>\n",
       "      <th>wb_123</th>\n",
       "      <th>wb_124</th>\n",
       "      <th>wb_125</th>\n",
       "      <th>wb_126</th>\n",
       "      <th>wb_127</th>\n",
       "      <th>wb_128</th>\n",
       "      <th>wb_129</th>\n",
       "      <th>wb_130</th>\n",
       "      <th>wb_131</th>\n",
       "      <th>wb_132</th>\n",
       "      <th>wb_133</th>\n",
       "      <th>wb_134</th>\n",
       "      <th>wb_135</th>\n",
       "      <th>wb_136</th>\n",
       "      <th>wb_137</th>\n",
       "      <th>wb_138</th>\n",
       "      <th>wb_139</th>\n",
       "      <th>wb_140</th>\n",
       "      <th>wb_141</th>\n",
       "      <th>wb_142</th>\n",
       "      <th>wb_143</th>\n",
       "      <th>wb_144</th>\n",
       "      <th>wb_145</th>\n",
       "      <th>wb_146</th>\n",
       "      <th>wb_147</th>\n",
       "      <th>wb_148</th>\n",
       "      <th>wb_149</th>\n",
       "      <th>wb_150</th>\n",
       "      <th>wb_151</th>\n",
       "      <th>wb_152</th>\n",
       "      <th>wb_153</th>\n",
       "      <th>wb_154</th>\n",
       "      <th>wb_155</th>\n",
       "      <th>wb_156</th>\n",
       "      <th>wb_157</th>\n",
       "      <th>wb_158</th>\n",
       "      <th>wb_159</th>\n",
       "      <th>wb_160</th>\n",
       "      <th>wb_161</th>\n",
       "      <th>wb_162</th>\n",
       "      <th>wb_163</th>\n",
       "      <th>wb_164</th>\n",
       "      <th>wb_165</th>\n",
       "      <th>wb_166</th>\n",
       "      <th>wb_167</th>\n",
       "      <th>wb_168</th>\n",
       "      <th>wb_169</th>\n",
       "      <th>wb_170</th>\n",
       "      <th>wb_171</th>\n",
       "      <th>wb_172</th>\n",
       "      <th>wb_173</th>\n",
       "      <th>wb_174</th>\n",
       "      <th>wb_175</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-84.503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.395</td>\n",
       "      <td>-2.612</td>\n",
       "      <td>0.000</td>\n",
       "      <td>83.254</td>\n",
       "      <td>-22.327</td>\n",
       "      <td>-0.548</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-83.661</td>\n",
       "      <td>-0.406</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.829</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.331</td>\n",
       "      <td>3.536</td>\n",
       "      <td>29.679</td>\n",
       "      <td>-4.243</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>80.918</td>\n",
       "      <td>-22.610</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.787</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>-85.256</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.559</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.464</td>\n",
       "      <td>30.396</td>\n",
       "      <td>-2.338</td>\n",
       "      <td>0.184</td>\n",
       "      <td>82.645</td>\n",
       "      <td>-22.637</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>3.762</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.332</td>\n",
       "      <td>1.907</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>-3.142</td>\n",
       "      <td>-3.125</td>\n",
       "      <td>0.390</td>\n",
       "      <td>2.886</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>0.471</td>\n",
       "      <td>1.971</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1.630</td>\n",
       "      <td>2.103</td>\n",
       "      <td>2.036</td>\n",
       "      <td>2.182</td>\n",
       "      <td>1.923</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.954</td>\n",
       "      <td>-2.057</td>\n",
       "      <td>-1.334</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.654</td>\n",
       "      <td>-2.769</td>\n",
       "      <td>-1.077</td>\n",
       "      <td>0.985</td>\n",
       "      <td>1.177</td>\n",
       "      <td>0.251</td>\n",
       "      <td>1.953</td>\n",
       "      <td>1.687</td>\n",
       "      <td>-1.064</td>\n",
       "      <td>-2.366</td>\n",
       "      <td>-0.407</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-1.041</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-1.175</td>\n",
       "      <td>1.248</td>\n",
       "      <td>2.608</td>\n",
       "      <td>0.793</td>\n",
       "      <td>2.652</td>\n",
       "      <td>1.775</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>2.268</td>\n",
       "      <td>1.294</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.325</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.557</td>\n",
       "      <td>0.141</td>\n",
       "      <td>1.151</td>\n",
       "      <td>3.562</td>\n",
       "      <td>-1.325</td>\n",
       "      <td>1.673</td>\n",
       "      <td>1.245</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1.809</td>\n",
       "      <td>1.410</td>\n",
       "      <td>1.045</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>1.494</td>\n",
       "      <td>-0.509</td>\n",
       "      <td>2.059</td>\n",
       "      <td>-2.802</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-2.069</td>\n",
       "      <td>2.236</td>\n",
       "      <td>-1.671</td>\n",
       "      <td>1.575</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.039</td>\n",
       "      <td>1.678</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>0.473</td>\n",
       "      <td>-3.638</td>\n",
       "      <td>0.262</td>\n",
       "      <td>1.916</td>\n",
       "      <td>1.349</td>\n",
       "      <td>1.445</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.009</td>\n",
       "      <td>0.566</td>\n",
       "      <td>1.590</td>\n",
       "      <td>2.121</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>1.448</td>\n",
       "      <td>2.238</td>\n",
       "      <td>1.731</td>\n",
       "      <td>1.792</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>-1.146</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-1.662</td>\n",
       "      <td>-0.964</td>\n",
       "      <td>-1.927</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-2.199</td>\n",
       "      <td>-1.741</td>\n",
       "      <td>-2.947</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-1.635</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-1.503</td>\n",
       "      <td>-1.179</td>\n",
       "      <td>-0.493</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-1.226</td>\n",
       "      <td>-2.844</td>\n",
       "      <td>-6.259</td>\n",
       "      <td>-1.095</td>\n",
       "      <td>6.530</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-4.204</td>\n",
       "      <td>-4.386</td>\n",
       "      <td>-6.729</td>\n",
       "      <td>-2.166</td>\n",
       "      <td>-4.521</td>\n",
       "      <td>-1.313</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-2.047</td>\n",
       "      <td>1.368</td>\n",
       "      <td>-5.941</td>\n",
       "      <td>5.493</td>\n",
       "      <td>6.067</td>\n",
       "      <td>1.784</td>\n",
       "      <td>3.611</td>\n",
       "      <td>2.411</td>\n",
       "      <td>-4.478</td>\n",
       "      <td>3.272</td>\n",
       "      <td>-3.387</td>\n",
       "      <td>-1.249</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>79.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-39.417</td>\n",
       "      <td>-86.861</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-50.679</td>\n",
       "      <td>-3.185</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-1.998</td>\n",
       "      <td>1.450</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>2.220</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>0.827</td>\n",
       "      <td>-2.848</td>\n",
       "      <td>-2.148</td>\n",
       "      <td>1.287</td>\n",
       "      <td>3.238</td>\n",
       "      <td>78.187</td>\n",
       "      <td>-0.534</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-1.562</td>\n",
       "      <td>-4.531</td>\n",
       "      <td>-36.637</td>\n",
       "      <td>-84.877</td>\n",
       "      <td>0.726</td>\n",
       "      <td>-50.371</td>\n",
       "      <td>-1.831</td>\n",
       "      <td>1.032</td>\n",
       "      <td>-1.998</td>\n",
       "      <td>1.423</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>1.377</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>-1.497</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.728</td>\n",
       "      <td>80.169</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-2.273</td>\n",
       "      <td>-38.701</td>\n",
       "      <td>-86.635</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-49.820</td>\n",
       "      <td>-1.898</td>\n",
       "      <td>2.147</td>\n",
       "      <td>1.825</td>\n",
       "      <td>2.379</td>\n",
       "      <td>2.339</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-4.335</td>\n",
       "      <td>1.959</td>\n",
       "      <td>2.135</td>\n",
       "      <td>1.810</td>\n",
       "      <td>2.736</td>\n",
       "      <td>2.720</td>\n",
       "      <td>2.040</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.555</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.937</td>\n",
       "      <td>-3.903</td>\n",
       "      <td>2.415</td>\n",
       "      <td>3.063</td>\n",
       "      <td>2.731</td>\n",
       "      <td>-3.073</td>\n",
       "      <td>2.219</td>\n",
       "      <td>1.896</td>\n",
       "      <td>1.337</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-2.153</td>\n",
       "      <td>2.302</td>\n",
       "      <td>1.394</td>\n",
       "      <td>0.451</td>\n",
       "      <td>1.758</td>\n",
       "      <td>0.520</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>1.499</td>\n",
       "      <td>2.594</td>\n",
       "      <td>0.095</td>\n",
       "      <td>-2.248</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.976</td>\n",
       "      <td>-0.658</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.602</td>\n",
       "      <td>-3.426</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.180</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-0.295</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.935</td>\n",
       "      <td>1.189</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.117</td>\n",
       "      <td>1.252</td>\n",
       "      <td>0.841</td>\n",
       "      <td>1.092</td>\n",
       "      <td>1.627</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.444</td>\n",
       "      <td>0.722</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.961</td>\n",
       "      <td>1.083</td>\n",
       "      <td>1.803</td>\n",
       "      <td>-1.211</td>\n",
       "      <td>-3.421</td>\n",
       "      <td>0.369</td>\n",
       "      <td>1.591</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.108</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-1.194</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.425</td>\n",
       "      <td>-1.213</td>\n",
       "      <td>-2.310</td>\n",
       "      <td>2.212</td>\n",
       "      <td>-1.937</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-1.627</td>\n",
       "      <td>-0.599</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-1.688</td>\n",
       "      <td>2.882</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-2.353</td>\n",
       "      <td>0.707</td>\n",
       "      <td>2.975</td>\n",
       "      <td>1.193</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>-1.921</td>\n",
       "      <td>3.139</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-1.711</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.980</td>\n",
       "      <td>-2.486</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-1.923</td>\n",
       "      <td>-1.775</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.774</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-2.236</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>-2.894</td>\n",
       "      <td>1.554</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>-0.807</td>\n",
       "      <td>-2.202</td>\n",
       "      <td>-3.653</td>\n",
       "      <td>-3.094</td>\n",
       "      <td>-2.277</td>\n",
       "      <td>4.612</td>\n",
       "      <td>8.400</td>\n",
       "      <td>-2.035</td>\n",
       "      <td>-5.050</td>\n",
       "      <td>-4.587</td>\n",
       "      <td>-2.166</td>\n",
       "      <td>-2.416</td>\n",
       "      <td>-2.309</td>\n",
       "      <td>-2.222</td>\n",
       "      <td>-2.201</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-5.010</td>\n",
       "      <td>5.650</td>\n",
       "      <td>-6.765</td>\n",
       "      <td>6.384</td>\n",
       "      <td>5.231</td>\n",
       "      <td>3.683</td>\n",
       "      <td>-4.704</td>\n",
       "      <td>-2.668</td>\n",
       "      <td>-2.439</td>\n",
       "      <td>-6.026</td>\n",
       "      <td>-0.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.488</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-24.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-83.560</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-25.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.288</td>\n",
       "      <td>3.221</td>\n",
       "      <td>-3.642</td>\n",
       "      <td>0.596</td>\n",
       "      <td>44.979</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-3.293</td>\n",
       "      <td>12.890</td>\n",
       "      <td>-2.602</td>\n",
       "      <td>-19.812</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-80.701</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-1.683</td>\n",
       "      <td>-22.505</td>\n",
       "      <td>-1.580</td>\n",
       "      <td>1.099</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.923</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.537</td>\n",
       "      <td>46.332</td>\n",
       "      <td>-0.745</td>\n",
       "      <td>0.190</td>\n",
       "      <td>13.396</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>-24.219</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-83.140</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-25.133</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>0.969</td>\n",
       "      <td>-0.477</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.618</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>1.497</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.355</td>\n",
       "      <td>0.561</td>\n",
       "      <td>1.702</td>\n",
       "      <td>-1.210</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.566</td>\n",
       "      <td>-1.263</td>\n",
       "      <td>0.736</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>1.132</td>\n",
       "      <td>-1.453</td>\n",
       "      <td>1.065</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>1.298</td>\n",
       "      <td>1.362</td>\n",
       "      <td>1.132</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.059</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.532</td>\n",
       "      <td>1.352</td>\n",
       "      <td>-3.296</td>\n",
       "      <td>2.362</td>\n",
       "      <td>1.215</td>\n",
       "      <td>0.911</td>\n",
       "      <td>1.035</td>\n",
       "      <td>2.004</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.588</td>\n",
       "      <td>-1.796</td>\n",
       "      <td>0.638</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>2.628</td>\n",
       "      <td>-1.320</td>\n",
       "      <td>2.298</td>\n",
       "      <td>2.701</td>\n",
       "      <td>0.666</td>\n",
       "      <td>1.356</td>\n",
       "      <td>1.085</td>\n",
       "      <td>0.356</td>\n",
       "      <td>1.490</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-2.306</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.823</td>\n",
       "      <td>3.102</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1.616</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>0.257</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.596</td>\n",
       "      <td>1.820</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>2.603</td>\n",
       "      <td>2.782</td>\n",
       "      <td>-2.512</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>1.433</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>1.317</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>1.997</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.379</td>\n",
       "      <td>1.727</td>\n",
       "      <td>2.484</td>\n",
       "      <td>-2.937</td>\n",
       "      <td>2.802</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>-1.919</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-1.205</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>-1.944</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>-2.218</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.197</td>\n",
       "      <td>1.556</td>\n",
       "      <td>-2.002</td>\n",
       "      <td>-1.651</td>\n",
       "      <td>0.401</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-1.543</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.382</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-1.294</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-1.796</td>\n",
       "      <td>-2.018</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-2.526</td>\n",
       "      <td>0.409</td>\n",
       "      <td>1.121</td>\n",
       "      <td>-1.390</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.422</td>\n",
       "      <td>-5.712</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>-1.426</td>\n",
       "      <td>-5.051</td>\n",
       "      <td>4.835</td>\n",
       "      <td>4.758</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>-1.140</td>\n",
       "      <td>-1.213</td>\n",
       "      <td>-2.527</td>\n",
       "      <td>-1.255</td>\n",
       "      <td>-5.222</td>\n",
       "      <td>-1.332</td>\n",
       "      <td>-1.116</td>\n",
       "      <td>5.989</td>\n",
       "      <td>-4.472</td>\n",
       "      <td>1.690</td>\n",
       "      <td>-4.979</td>\n",
       "      <td>3.695</td>\n",
       "      <td>4.583</td>\n",
       "      <td>-4.741</td>\n",
       "      <td>-1.126</td>\n",
       "      <td>-0.981</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-1.144</td>\n",
       "      <td>-0.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>71.763</td>\n",
       "      <td>35.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.671</td>\n",
       "      <td>-12.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-67.927</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-21.386</td>\n",
       "      <td>38.416</td>\n",
       "      <td>-19.454</td>\n",
       "      <td>18.104</td>\n",
       "      <td>-16.339</td>\n",
       "      <td>-3.620</td>\n",
       "      <td>7.860</td>\n",
       "      <td>-3.157</td>\n",
       "      <td>3.148</td>\n",
       "      <td>-18.956</td>\n",
       "      <td>43.026</td>\n",
       "      <td>49.129</td>\n",
       "      <td>18.726</td>\n",
       "      <td>1.607</td>\n",
       "      <td>-25.328</td>\n",
       "      <td>52.248</td>\n",
       "      <td>-6.672</td>\n",
       "      <td>1.631</td>\n",
       "      <td>-46.037</td>\n",
       "      <td>2.891</td>\n",
       "      <td>-19.140</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.555</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>-0.647</td>\n",
       "      <td>-1.333</td>\n",
       "      <td>71.653</td>\n",
       "      <td>35.994</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.541</td>\n",
       "      <td>49.627</td>\n",
       "      <td>-12.814</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-66.974</td>\n",
       "      <td>0.927</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.954</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.193</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-3.070</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>1.230</td>\n",
       "      <td>1.292</td>\n",
       "      <td>1.083</td>\n",
       "      <td>0.885</td>\n",
       "      <td>1.186</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.087</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.909</td>\n",
       "      <td>2.020</td>\n",
       "      <td>1.374</td>\n",
       "      <td>2.172</td>\n",
       "      <td>-2.640</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>1.392</td>\n",
       "      <td>1.797</td>\n",
       "      <td>2.153</td>\n",
       "      <td>0.024</td>\n",
       "      <td>2.756</td>\n",
       "      <td>1.757</td>\n",
       "      <td>-1.679</td>\n",
       "      <td>-1.594</td>\n",
       "      <td>-1.156</td>\n",
       "      <td>-1.591</td>\n",
       "      <td>1.338</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.819</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.372</td>\n",
       "      <td>2.039</td>\n",
       "      <td>1.866</td>\n",
       "      <td>-2.924</td>\n",
       "      <td>2.264</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-2.235</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>-0.911</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-1.437</td>\n",
       "      <td>-3.147</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.812</td>\n",
       "      <td>2.462</td>\n",
       "      <td>2.369</td>\n",
       "      <td>2.153</td>\n",
       "      <td>2.330</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.815</td>\n",
       "      <td>2.017</td>\n",
       "      <td>1.861</td>\n",
       "      <td>-0.906</td>\n",
       "      <td>0.849</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>0.228</td>\n",
       "      <td>1.692</td>\n",
       "      <td>1.110</td>\n",
       "      <td>1.361</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>1.717</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.483</td>\n",
       "      <td>1.487</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.877</td>\n",
       "      <td>1.113</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.203</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.089</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>0.808</td>\n",
       "      <td>-2.258</td>\n",
       "      <td>2.466</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.538</td>\n",
       "      <td>1.715</td>\n",
       "      <td>1.412</td>\n",
       "      <td>1.433</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-2.702</td>\n",
       "      <td>1.410</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>1.817</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>1.656</td>\n",
       "      <td>2.049</td>\n",
       "      <td>1.561</td>\n",
       "      <td>1.692</td>\n",
       "      <td>1.866</td>\n",
       "      <td>2.361</td>\n",
       "      <td>1.957</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-1.136</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>1.077</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.536</td>\n",
       "      <td>3.047</td>\n",
       "      <td>0.073</td>\n",
       "      <td>1.227</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.941</td>\n",
       "      <td>1.072</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>1.089</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.052</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.144</td>\n",
       "      <td>-0.914</td>\n",
       "      <td>1.146</td>\n",
       "      <td>-1.841</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-5.097</td>\n",
       "      <td>-5.330</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.182</td>\n",
       "      <td>1.798</td>\n",
       "      <td>1.604</td>\n",
       "      <td>1.742</td>\n",
       "      <td>-8.189</td>\n",
       "      <td>-5.188</td>\n",
       "      <td>1.617</td>\n",
       "      <td>-2.508</td>\n",
       "      <td>-2.531</td>\n",
       "      <td>-2.399</td>\n",
       "      <td>-2.482</td>\n",
       "      <td>1.538</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>2.153</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.730</td>\n",
       "      <td>1.709</td>\n",
       "      <td>1.501</td>\n",
       "      <td>-5.310</td>\n",
       "      <td>1.718</td>\n",
       "      <td>-5.299</td>\n",
       "      <td>-2.760</td>\n",
       "      <td>0.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-4.682</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-55.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>37.671</td>\n",
       "      <td>94.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>69.192</td>\n",
       "      <td>-9.112</td>\n",
       "      <td>-25.347</td>\n",
       "      <td>-1.660</td>\n",
       "      <td>-26.021</td>\n",
       "      <td>-1.442</td>\n",
       "      <td>0.539</td>\n",
       "      <td>1.165</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>-0.713</td>\n",
       "      <td>37.419</td>\n",
       "      <td>-0.484</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.162</td>\n",
       "      <td>161.145</td>\n",
       "      <td>2.299</td>\n",
       "      <td>1.837</td>\n",
       "      <td>1.588</td>\n",
       "      <td>0.283</td>\n",
       "      <td>-1.426</td>\n",
       "      <td>-5.125</td>\n",
       "      <td>3.805</td>\n",
       "      <td>-1.897</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-56.041</td>\n",
       "      <td>1.545</td>\n",
       "      <td>-3.285</td>\n",
       "      <td>0.656</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>2.111</td>\n",
       "      <td>0.505</td>\n",
       "      <td>-2.806</td>\n",
       "      <td>-1.254</td>\n",
       "      <td>1.076</td>\n",
       "      <td>38.032</td>\n",
       "      <td>94.052</td>\n",
       "      <td>-2.193</td>\n",
       "      <td>-0.472</td>\n",
       "      <td>1.340</td>\n",
       "      <td>0.887</td>\n",
       "      <td>69.183</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-3.145</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.014</td>\n",
       "      <td>3.302</td>\n",
       "      <td>3.203</td>\n",
       "      <td>3.465</td>\n",
       "      <td>-3.349</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>3.607</td>\n",
       "      <td>0.265</td>\n",
       "      <td>-2.998</td>\n",
       "      <td>-2.760</td>\n",
       "      <td>0.171</td>\n",
       "      <td>3.248</td>\n",
       "      <td>0.198</td>\n",
       "      <td>3.018</td>\n",
       "      <td>3.456</td>\n",
       "      <td>3.357</td>\n",
       "      <td>3.446</td>\n",
       "      <td>3.272</td>\n",
       "      <td>-2.564</td>\n",
       "      <td>3.303</td>\n",
       "      <td>-2.545</td>\n",
       "      <td>-3.908</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>1.304</td>\n",
       "      <td>1.729</td>\n",
       "      <td>1.954</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.172</td>\n",
       "      <td>1.660</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.303</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>1.271</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>1.710</td>\n",
       "      <td>1.998</td>\n",
       "      <td>1.300</td>\n",
       "      <td>1.939</td>\n",
       "      <td>1.786</td>\n",
       "      <td>0.318</td>\n",
       "      <td>2.120</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>1.217</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.634</td>\n",
       "      <td>1.357</td>\n",
       "      <td>1.185</td>\n",
       "      <td>1.130</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.298</td>\n",
       "      <td>1.169</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.260</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.642</td>\n",
       "      <td>1.407</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.749</td>\n",
       "      <td>1.429</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.086</td>\n",
       "      <td>1.441</td>\n",
       "      <td>0.169</td>\n",
       "      <td>1.742</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.846</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.934</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.237</td>\n",
       "      <td>1.420</td>\n",
       "      <td>0.229</td>\n",
       "      <td>1.445</td>\n",
       "      <td>1.967</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>1.777</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.347</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>1.954</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>1.533</td>\n",
       "      <td>1.613</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>0.735</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.646</td>\n",
       "      <td>0.663</td>\n",
       "      <td>1.267</td>\n",
       "      <td>2.202</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>1.583</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.031</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.673</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>1.248</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>1.600</td>\n",
       "      <td>1.491</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>1.070</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.127</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.104</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.329</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1.297</td>\n",
       "      <td>1.987</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>-3.568</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>2.520</td>\n",
       "      <td>2.442</td>\n",
       "      <td>2.395</td>\n",
       "      <td>-3.762</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>2.400</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-3.477</td>\n",
       "      <td>-3.329</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>2.335</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>2.831</td>\n",
       "      <td>2.156</td>\n",
       "      <td>2.591</td>\n",
       "      <td>2.432</td>\n",
       "      <td>2.266</td>\n",
       "      <td>-3.075</td>\n",
       "      <td>2.335</td>\n",
       "      <td>-2.558</td>\n",
       "      <td>-4.223</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           seed  00000-target  00001-target  00002-target  00010-target  \\\n",
       "666  1373158606         0.000         0.000         0.000         0.000   \n",
       "129  1373158606         0.000         0.000         0.000         0.000   \n",
       "784  1373158606         0.000         0.000         0.000         0.000   \n",
       "152  1373158606         0.000         0.000         0.000         0.000   \n",
       "920  1373158606        -4.682         0.000         0.000         0.000   \n",
       "\n",
       "     00011-target  00020-target  00100-target  00101-target  00110-target  \\\n",
       "666       -84.503         0.000         0.000         0.000         0.000   \n",
       "129         0.000         0.000         0.000         0.000         0.000   \n",
       "784        45.889         0.000         0.000        13.488         0.000   \n",
       "152         0.000         0.000         0.000         0.000         0.000   \n",
       "920       -55.131         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     00200-target  01000-target  01001-target  01010-target  01100-target  \\\n",
       "666         0.000         0.000         0.000         0.000         0.000   \n",
       "129         0.000         0.000        79.712         0.000         0.000   \n",
       "784       -24.402         0.000         0.000       -83.560         0.000   \n",
       "152         0.000         0.000        71.763        35.388         0.000   \n",
       "920         0.000         0.000         0.000         0.000         0.000   \n",
       "\n",
       "     02000-target  10000-target  10001-target  10010-target  10100-target  \\\n",
       "666         0.000         0.000        30.395        -2.612         0.000   \n",
       "129         0.000         0.000       -39.417       -86.861         0.000   \n",
       "784         0.000         0.000       -25.515         0.000         0.000   \n",
       "152         0.000        49.671       -12.158         0.000       -67.927   \n",
       "920        37.671        94.206         0.000         0.000         0.000   \n",
       "\n",
       "     11000-target  20000-target  00000-lstsq_lambda  00001-lstsq_lambda  \\\n",
       "666        83.254       -22.327              -0.548              -0.237   \n",
       "129       -50.679        -3.185               0.602              -1.998   \n",
       "784         0.000         0.000               0.288               3.221   \n",
       "152         0.000         0.000             -21.386              38.416   \n",
       "920         0.000        69.192              -9.112             -25.347   \n",
       "\n",
       "     00002-lstsq_lambda  00010-lstsq_lambda  00011-lstsq_lambda  \\\n",
       "666              -0.565               0.332             -83.661   \n",
       "129               1.450              -0.823               2.220   \n",
       "784              -3.642               0.596              44.979   \n",
       "152             -19.454              18.104             -16.339   \n",
       "920              -1.660             -26.021              -1.442   \n",
       "\n",
       "     00020-lstsq_lambda  00100-lstsq_lambda  00101-lstsq_lambda  \\\n",
       "666              -0.406              -0.321               0.253   \n",
       "129              -0.804               0.827              -2.848   \n",
       "784               0.462              -3.293              12.890   \n",
       "152              -3.620               7.860              -3.157   \n",
       "920               0.539               1.165              -1.700   \n",
       "\n",
       "     00110-lstsq_lambda  00200-lstsq_lambda  01000-lstsq_lambda  \\\n",
       "666               0.829              -0.622              -0.527   \n",
       "129              -2.148               1.287               3.238   \n",
       "784              -2.602             -19.812               0.031   \n",
       "152               3.148             -18.956              43.026   \n",
       "920              -0.789              -0.713              37.419   \n",
       "\n",
       "     01001-lstsq_lambda  01010-lstsq_lambda  01100-lstsq_lambda  \\\n",
       "666               0.179               0.636               1.024   \n",
       "129              78.187              -0.534              -0.116   \n",
       "784              -0.294             -80.701              -0.683   \n",
       "152              49.129              18.726               1.607   \n",
       "920              -0.484              -0.250              -0.331   \n",
       "\n",
       "     02000-lstsq_lambda  10000-lstsq_lambda  10001-lstsq_lambda  \\\n",
       "666               0.331               3.536              29.679   \n",
       "129              -1.562              -4.531             -36.637   \n",
       "784              -0.671              -1.683             -22.505   \n",
       "152             -25.328              52.248              -6.672   \n",
       "920               0.162             161.145               2.299   \n",
       "\n",
       "     10010-lstsq_lambda  10100-lstsq_lambda  11000-lstsq_lambda  \\\n",
       "666              -4.243              -0.844              80.918   \n",
       "129             -84.877               0.726             -50.371   \n",
       "784              -1.580               1.099               0.074   \n",
       "152               1.631             -46.037               2.891   \n",
       "920               1.837               1.588               0.283   \n",
       "\n",
       "     20000-lstsq_lambda  00000-lstsq_target  00001-lstsq_target  \\\n",
       "666             -22.610               0.000               0.787   \n",
       "129              -1.831               1.032              -1.998   \n",
       "784               0.166               0.021              -0.923   \n",
       "152             -19.140              -0.105               0.555   \n",
       "920              -1.426              -5.125               3.805   \n",
       "\n",
       "     00002-lstsq_target  00010-lstsq_target  00011-lstsq_target  \\\n",
       "666              -0.419              -0.784             -85.256   \n",
       "129               1.423              -0.048               1.377   \n",
       "784               0.469               0.537              46.332   \n",
       "152              -0.326               0.480               0.023   \n",
       "920              -1.897               0.859             -56.041   \n",
       "\n",
       "     00020-lstsq_target  00100-lstsq_target  00101-lstsq_target  \\\n",
       "666               0.836               0.070               0.266   \n",
       "129              -0.737              -0.648              -1.497   \n",
       "784              -0.745               0.190              13.396   \n",
       "152              -0.223               0.643               0.234   \n",
       "920               1.545              -3.285               0.656   \n",
       "\n",
       "     00110-lstsq_target  00200-lstsq_target  01000-lstsq_target  \\\n",
       "666               0.042              -0.559              -0.229   \n",
       "129              -0.843               1.490              -0.728   \n",
       "784              -0.433             -24.219               0.757   \n",
       "152              -0.812              -0.647              -1.333   \n",
       "920              -0.784               2.111               0.505   \n",
       "\n",
       "     01001-lstsq_target  01010-lstsq_target  01100-lstsq_target  \\\n",
       "666              -0.252               0.021               0.456   \n",
       "129              80.169               0.158               0.953   \n",
       "784               0.091             -83.140              -0.851   \n",
       "152              71.653              35.994               0.137   \n",
       "920              -2.806              -1.254               1.076   \n",
       "\n",
       "     02000-lstsq_target  10000-lstsq_target  10001-lstsq_target  \\\n",
       "666               0.364               0.464              30.396   \n",
       "129              -0.038              -2.273             -38.701   \n",
       "784              -0.446              -0.366             -25.133   \n",
       "152               0.541              49.627             -12.814   \n",
       "920              38.032              94.052              -2.193   \n",
       "\n",
       "     10010-lstsq_target  10100-lstsq_target  11000-lstsq_target  \\\n",
       "666              -2.338               0.184              82.645   \n",
       "129             -86.635               0.328             -49.820   \n",
       "784              -0.099               0.969              -0.477   \n",
       "152              -0.014             -66.974               0.927   \n",
       "920              -0.472               1.340               0.887   \n",
       "\n",
       "     20000-lstsq_target   wb_0   wb_1  wb_2   wb_3  wb_4   wb_5   wb_6   wb_7  \\\n",
       "666             -22.637 -0.593 -1.835 3.762  0.145 0.332  1.907 -1.257 -3.142   \n",
       "129              -1.898  2.147  1.825 2.379  2.339 0.109 -4.335  1.959  2.135   \n",
       "784              -0.004 -0.926  0.955 1.618 -1.110 1.497  0.833  1.492  0.917   \n",
       "152              -0.553 -0.954  0.560 0.193 -0.001 0.987  0.887  0.679 -3.070   \n",
       "920              69.183 -0.077 -3.145 0.201  0.014 3.302  3.203  3.465 -3.349   \n",
       "\n",
       "      wb_8  wb_9  wb_10  wb_11  wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  \\\n",
       "666 -3.125 0.390  2.886 -0.507 -0.154  0.471  1.971  0.697  1.630  2.103   \n",
       "129  1.810 2.736  2.720  2.040  2.013  2.555  0.004  1.937 -3.903  2.415   \n",
       "784  1.355 0.561  1.702 -1.210  1.207  1.566 -1.263  0.736 -0.553  1.132   \n",
       "152 -0.013 1.230  1.292  1.083  0.885  1.186  0.916  0.206  0.778  1.087   \n",
       "920 -0.002 3.607  0.265 -2.998 -2.760  0.171  3.248  0.198  3.018  3.456   \n",
       "\n",
       "     wb_18  wb_19  wb_20  wb_21  wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  \\\n",
       "666  2.036  2.182  1.923  0.002  1.954 -2.057 -1.334 -0.283  0.654 -2.769   \n",
       "129  3.063  2.731 -3.073  2.219  1.896  1.337  0.950  0.358  0.094 -0.994   \n",
       "784 -1.453  1.065 -0.422  1.298  1.362  1.132  0.932  1.059  0.763  0.532   \n",
       "152  1.042  1.092  0.908  0.045  0.909  2.020  1.374  2.172 -2.640 -0.354   \n",
       "920  3.357  3.446  3.272 -2.564  3.303 -2.545 -3.908 -0.145  0.384 -0.341   \n",
       "\n",
       "     wb_28  wb_29  wb_30  wb_31  wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  \\\n",
       "666 -1.077  0.985  1.177  0.251  1.953  1.687 -1.064 -2.366 -0.407 -0.142   \n",
       "129  0.099 -2.153  2.302  1.394  0.451  1.758  0.520 -0.532 -0.233 -0.538   \n",
       "784  1.352 -3.296  2.362  1.215  0.911  1.035  2.004  0.510  0.638  0.657   \n",
       "152 -0.310  1.392  1.797  2.153  0.024  2.756  1.757 -1.679 -1.594 -1.156   \n",
       "920 -0.305  1.304  1.729  1.954  0.395  0.172  1.660 -0.435  0.207  0.303   \n",
       "\n",
       "     wb_38  wb_39  wb_40  wb_41  wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  \\\n",
       "666 -1.041  0.830 -1.175  1.248  2.608  0.793  2.652  1.775 -0.371  2.268   \n",
       "129 -0.571 -0.386  1.499  2.594  0.095 -2.248  0.127  2.976 -0.658  1.031   \n",
       "784  0.588 -1.796  0.638 -0.439  2.628 -1.320  2.298  2.701  0.666  1.356   \n",
       "152 -1.591  1.338  0.005  1.819  2.076  1.372  2.039  1.866 -2.924  2.264   \n",
       "920 -0.289  1.271 -0.003  1.710  1.998  1.300  1.939  1.786  0.318  2.120   \n",
       "\n",
       "     wb_48  wb_49  wb_50  wb_51  wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  \\\n",
       "666  1.294 -0.030  0.350  0.096 -0.130  0.284 -0.043  0.271 -0.076 -0.102   \n",
       "129  1.602 -3.426  0.293  0.344 -0.223  0.372  0.852  0.010  0.108 -0.287   \n",
       "784  1.085  0.356  1.490  0.579  0.197 -2.306 -0.173 -0.053  0.933  0.030   \n",
       "152  0.620 -2.235 -0.903  0.183 -0.441 -0.182 -0.911 -0.140 -1.437 -3.147   \n",
       "920  0.529  0.340 -0.099  1.217 -0.445 -0.206  0.634  1.357  1.185  1.130   \n",
       "\n",
       "     wb_58  wb_59  wb_60  wb_61  wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  \\\n",
       "666 -0.029 -0.168  0.154  0.696  0.175  0.274  0.698  0.008 -0.036  0.114   \n",
       "129  0.180 -0.441  0.511  0.722  0.461  0.406  0.115 -0.265  0.484 -0.295   \n",
       "784  0.384  0.208  0.823  3.102  0.756  0.653  0.283  1.616  0.030 -0.881   \n",
       "152  0.216 -0.812  2.462  2.369  2.153  2.330 -0.278 -0.235 -0.821 -0.202   \n",
       "920 -0.147  0.877  0.009  1.298  1.169  0.026  1.260 -0.239  0.642  1.407   \n",
       "\n",
       "     wb_68  wb_69  wb_70  wb_71  wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  \\\n",
       "666  0.601 -0.043  0.008 -0.162 -0.186  0.056  0.325  1.065  1.557  0.141   \n",
       "129 -0.018  0.648  0.556  0.175 -0.059  0.395  0.341  0.935  1.189  1.078   \n",
       "784  0.257 -0.033  0.252 -0.008  0.564  0.850  0.596  1.820  0.182 -0.464   \n",
       "152 -0.500 -0.846 -0.192 -0.012 -0.815  2.017  1.861 -0.906  0.849 -0.455   \n",
       "920  0.998  0.749  1.429  0.775  1.037  1.086  1.441  0.169  1.742 -0.460   \n",
       "\n",
       "     wb_78  wb_79  wb_80  wb_81  wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  \\\n",
       "666  1.151  3.562 -1.325  1.673  1.245  0.981  1.809  1.410  1.045 -0.332   \n",
       "129  1.117  1.252  0.841  1.092  1.627  1.046  1.444  0.722  1.008  0.308   \n",
       "784  2.603  2.782 -2.512  0.034  0.030 -0.260  1.433 -0.183  1.317 -0.596   \n",
       "152  0.228  1.692  1.110  1.361  0.125 -0.996  1.717  0.500  0.540  0.128   \n",
       "920  0.226  0.993  0.431  0.375  1.846 -0.139  0.934 -0.270  1.739  1.300   \n",
       "\n",
       "     wb_88  wb_89  wb_90  wb_91  wb_92  wb_93  wb_94  wb_95  wb_96  wb_97  \\\n",
       "666  1.494 -0.509  2.059 -2.802 -0.311 -0.613 -0.137 -2.069  2.236 -1.671   \n",
       "129  0.691  0.046  0.961  1.083  1.803 -1.211 -3.421  0.369  1.591  1.280   \n",
       "784 -0.277  1.997  0.669  0.379  1.727  2.484 -2.937  2.802 -0.093 -0.454   \n",
       "152  0.483  1.487  0.021  0.877  1.113  1.280  1.203  0.975  1.054  1.089   \n",
       "920 -0.210  0.796  0.005  0.211  0.381  0.599  0.461  0.237  1.420  0.229   \n",
       "\n",
       "     wb_98  wb_99  wb_100  wb_101  wb_102  wb_103  wb_104  wb_105  wb_106  \\\n",
       "666  1.575  0.763   0.039   1.678  -0.478   0.473  -3.638   0.262   1.916   \n",
       "129  1.108  0.684  -1.194   0.006   1.425  -1.213  -2.310   2.212  -1.937   \n",
       "784  0.189 -0.422  -1.919   0.020   0.046  -1.205  -0.614  -1.944  -0.532   \n",
       "152 -0.568  0.808  -2.258   2.466  -0.034  -0.538   1.715   1.412   1.433   \n",
       "920  1.445  1.967  -0.367   1.777  -0.029  -0.530   0.622   0.347  -0.110   \n",
       "\n",
       "     wb_107  wb_108  wb_109  wb_110  wb_111  wb_112  wb_113  wb_114  wb_115  \\\n",
       "666   1.349   1.445   1.164   0.670  -0.099   0.048   1.009   0.566   1.590   \n",
       "129   0.328  -1.627  -0.599  -0.909  -1.028  -0.436  -0.606   0.033  -1.688   \n",
       "784   0.326  -0.146  -0.817  -0.597  -2.218   0.112  -0.222   2.100   0.370   \n",
       "152  -0.036  -2.702   1.410  -0.112  -0.284  -0.215  -0.070   1.817  -0.400   \n",
       "920   1.954  -0.125   0.211  -0.521   1.533   1.613  -0.161   0.735  -0.401   \n",
       "\n",
       "     wb_116  wb_117  wb_118  wb_119  wb_120  wb_121  wb_122  wb_123  wb_124  \\\n",
       "666   2.121   0.318   0.472  -0.329   1.448   2.238   1.731   1.792   0.904   \n",
       "129   2.882   0.168  -2.353   0.707   2.975   1.193  -0.675  -1.921   3.139   \n",
       "784   0.970   0.197   1.556  -2.002  -1.651   0.401  -0.048  -0.364   0.440   \n",
       "152   1.656   2.049   1.561   1.692   1.866   2.361   1.957  -0.071   0.810   \n",
       "920   0.609   0.913   0.493   0.545   0.725   1.646   0.663   1.267   2.202   \n",
       "\n",
       "     wb_125  wb_126  wb_127  wb_128  wb_129  wb_130  wb_131  wb_132  wb_133  \\\n",
       "666   0.811  -0.709  -1.146   0.320  -0.773  -0.015  -1.662  -0.964  -1.927   \n",
       "129   0.373  -1.711  -0.702   0.490   0.980  -2.486  -0.294  -1.923  -1.775   \n",
       "784  -1.543   0.272   0.382  -0.022  -0.002  -0.139   0.104   0.311   0.232   \n",
       "152   0.506  -1.136  -0.093  -0.163   1.077   1.045   0.536   3.047   0.073   \n",
       "920  -0.078   1.583  -0.086  -0.163   1.071   1.031   0.969   1.673  -0.134   \n",
       "\n",
       "     wb_134  wb_135  wb_136  wb_137  wb_138  wb_139  wb_140  wb_141  wb_142  \\\n",
       "666  -0.947  -0.423   0.891  -0.143  -0.711   0.235  -2.199  -1.741  -2.947   \n",
       "129  -0.913   1.004   0.749   0.700   0.774  -0.167  -2.236  -0.586  -2.894   \n",
       "784  -1.294   0.121  -0.236   0.318   0.245  -1.796  -2.018   0.982  -2.526   \n",
       "152   1.227   0.993   0.957   0.865   0.941   1.072  -0.164   1.089   1.112   \n",
       "920   1.248  -0.147   1.600   1.491  -0.112   1.070  -0.174   1.047   1.127   \n",
       "\n",
       "     wb_143  wb_144  wb_145  wb_146  wb_147  wb_148  wb_149  wb_150  wb_151  \\\n",
       "666   0.437  -1.635  -0.367  -1.503  -1.179  -0.493   0.394  -1.226  -2.844   \n",
       "129   1.554  -0.636   0.221  -1.995  -1.378  -0.696  -0.807  -2.202  -3.653   \n",
       "784   0.409   1.121  -1.390   0.327   0.186   0.170   0.422  -5.712  -1.001   \n",
       "152   1.052   1.101   1.144  -0.914   1.146  -1.841   0.386  -5.097  -5.330   \n",
       "920   1.037   1.104   1.158   1.329   1.192   1.297   1.987  -0.223  -3.568   \n",
       "\n",
       "     wb_152  wb_153  wb_154  wb_155  wb_156  wb_157  wb_158  wb_159  wb_160  \\\n",
       "666  -6.259  -1.095   6.530   1.602  -4.204  -4.386  -6.729  -2.166  -4.521   \n",
       "129  -3.094  -2.277   4.612   8.400  -2.035  -5.050  -4.587  -2.166  -2.416   \n",
       "784  -1.426  -5.051   4.835   4.758  -0.858  -1.140  -1.213  -2.527  -1.255   \n",
       "152  -0.252  -0.182   1.798   1.604   1.742  -8.189  -5.188   1.617  -2.508   \n",
       "920  -0.261  -0.197   2.520   2.442   2.395  -3.762  -0.262   2.400  -0.050   \n",
       "\n",
       "     wb_161  wb_162  wb_163  wb_164  wb_165  wb_166  wb_167  wb_168  wb_169  \\\n",
       "666  -1.313  -0.309  -2.047   1.368  -5.941   5.493   6.067   1.784   3.611   \n",
       "129  -2.309  -2.222  -2.201   0.086  -5.010   5.650  -6.765   6.384   5.231   \n",
       "784  -5.222  -1.332  -1.116   5.989  -4.472   1.690  -4.979   3.695   4.583   \n",
       "152  -2.531  -2.399  -2.482   1.538  -0.157   2.153   1.420   1.730   1.709   \n",
       "920  -3.477  -3.329  -0.088   2.335  -0.156   2.831   2.156   2.591   2.432   \n",
       "\n",
       "     wb_170  wb_171  wb_172  wb_173  wb_174  wb_175  \n",
       "666   2.411  -4.478   3.272  -3.387  -1.249   0.038  \n",
       "129   3.683  -4.704  -2.668  -2.439  -6.026  -0.794  \n",
       "784  -4.741  -1.126  -0.981  -0.880  -1.144  -0.415  \n",
       "152   1.501  -5.310   1.718  -5.299  -2.760   0.881  \n",
       "920   2.266  -3.075   2.335  -2.558  -4.223   0.723  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------- TRAINING INTERPRETATION NET -----------------------------------------------\n",
      "[<function inet_coefficient_loss_wrapper.<locals>.inet_coefficient_loss at 0x7fd2954a9dc0>, <function inet_lambda_fv_loss_wrapper.<locals>.inet_lambda_fv_loss at 0x7fd2954a9f70>, <function inet_coefficient_loss_wrapper.<locals>.inet_coefficient_loss at 0x7fd2af0df040>, <function inet_lambda_fv_loss_wrapper.<locals>.inet_lambda_fv_loss at 0x7fd2af0df0d0>]\n",
      "Epoch 1/500\n",
      "40/40 [==============================] - 20s 336ms/step - loss: 35.9351 - r2_inet_coefficient_loss: 0.1610 - r2_inet_lambda_fv_loss: 1.6137 - mae_inet_coefficient_loss: 21.0015 - mae_inet_lambda_fv_loss: 35.9325 - val_loss: 25.3585 - val_r2_inet_coefficient_loss: 0.4511 - val_r2_inet_lambda_fv_loss: 0.2473 - val_mae_inet_coefficient_loss: 26.1188 - val_mae_inet_lambda_fv_loss: 25.3519\n",
      "Epoch 2/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 25.7839 - r2_inet_coefficient_loss: 0.4173 - r2_inet_lambda_fv_loss: 0.2355 - mae_inet_coefficient_loss: 26.3016 - mae_inet_lambda_fv_loss: 25.7810 - val_loss: 22.7862 - val_r2_inet_coefficient_loss: 0.4191 - val_r2_inet_lambda_fv_loss: 0.1036 - val_mae_inet_coefficient_loss: 27.0563 - val_mae_inet_lambda_fv_loss: 22.8539\n",
      "Epoch 3/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 22.7600 - r2_inet_coefficient_loss: 0.4685 - r2_inet_lambda_fv_loss: 0.0531 - mae_inet_coefficient_loss: 27.9304 - mae_inet_lambda_fv_loss: 22.7597 - val_loss: 21.4942 - val_r2_inet_coefficient_loss: 0.8047 - val_r2_inet_lambda_fv_loss: 5.2118e-04 - val_mae_inet_coefficient_loss: 29.2212 - val_mae_inet_lambda_fv_loss: 21.5039\n",
      "Epoch 4/500\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 22.2215 - r2_inet_coefficient_loss: 0.8699 - r2_inet_lambda_fv_loss: 0.0399 - mae_inet_coefficient_loss: 29.5273 - mae_inet_lambda_fv_loss: 22.2236 - val_loss: 21.8544 - val_r2_inet_coefficient_loss: 1.0708 - val_r2_inet_lambda_fv_loss: 0.0602 - val_mae_inet_coefficient_loss: 30.3333 - val_mae_inet_lambda_fv_loss: 21.8816\n",
      "Epoch 5/500\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 22.4602 - r2_inet_coefficient_loss: 1.0811 - r2_inet_lambda_fv_loss: 0.0629 - mae_inet_coefficient_loss: 30.5576 - mae_inet_lambda_fv_loss: 22.4668 - val_loss: 22.1139 - val_r2_inet_coefficient_loss: 1.2574 - val_r2_inet_lambda_fv_loss: 0.0363 - val_mae_inet_coefficient_loss: 30.2966 - val_mae_inet_lambda_fv_loss: 22.1322\n",
      "Epoch 6/500\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 22.7406 - r2_inet_coefficient_loss: 1.1527 - r2_inet_lambda_fv_loss: 0.0883 - mae_inet_coefficient_loss: 30.9874 - mae_inet_lambda_fv_loss: 22.7355 - val_loss: 22.0137 - val_r2_inet_coefficient_loss: 1.3917 - val_r2_inet_lambda_fv_loss: 0.0381 - val_mae_inet_coefficient_loss: 30.6863 - val_mae_inet_lambda_fv_loss: 22.0173\n",
      "Epoch 7/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 22.5163 - r2_inet_coefficient_loss: 1.2291 - r2_inet_lambda_fv_loss: 0.0815 - mae_inet_coefficient_loss: 31.3969 - mae_inet_lambda_fv_loss: 22.5146 - val_loss: 21.8656 - val_r2_inet_coefficient_loss: 1.6239 - val_r2_inet_lambda_fv_loss: 0.0314 - val_mae_inet_coefficient_loss: 32.3726 - val_mae_inet_lambda_fv_loss: 21.9299\n",
      "Epoch 8/500\n",
      "40/40 [==============================] - 12s 299ms/step - loss: 22.2705 - r2_inet_coefficient_loss: 1.4104 - r2_inet_lambda_fv_loss: 0.0526 - mae_inet_coefficient_loss: 32.1952 - mae_inet_lambda_fv_loss: 22.2679 - val_loss: 21.2064 - val_r2_inet_coefficient_loss: 1.3185 - val_r2_inet_lambda_fv_loss: -0.0183 - val_mae_inet_coefficient_loss: 31.7245 - val_mae_inet_lambda_fv_loss: 21.2869\n",
      "Epoch 9/500\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 21.8919 - r2_inet_coefficient_loss: 1.3279 - r2_inet_lambda_fv_loss: 0.0203 - mae_inet_coefficient_loss: 31.9625 - mae_inet_lambda_fv_loss: 21.8940 - val_loss: 21.6306 - val_r2_inet_coefficient_loss: 1.5199 - val_r2_inet_lambda_fv_loss: 0.0127 - val_mae_inet_coefficient_loss: 32.4887 - val_mae_inet_lambda_fv_loss: 21.6148\n",
      "Epoch 10/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 22.3819 - r2_inet_coefficient_loss: 1.3542 - r2_inet_lambda_fv_loss: 0.0453 - mae_inet_coefficient_loss: 31.5432 - mae_inet_lambda_fv_loss: 22.3825 - val_loss: 21.7286 - val_r2_inet_coefficient_loss: 1.1871 - val_r2_inet_lambda_fv_loss: -0.0257 - val_mae_inet_coefficient_loss: 29.7281 - val_mae_inet_lambda_fv_loss: 21.6846\n",
      "Epoch 11/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 22.3912 - r2_inet_coefficient_loss: 1.2016 - r2_inet_lambda_fv_loss: 0.0146 - mae_inet_coefficient_loss: 30.1451 - mae_inet_lambda_fv_loss: 22.3869 - val_loss: 20.9590 - val_r2_inet_coefficient_loss: 1.2143 - val_r2_inet_lambda_fv_loss: -0.1241 - val_mae_inet_coefficient_loss: 29.7900 - val_mae_inet_lambda_fv_loss: 20.9115\n",
      "Epoch 12/500\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 21.5259 - r2_inet_coefficient_loss: 1.4515 - r2_inet_lambda_fv_loss: -0.0541 - mae_inet_coefficient_loss: 31.5513 - mae_inet_lambda_fv_loss: 21.5255 - val_loss: 20.6400 - val_r2_inet_coefficient_loss: 1.5297 - val_r2_inet_lambda_fv_loss: -0.0874 - val_mae_inet_coefficient_loss: 31.6395 - val_mae_inet_lambda_fv_loss: 20.6861\n",
      "Epoch 13/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 21.4187 - r2_inet_coefficient_loss: 1.5651 - r2_inet_lambda_fv_loss: -0.0421 - mae_inet_coefficient_loss: 32.4374 - mae_inet_lambda_fv_loss: 21.4219 - val_loss: 20.5720 - val_r2_inet_coefficient_loss: 1.5204 - val_r2_inet_lambda_fv_loss: -0.1095 - val_mae_inet_coefficient_loss: 31.2559 - val_mae_inet_lambda_fv_loss: 20.5751\n",
      "Epoch 14/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 20.9228 - r2_inet_coefficient_loss: 1.4863 - r2_inet_lambda_fv_loss: -0.0807 - mae_inet_coefficient_loss: 31.8629 - mae_inet_lambda_fv_loss: 20.9271 - val_loss: 20.1364 - val_r2_inet_coefficient_loss: 1.5736 - val_r2_inet_lambda_fv_loss: -0.1352 - val_mae_inet_coefficient_loss: 31.0946 - val_mae_inet_lambda_fv_loss: 20.1656\n",
      "Epoch 15/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 20.7561 - r2_inet_coefficient_loss: 1.5758 - r2_inet_lambda_fv_loss: -0.0999 - mae_inet_coefficient_loss: 31.8830 - mae_inet_lambda_fv_loss: 20.7547 - val_loss: 19.9593 - val_r2_inet_coefficient_loss: 1.6777 - val_r2_inet_lambda_fv_loss: -0.1553 - val_mae_inet_coefficient_loss: 31.8416 - val_mae_inet_lambda_fv_loss: 19.9721\n",
      "Epoch 16/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 20.5505 - r2_inet_coefficient_loss: 1.6867 - r2_inet_lambda_fv_loss: -0.1283 - mae_inet_coefficient_loss: 32.6041 - mae_inet_lambda_fv_loss: 20.5488 - val_loss: 19.9726 - val_r2_inet_coefficient_loss: 1.5847 - val_r2_inet_lambda_fv_loss: -0.1504 - val_mae_inet_coefficient_loss: 31.9507 - val_mae_inet_lambda_fv_loss: 19.9154\n",
      "Epoch 17/500\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 20.7153 - r2_inet_coefficient_loss: 1.6619 - r2_inet_lambda_fv_loss: -0.0956 - mae_inet_coefficient_loss: 32.6873 - mae_inet_lambda_fv_loss: 20.7159 - val_loss: 20.3274 - val_r2_inet_coefficient_loss: 1.6960 - val_r2_inet_lambda_fv_loss: -0.1416 - val_mae_inet_coefficient_loss: 31.7971 - val_mae_inet_lambda_fv_loss: 20.3315\n",
      "Epoch 18/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 20.7869 - r2_inet_coefficient_loss: 1.5558 - r2_inet_lambda_fv_loss: -0.1145 - mae_inet_coefficient_loss: 32.0787 - mae_inet_lambda_fv_loss: 20.7860 - val_loss: 19.6694 - val_r2_inet_coefficient_loss: 1.5910 - val_r2_inet_lambda_fv_loss: -0.1911 - val_mae_inet_coefficient_loss: 31.3457 - val_mae_inet_lambda_fv_loss: 19.6620\n",
      "Epoch 19/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 20.2447 - r2_inet_coefficient_loss: 1.5672 - r2_inet_lambda_fv_loss: -0.1593 - mae_inet_coefficient_loss: 31.9202 - mae_inet_lambda_fv_loss: 20.2436 - val_loss: 19.7595 - val_r2_inet_coefficient_loss: 1.4780 - val_r2_inet_lambda_fv_loss: -0.1947 - val_mae_inet_coefficient_loss: 30.2662 - val_mae_inet_lambda_fv_loss: 19.8136\n",
      "Epoch 20/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 20.4096 - r2_inet_coefficient_loss: 1.4275 - r2_inet_lambda_fv_loss: -0.1569 - mae_inet_coefficient_loss: 31.2655 - mae_inet_lambda_fv_loss: 20.4103 - val_loss: 19.5965 - val_r2_inet_coefficient_loss: 1.4199 - val_r2_inet_lambda_fv_loss: -0.2383 - val_mae_inet_coefficient_loss: 30.4699 - val_mae_inet_lambda_fv_loss: 19.5630\n",
      "Epoch 21/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 20.6185 - r2_inet_coefficient_loss: 1.4075 - r2_inet_lambda_fv_loss: -0.1601 - mae_inet_coefficient_loss: 31.3547 - mae_inet_lambda_fv_loss: 20.6171 - val_loss: 20.3669 - val_r2_inet_coefficient_loss: 1.5961 - val_r2_inet_lambda_fv_loss: -0.1894 - val_mae_inet_coefficient_loss: 30.7345 - val_mae_inet_lambda_fv_loss: 20.3966\n",
      "Epoch 22/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 21.4464 - r2_inet_coefficient_loss: 1.7194 - r2_inet_lambda_fv_loss: -0.0943 - mae_inet_coefficient_loss: 32.1028 - mae_inet_lambda_fv_loss: 21.4428 - val_loss: 20.9070 - val_r2_inet_coefficient_loss: 1.6744 - val_r2_inet_lambda_fv_loss: -0.1168 - val_mae_inet_coefficient_loss: 30.3376 - val_mae_inet_lambda_fv_loss: 20.9234\n",
      "Epoch 23/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 21.3237 - r2_inet_coefficient_loss: 1.6574 - r2_inet_lambda_fv_loss: -0.0958 - mae_inet_coefficient_loss: 31.3111 - mae_inet_lambda_fv_loss: 21.3252 - val_loss: 20.5990 - val_r2_inet_coefficient_loss: 1.8233 - val_r2_inet_lambda_fv_loss: -0.1234 - val_mae_inet_coefficient_loss: 30.9825 - val_mae_inet_lambda_fv_loss: 20.7102\n",
      "Epoch 24/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 21.1779 - r2_inet_coefficient_loss: 1.6480 - r2_inet_lambda_fv_loss: -0.0969 - mae_inet_coefficient_loss: 31.6442 - mae_inet_lambda_fv_loss: 21.1732 - val_loss: 20.3533 - val_r2_inet_coefficient_loss: 1.8645 - val_r2_inet_lambda_fv_loss: -0.1776 - val_mae_inet_coefficient_loss: 31.6044 - val_mae_inet_lambda_fv_loss: 20.3709\n",
      "Epoch 25/500\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 21.0005 - r2_inet_coefficient_loss: 1.7496 - r2_inet_lambda_fv_loss: -0.1171 - mae_inet_coefficient_loss: 32.3608 - mae_inet_lambda_fv_loss: 20.9992 - val_loss: 20.7500 - val_r2_inet_coefficient_loss: 1.8480 - val_r2_inet_lambda_fv_loss: -0.1375 - val_mae_inet_coefficient_loss: 31.6429 - val_mae_inet_lambda_fv_loss: 20.7972\n",
      "Epoch 26/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 21.2566 - r2_inet_coefficient_loss: 1.8608 - r2_inet_lambda_fv_loss: -0.0963 - mae_inet_coefficient_loss: 32.0218 - mae_inet_lambda_fv_loss: 21.2562 - val_loss: 20.9633 - val_r2_inet_coefficient_loss: 1.8329 - val_r2_inet_lambda_fv_loss: -0.0978 - val_mae_inet_coefficient_loss: 31.9058 - val_mae_inet_lambda_fv_loss: 20.9559\n",
      "Epoch 27/500\n",
      "40/40 [==============================] - 12s 288ms/step - loss: 21.5502 - r2_inet_coefficient_loss: 1.6825 - r2_inet_lambda_fv_loss: -0.0729 - mae_inet_coefficient_loss: 32.3063 - mae_inet_lambda_fv_loss: 21.5498 - val_loss: 20.7445 - val_r2_inet_coefficient_loss: 1.4989 - val_r2_inet_lambda_fv_loss: -0.1181 - val_mae_inet_coefficient_loss: 30.4536 - val_mae_inet_lambda_fv_loss: 20.7390\n",
      "Epoch 28/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 21.1101 - r2_inet_coefficient_loss: 1.5504 - r2_inet_lambda_fv_loss: -0.1154 - mae_inet_coefficient_loss: 31.7484 - mae_inet_lambda_fv_loss: 21.1106 - val_loss: 20.9474 - val_r2_inet_coefficient_loss: 1.5495 - val_r2_inet_lambda_fv_loss: -0.1120 - val_mae_inet_coefficient_loss: 31.3401 - val_mae_inet_lambda_fv_loss: 20.9675\n",
      "Epoch 29/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 21.6063 - r2_inet_coefficient_loss: 1.6629 - r2_inet_lambda_fv_loss: -0.0791 - mae_inet_coefficient_loss: 32.1615 - mae_inet_lambda_fv_loss: 21.6057 - val_loss: 20.9334 - val_r2_inet_coefficient_loss: 1.6290 - val_r2_inet_lambda_fv_loss: -0.1093 - val_mae_inet_coefficient_loss: 31.3325 - val_mae_inet_lambda_fv_loss: 20.9693\n",
      "Epoch 30/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 21.1509 - r2_inet_coefficient_loss: 1.7293 - r2_inet_lambda_fv_loss: -0.1039 - mae_inet_coefficient_loss: 32.1934 - mae_inet_lambda_fv_loss: 21.1480 - val_loss: 20.8225 - val_r2_inet_coefficient_loss: 1.6552 - val_r2_inet_lambda_fv_loss: -0.1024 - val_mae_inet_coefficient_loss: 31.9643 - val_mae_inet_lambda_fv_loss: 20.8057\n",
      "Epoch 31/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 20.9883 - r2_inet_coefficient_loss: 1.8117 - r2_inet_lambda_fv_loss: -0.1144 - mae_inet_coefficient_loss: 32.7476 - mae_inet_lambda_fv_loss: 20.9876 - val_loss: 20.3825 - val_r2_inet_coefficient_loss: 1.9014 - val_r2_inet_lambda_fv_loss: -0.1348 - val_mae_inet_coefficient_loss: 32.3175 - val_mae_inet_lambda_fv_loss: 20.3981\n",
      "Epoch 32/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 20.3624 - r2_inet_coefficient_loss: 1.9337 - r2_inet_lambda_fv_loss: -0.1219 - mae_inet_coefficient_loss: 33.0713 - mae_inet_lambda_fv_loss: 20.3637 - val_loss: 19.5134 - val_r2_inet_coefficient_loss: 2.1473 - val_r2_inet_lambda_fv_loss: -0.2038 - val_mae_inet_coefficient_loss: 33.5650 - val_mae_inet_lambda_fv_loss: 19.4814\n",
      "Epoch 33/500\n",
      "40/40 [==============================] - 12s 291ms/step - loss: 20.0677 - r2_inet_coefficient_loss: 2.2426 - r2_inet_lambda_fv_loss: -0.1453 - mae_inet_coefficient_loss: 34.4188 - mae_inet_lambda_fv_loss: 20.0678 - val_loss: 19.1268 - val_r2_inet_coefficient_loss: 2.1595 - val_r2_inet_lambda_fv_loss: -0.2294 - val_mae_inet_coefficient_loss: 33.9405 - val_mae_inet_lambda_fv_loss: 19.1030\n",
      "Epoch 34/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 19.6208 - r2_inet_coefficient_loss: 2.1675 - r2_inet_lambda_fv_loss: -0.1803 - mae_inet_coefficient_loss: 34.2306 - mae_inet_lambda_fv_loss: 19.6228 - val_loss: 18.9655 - val_r2_inet_coefficient_loss: 2.3501 - val_r2_inet_lambda_fv_loss: -0.2367 - val_mae_inet_coefficient_loss: 35.2629 - val_mae_inet_lambda_fv_loss: 18.9315\n",
      "Epoch 35/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 19.5449 - r2_inet_coefficient_loss: 2.3344 - r2_inet_lambda_fv_loss: -0.1821 - mae_inet_coefficient_loss: 35.4297 - mae_inet_lambda_fv_loss: 19.5434 - val_loss: 18.6262 - val_r2_inet_coefficient_loss: 2.1693 - val_r2_inet_lambda_fv_loss: -0.2652 - val_mae_inet_coefficient_loss: 34.3603 - val_mae_inet_lambda_fv_loss: 18.6078\n",
      "Epoch 36/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 19.2171 - r2_inet_coefficient_loss: 2.1847 - r2_inet_lambda_fv_loss: -0.2194 - mae_inet_coefficient_loss: 35.1008 - mae_inet_lambda_fv_loss: 19.2206 - val_loss: 18.3559 - val_r2_inet_coefficient_loss: 2.3849 - val_r2_inet_lambda_fv_loss: -0.2712 - val_mae_inet_coefficient_loss: 35.0996 - val_mae_inet_lambda_fv_loss: 18.3471\n",
      "Epoch 37/500\n",
      "40/40 [==============================] - 12s 291ms/step - loss: 19.1417 - r2_inet_coefficient_loss: 2.3666 - r2_inet_lambda_fv_loss: -0.2247 - mae_inet_coefficient_loss: 35.4764 - mae_inet_lambda_fv_loss: 19.1423 - val_loss: 18.2305 - val_r2_inet_coefficient_loss: 2.4090 - val_r2_inet_lambda_fv_loss: -0.2811 - val_mae_inet_coefficient_loss: 34.9366 - val_mae_inet_lambda_fv_loss: 18.2190\n",
      "Epoch 38/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 19.0347 - r2_inet_coefficient_loss: 2.4004 - r2_inet_lambda_fv_loss: -0.2422 - mae_inet_coefficient_loss: 36.1397 - mae_inet_lambda_fv_loss: 19.0353 - val_loss: 18.2851 - val_r2_inet_coefficient_loss: 2.5285 - val_r2_inet_lambda_fv_loss: -0.2816 - val_mae_inet_coefficient_loss: 35.6659 - val_mae_inet_lambda_fv_loss: 18.2685\n",
      "Epoch 39/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 18.8645 - r2_inet_coefficient_loss: 2.3700 - r2_inet_lambda_fv_loss: -0.2506 - mae_inet_coefficient_loss: 35.6815 - mae_inet_lambda_fv_loss: 18.8658 - val_loss: 18.3227 - val_r2_inet_coefficient_loss: 2.4528 - val_r2_inet_lambda_fv_loss: -0.2777 - val_mae_inet_coefficient_loss: 35.1911 - val_mae_inet_lambda_fv_loss: 18.3371\n",
      "Epoch 40/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 18.8319 - r2_inet_coefficient_loss: 2.3940 - r2_inet_lambda_fv_loss: -0.2418 - mae_inet_coefficient_loss: 35.8093 - mae_inet_lambda_fv_loss: 18.8365 - val_loss: 18.2891 - val_r2_inet_coefficient_loss: 2.5244 - val_r2_inet_lambda_fv_loss: -0.2812 - val_mae_inet_coefficient_loss: 35.1842 - val_mae_inet_lambda_fv_loss: 18.2769\n",
      "Epoch 41/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 19.0238 - r2_inet_coefficient_loss: 2.4444 - r2_inet_lambda_fv_loss: -0.2255 - mae_inet_coefficient_loss: 35.8346 - mae_inet_lambda_fv_loss: 19.0327 - val_loss: 18.4973 - val_r2_inet_coefficient_loss: 2.4931 - val_r2_inet_lambda_fv_loss: -0.2730 - val_mae_inet_coefficient_loss: 35.2178 - val_mae_inet_lambda_fv_loss: 18.4595\n",
      "Epoch 42/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 19.2057 - r2_inet_coefficient_loss: 2.3997 - r2_inet_lambda_fv_loss: -0.2125 - mae_inet_coefficient_loss: 35.7147 - mae_inet_lambda_fv_loss: 19.2055 - val_loss: 18.7215 - val_r2_inet_coefficient_loss: 2.4494 - val_r2_inet_lambda_fv_loss: -0.2560 - val_mae_inet_coefficient_loss: 35.4009 - val_mae_inet_lambda_fv_loss: 18.6871\n",
      "Epoch 43/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 19.2107 - r2_inet_coefficient_loss: 2.3867 - r2_inet_lambda_fv_loss: -0.2193 - mae_inet_coefficient_loss: 36.0227 - mae_inet_lambda_fv_loss: 19.2077 - val_loss: 18.6684 - val_r2_inet_coefficient_loss: 2.5683 - val_r2_inet_lambda_fv_loss: -0.2590 - val_mae_inet_coefficient_loss: 35.4049 - val_mae_inet_lambda_fv_loss: 18.6098\n",
      "Epoch 44/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 19.1496 - r2_inet_coefficient_loss: 2.4471 - r2_inet_lambda_fv_loss: -0.2260 - mae_inet_coefficient_loss: 36.0684 - mae_inet_lambda_fv_loss: 19.1532 - val_loss: 18.5949 - val_r2_inet_coefficient_loss: 2.5190 - val_r2_inet_lambda_fv_loss: -0.2598 - val_mae_inet_coefficient_loss: 35.2752 - val_mae_inet_lambda_fv_loss: 18.5375\n",
      "Epoch 45/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 19.3384 - r2_inet_coefficient_loss: 2.4703 - r2_inet_lambda_fv_loss: -0.2264 - mae_inet_coefficient_loss: 36.0506 - mae_inet_lambda_fv_loss: 19.3389 - val_loss: 18.8561 - val_r2_inet_coefficient_loss: 2.4151 - val_r2_inet_lambda_fv_loss: -0.2475 - val_mae_inet_coefficient_loss: 34.5566 - val_mae_inet_lambda_fv_loss: 18.7951\n",
      "Epoch 46/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 19.2384 - r2_inet_coefficient_loss: 2.3864 - r2_inet_lambda_fv_loss: -0.2244 - mae_inet_coefficient_loss: 35.7278 - mae_inet_lambda_fv_loss: 19.2371 - val_loss: 19.1723 - val_r2_inet_coefficient_loss: 2.4851 - val_r2_inet_lambda_fv_loss: -0.2193 - val_mae_inet_coefficient_loss: 34.4669 - val_mae_inet_lambda_fv_loss: 19.0784\n",
      "Epoch 47/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 19.5511 - r2_inet_coefficient_loss: 2.4142 - r2_inet_lambda_fv_loss: -0.1961 - mae_inet_coefficient_loss: 35.6201 - mae_inet_lambda_fv_loss: 19.5545 - val_loss: 19.1175 - val_r2_inet_coefficient_loss: 2.6053 - val_r2_inet_lambda_fv_loss: -0.2199 - val_mae_inet_coefficient_loss: 35.0475 - val_mae_inet_lambda_fv_loss: 19.0135\n",
      "Epoch 48/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 19.7457 - r2_inet_coefficient_loss: 2.3998 - r2_inet_lambda_fv_loss: -0.1990 - mae_inet_coefficient_loss: 35.7152 - mae_inet_lambda_fv_loss: 19.7467 - val_loss: 19.6957 - val_r2_inet_coefficient_loss: 2.6548 - val_r2_inet_lambda_fv_loss: -0.1768 - val_mae_inet_coefficient_loss: 34.6646 - val_mae_inet_lambda_fv_loss: 19.6273\n",
      "Epoch 49/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 20.6433 - r2_inet_coefficient_loss: 2.7046 - r2_inet_lambda_fv_loss: -0.1208 - mae_inet_coefficient_loss: 35.8616 - mae_inet_lambda_fv_loss: 20.6506 - val_loss: 20.2715 - val_r2_inet_coefficient_loss: 2.6561 - val_r2_inet_lambda_fv_loss: -0.1393 - val_mae_inet_coefficient_loss: 34.3866 - val_mae_inet_lambda_fv_loss: 20.1955\n",
      "Epoch 50/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 20.8417 - r2_inet_coefficient_loss: 2.8331 - r2_inet_lambda_fv_loss: -0.1135 - mae_inet_coefficient_loss: 35.6460 - mae_inet_lambda_fv_loss: 20.8418 - val_loss: 20.1879 - val_r2_inet_coefficient_loss: 2.5920 - val_r2_inet_lambda_fv_loss: -0.1542 - val_mae_inet_coefficient_loss: 34.0684 - val_mae_inet_lambda_fv_loss: 20.1109\n",
      "Epoch 51/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 20.7998 - r2_inet_coefficient_loss: 2.6986 - r2_inet_lambda_fv_loss: -0.1114 - mae_inet_coefficient_loss: 35.3493 - mae_inet_lambda_fv_loss: 20.8054 - val_loss: 20.2235 - val_r2_inet_coefficient_loss: 2.4309 - val_r2_inet_lambda_fv_loss: -0.1557 - val_mae_inet_coefficient_loss: 33.7274 - val_mae_inet_lambda_fv_loss: 20.1274\n",
      "Epoch 52/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 20.8863 - r2_inet_coefficient_loss: 2.4564 - r2_inet_lambda_fv_loss: -0.0878 - mae_inet_coefficient_loss: 34.8893 - mae_inet_lambda_fv_loss: 20.8910 - val_loss: 20.3884 - val_r2_inet_coefficient_loss: 2.3005 - val_r2_inet_lambda_fv_loss: -0.1465 - val_mae_inet_coefficient_loss: 33.4058 - val_mae_inet_lambda_fv_loss: 20.3393\n",
      "Epoch 53/500\n",
      "40/40 [==============================] - 12s 291ms/step - loss: 20.9187 - r2_inet_coefficient_loss: 2.4519 - r2_inet_lambda_fv_loss: -0.0932 - mae_inet_coefficient_loss: 34.1341 - mae_inet_lambda_fv_loss: 20.9211 - val_loss: 20.7658 - val_r2_inet_coefficient_loss: 1.8251 - val_r2_inet_lambda_fv_loss: -0.0939 - val_mae_inet_coefficient_loss: 32.1206 - val_mae_inet_lambda_fv_loss: 20.7017\n",
      "Epoch 54/500\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 20.9972 - r2_inet_coefficient_loss: 2.3528 - r2_inet_lambda_fv_loss: -0.0656 - mae_inet_coefficient_loss: 33.7829 - mae_inet_lambda_fv_loss: 20.9996 - val_loss: 20.2735 - val_r2_inet_coefficient_loss: 2.2074 - val_r2_inet_lambda_fv_loss: -0.1122 - val_mae_inet_coefficient_loss: 33.0165 - val_mae_inet_lambda_fv_loss: 20.1644\n",
      "Epoch 55/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 20.9562 - r2_inet_coefficient_loss: 2.4966 - r2_inet_lambda_fv_loss: -0.1019 - mae_inet_coefficient_loss: 34.2105 - mae_inet_lambda_fv_loss: 20.9579 - val_loss: 20.1891 - val_r2_inet_coefficient_loss: 2.1389 - val_r2_inet_lambda_fv_loss: -0.1075 - val_mae_inet_coefficient_loss: 32.5445 - val_mae_inet_lambda_fv_loss: 20.1164\n",
      "Epoch 56/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 20.8436 - r2_inet_coefficient_loss: 2.4143 - r2_inet_lambda_fv_loss: -0.1158 - mae_inet_coefficient_loss: 33.5573 - mae_inet_lambda_fv_loss: 20.8390 - val_loss: 20.4099 - val_r2_inet_coefficient_loss: 2.3201 - val_r2_inet_lambda_fv_loss: -0.0879 - val_mae_inet_coefficient_loss: 33.0257 - val_mae_inet_lambda_fv_loss: 20.3645\n",
      "Epoch 57/500\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 20.9057 - r2_inet_coefficient_loss: 2.5833 - r2_inet_lambda_fv_loss: -0.0891 - mae_inet_coefficient_loss: 34.2353 - mae_inet_lambda_fv_loss: 20.9098 - val_loss: 20.3078 - val_r2_inet_coefficient_loss: 2.0303 - val_r2_inet_lambda_fv_loss: -0.1149 - val_mae_inet_coefficient_loss: 31.9919 - val_mae_inet_lambda_fv_loss: 20.2419\n",
      "Epoch 58/500\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 20.6100 - r2_inet_coefficient_loss: 2.5644 - r2_inet_lambda_fv_loss: -0.1075 - mae_inet_coefficient_loss: 34.0885 - mae_inet_lambda_fv_loss: 20.6098 - val_loss: 20.0043 - val_r2_inet_coefficient_loss: 2.1929 - val_r2_inet_lambda_fv_loss: -0.1378 - val_mae_inet_coefficient_loss: 33.2090 - val_mae_inet_lambda_fv_loss: 19.9289\n",
      "Epoch 59/500\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 20.5346 - r2_inet_coefficient_loss: 2.6632 - r2_inet_lambda_fv_loss: -0.1163 - mae_inet_coefficient_loss: 34.5618 - mae_inet_lambda_fv_loss: 20.5321 - val_loss: 20.0997 - val_r2_inet_coefficient_loss: 2.2620 - val_r2_inet_lambda_fv_loss: -0.1362 - val_mae_inet_coefficient_loss: 32.7651 - val_mae_inet_lambda_fv_loss: 20.0464\n",
      "Epoch 60/500\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 20.5822 - r2_inet_coefficient_loss: 2.7789 - r2_inet_lambda_fv_loss: -0.1210 - mae_inet_coefficient_loss: 34.9646 - mae_inet_lambda_fv_loss: 20.5822 - val_loss: 19.9321 - val_r2_inet_coefficient_loss: 2.3156 - val_r2_inet_lambda_fv_loss: -0.1234 - val_mae_inet_coefficient_loss: 33.3424 - val_mae_inet_lambda_fv_loss: 19.9310\n",
      "Epoch 61/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 20.4002 - r2_inet_coefficient_loss: 2.6532 - r2_inet_lambda_fv_loss: -0.1135 - mae_inet_coefficient_loss: 34.7069 - mae_inet_lambda_fv_loss: 20.3994 - val_loss: 19.8860 - val_r2_inet_coefficient_loss: 2.2807 - val_r2_inet_lambda_fv_loss: -0.1298 - val_mae_inet_coefficient_loss: 33.7471 - val_mae_inet_lambda_fv_loss: 19.8804\n",
      "Epoch 62/500\n",
      "40/40 [==============================] - 12s 291ms/step - loss: 20.2347 - r2_inet_coefficient_loss: 2.7123 - r2_inet_lambda_fv_loss: -0.1068 - mae_inet_coefficient_loss: 34.8622 - mae_inet_lambda_fv_loss: 20.2339 - val_loss: 19.6657 - val_r2_inet_coefficient_loss: 2.3387 - val_r2_inet_lambda_fv_loss: -0.1411 - val_mae_inet_coefficient_loss: 33.4494 - val_mae_inet_lambda_fv_loss: 19.6515\n",
      "Epoch 63/500\n",
      "40/40 [==============================] - 12s 297ms/step - loss: 20.1563 - r2_inet_coefficient_loss: 2.7239 - r2_inet_lambda_fv_loss: -0.1482 - mae_inet_coefficient_loss: 34.8346 - mae_inet_lambda_fv_loss: 20.1577 - val_loss: 19.7940 - val_r2_inet_coefficient_loss: 2.1217 - val_r2_inet_lambda_fv_loss: -0.1350 - val_mae_inet_coefficient_loss: 33.0102 - val_mae_inet_lambda_fv_loss: 19.8107\n",
      "Epoch 64/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 20.2417 - r2_inet_coefficient_loss: 2.4520 - r2_inet_lambda_fv_loss: -0.1407 - mae_inet_coefficient_loss: 34.4428 - mae_inet_lambda_fv_loss: 20.2406 - val_loss: 19.6048 - val_r2_inet_coefficient_loss: 2.1647 - val_r2_inet_lambda_fv_loss: -0.1479 - val_mae_inet_coefficient_loss: 33.3038 - val_mae_inet_lambda_fv_loss: 19.5943\n",
      "Epoch 65/500\n",
      "40/40 [==============================] - 12s 288ms/step - loss: 19.9843 - r2_inet_coefficient_loss: 2.4584 - r2_inet_lambda_fv_loss: -0.1390 - mae_inet_coefficient_loss: 34.3408 - mae_inet_lambda_fv_loss: 19.9858 - val_loss: 19.5100 - val_r2_inet_coefficient_loss: 1.9959 - val_r2_inet_lambda_fv_loss: -0.1638 - val_mae_inet_coefficient_loss: 32.5469 - val_mae_inet_lambda_fv_loss: 19.5270\n",
      "Epoch 66/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 20.0479 - r2_inet_coefficient_loss: 2.1995 - r2_inet_lambda_fv_loss: -0.1488 - mae_inet_coefficient_loss: 33.9644 - mae_inet_lambda_fv_loss: 20.0455 - val_loss: 19.5002 - val_r2_inet_coefficient_loss: 2.0899 - val_r2_inet_lambda_fv_loss: -0.1607 - val_mae_inet_coefficient_loss: 32.9360 - val_mae_inet_lambda_fv_loss: 19.5109\n",
      "Epoch 67/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 20.0622 - r2_inet_coefficient_loss: 2.4404 - r2_inet_lambda_fv_loss: -0.1353 - mae_inet_coefficient_loss: 34.6492 - mae_inet_lambda_fv_loss: 20.0647 - val_loss: 19.4908 - val_r2_inet_coefficient_loss: 2.2083 - val_r2_inet_lambda_fv_loss: -0.1622 - val_mae_inet_coefficient_loss: 33.4504 - val_mae_inet_lambda_fv_loss: 19.5170\n",
      "Epoch 68/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 19.9724 - r2_inet_coefficient_loss: 2.4942 - r2_inet_lambda_fv_loss: -0.1453 - mae_inet_coefficient_loss: 34.5151 - mae_inet_lambda_fv_loss: 19.9763 - val_loss: 19.7720 - val_r2_inet_coefficient_loss: 1.9358 - val_r2_inet_lambda_fv_loss: -0.1794 - val_mae_inet_coefficient_loss: 31.9345 - val_mae_inet_lambda_fv_loss: 19.8383\n",
      "Epoch 69/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 20.2958 - r2_inet_coefficient_loss: 2.4047 - r2_inet_lambda_fv_loss: -0.1101 - mae_inet_coefficient_loss: 34.5793 - mae_inet_lambda_fv_loss: 20.2969 - val_loss: 19.6412 - val_r2_inet_coefficient_loss: 2.2778 - val_r2_inet_lambda_fv_loss: -0.1557 - val_mae_inet_coefficient_loss: 33.7132 - val_mae_inet_lambda_fv_loss: 19.6607\n",
      "Epoch 70/500\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 19.9738 - r2_inet_coefficient_loss: 2.4652 - r2_inet_lambda_fv_loss: -0.1424 - mae_inet_coefficient_loss: 34.7452 - mae_inet_lambda_fv_loss: 19.9734 - val_loss: 19.3860 - val_r2_inet_coefficient_loss: 2.2336 - val_r2_inet_lambda_fv_loss: -0.1721 - val_mae_inet_coefficient_loss: 33.4992 - val_mae_inet_lambda_fv_loss: 19.4036\n",
      "Epoch 71/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 19.8330 - r2_inet_coefficient_loss: 2.5392 - r2_inet_lambda_fv_loss: -0.1691 - mae_inet_coefficient_loss: 34.7753 - mae_inet_lambda_fv_loss: 19.8305 - val_loss: 19.3408 - val_r2_inet_coefficient_loss: 2.1836 - val_r2_inet_lambda_fv_loss: -0.1842 - val_mae_inet_coefficient_loss: 33.0490 - val_mae_inet_lambda_fv_loss: 19.3349\n",
      "Epoch 72/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 19.6970 - r2_inet_coefficient_loss: 2.3184 - r2_inet_lambda_fv_loss: -0.1643 - mae_inet_coefficient_loss: 34.3592 - mae_inet_lambda_fv_loss: 19.6937 - val_loss: 19.2411 - val_r2_inet_coefficient_loss: 2.2059 - val_r2_inet_lambda_fv_loss: -0.1854 - val_mae_inet_coefficient_loss: 33.2113 - val_mae_inet_lambda_fv_loss: 19.2895\n",
      "Epoch 73/500\n",
      "40/40 [==============================] - 12s 295ms/step - loss: 19.8311 - r2_inet_coefficient_loss: 2.3982 - r2_inet_lambda_fv_loss: -0.1675 - mae_inet_coefficient_loss: 34.9533 - mae_inet_lambda_fv_loss: 19.8336 - val_loss: 19.0607 - val_r2_inet_coefficient_loss: 2.2651 - val_r2_inet_lambda_fv_loss: -0.1873 - val_mae_inet_coefficient_loss: 34.1517 - val_mae_inet_lambda_fv_loss: 19.1184\n",
      "Epoch 74/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 19.7020 - r2_inet_coefficient_loss: 2.4783 - r2_inet_lambda_fv_loss: -0.1639 - mae_inet_coefficient_loss: 35.5298 - mae_inet_lambda_fv_loss: 19.7007 - val_loss: 19.0770 - val_r2_inet_coefficient_loss: 2.1686 - val_r2_inet_lambda_fv_loss: -0.1875 - val_mae_inet_coefficient_loss: 33.8864 - val_mae_inet_lambda_fv_loss: 19.1136\n",
      "Epoch 75/500\n",
      "40/40 [==============================] - 12s 296ms/step - loss: 19.6952 - r2_inet_coefficient_loss: 2.3580 - r2_inet_lambda_fv_loss: -0.1416 - mae_inet_coefficient_loss: 34.8805 - mae_inet_lambda_fv_loss: 19.6920 - val_loss: 18.9735 - val_r2_inet_coefficient_loss: 2.2462 - val_r2_inet_lambda_fv_loss: -0.1965 - val_mae_inet_coefficient_loss: 34.2177 - val_mae_inet_lambda_fv_loss: 18.9743\n",
      "Epoch 76/500\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 19.7018 - r2_inet_coefficient_loss: 2.4635 - r2_inet_lambda_fv_loss: -0.1446 - mae_inet_coefficient_loss: 35.1304 - mae_inet_lambda_fv_loss: 19.7021 - val_loss: 19.0917 - val_r2_inet_coefficient_loss: 2.0524 - val_r2_inet_lambda_fv_loss: -0.1793 - val_mae_inet_coefficient_loss: 33.4657 - val_mae_inet_lambda_fv_loss: 19.0945\n",
      "Epoch 77/500\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 19.6043 - r2_inet_coefficient_loss: 2.2469 - r2_inet_lambda_fv_loss: -0.1686 - mae_inet_coefficient_loss: 34.6717 - mae_inet_lambda_fv_loss: 19.6092 - val_loss: 19.1459 - val_r2_inet_coefficient_loss: 2.2668 - val_r2_inet_lambda_fv_loss: -0.1745 - val_mae_inet_coefficient_loss: 34.5396 - val_mae_inet_lambda_fv_loss: 19.1001\n",
      "Epoch 78/500\n",
      "40/40 [==============================] - 12s 293ms/step - loss: 19.6975 - r2_inet_coefficient_loss: 2.4444 - r2_inet_lambda_fv_loss: -0.1489 - mae_inet_coefficient_loss: 34.8072 - mae_inet_lambda_fv_loss: 19.6968 - val_loss: 19.3270 - val_r2_inet_coefficient_loss: 2.1325 - val_r2_inet_lambda_fv_loss: -0.1726 - val_mae_inet_coefficient_loss: 33.4025 - val_mae_inet_lambda_fv_loss: 19.2855\n",
      "Epoch 79/500\n",
      "40/40 [==============================] - 12s 291ms/step - loss: 19.8186 - r2_inet_coefficient_loss: 2.2433 - r2_inet_lambda_fv_loss: -0.1411 - mae_inet_coefficient_loss: 34.2222 - mae_inet_lambda_fv_loss: 19.8213 - val_loss: 19.2245 - val_r2_inet_coefficient_loss: 2.2131 - val_r2_inet_lambda_fv_loss: -0.1750 - val_mae_inet_coefficient_loss: 33.9423 - val_mae_inet_lambda_fv_loss: 19.2236\n",
      "Epoch 80/500\n",
      "40/40 [==============================] - 12s 291ms/step - loss: 19.6549 - r2_inet_coefficient_loss: 2.3442 - r2_inet_lambda_fv_loss: -0.1774 - mae_inet_coefficient_loss: 34.6855 - mae_inet_lambda_fv_loss: 19.6588 - val_loss: 19.2726 - val_r2_inet_coefficient_loss: 2.0190 - val_r2_inet_lambda_fv_loss: -0.2005 - val_mae_inet_coefficient_loss: 33.3338 - val_mae_inet_lambda_fv_loss: 19.2613\n",
      "Epoch 81/500\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 19.7511 - r2_inet_coefficient_loss: 2.1171 - r2_inet_lambda_fv_loss: -0.1495 - mae_inet_coefficient_loss: 34.0274 - mae_inet_lambda_fv_loss: 19.7527 - val_loss: 19.2957 - val_r2_inet_coefficient_loss: 2.1462 - val_r2_inet_lambda_fv_loss: -0.1681 - val_mae_inet_coefficient_loss: 33.0258 - val_mae_inet_lambda_fv_loss: 19.2899\n",
      "Epoch 82/500\n",
      "40/40 [==============================] - 10s 256ms/step - loss: 19.7241 - r2_inet_coefficient_loss: 2.2813 - r2_inet_lambda_fv_loss: -0.1643 - mae_inet_coefficient_loss: 34.5383 - mae_inet_lambda_fv_loss: 19.7250 - val_loss: 19.2871 - val_r2_inet_coefficient_loss: 2.3677 - val_r2_inet_lambda_fv_loss: -0.1689 - val_mae_inet_coefficient_loss: 33.5285 - val_mae_inet_lambda_fv_loss: 19.3088\n",
      "Epoch 83/500\n",
      "40/40 [==============================] - 12s 309ms/step - loss: 19.8121 - r2_inet_coefficient_loss: 2.1915 - r2_inet_lambda_fv_loss: -0.1620 - mae_inet_coefficient_loss: 34.1617 - mae_inet_lambda_fv_loss: 19.8131 - val_loss: 19.4734 - val_r2_inet_coefficient_loss: 2.3700 - val_r2_inet_lambda_fv_loss: -0.1576 - val_mae_inet_coefficient_loss: 33.3603 - val_mae_inet_lambda_fv_loss: 19.4518\n",
      "Epoch 84/500\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 19.5776 - r2_inet_coefficient_loss: 2.2470 - r2_inet_lambda_fv_loss: -0.1757 - mae_inet_coefficient_loss: 34.0483 - mae_inet_lambda_fv_loss: 19.5779 - val_loss: 19.1935 - val_r2_inet_coefficient_loss: 1.9491 - val_r2_inet_lambda_fv_loss: -0.2051 - val_mae_inet_coefficient_loss: 32.5799 - val_mae_inet_lambda_fv_loss: 19.2037\n",
      "Epoch 85/500\n",
      "40/40 [==============================] - 13s 322ms/step - loss: 19.3745 - r2_inet_coefficient_loss: 1.9872 - r2_inet_lambda_fv_loss: -0.2149 - mae_inet_coefficient_loss: 33.8350 - mae_inet_lambda_fv_loss: 19.3728 - val_loss: 18.9280 - val_r2_inet_coefficient_loss: 1.9172 - val_r2_inet_lambda_fv_loss: -0.2275 - val_mae_inet_coefficient_loss: 33.2366 - val_mae_inet_lambda_fv_loss: 18.9655\n",
      "Epoch 86/500\n",
      "40/40 [==============================] - 13s 319ms/step - loss: 19.2984 - r2_inet_coefficient_loss: 1.9230 - r2_inet_lambda_fv_loss: -0.2386 - mae_inet_coefficient_loss: 33.6536 - mae_inet_lambda_fv_loss: 19.2952 - val_loss: 18.8081 - val_r2_inet_coefficient_loss: 1.7534 - val_r2_inet_lambda_fv_loss: -0.2104 - val_mae_inet_coefficient_loss: 32.3999 - val_mae_inet_lambda_fv_loss: 18.8516\n",
      "Epoch 87/500\n",
      "40/40 [==============================] - 13s 318ms/step - loss: 19.2121 - r2_inet_coefficient_loss: 1.8913 - r2_inet_lambda_fv_loss: -0.2034 - mae_inet_coefficient_loss: 33.5565 - mae_inet_lambda_fv_loss: 19.2128 - val_loss: 18.8033 - val_r2_inet_coefficient_loss: 1.7598 - val_r2_inet_lambda_fv_loss: -0.2497 - val_mae_inet_coefficient_loss: 32.4990 - val_mae_inet_lambda_fv_loss: 18.8430\n",
      "Training Time: 0:17:16\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------ LOADING MODELS -----------------------------------------------------\n",
      "Loading Time: 0:00:01\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------- PREDICT INET ------------------------------------------------------\n",
      "Predict Time: 0:00:00\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------- CALCULATE SYMBOLIC REGRESSION FUNCTION ------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=10)]: Done   2 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=10)]: Done   3 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=10)]: Done   4 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=10)]: Done   5 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=10)]: Done   6 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=10)]: Done   7 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=10)]: Done   8 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=10)]: Done   9 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=10)]: Done  10 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=10)]: Done  11 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=10)]: Done  12 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=10)]: Done  13 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=10)]: Done  14 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=10)]: Done  15 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=10)]: Done  16 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=10)]: Done  17 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=10)]: Done  18 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=10)]: Done  19 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=10)]: Done  20 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=10)]: Done  22 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=10)]: Done  23 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=10)]: Done  24 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=10)]: Done  25 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=10)]: Done  26 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=10)]: Done  27 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=10)]: Done  28 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=10)]: Done  29 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=10)]: Done  31 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=10)]: Done  32 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=10)]: Done  33 tasks      | elapsed: 22.2min\n",
      "[Parallel(n_jobs=10)]: Done  34 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=10)]: Done  35 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=10)]: Done  36 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=10)]: Done  37 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=10)]: Done  38 tasks      | elapsed: 25.0min\n",
      "[Parallel(n_jobs=10)]: Done  39 tasks      | elapsed: 26.4min\n",
      "[Parallel(n_jobs=10)]: Done  40 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=10)]: Done  41 tasks      | elapsed: 27.6min\n",
      "[Parallel(n_jobs=10)]: Done  42 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=10)]: Done  43 tasks      | elapsed: 28.3min\n",
      "[Parallel(n_jobs=10)]: Done  44 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=10)]: Done  45 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=10)]: Done  46 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=10)]: Done  47 tasks      | elapsed: 30.6min\n",
      "[Parallel(n_jobs=10)]: Done  48 tasks      | elapsed: 30.8min\n",
      "[Parallel(n_jobs=10)]: Done  49 tasks      | elapsed: 31.1min\n",
      "[Parallel(n_jobs=10)]: Done  50 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=10)]: Done  51 tasks      | elapsed: 33.0min\n",
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed: 33.1min\n",
      "[Parallel(n_jobs=10)]: Done  53 tasks      | elapsed: 33.6min\n",
      "[Parallel(n_jobs=10)]: Done  54 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=10)]: Done  55 tasks      | elapsed: 34.0min\n",
      "[Parallel(n_jobs=10)]: Done  56 tasks      | elapsed: 34.3min\n",
      "[Parallel(n_jobs=10)]: Done  57 tasks      | elapsed: 35.4min\n",
      "[Parallel(n_jobs=10)]: Done  58 tasks      | elapsed: 36.4min\n",
      "[Parallel(n_jobs=10)]: Done  59 tasks      | elapsed: 37.8min\n",
      "[Parallel(n_jobs=10)]: Done  60 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=10)]: Done  61 tasks      | elapsed: 38.7min\n",
      "[Parallel(n_jobs=10)]: Done  62 tasks      | elapsed: 39.1min\n",
      "[Parallel(n_jobs=10)]: Done  63 tasks      | elapsed: 39.1min\n",
      "[Parallel(n_jobs=10)]: Done  64 tasks      | elapsed: 40.7min\n",
      "[Parallel(n_jobs=10)]: Done  65 tasks      | elapsed: 40.9min\n",
      "[Parallel(n_jobs=10)]: Done  66 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=10)]: Done  67 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=10)]: Done  68 tasks      | elapsed: 42.8min\n",
      "[Parallel(n_jobs=10)]: Done  69 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=10)]: Done  70 tasks      | elapsed: 43.7min\n",
      "[Parallel(n_jobs=10)]: Done  71 tasks      | elapsed: 44.2min\n",
      "[Parallel(n_jobs=10)]: Done  72 tasks      | elapsed: 44.9min\n",
      "[Parallel(n_jobs=10)]: Done  73 tasks      | elapsed: 45.5min\n",
      "[Parallel(n_jobs=10)]: Done  74 tasks      | elapsed: 46.1min\n",
      "[Parallel(n_jobs=10)]: Done  75 tasks      | elapsed: 46.2min\n",
      "[Parallel(n_jobs=10)]: Done  76 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=10)]: Done  77 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=10)]: Done  78 tasks      | elapsed: 48.8min\n",
      "[Parallel(n_jobs=10)]: Done  79 tasks      | elapsed: 49.6min\n",
      "[Parallel(n_jobs=10)]: Done  80 tasks      | elapsed: 49.9min\n",
      "[Parallel(n_jobs=10)]: Done  81 tasks      | elapsed: 50.9min\n",
      "[Parallel(n_jobs=10)]: Done  91 out of 100 | elapsed: 56.4min remaining:  5.6min\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed: 60.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Regression Optimization Time: 1:00:41\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------ CALCULATE FUNCTION VALUES ------------------------------------------------\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metamodel_poly\n",
      "Exit <class 'KeyError'>\n",
      "metamodel_functions\n",
      "Exit <class 'KeyError'>\n",
      "metamodel_functions_no_GD\n",
      "Exit <class 'KeyError'>\n",
      "symbolic_regression_functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=10)]: Done 100 out of 100 | elapsed:   45.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_network_polynomials\n",
      "Exit <class 'KeyError'>\n",
      "FV Calculation Time: 0:01:01\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------- CALCULATE SCORES ----------------------------------------------------\n",
      "lambda_preds_VS_target_polynomials\n",
      "lambda_preds_VS_lstsq_lambda_pred_polynomials\n",
      "lambda_preds_VS_lstsq_target_polynomials\n",
      "lambda_preds_VS_inet_polynomials\n",
      "lambda_preds_VS_symbolic_regression_functions\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c5d89a0a4fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdistrib_dict_test_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_interpretation_net_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_net_train_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                    \u001b[0mlambda_net_valid_dataset_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                    lambda_net_test_dataset_list)\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mcalculate_interpretation_net_results\u001b[0;34m(lambda_net_train_dataset_list, lambda_net_valid_dataset_list, lambda_net_test_dataset_list)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfunction_values_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_values_test_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mscores_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistrib_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_all_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_values_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolynomial_dict_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mscores_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mdistrib_dict_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistrib_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/InterpretationNet.py\u001b[0m in \u001b[0;36mevaluate_all_predictions\u001b[0;34m(function_value_dict, polynomial_dict)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0mevaluation_key_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m         evaluation_scores, evaluation_distrib = evaluate_interpretation_net(polynomials_1, \n\u001b[0m\u001b[1;32m   1169\u001b[0m                                                                             \u001b[0mpolynomials_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                                                             \u001b[0mfunction_values_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/metrics.py\u001b[0m in \u001b[0;36mevaluate_interpretation_net\u001b[0;34m(function_1_coefficients, function_2_coefficients, function_1_fv, function_2_fv)\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0mmape_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_percentage_error_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m     \u001b[0mr2_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0mraae_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_absolute_average_error_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0mrmae_fv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_maximum_average_error_function_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_1_fv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_2_fv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work-ceph/smarton/InES_XAI/smarton/utilities/metrics.py\u001b[0m in \u001b[0;36mr2_score_function_values\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mresult_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0mresult_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \"\"\"\n\u001b[0;32m--> 676\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    677\u001b[0m         y_true, y_pred, multioutput)\n\u001b[1;32m    678\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xai/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "#%autoreload 2\n",
    "(history_list, \n",
    "\n",
    "#scores_valid_list,\n",
    "scores_test_list, \n",
    "\n",
    "#function_values_valid_list, \n",
    "function_values_test_list, \n",
    "\n",
    "#polynomial_dict_valid_list,\n",
    "polynomial_dict_test_list,\n",
    "\n",
    "#distrib_dict_valid_list,\n",
    "distrib_dict_test_list,\n",
    "\n",
    "model_list) = calculate_interpretation_net_results(lambda_net_train_dataset_list, \n",
    "                                                   lambda_net_valid_dataset_list, \n",
    "                                                   lambda_net_test_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_net_output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interpretation_net_output_monomials+1)*sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list[-1]['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list[-1]['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_dict_test_list[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize = tf.constant([float(i) for i in range(interpretation_net_output_shape)])\n",
    "\n",
    "if interpretation_net_output_monomials != None:\n",
    "    poly_optimize_coeffs = poly_optimize[:interpretation_net_output_monomials]\n",
    "\n",
    "    poly_optimize_identifiers_list = []\n",
    "    for i in range(interpretation_net_output_monomials):\n",
    "        poly_optimize_identifiers = tf.math.softmax(poly_optimize[sparsity*i+interpretation_net_output_monomials:sparsity*(i+1)+interpretation_net_output_monomials])\n",
    "        poly_optimize_identifiers_list.append(poly_optimize_identifiers)\n",
    "    poly_optimize_identifiers_list = tf.keras.backend.flatten(poly_optimize_identifiers_list)\n",
    "    poly_optimize = tf.concat([poly_optimize_coeffs, poly_optimize_identifiers_list], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_test_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']))\n",
    "\n",
    "print(distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_inet_polynomials'][distrib_dict_test_list[-1]['R2'].loc['lambda_preds_VS_inet_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "#p.set(xlim=(0, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'][distrib_dict_test_list[-1]['MAE'].loc['lambda_preds_VS_lstsq_lambda_pred_polynomials'] < 50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_test_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_test_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "custom_representation_keys_fixed = ['target_polynomials', 'lstsq_target_polynomials', 'lstsq_lambda_pred_polynomials', 'lstsq_lambda_pred_polynomials']\n",
    "custom_representation_keys_dynamic = ['inet_polynomials', 'per_network_polynomials']\n",
    "sympy_representation_keys = ['metamodel_functions']\n",
    "\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for key in polynomial_dict_test_list[-1].keys():\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(key)\n",
    "    if key in custom_representation_keys_fixed:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], force_complete_poly_representation=True, round_digits=4)\n",
    "    elif key in custom_representation_keys_dynamic:\n",
    "        print_polynomial_from_coefficients(polynomial_dict_test_list[-1][key][index], round_digits=4)\n",
    "    else:\n",
    "        display(polynomial_dict_test_list[-1][key][index])\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      polynomial_dict_test_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "current_jobs = 1\n",
    "\n",
    "lr=0.5\n",
    "max_steps = 100\n",
    "early_stopping=10\n",
    "restarts=2\n",
    "per_network_dataset_size = 500\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "if n_jobs != -1:\n",
    "    n_jobs_per_network = min(n_jobs, os.cpu_count() // current_jobs)\n",
    "else: \n",
    "    n_jobs_per_network = os.cpu_count() // current_jobs - 1\n",
    "\n",
    "printing = True if n_jobs_per_network == 1 else False\n",
    "\n",
    "\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "if evaluate_with_real_function: #target polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.target_polynomial_list)\n",
    "else: #lstsq lambda pred polynomial as inet target\n",
    "    poly_representation_list = np.array(lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list)\n",
    "\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "lambda_network_weights = lambda_network_weights_list[0]\n",
    "poly_representation = poly_representation_list[0]\n",
    "\n",
    "\n",
    "\n",
    "per_network_poly_optimization_tf(per_network_dataset_size, \n",
    "                                lambda_network_weights, \n",
    "                                  list_of_monomial_identifiers_numbers, \n",
    "                                  config, \n",
    "                                  lr=lr, \n",
    "                                  max_steps = max_steps, \n",
    "                                  early_stopping=early_stopping, \n",
    "                                  restarts=restarts, \n",
    "                                  printing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Real Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Auto MPG-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation_possible_autoMPG = False\n",
    "print_head_autoMPG = None\n",
    "\n",
    "url_autoMPG = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names_autoMPG = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset_autoMPG = pd.read_csv(url_autoMPG, names=column_names_autoMPG,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset_autoMPG = raw_dataset_autoMPG.dropna()\n",
    "\n",
    "dataset_autoMPG['Origin'] = dataset_autoMPG['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "dataset_autoMPG = pd.get_dummies(dataset_autoMPG, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "features_autoMPG = dataset_autoMPG.copy()\n",
    "\n",
    "labels_autoMPG = features_autoMPG.pop('MPG')\n",
    "\n",
    "features_autoMPG_normalized = (features_autoMPG-features_autoMPG.min())/(features_autoMPG.max()-features_autoMPG.min())\n",
    "\n",
    "#labels_autoMPG = (labels_autoMPG-labels_autoMPG.min())/(labels_autoMPG.max()-labels_autoMPG.min())\n",
    "\n",
    "\n",
    "if features_autoMPG_normalized.shape[1] >= n:\n",
    "    if n == 1:\n",
    "        features_autoMPG_model = features_autoMPG_normalized[['Horsepower']]\n",
    "    elif n == features_autoMPG_normalized.shape[1]:\n",
    "        features_autoMPG_model = features_autoMPG_normalized\n",
    "    else:\n",
    "        features_autoMPG_model = features_autoMPG_normalized.sample(n=n, axis='columns')\n",
    "        \n",
    "    print_head_autoMPG = features_autoMPG_model.head()\n",
    "    interpretation_possible_autoMPG = True\n",
    "\n",
    "print_head_autoMPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    ((lambda_index_autoMPG, \n",
    "     current_seed_autoMPG, \n",
    "     polynomial_autoMPG, \n",
    "     polynomial_lstsq_pred_list_autoMPG, \n",
    "     polynomial_lstsq_true_list_autoMPG), \n",
    "    scores_list_autoMPG, \n",
    "    pred_list_autoMPG, \n",
    "    history_autoMPG, \n",
    "    model_autoMPG) = train_nn(lambda_index=0, \n",
    "                              X_data_lambda=features_autoMPG_model.values, \n",
    "                              y_data_real_lambda=labels_autoMPG.values, \n",
    "                              polynomial=None, \n",
    "                              seed_list=[RANDOM_SEED], \n",
    "                              callbacks=[PlotLossesKerasTF()], \n",
    "                              return_history=True, \n",
    "                              each_epochs_save=None, \n",
    "                              printing=False, \n",
    "                              return_model=True)\n",
    "    \n",
    "    polynomial_lstsq_pred_autoMPG = polynomial_lstsq_pred_list_autoMPG[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    x = tf.linspace(0.0, 250, 251)\n",
    "    y = model_autoMPG.predict(x)\n",
    "\n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'n': n,\n",
    "        'd': d,\n",
    "        'inet_loss': inet_loss,\n",
    "        'sparsity': sparsity,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "        'RANDOM_SEED': RANDOM_SEED,\n",
    "        'nas': nas,\n",
    "        'number_of_lambda_weights': number_of_lambda_weights,\n",
    "        'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "        'fixed_initialization_lambda_training': fixed_initialization_lambda_training,\n",
    "        'dropout': dropout,\n",
    "        'lambda_network_layers': lambda_network_layers,\n",
    "        'optimizer_lambda': optimizer_lambda,\n",
    "        'loss_lambda': loss_lambda,        \n",
    "         #'list_of_monomial_identifiers': list_of_monomial_identifiers,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "weights_autoMPG = model_autoMPG.get_weights()\n",
    "\n",
    "weights_flat_autoMPG = []\n",
    "for layer_weights, biases in pairwise(weights_autoMPG):    #clf.get_weights()\n",
    "    for neuron in layer_weights:\n",
    "        for weight in neuron:\n",
    "            weights_flat_autoMPG.append(weight)\n",
    "    for bias in biases:\n",
    "        weights_flat_autoMPG.append(bias)\n",
    "        \n",
    "weights_flat_autoMPG = np.array(weights_flat_autoMPG)\n",
    "\n",
    "\n",
    "x = pred_list_autoMPG['X_test_lambda']\n",
    "y = pred_list_autoMPG['y_test_real_lambda']\n",
    "\n",
    "y_model_autoMPG = model_autoMPG.predict(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    y_polynomial_lstsq_pred_autoMPG = calculate_function_values_from_polynomial(polynomial_lstsq_pred_autoMPG, x, force_complete_poly_representation=True)\n",
    "\n",
    "    mae_model_polynomial_lstsq_pred_autoMPGy = mean_absolute_error(y_model_autoMPG, y_polynomial_lstsq_pred_autoMPG)\n",
    "    mae_data_polynomial_lstsq_pred_autoMPG = mean_absolute_error(y, y_polynomial_lstsq_pred_autoMPG)\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQt Poly:')\n",
    "    print_polynomial_from_coefficients(y_polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('MAE Model: ', mae_model_polynomial_lstsq_pred_autoMPGy)\n",
    "    print('MAE Data: ', mae_data_polynomial_lstsq_pred_autoMPG)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    interpretation_net = model_list[-1]\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    #interpretation_net_poly = interpretation_net.predict(np.array([weights_flat_autoMPG]))[0]\n",
    "    interpretation_net_poly = make_inet_prediction(interpretation_net, weights_flat_autoMPG, network_data=None, lambda_trained_normalized=False, inet_training_normalized=normalize_inet_data, normalization_parameter_dict=None)\n",
    "    \n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_interpretation_net_poly = calculate_function_values_from_polynomial(interpretation_net_poly, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_interpretation_net_poly)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_interpretation_net_poly)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)    \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    if False:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer':  'Powell',\n",
    "            'jac': 'fprime',\n",
    "            'max_steps': 5000,#100,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 500,\n",
    "        }      \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_scipy(per_network_dataset_size, \n",
    "                                                                  weights_flat_autoMPG, \n",
    "                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                  config, \n",
    "                                                                  optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                  jac = per_network_hyperparams['jac'],\n",
    "                                                                  max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                  restarts=per_network_hyperparams['restarts'], \n",
    "                                                                  printing=True,\n",
    "                                                                  return_error=False)\n",
    "    else:\n",
    "        per_network_hyperparams = {\n",
    "            'optimizer': tf.keras.optimizers.RMSprop,\n",
    "            'lr': 0.02,\n",
    "            'max_steps': 500,\n",
    "            'early_stopping': 10,\n",
    "            'restarts': 3,\n",
    "            'per_network_dataset_size': 5000,\n",
    "        }   \n",
    "        \n",
    "        per_network_function =  per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                              weights_flat_autoMPG, \n",
    "                                                              list_of_monomial_identifiers_numbers, \n",
    "                                                              config, \n",
    "                                                              optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                              lr=per_network_hyperparams['lr'], \n",
    "                                                              max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                              early_stopping=per_network_hyperparams['early_stopping'], \n",
    "                                                              restarts=per_network_hyperparams['restarts'], \n",
    "                                                              printing=True,\n",
    "                                                              return_error=False)\n",
    "            \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)  \n",
    "    \n",
    "    y_per_network_function = calculate_function_values_from_polynomial(per_network_function, x, force_complete_poly_representation=False)\n",
    "    \n",
    "    mae_model_interpretation_net_poly = mean_absolute_error(y_model_autoMPG, y_per_network_function)\n",
    "    mae_data_interpretation_net_poly = mean_absolute_error(y, y_per_network_function)    \n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function)\n",
    "    print('MAE Model: ', mae_model_interpretation_net_poly)\n",
    "    print('MAE Data: ', mae_data_interpretation_net_poly)       \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG:\n",
    "    \n",
    "    symbolic_regression_hyperparams = {\n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "\n",
    "    start = time.time() \n",
    "    \n",
    "    symbolic_regression_function =  symbolic_regression(model_autoMPG, \n",
    "                                                      config,\n",
    "                                                      symbolic_regression_hyperparams,\n",
    "                                                      #printing = True,\n",
    "                                                      return_error = False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    variable_names = ['X' + str(i) for i in range(n)]\n",
    "    \n",
    "    y_symbolic_regression_function = calculate_function_values_from_sympy(symbolic_regression_function, x, variable_names=variable_names)\n",
    "    \n",
    "    mae_model_symbolic_regression_function = mean_absolute_error(y_model_autoMPG, y_symbolic_regression_function)\n",
    "    mae_data_symbolic_regression_function = mean_absolute_error(y, y_symbolic_regression_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Poly:')    \n",
    "    display(symbolic_regression_function)\n",
    "    print('MAE Model: ', mae_model_symbolic_regression_function)\n",
    "    print('MAE Data: ', mae_data_symbolic_regression_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "if interpretation_possible_autoMPG and True:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = False,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function = calculate_function_values_from_sympy(metamodel_function, x)\n",
    "    \n",
    "    mae_model_metamodel_function = mean_absolute_error(y_model_autoMPG, y_metamodel_function)\n",
    "    mae_data_metamodel_function = mean_absolute_error(y, y_metamodel_function)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')    \n",
    "    display(metamodel_function)\n",
    "    print('MAE Model: ', mae_model_metamodel_function)\n",
    "    print('MAE Data: ', mae_data_metamodel_function)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and False:\n",
    "    metamodeling_hyperparams = {\n",
    "        'num_iter': 500,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': 0.01,        \n",
    "        'dataset_size': 500,\n",
    "    }\n",
    "    \n",
    "    start = time.time() \n",
    "\n",
    "    metamodel_function_basic =  symbolic_metamodeling(model_autoMPG, \n",
    "                                              config,\n",
    "                                              metamodeling_hyperparams,\n",
    "                                              #printing = True,\n",
    "                                              return_error = False,\n",
    "                                              return_expression = 'approx', #'approx', #'exact',\n",
    "                                              function_metamodeling = True,\n",
    "                                              force_polynomial=False)\n",
    "    \n",
    "    end = time.time()     \n",
    "    generation_time = (end - start) \n",
    "    minutes, seconds = divmod(int(generation_time), 60)\n",
    "    hours, minutes = divmod(minutes, 60)        \n",
    "    \n",
    "    y_metamodel_function_basic = calculate_function_values_from_sympy(metamodel_function_basic, x)\n",
    "    \n",
    "    mae_metamodel_function_basic = mean_absolute_error(y_model_autoMPG, y_metamodel_function_basic)\n",
    "    mae_metamodel_function_basic = mean_absolute_error(y, y_metamodel_function_basic)\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function Basic:')    \n",
    "    display(metamodel_function_basic)\n",
    "    print('MAE Model: ', mae_metamodel_function_basic)\n",
    "    print('MAE Data: ', mae_metamodel_function_basic)      \n",
    "    print('Computation Time: ' +  f'{hours:d}:{minutes:02d}:{seconds:02d}')    \n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG:\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Interpretation Net Poly:')\n",
    "    print_polynomial_from_coefficients(interpretation_net_poly, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Per Network Poly:')\n",
    "    print_polynomial_from_coefficients(per_network_function, force_complete_poly_representation=False)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('LSTSQ Poly:')\n",
    "    print_polynomial_from_coefficients(polynomial_lstsq_pred_autoMPG, force_complete_poly_representation=True)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Symbolic Regression Function:')\n",
    "    display(symbolic_regression_function)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('Metamodel Function:')\n",
    "    display(metamodel_function)\n",
    "    #print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    #print('Metamodel Function Basic:')\n",
    "    #display(metamodel_function_basic)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interpretation_possible_autoMPG and n==1:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "    \n",
    "    ax.set_ylim([0,50])\n",
    "    \n",
    "    plt.scatter(features_autoMPG_model['Horsepower'], labels_autoMPG, label='Data')\n",
    "    plt.scatter(x, y, label='Test Data')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_model_autoMPG))]) , label='Model Predictions')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_interpretation_net_poly))]) , label='Interpretation Net Poly')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_per_network_function))]) , label='Per Network Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_polynomial_lstsq_pred_autoMPG))]) , label='LSTSQ Poly')\n",
    "    plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_symbolic_regression_function))]) , label='Symbolic Regression Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y_metamodel_function))]) , label='Metamodel Function')\n",
    "    #plt.plot(np.sort(x, axis=0), np.array([y for _, y in sorted(zip(x, y))]) y_metamodel_function_basic, label='Metamodel Function Basic')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_X = np.array([i for i in range(1000)])\n",
    "sample_data_y = np.array([3*i for i in range(1000)])\n",
    "\n",
    "current_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y*1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(current_seed)\n",
    "np.random.seed(current_seed)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(current_seed)\n",
    "else:\n",
    "    tf.set_random_seed(current_seed) \n",
    "    \n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Dense(5, input_shape=(1,), activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "          \n",
    "    \n",
    "model.summary()\n",
    "\n",
    "model.fit(sample_data_X,\n",
    "         sample_data_y+1000,\n",
    "         epochs=5000,\n",
    "         verbose=0)\n",
    "\n",
    "print(model.get_weights())\n",
    "\n",
    "print(model.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_2_weights = model.get_weights()\n",
    "model_2_normalized_weights = model_2_weights #[weights/10 for weights in model_2_weights]\n",
    "\n",
    "\n",
    "model_2_normalized_weights[-6] = model_2_normalized_weights[-6]/10\n",
    "model_2_normalized_weights[-5] = model_2_normalized_weights[-5]/10\n",
    "\n",
    "model_2_normalized_weights[-4] = model_2_normalized_weights[-4]/10\n",
    "model_2_normalized_weights[-3] = model_2_normalized_weights[-3]/100\n",
    "\n",
    "model_2_normalized_weights[-2] = model_2_normalized_weights[-2]/10\n",
    "model_2_normalized_weights[-1] = model_2_normalized_weights[-1]/1000\n",
    "\n",
    "model_2.set_weights(model_2_normalized_weights)\n",
    "\n",
    "print(model_2.get_weights())\n",
    "print(model_2.predict([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Per-Network Poly Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Common Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'jac': 'fprime',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      jac = per_network_hyperparams['jac'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Optimization (Neural Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Common Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  [\n",
    "                   'Nelder-Mead', \n",
    "                   'Powell', \n",
    "        \n",
    "                   'CG',\n",
    "                   'BFGS',\n",
    "                   'Newton-CG', \n",
    "                   #'L-BFGS-B', #'>' not supported between instances of 'int' and 'NoneType'\n",
    "                   'TNC', \n",
    "                   \n",
    "                   'COBYLA', \n",
    "                   'SLSQP', \n",
    "                   \n",
    "                   #'trust-constr', # TypeError: _minimize_trustregion_constr() got an unexpected keyword argument 'maxfun'\n",
    "                   #'dogleg', # ValueError: Hessian is required for dogleg minimization\n",
    "                   #'trust-ncg', #ValueError: Either the Hessian or the Hessian-vector product is required for Newton-CG trust-region minimization\n",
    "                   #'trust-exact', # ValueError: Hessian matrix is required for trust region exact minimization.\n",
    "                   #'trust-krylov' #ValueError: Either the Hessian or the Hessian-vector product is required for Krylov trust-region minimization\n",
    "                   ], \n",
    "    'jac': ['fprime'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  jac = params['jac'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Neural Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [5000],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
