{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inerpretation-Net Training\n",
    "\n",
    "# Experiment 1: I-Net Performance for Different Algebras and Complexities\n",
    "# Experiment 2: I-Net Performance Comparison for Î»-Nets with Different Training Levels\n",
    "# Experiment 3: I-Net Performance Comparison Different Training Data Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "\n",
    "config = {\n",
    "    'data': {\n",
    "        'd': 5, #degree\n",
    "        'n': 1, #number of variables\n",
    "        'sparsity': None,\n",
    "        'sample_sparsity': None,\n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        'a_max': 1,\n",
    "        'a_min': -1,\n",
    "        'lambda_nets_total': 10000,\n",
    "        'noise': 0.1,\n",
    "        'noise_distrib': 'normal', #'normal', 'uniform', 'beta', 'Gamma', 'laplace'\n",
    "        \n",
    "        'same_training_all_lambda_nets': False,\n",
    "\n",
    "        'fixed_seed_lambda_training': True,\n",
    "        'fixed_initialization_lambda_training': False,\n",
    "        'number_different_lambda_trainings': 1,\n",
    "    },\n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True,  #if early stopping is used, multi_epoch_analysis is deactivated\n",
    "        'early_stopping_min_delta_lambda': 1e-4,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout': 0,\n",
    "        'lambda_network_layers': [5*'sparsity'],\n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'mae',\n",
    "        'number_of_lambda_weights': None,\n",
    "        'lambda_dataset_size': 5000,\n",
    "    },\n",
    "    'i_net': {\n",
    "        'optimizer': 'custom',#adam\n",
    "        'inet_loss': 'mae',\n",
    "        'inet_metrics': ['r2'],\n",
    "        'dropout': 0.25,\n",
    "        'dropout_output': 0,\n",
    "        'epochs': 500,\n",
    "        'early_stopping': True,\n",
    "        'batch_size': 512,\n",
    "        'dense_layers': [512, 1024],\n",
    "        'convolution_layers': None,\n",
    "        'lstm_layers': None,\n",
    "        'interpretation_dataset_size': 1000,\n",
    "                \n",
    "        'interpretation_net_output_monomials': 3, #(None, int)\n",
    "        'interpretation_net_output_shape': None, #calculated automatically later\n",
    "        \n",
    "        'evaluate_with_real_function': False,\n",
    "        'consider_labels_training': False,\n",
    "                      \n",
    "        'data_reshape_version': None, #default to 2 options:(None, 0,1 2)\n",
    "        'nas': True,\n",
    "        'nas_type': 'SEQUENTIAL', #options:(None, 'SEQUENTIAL', 'CNN', 'LSTM', 'CNN-LSTM', 'CNN-LSTM-parallel')      \n",
    "        'nas_trials': 1,\n",
    "    },\n",
    "    'evaluation': {   \n",
    "        'inet_holdout_seed_evaluation': False,\n",
    "        \n",
    "        #set if multi_epoch_analysis should be performed\n",
    "        'multi_epoch_analysis': True,\n",
    "        'each_epochs_save_lambda': 20,\n",
    "        'epoch_start': 0, #use to skip first epochs in multi_epoch_analysis\n",
    "        \n",
    "        #set if samples analysis should be performed\n",
    "        'samples_list': None,#[100, 500, 750, 1000, 2500, 5000, 7500, 10000, 15000, 20000, 25000, 28125] \n",
    "       \n",
    "        'random_evaluation_dataset_size': 500,\n",
    "    },\n",
    "    'computation':{\n",
    "        'n_jobs': 11,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:36.233201Z",
     "start_time": "2021-01-08T11:56:36.208062Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "##################################################### IMPORT LIBRARIES ################################################################\n",
    "#######################################################################################################################################\n",
    "from itertools import product       \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import timeit\n",
    "import psutil\n",
    "\n",
    "from functools import reduce\n",
    "from more_itertools import random_product \n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import colored\n",
    "import math\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "import keras.backend as K\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from IPython.display import display, Math, Latex, clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "from utilities.InterpretationNet import *\n",
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "################################################### VARIABLE ADJUSTMENTS ##############################################################\n",
    "#######################################################################################################################################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "n_jobs = min((epochs_lambda//each_epochs_save_lambda+1, n_jobs)) if multi_epoch_analysis else min(len(samples_list), n_jobs) if samples_list!=None else 1\n",
    "\n",
    "multi_epoch_analysis = False if early_stopping_lambda else multi_epoch_analysis #deactivate multi_epoch_analysis if early stopping is used\n",
    "\n",
    "each_epochs_save_lambda = each_epochs_save_lambda if multi_epoch_analysis else epochs_lambda\n",
    "epochs_save_range_lambda = range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda) if each_epochs_save_lambda == 1 else range(epoch_start//each_epochs_save_lambda, epochs_lambda//each_epochs_save_lambda+1) if multi_epoch_analysis else range(1,2)\n",
    "\n",
    "data_reshape_version = 2 if data_reshape_version == None and (convolution_layers != None or lstm_layers != None or (nas and nas_type != 'SEQUENTIAL')) else data_reshape_version\n",
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "config['evaluation']['multi_epoch_analysis'] = multi_epoch_analysis\n",
    "config['evaluation']['each_epochs_save_lambda'] = each_epochs_save_lambda\n",
    "config['i_net']['data_reshape_version'] = data_reshape_version\n",
    "\n",
    "config['data']['sparsity'] = nCr(config['data']['n']+config['data']['d'], config['data']['d'])\n",
    "config['data']['sample_sparsity'] = config['data']['sparsity'] if config['data']['sample_sparsity'] == None else config['data']['sample_sparsity']\n",
    "\n",
    "config['i_net']['interpretation_net_output_shape'] = config['data']['sparsity'] if config['i_net']['interpretation_net_output_monomials'] is None else config['data']['sparsity']*config['i_net']['interpretation_net_output_monomials']+config['i_net']['interpretation_net_output_monomials']\n",
    "\n",
    "\n",
    "transformed_layers = []\n",
    "for layer in config['lambda_net']['lambda_network_layers']:\n",
    "    if type(layer) == str:\n",
    "        transformed_layers.append(layer.count('sparsity')*config['data']['sparsity'])\n",
    "    else:\n",
    "        transformed_layers.append(layer)\n",
    "config['lambda_net']['lambda_network_layers'] = transformed_layers\n",
    "\n",
    "layers_with_input_output = list(flatten([[config['data']['n']], config['lambda_net']['lambda_network_layers'], [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]  \n",
    "config['lambda_net']['number_of_lambda_weights'] = number_of_lambda_weights\n",
    "    \n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['i_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "\n",
    "\n",
    "initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "initialize_metrics_config_from_curent_notebook(config)\n",
    "initialize_utility_functions_config_from_curent_notebook(config)\n",
    "initialize_InterpretationNet_config_from_curent_notebook(config)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(path_type='interpretation_net'))\n",
    "create_folders_inet()\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inet_dense512-1024-output_21_drop0.25e500b512_custom/lnets_1000_30-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_5_sparsity_6_amin_-1_amax_1_xdist_uniform_noise_normal_0.1\n",
      "lnets_10000_30-1000e_ES0.0001_64b_adam_mae_train_5000_diffX_1-FixSeed_42/var_1_d_5_sparsity_6_amin_-1_amax_1_xmin_0_xmax_1_xdist_uniform_noise_normal_0.1\n"
     ]
    }
   ],
   "source": [
    "print(path_identifier_interpretation_net_data)\n",
    "\n",
    "print(path_identifier_lambda_net_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.600530Z",
     "start_time": "2021-01-05T08:33:49.583928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.789212Z",
     "start_time": "2021-01-05T08:33:49.725485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc65d1d25064425ca4e6da4d14010a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 6\n",
      "Number of monomials in a polynomial with 1 variables and degree 5: 6\n",
      "Sparsity: 6\n",
      "['0', '1', '2', '3', '4', '5']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cf2b7a0e2e4ec9893f29a7129e7549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 6\n",
      "Number of monomials in a polynomial with 1 variables and degree 5: 6\n",
      "Sparsity: 6\n",
      "['0', '1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)\n",
    "\n",
    "\n",
    "layers_with_input_output = list(flatten([[n], lambda_network_layers, [1]]))\n",
    "number_of_lambda_weights = 0\n",
    "for i in range(len(layers_with_input_output)-1):\n",
    "    number_of_lambda_weights += (layers_with_input_output[i]+1)*layers_with_input_output[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T08:33:49.994944Z",
     "start_time": "2021-01-05T08:33:49.957264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_lambda_nets(index):\n",
    "    \n",
    "    if psutil.virtual_memory().percent > 80:\n",
    "        raise SystemExit(\"Out of RAM!\")\n",
    "    \n",
    "    directory = './data/weights/' + 'weights_' + path_identifier_lambda_net_data + '/'\n",
    "    path_weights = directory + 'weights_epoch_' + str(index).zfill(3) + '.txt'\n",
    "    path_X_data = directory + 'lambda_X_test_data.txt'\n",
    "    path_y_data = directory + 'lambda_y_test_data.txt'        \n",
    "    \n",
    "    weight_data = pd.read_csv(path_weights, sep=\",\", header=None)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    weight_data = weight_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_X_test_data = pd.read_csv(path_X_data, sep=\",\", header=None)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    lambda_X_test_data = lambda_X_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "    \n",
    "    lambda_y_test_data = pd.read_csv(path_y_data, sep=\",\", header=None)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(frac=1, random_state=RANDOM_SEED)\n",
    "    lambda_y_test_data = lambda_y_test_data.sort_values(by=0).sample(n=interpretation_dataset_size, random_state=RANDOM_SEED)\n",
    "        \n",
    "    lambda_nets = [None] * weight_data.shape[0]\n",
    "    for i, (row_weights, row_lambda_X_test_data, row_lambda_y_test_data) in enumerate(zip(weight_data.values, lambda_X_test_data.values, lambda_y_test_data.values)):        \n",
    "        lambda_net = LambdaNet(row_weights, row_lambda_X_test_data, row_lambda_y_test_data)\n",
    "        lambda_nets[i] = lambda_net\n",
    "                \n",
    "    lambda_net_dataset = LambdaNetDataset(lambda_nets)\n",
    "        \n",
    "    return lambda_net_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:29:48.869797Z",
     "start_time": "2021-01-05T08:33:49.997149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=11)]: Using backend MultiprocessingBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=11)]: Done   1 out of   1 | elapsed:    7.9s finished\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "\n",
    "parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "lambda_net_dataset_list = parallel(delayed(load_lambda_nets)((i+1)*each_epochs_save_lambda if each_epochs_save_lambda==1 else i*each_epochs_save_lambda if i > 1 else each_epochs_save_lambda if i==1 else 1) for i in epochs_save_range_lambda)  \n",
    "del parallel\n",
    "\n",
    "lambda_net_dataset = lambda_net_dataset_list[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:01:21.350996Z",
     "start_time": "2020-09-16T18:01:21.343717Z"
    }
   },
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:30:49.711839Z",
     "start_time": "2021-01-05T09:29:48.873305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-1.533</td>\n",
       "      <td>6.634</td>\n",
       "      <td>-11.500</td>\n",
       "      <td>5.498</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.605</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>0.915</td>\n",
       "      <td>-2.125</td>\n",
       "      <td>0.759</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.523</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.229</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.244</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.690</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.789</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.452</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>-0.768</td>\n",
       "      <td>1.420</td>\n",
       "      <td>-14.454</td>\n",
       "      <td>48.213</td>\n",
       "      <td>-63.525</td>\n",
       "      <td>27.058</td>\n",
       "      <td>-0.794</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>0.376</td>\n",
       "      <td>-1.277</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.358</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.474</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.035</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.172</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-2.337</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.721</td>\n",
       "      <td>0.346</td>\n",
       "      <td>4.731</td>\n",
       "      <td>-9.701</td>\n",
       "      <td>12.714</td>\n",
       "      <td>-4.814</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.980</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.528</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.689</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>0.398</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.525</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.791</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.010</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>1.515</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.093</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.173</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.746</td>\n",
       "      <td>-0.828</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-2.491</td>\n",
       "      <td>8.142</td>\n",
       "      <td>-10.118</td>\n",
       "      <td>3.139</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.885</td>\n",
       "      <td>-1.338</td>\n",
       "      <td>1.146</td>\n",
       "      <td>-1.629</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.678</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-1.085</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.106</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-1.272</td>\n",
       "      <td>-1.217</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-1.255</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.697</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>2.311</td>\n",
       "      <td>-0.934</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.293</td>\n",
       "      <td>2.685</td>\n",
       "      <td>-2.779</td>\n",
       "      <td>1.703</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.108</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.400</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.462</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.717</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "6252  1373158606     0.924     0.492    -0.142    -0.402    -0.741     0.255   \n",
       "4684  1373158606    -0.794     0.241    -0.127    -0.188    -0.535    -0.939   \n",
       "1731  1373158606    -0.739     0.951     0.737     0.786     0.773     0.083   \n",
       "4742  1373158606     0.173    -0.607    -0.085     0.746    -0.828    -0.939   \n",
       "4521  1373158606     0.131     0.208     0.871     0.323     0.573     0.164   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "6252           0.941           0.421          -1.533           6.634   \n",
       "4684          -0.768           1.420         -14.454          48.213   \n",
       "1731          -0.721           0.346           4.731          -9.701   \n",
       "4742           0.161          -0.337          -2.491           8.142   \n",
       "4521           0.124           0.308           0.697          -0.264   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "6252         -11.500           5.498           0.911           0.605   \n",
       "4684         -63.525          27.058          -0.794           0.232   \n",
       "1731          12.714          -4.814          -0.744           0.972   \n",
       "4742         -10.118           3.139           0.181          -0.787   \n",
       "4521           2.311          -0.934           0.132           0.241   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "6252          -0.683           0.915          -2.125           0.759 -0.009   \n",
       "4684           0.112          -0.997           0.376          -1.277 -0.009   \n",
       "1731           0.672           0.758           0.980          -0.046 -0.009   \n",
       "4742           0.885          -1.338           1.146          -1.629 -0.009   \n",
       "4521           0.293           2.685          -2.779           1.703 -0.009   \n",
       "\n",
       "       wb_1   wb_2  wb_3  wb_4   wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "6252 -0.227  0.414 0.340 0.169  0.103 0.419 -0.297 0.334 0.062  0.544 -0.088   \n",
       "4684 -0.227  0.231 0.122 0.313  0.080 0.185 -0.297 0.089 0.253  0.358 -0.088   \n",
       "1731 -0.227 -0.173 0.082 0.724  0.671 0.528 -0.297 0.067 0.916  0.416 -0.088   \n",
       "4742 -0.227  0.511 0.469 0.050 -0.029 0.515 -0.297 0.433 0.595  0.678 -0.088   \n",
       "4521 -0.227  0.128 0.083 0.634  0.562 0.108 -0.297 0.067 0.951  0.400 -0.088   \n",
       "\n",
       "      wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "6252 -0.019  0.451  0.090  0.523 -0.042  0.202  0.265  0.229 -0.054  0.043   \n",
       "4684 -0.019  0.234  0.078  0.342  0.008  1.474  0.405  0.393  0.033  0.035   \n",
       "1731 -0.019  0.349  0.689 -0.003  0.528  0.849  0.892  0.883  0.649  0.039   \n",
       "4742 -0.019  0.540 -0.019  0.653  0.006  0.278  0.134  0.182  0.030  0.396   \n",
       "4521 -0.019  0.126  0.521  0.243  0.398  0.706  0.723  0.705  0.612  0.041   \n",
       "\n",
       "      wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "6252  0.008 -0.042 -0.404 -0.090  0.333 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "4684 -0.043 -0.042 -0.404 -0.090  0.048 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "1731  0.007 -0.042 -0.404 -0.090  0.046 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "4742  0.433 -0.042 -0.404 -0.090  0.397 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "4521  0.008 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "\n",
       "      wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "6252 -0.212 -0.233  0.234  0.234 -0.286  0.000 -0.234  0.296 -0.243  0.000   \n",
       "4684  0.172  0.175  0.001 -0.081  0.178  0.000  0.175  0.190  0.173  0.000   \n",
       "1731  0.437 -0.092 -0.412 -0.438 -0.438  0.000 -0.078 -0.404  0.001  0.000   \n",
       "4742 -0.264 -0.331  0.090  0.099 -0.363  0.000 -0.339 -0.404 -0.007  0.000   \n",
       "4521 -0.128 -0.084 -0.130 -0.215 -0.108  0.000 -0.070 -0.709 -0.337  0.000   \n",
       "\n",
       "      wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "6252  0.000 -0.255  0.244 -0.205  0.232  0.254  0.227  0.242  0.288 -0.046   \n",
       "4684  0.000  0.175 -0.079  0.172 -0.023 -0.949  0.001  0.000 -0.040  0.176   \n",
       "1731  0.000  0.001 -0.463  0.398 -0.438 -0.280 -0.000 -0.140 -0.525 -0.054   \n",
       "4742  0.000 -0.279  0.089 -0.006 -0.025 -0.002  0.094  0.015 -0.044 -0.335   \n",
       "4521  0.000 -0.126 -0.436 -0.173 -0.186 -0.442  0.066 -0.330 -0.462 -0.047   \n",
       "\n",
       "      wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "6252 -0.023  0.000  0.000  0.000 -0.233  0.000  0.000  0.000 -0.254 -0.359   \n",
       "4684  0.200  0.000  0.000  0.000  0.175  0.000  0.000  0.000 -0.254 -0.359   \n",
       "1731 -0.024  0.000  0.000  0.000 -0.060  0.000  0.000  0.000 -0.254 -0.359   \n",
       "4742 -0.366  0.000  0.000  0.000 -0.337  0.000  0.000  0.000 -0.254 -0.359   \n",
       "4521 -0.023  0.000  0.000  0.000 -0.053  0.000  0.000  0.000 -0.254 -0.359   \n",
       "\n",
       "      wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "6252 -0.566 -0.690  0.341  0.400 -0.479 -0.408 -0.789  0.120 -0.376 -0.352   \n",
       "4684 -0.339 -0.360  0.278  0.187 -0.191 -0.408 -0.396 -0.074 -0.188 -0.352   \n",
       "1731 -0.466 -0.215  0.917  1.043  0.791 -0.408 -0.251  0.617  0.148 -0.352   \n",
       "4742 -0.691 -0.997  0.091  0.166 -0.746 -0.408 -1.085 -0.558 -0.418 -0.352   \n",
       "4521 -0.193 -0.225  0.614  0.717 -0.034 -0.408 -0.258  0.870  0.361 -0.352   \n",
       "\n",
       "      wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "6252 -0.364 -0.443  0.341 -0.452  0.687  0.215  0.348  0.244  0.337 -0.311   \n",
       "4684 -0.364 -0.199  0.121 -0.265  0.393 -2.337  0.330  0.208  0.049 -0.458   \n",
       "1731 -0.364  0.190  1.010 -0.240  1.515  0.684  0.819  0.700  1.093 -0.306   \n",
       "4742 -0.364 -0.580  0.091 -0.495  0.391 -0.077  0.106 -0.017  0.043 -1.272   \n",
       "4521 -0.364 -0.008  0.835  0.164  0.961  0.674  0.627  0.624  0.986 -0.309   \n",
       "\n",
       "      wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  \n",
       "6252 -0.044 -0.261 -0.383 -0.059 -0.969  0.144 -0.258 -0.315  0.202  \n",
       "4684 -0.229 -0.261 -0.383 -0.059 -0.479  0.144 -0.258 -0.315 -0.168  \n",
       "1731 -0.042 -0.261 -0.383 -0.059 -0.331  0.144 -0.258 -0.315 -0.437  \n",
       "4742 -1.217 -0.261 -0.383 -0.059 -1.255  0.144 -0.258 -0.315  0.121  \n",
       "4521 -0.044 -0.261 -0.383 -0.059 -0.334  0.144 -0.258 -0.315  0.078  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:31:56.898548Z",
     "start_time": "2021-01-05T09:30:49.715497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.611</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.278</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.476</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.037</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.363</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.549</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.653</td>\n",
       "      <td>3.592</td>\n",
       "      <td>10.852</td>\n",
       "      <td>14.163</td>\n",
       "      <td>5.881</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.633</td>\n",
       "      <td>1.564</td>\n",
       "      <td>3.645</td>\n",
       "      <td>3.985</td>\n",
       "      <td>1.704</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-0.997</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.998</td>\n",
       "      <td>-1.038</td>\n",
       "      <td>-1.997</td>\n",
       "      <td>-14.454</td>\n",
       "      <td>-73.162</td>\n",
       "      <td>-64.644</td>\n",
       "      <td>-41.140</td>\n",
       "      <td>-1.002</td>\n",
       "      <td>-1.431</td>\n",
       "      <td>-4.763</td>\n",
       "      <td>-10.661</td>\n",
       "      <td>-11.225</td>\n",
       "      <td>-4.626</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.289</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.099</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.491</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>-0.818</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-1.131</td>\n",
       "      <td>-1.352</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.092</td>\n",
       "      <td>-1.185</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-1.452</td>\n",
       "      <td>-1.350</td>\n",
       "      <td>-0.915</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-1.058</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-2.407</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-2.723</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-1.758</td>\n",
       "      <td>-2.052</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-1.720</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.501</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.487</td>\n",
       "      <td>-1.948</td>\n",
       "      <td>-4.897</td>\n",
       "      <td>-6.805</td>\n",
       "      <td>-2.656</td>\n",
       "      <td>-0.481</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>-2.480</td>\n",
       "      <td>-2.638</td>\n",
       "      <td>-1.197</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.168</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.269</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-0.441</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.543</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.573</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.433</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.386</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.456</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.392</td>\n",
       "      <td>1.864</td>\n",
       "      <td>5.383</td>\n",
       "      <td>6.678</td>\n",
       "      <td>2.523</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.505</td>\n",
       "      <td>1.099</td>\n",
       "      <td>2.270</td>\n",
       "      <td>2.749</td>\n",
       "      <td>1.068</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.356</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.525</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.084</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.431</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1373158606.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1.983</td>\n",
       "      <td>21.905</td>\n",
       "      <td>48.213</td>\n",
       "      <td>96.505</td>\n",
       "      <td>28.522</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.415</td>\n",
       "      <td>4.774</td>\n",
       "      <td>10.300</td>\n",
       "      <td>10.710</td>\n",
       "      <td>4.676</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.784</td>\n",
       "      <td>1.057</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.621</td>\n",
       "      <td>1.611</td>\n",
       "      <td>1.834</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>1.442</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.649</td>\n",
       "      <td>1.544</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.133</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.931</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>1.217</td>\n",
       "      <td>1.479</td>\n",
       "      <td>2.119</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.854</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>2.402</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.148</td>\n",
       "      <td>1.856</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.720</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.247</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                seed  0-target  1-target  2-target  3-target  4-target  \\\n",
       "count       1000.000  1000.000  1000.000  1000.000  1000.000  1000.000   \n",
       "mean  1373158606.000     0.002    -0.013     0.004    -0.006     0.004   \n",
       "std            0.000     0.569     0.591     0.580     0.574     0.563   \n",
       "min   1373158606.000    -1.000    -1.000    -0.998    -0.997    -1.000   \n",
       "25%   1373158606.000    -0.474    -0.557    -0.500    -0.501    -0.469   \n",
       "50%   1373158606.000     0.021    -0.008     0.009    -0.030    -0.003   \n",
       "75%   1373158606.000     0.488     0.508     0.481     0.500     0.457   \n",
       "max   1373158606.000     1.000     0.992     0.998     0.999     0.999   \n",
       "\n",
       "       5-target  0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  \\\n",
       "count  1000.000        1000.000        1000.000        1000.000   \n",
       "mean     -0.015           0.006          -0.031           0.087   \n",
       "std       0.585           0.536           0.653           3.592   \n",
       "min      -0.998          -1.038          -1.997         -14.454   \n",
       "25%      -0.527          -0.423          -0.487          -1.948   \n",
       "50%      -0.038           0.010          -0.019           0.002   \n",
       "75%       0.495           0.435           0.392           1.864   \n",
       "max       0.999           0.990           1.983          21.905   \n",
       "\n",
       "       3-lstsq_lambda  4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  \\\n",
       "count        1000.000        1000.000        1000.000        1000.000   \n",
       "mean           -0.396           0.611          -0.292           0.002   \n",
       "std            10.852          14.163           5.881           0.569   \n",
       "min           -73.162         -64.644         -41.140          -1.002   \n",
       "25%            -4.897          -6.805          -2.656          -0.481   \n",
       "50%            -0.004           0.005          -0.001           0.024   \n",
       "75%             5.383           6.678           2.523           0.484   \n",
       "max            48.213          96.505          28.522           1.005   \n",
       "\n",
       "       1-lstsq_target  2-lstsq_target  3-lstsq_target  4-lstsq_target  \\\n",
       "count        1000.000        1000.000        1000.000        1000.000   \n",
       "mean           -0.018           0.028          -0.055           0.046   \n",
       "std             0.633           1.564           3.645           3.985   \n",
       "min            -1.431          -4.763         -10.661         -11.225   \n",
       "25%            -0.551          -1.060          -2.480          -2.638   \n",
       "50%             0.002           0.045           0.014           0.018   \n",
       "75%             0.505           1.099           2.270           2.749   \n",
       "max             1.415           4.774          10.300          10.710   \n",
       "\n",
       "       5-lstsq_target     wb_0     wb_1     wb_2     wb_3     wb_4     wb_5  \\\n",
       "count        1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean           -0.028   -0.009   -0.227    0.289    0.196    0.282    0.221   \n",
       "std             1.704    0.000    0.000    0.142    0.142    0.162    0.162   \n",
       "min            -4.626   -0.009   -0.227   -0.173   -0.289   -0.134   -0.199   \n",
       "25%            -1.197   -0.009   -0.227    0.191    0.083    0.174    0.087   \n",
       "50%            -0.081   -0.009   -0.227    0.294    0.191    0.261    0.202   \n",
       "75%             1.068   -0.009   -0.227    0.364    0.272    0.371    0.312   \n",
       "max             4.676   -0.009   -0.227    0.761    0.660    0.847    0.784   \n",
       "\n",
       "          wb_6     wb_7     wb_8     wb_9    wb_10    wb_11    wb_12    wb_13  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.278   -0.297    0.166    0.449    0.476   -0.088   -0.019    0.324   \n",
       "std      0.160    0.000    0.141    0.263    0.204    0.000    0.000    0.161   \n",
       "min     -0.158   -0.297   -0.254   -0.132    0.099   -0.088   -0.019   -0.120   \n",
       "25%      0.168   -0.297    0.067    0.284    0.360   -0.088   -0.019    0.225   \n",
       "50%      0.263   -0.297    0.155    0.375    0.433   -0.088   -0.019    0.308   \n",
       "75%      0.356   -0.297    0.245    0.563    0.525   -0.088   -0.019    0.390   \n",
       "max      1.057   -0.297    0.621    1.611    1.834   -0.088   -0.019    1.442   \n",
       "\n",
       "         wb_14    wb_15    wb_16    wb_17    wb_18    wb_19    wb_20    wb_21  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.221    0.405    0.080    0.415    0.371    0.393    0.147    0.115   \n",
       "std      0.167    0.144    0.152    0.246    0.166    0.193    0.205    0.141   \n",
       "min     -0.209   -0.003   -0.280   -0.098   -0.022   -0.063   -0.317   -0.286   \n",
       "25%      0.084    0.317    0.008    0.258    0.269    0.264    0.033    0.041   \n",
       "50%      0.199    0.400    0.013    0.353    0.347    0.351    0.079    0.071   \n",
       "75%      0.314    0.473    0.145    0.501    0.454    0.477    0.246    0.179   \n",
       "max      0.833    1.070    0.649    1.544    1.017    1.133    0.848    0.568   \n",
       "\n",
       "         wb_22    wb_23    wb_24    wb_25    wb_26    wb_27    wb_28    wb_29  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.061   -0.042   -0.404   -0.090    0.127   -0.291   -0.209   -0.233   \n",
       "std      0.178    0.000    0.000    0.000    0.140    0.000    0.000    0.000   \n",
       "min     -0.411   -0.042   -0.404   -0.090   -0.273   -0.291   -0.209   -0.233   \n",
       "25%      0.008   -0.042   -0.404   -0.090    0.047   -0.291   -0.209   -0.233   \n",
       "50%      0.008   -0.042   -0.404   -0.090    0.094   -0.291   -0.209   -0.233   \n",
       "75%      0.084   -0.042   -0.404   -0.090    0.200   -0.291   -0.209   -0.233   \n",
       "max      0.931   -0.042   -0.404   -0.090    0.568   -0.291   -0.209   -0.233   \n",
       "\n",
       "         wb_30    wb_31    wb_32    wb_33    wb_34    wb_35    wb_36    wb_37  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.000    0.000   -0.017   -0.028   -0.011   -0.017   -0.054    0.000   \n",
       "std      0.000    0.000    0.146    0.157    0.171    0.180    0.186    0.000   \n",
       "min      0.000    0.000   -0.401   -0.467   -0.545   -0.601   -0.705    0.000   \n",
       "25%      0.000    0.000   -0.130   -0.093   -0.108   -0.082   -0.185    0.000   \n",
       "50%      0.000    0.000   -0.003   -0.008   -0.001   -0.001   -0.008    0.000   \n",
       "75%      0.000    0.000    0.110    0.109    0.128    0.131    0.105    0.000   \n",
       "max      0.000    0.000    0.437    0.332    0.241    0.242    0.245    0.000   \n",
       "\n",
       "         wb_38    wb_39    wb_40    wb_41    wb_42    wb_43    wb_44    wb_45  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.029   -0.077   -0.045    0.000    0.000   -0.049   -0.025   -0.005   \n",
       "std      0.158    0.287    0.212    0.000    0.000    0.182    0.194    0.143   \n",
       "min     -0.491   -1.001   -1.180    0.000    0.000   -0.880   -0.587   -0.818   \n",
       "25%     -0.079   -0.325   -0.114    0.000    0.000   -0.167   -0.083   -0.077   \n",
       "50%     -0.016    0.011   -0.004    0.000    0.000   -0.006   -0.001   -0.002   \n",
       "75%      0.109    0.152    0.102    0.000    0.000    0.104    0.132    0.108   \n",
       "max      0.331    0.306    0.221    0.000    0.000    0.235    0.252    0.398   \n",
       "\n",
       "         wb_46    wb_47    wb_48    wb_49    wb_50    wb_51    wb_52    wb_53  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.011   -0.069    0.018   -0.033   -0.018   -0.017    0.003    0.000   \n",
       "std      0.157    0.249    0.135    0.214    0.214    0.152    0.174    0.000   \n",
       "min     -0.551   -0.959   -0.329   -0.907   -0.704   -0.486   -0.761    0.000   \n",
       "25%     -0.023   -0.269   -0.036   -0.138   -0.041   -0.049   -0.023    0.000   \n",
       "50%     -0.018   -0.004   -0.000   -0.002   -0.008   -0.046   -0.022    0.000   \n",
       "75%      0.135    0.127    0.128    0.129    0.153    0.111    0.123    0.000   \n",
       "max      0.283    0.260    0.233    0.249    0.320    0.287    0.411    0.000   \n",
       "\n",
       "         wb_54    wb_55    wb_56    wb_57    wb_58    wb_59    wb_60    wb_61  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.000    0.000   -0.021    0.000    0.000    0.000   -0.254   -0.359   \n",
       "std      0.000    0.000    0.153    0.000    0.000    0.000    0.000    0.000   \n",
       "min      0.000    0.000   -0.486    0.000    0.000    0.000   -0.254   -0.359   \n",
       "25%      0.000    0.000   -0.056    0.000    0.000    0.000   -0.254   -0.359   \n",
       "50%      0.000    0.000   -0.051    0.000    0.000    0.000   -0.254   -0.359   \n",
       "75%      0.000    0.000    0.111    0.000    0.000    0.000   -0.254   -0.359   \n",
       "max      0.000    0.000    0.275    0.000    0.000    0.000   -0.254   -0.359   \n",
       "\n",
       "         wb_62    wb_63    wb_64    wb_65    wb_66    wb_67    wb_68    wb_69  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.373   -0.425    0.354    0.418   -0.216   -0.408   -0.469    0.037   \n",
       "std      0.174    0.231    0.218    0.254    0.323    0.000    0.258    0.427   \n",
       "min     -1.131   -1.352    0.003    0.092   -1.185   -0.408   -1.452   -1.350   \n",
       "25%     -0.417   -0.441    0.209    0.252   -0.278   -0.408   -0.496   -0.113   \n",
       "50%     -0.339   -0.356    0.311    0.354   -0.189   -0.408   -0.386   -0.023   \n",
       "75%     -0.260   -0.260    0.381    0.431   -0.098   -0.408   -0.270    0.112   \n",
       "max     -0.057   -0.145    1.217    1.479    2.119   -0.408   -0.180    2.272   \n",
       "\n",
       "         wb_70    wb_71    wb_72    wb_73    wb_74    wb_75    wb_76    wb_77  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean    -0.115   -0.352   -0.364   -0.197    0.363   -0.269    0.627    0.117   \n",
       "std      0.438    0.000    0.000    0.330    0.270    0.218    0.315    0.503   \n",
       "min     -0.915   -0.352   -0.364   -1.058    0.025   -0.982    0.329   -2.407   \n",
       "25%     -0.264   -0.352   -0.364   -0.281    0.187   -0.346    0.394    0.079   \n",
       "50%     -0.193   -0.352   -0.364   -0.199    0.291   -0.271    0.544    0.182   \n",
       "75%     -0.102   -0.352   -0.364   -0.108    0.373   -0.186    0.675    0.263   \n",
       "max      2.854   -0.352   -0.364    2.402    1.467    2.148    1.856    1.101   \n",
       "\n",
       "         wb_78    wb_79    wb_80    wb_81    wb_82    wb_83    wb_84    wb_85  \\\n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000   \n",
       "mean     0.359    0.177    0.327   -0.522   -0.251   -0.261   -0.383   -0.059   \n",
       "std      0.179    0.437    0.350    0.290    0.333    0.000    0.000    0.000   \n",
       "min     -0.001   -2.723   -0.308   -1.758   -2.052   -0.261   -0.383   -0.059   \n",
       "25%      0.246    0.124    0.049   -0.543   -0.275   -0.261   -0.383   -0.059   \n",
       "50%      0.329    0.218    0.216   -0.425   -0.122   -0.261   -0.383   -0.059   \n",
       "75%      0.410    0.296    0.354   -0.310   -0.044   -0.261   -0.383   -0.059   \n",
       "max      1.096    1.035    1.720   -0.254    0.029   -0.261   -0.383   -0.059   \n",
       "\n",
       "         wb_86    wb_87    wb_88    wb_89    wb_90  \n",
       "count 1000.000 1000.000 1000.000 1000.000 1000.000  \n",
       "mean    -0.549    0.144   -0.258   -0.315    0.004  \n",
       "std      0.284    0.000    0.000    0.000    0.138  \n",
       "min     -1.720    0.144   -0.258   -0.315   -0.476  \n",
       "25%     -0.573    0.144   -0.258   -0.315   -0.118  \n",
       "50%     -0.456    0.144   -0.258   -0.315    0.011  \n",
       "75%     -0.335    0.144   -0.258   -0.315    0.128  \n",
       "max     -0.247    0.144   -0.258   -0.315    0.326  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.as_pandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04991591],\n",
       "       [0.55931928],\n",
       "       [0.41215429],\n",
       "       [0.47204459],\n",
       "       [0.50702493],\n",
       "       [0.36030918],\n",
       "       [0.48300684],\n",
       "       [0.91632547],\n",
       "       [0.27265516],\n",
       "       [0.88145597]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0879322 ],\n",
       "       [0.92128287],\n",
       "       [0.94887895],\n",
       "       [1.06356541],\n",
       "       [1.04696906],\n",
       "       [0.91303792],\n",
       "       [0.84312583],\n",
       "       [0.62768837],\n",
       "       [1.02064446],\n",
       "       [0.61265825]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_dataset.y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets for Interpretation-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:32:09.782470Z",
     "start_time": "2021-01-05T09:31:56.901018Z"
    }
   },
   "outputs": [],
   "source": [
    "#generate train, test and validation data for training\n",
    "\n",
    "lambda_net_train_dataset_list = []\n",
    "lambda_net_valid_dataset_list = []\n",
    "lambda_net_test_dataset_list = []\n",
    "\n",
    "for lambda_net_dataset in lambda_net_dataset_list:\n",
    "    \n",
    "    \n",
    "    if inet_holdout_seed_evaluation:\n",
    "        complete_seed_list = list(set(lambda_net_dataset.train_settings_list['seed']))#list(weight_data.iloc[:,1].unique())\n",
    "\n",
    "        random.seed(RANDOM_SEED)\n",
    "        test_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/0.9)))\n",
    "        lambda_net_test_dataset = lambda_net_dataset.get_lambda_nets_by_seed(test_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(test_seeds))#complete_seed_list.remove(test_seeds)\n",
    "        \n",
    "        random.seed(RANDOM_SEED)\n",
    "        valid_seeds = random.sample(complete_seed_list, int(len(complete_seed_list)-len(complete_seed_list)/(1/0.9)))\n",
    "        lambda_net_valid_dataset = lambda_net_dataset.get_lambda_nets_by_seed(valid_seeds)\n",
    "        complete_seed_list = list(set(complete_seed_list) - set(valid_seeds))\n",
    "\n",
    "        train_seeds = complete_seed_list\n",
    "        lambda_net_train_dataset = lambda_net_dataset.get_lambda_nets_by_seed(train_seeds)       \n",
    "        \n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "        \n",
    "        del lambda_net_dataset\n",
    "    else:\n",
    "        lambda_net_train_with_valid_dataset, lambda_net_test_dataset = split_LambdaNetDataset(lambda_net_dataset, test_split=0.1)\n",
    "        lambda_net_train_dataset, lambda_net_valid_dataset = split_LambdaNetDataset(lambda_net_train_with_valid_dataset, test_split=0.1)\n",
    "\n",
    "        lambda_net_train_dataset_list.append(lambda_net_train_dataset)\n",
    "        lambda_net_valid_dataset_list.append(lambda_net_valid_dataset)\n",
    "        lambda_net_test_dataset_list.append(lambda_net_test_dataset)\n",
    "    \n",
    "        del lambda_net_dataset, lambda_net_train_with_valid_dataset\n",
    "\n",
    "        \n",
    "del lambda_net_dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:06.495716Z",
     "start_time": "2021-01-05T09:32:09.784760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810, 110)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:08.945802Z",
     "start_time": "2021-01-05T09:33:06.499150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 110)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:33:11.543306Z",
     "start_time": "2021-01-05T09:33:08.947468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 110)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:04.155343Z",
     "start_time": "2021-01-05T09:33:11.544785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.411</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.839</td>\n",
       "      <td>-0.515</td>\n",
       "      <td>1.365</td>\n",
       "      <td>-2.339</td>\n",
       "      <td>3.144</td>\n",
       "      <td>-1.464</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.247</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.191</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.197</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.224</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.083</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.925</td>\n",
       "      <td>-0.744</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-2.645</td>\n",
       "      <td>8.657</td>\n",
       "      <td>-12.880</td>\n",
       "      <td>5.510</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>1.878</td>\n",
       "      <td>-2.575</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.571</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>-0.288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.521</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.499</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-1.013</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.943</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>0.222</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.570</td>\n",
       "      <td>-5.309</td>\n",
       "      <td>12.721</td>\n",
       "      <td>-14.935</td>\n",
       "      <td>5.854</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>1.099</td>\n",
       "      <td>-4.081</td>\n",
       "      <td>4.435</td>\n",
       "      <td>-2.219</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.605</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.464</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.576</td>\n",
       "      <td>-0.153</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.310</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.772</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.357</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.402</td>\n",
       "      <td>-0.947</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.931</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6765</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-0.598</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-0.879</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-1.050</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>0.772</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.898</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>1.508</td>\n",
       "      <td>-5.479</td>\n",
       "      <td>7.188</td>\n",
       "      <td>-3.511</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.485</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.241</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.691</td>\n",
       "      <td>0.827</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>0.272</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>0.260</td>\n",
       "      <td>3.634</td>\n",
       "      <td>-9.601</td>\n",
       "      <td>9.302</td>\n",
       "      <td>-3.448</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.353</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.327</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>0.274</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-0.448</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "3105  1373158606     0.830    -0.392     0.484     0.428    -0.411     0.102   \n",
       "483   1373158606    -0.150    -0.925    -0.744     0.531    -1.000    -0.167   \n",
       "381   1373158606     0.943    -0.337    -0.036    -0.608     0.222    -0.439   \n",
       "6765  1373158606    -0.901    -0.598    -0.416     0.189     0.620    -0.879   \n",
       "3999  1373158606    -0.691     0.827    -0.143    -0.244    -0.537     0.272   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "3105           0.724           0.240          -0.119           0.268   \n",
       "483           -0.130          -0.901          -2.645           8.657   \n",
       "381            0.897           0.570          -5.309          12.721   \n",
       "6765          -0.810          -1.050           0.361          -0.788   \n",
       "3999          -0.560           0.233           0.002          -0.005   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "3105          -0.268           0.098           0.839          -0.515   \n",
       "483          -12.880           5.510          -0.153          -0.868   \n",
       "381          -14.935           5.854           0.946          -0.451   \n",
       "6765           0.772          -0.279          -0.898          -0.803   \n",
       "3999           0.006          -0.002          -0.676           0.260   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "3105           1.365          -2.339           3.144          -1.464 -0.009   \n",
       "483           -1.191           1.878          -2.575           0.450 -0.009   \n",
       "381            1.099          -4.081           4.435          -2.219 -0.009   \n",
       "6765           1.508          -5.479           7.188          -3.511 -0.009   \n",
       "3999           3.634          -9.601           9.302          -3.448 -0.009   \n",
       "\n",
       "       wb_1  wb_2  wb_3  wb_4   wb_5  wb_6   wb_7  wb_8   wb_9  wb_10  wb_11  \\\n",
       "3105 -0.227 0.284 0.190 0.268  0.205 0.247 -0.297 0.068  0.251  0.399 -0.088   \n",
       "483  -0.227 0.509 0.404 0.108  0.080 0.586 -0.297 0.369  0.814  0.682 -0.088   \n",
       "381  -0.227 0.454 0.358 0.065 -0.004 0.437 -0.297 0.334 -0.017  0.605 -0.088   \n",
       "6765 -0.227 0.361 0.254 0.192  0.081 0.317 -0.297 0.221  0.387  0.485 -0.088   \n",
       "3999 -0.227 0.227 0.118 0.318  0.256 0.181 -0.297 0.085  0.244  0.353 -0.088   \n",
       "\n",
       "      wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "3105 -0.019  0.288  0.199  0.384  0.063  0.314  0.356  0.332  0.095  0.041   \n",
       "483  -0.019  0.638  0.079  0.622  0.008  0.553  0.139  0.391  0.033  0.315   \n",
       "381  -0.019  0.464 -0.020  0.576 -0.153  0.096  0.167  0.127 -0.177  0.317   \n",
       "6765 -0.019  0.364  0.079  0.470  0.008  0.236  0.276  0.254  0.033  0.169   \n",
       "3999 -0.019  0.229  0.256  0.337  0.009  0.377  0.400  0.387  0.191  0.030   \n",
       "\n",
       "      wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "3105  0.009 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "483   0.571 -0.042 -0.404 -0.090  0.325 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "381   0.011 -0.042 -0.404 -0.090  0.310 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "6765  0.109 -0.042 -0.404 -0.090  0.181 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "3999 -0.080 -0.042 -0.404 -0.090  0.043 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "\n",
       "      wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "3105 -0.003 -0.001  0.191  0.191 -0.002  0.000 -0.069  0.204 -0.008  0.000   \n",
       "483   0.030  0.024 -0.108 -0.081 -0.372  0.000  0.030 -0.522 -0.288  0.000   \n",
       "381  -0.174 -0.227  0.223  0.226 -0.275  0.000 -0.233  0.201 -0.106  0.000   \n",
       "6765  0.165  0.166 -0.007 -0.081  0.176  0.000  0.165  0.206  0.172  0.000   \n",
       "3999  0.134  0.138 -0.000 -0.000  0.136  0.000  0.140  0.128  0.130  0.000   \n",
       "\n",
       "      wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "3105  0.000 -0.003  0.197 -0.008  0.188  0.205  0.187  0.198  0.224 -0.047   \n",
       "483   0.000 -0.292 -0.079 -0.045 -0.023 -0.437 -0.139 -0.307 -0.039  0.036   \n",
       "381   0.000 -0.252  0.236 -0.095  0.230  0.239  0.214  0.227  0.291 -0.246   \n",
       "6765  0.000  0.174 -0.079  0.167 -0.023 -0.021 -0.014 -0.015 -0.039  0.165   \n",
       "3999  0.000  0.134 -0.000  0.132 -0.022 -0.000 -0.000 -0.000  0.000  0.143   \n",
       "\n",
       "      wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "3105 -0.022  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254 -0.359   \n",
       "483  -0.439  0.000  0.000  0.000  0.039  0.000  0.000  0.000 -0.254 -0.359   \n",
       "381  -0.021  0.000  0.000  0.000 -0.241  0.000  0.000  0.000 -0.254 -0.359   \n",
       "6765  0.196  0.000  0.000  0.000  0.164  0.000  0.000  0.000 -0.254 -0.359   \n",
       "3999  0.208  0.000  0.000  0.000  0.142  0.000  0.000  0.000 -0.254 -0.359   \n",
       "\n",
       "      wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "3105 -0.300 -0.307  0.336  0.380 -0.144 -0.408 -0.258  0.083 -0.154 -0.352   \n",
       "483  -0.500 -0.496  0.135  0.187 -0.646 -0.408 -0.521 -0.654 -0.499 -0.352   \n",
       "381  -0.561 -0.693  0.331  0.411 -0.510 -0.408 -0.772  0.049 -0.366 -0.352   \n",
       "6765 -0.415 -0.421  0.160  0.187 -0.260 -0.408 -0.449 -0.148 -0.274 -0.352   \n",
       "3999 -0.302 -0.327  0.304  0.339 -0.152 -0.408 -0.367 -0.030 -0.151 -0.352   \n",
       "\n",
       "      wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "3105 -0.364 -0.156  0.316 -0.232  0.610  0.214  0.361  0.249  0.261 -0.310   \n",
       "483  -0.364 -0.595  0.121 -0.442  0.393 -0.661  0.130 -0.444  0.048 -0.562   \n",
       "381  -0.364 -0.460  0.357 -0.443  0.757  0.196  0.315  0.218  0.402 -0.947   \n",
       "6765 -0.364 -0.274  0.121 -0.351  0.393  0.042  0.204  0.085  0.048 -0.498   \n",
       "3999 -0.364 -0.160  0.274 -0.229  0.394  0.184  0.338  0.222  0.208 -0.448   \n",
       "\n",
       "      wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  \n",
       "3105 -0.045 -0.261 -0.383 -0.059 -0.335  0.144 -0.258 -0.315  0.170  \n",
       "483  -1.013 -0.261 -0.383 -0.059 -0.589  0.144 -0.258 -0.315 -0.044  \n",
       "381  -0.046 -0.261 -0.383 -0.059 -0.931  0.144 -0.258 -0.315  0.198  \n",
       "6765 -0.241 -0.261 -0.383 -0.059 -0.523  0.144 -0.258 -0.315 -0.151  \n",
       "3999 -0.297 -0.261 -0.383 -0.059 -0.463  0.144 -0.258 -0.315 -0.135  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_train_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:07.407453Z",
     "start_time": "2021-01-05T09:34:04.157787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.697</td>\n",
       "      <td>-0.796</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-1.026</td>\n",
       "      <td>10.588</td>\n",
       "      <td>-39.312</td>\n",
       "      <td>56.247</td>\n",
       "      <td>-25.293</td>\n",
       "      <td>0.692</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>0.962</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>1.734</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.517</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>1.442</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.508</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.184</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>2.354</td>\n",
       "      <td>0.264</td>\n",
       "      <td>-0.324</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.209</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7942</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.702</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-2.466</td>\n",
       "      <td>5.243</td>\n",
       "      <td>-5.050</td>\n",
       "      <td>1.805</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.169</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>2.993</td>\n",
       "      <td>-1.764</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.444</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.124</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.346</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.290</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.206</td>\n",
       "      <td>0.226</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8142</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.819</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.591</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-3.919</td>\n",
       "      <td>10.000</td>\n",
       "      <td>-13.687</td>\n",
       "      <td>5.853</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>1.298</td>\n",
       "      <td>-3.998</td>\n",
       "      <td>2.360</td>\n",
       "      <td>-0.754</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.507</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.656</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.366</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.390</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.096</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.881</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6599</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.031</td>\n",
       "      <td>12.196</td>\n",
       "      <td>-44.799</td>\n",
       "      <td>63.610</td>\n",
       "      <td>-28.474</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.713</td>\n",
       "      <td>-1.299</td>\n",
       "      <td>5.794</td>\n",
       "      <td>-5.286</td>\n",
       "      <td>2.919</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.519</td>\n",
       "      <td>1.652</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.443</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2.296</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3570</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.581</td>\n",
       "      <td>-0.597</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.549</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-1.913</td>\n",
       "      <td>4.173</td>\n",
       "      <td>-4.090</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0.574</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>1.096</td>\n",
       "      <td>-1.191</td>\n",
       "      <td>0.657</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.311</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.451</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.261</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.468</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "5459  1373158606     0.697    -0.796     0.848     0.249     0.738     0.369   \n",
       "7942  1373158606     0.404    -0.399    -0.503     0.509     0.467    -0.702   \n",
       "8142  1373158606     0.086     0.024    -0.819    -0.190    -0.591     0.067   \n",
       "6599  1373158606     0.957     0.365     0.795     0.692     0.164     0.787   \n",
       "3570  1373158606     0.581    -0.597    -0.030     0.215    -0.680     0.557   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "5459           0.625          -1.026          10.588         -39.312   \n",
       "7942           0.396          -0.056          -2.466           5.243   \n",
       "8142           0.087           0.313          -3.919          10.000   \n",
       "6599           0.880           0.031          12.196         -44.799   \n",
       "3570           0.549          -0.204          -1.913           4.173   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "5459          56.247         -25.293           0.692          -0.775   \n",
       "7942          -5.050           1.805           0.412          -0.497   \n",
       "8142         -13.687           5.853           0.111          -0.440   \n",
       "6599          63.610         -28.474           0.939           0.713   \n",
       "3570          -4.090           1.480           0.574          -0.457   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "5459           0.962          -0.417           1.734          -0.100 -0.009   \n",
       "7942           0.169          -1.526           2.993          -1.764 -0.009   \n",
       "8142           1.298          -3.998           2.360          -0.754 -0.009   \n",
       "6599          -1.299           5.794          -5.286           2.919 -0.009   \n",
       "3570          -0.615           1.096          -1.191           0.657 -0.009   \n",
       "\n",
       "       wb_1  wb_2  wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "5459 -0.227 0.119 0.083 0.181 0.115 0.105 -0.297 0.067 0.333  0.517 -0.088   \n",
       "7942 -0.227 0.334 0.234 0.194 0.131 0.298 -0.297 0.203 0.384  0.461 -0.088   \n",
       "8142 -0.227 0.509 0.408 0.079 0.002 0.507 -0.297 0.374 0.617  0.656 -0.088   \n",
       "6599 -0.227 0.131 0.083 0.395 0.329 0.110 -0.297 0.067 0.519  1.652 -0.088   \n",
       "3570 -0.227 0.345 0.244 0.193 0.130 0.311 -0.297 0.213 0.411  0.469 -0.088   \n",
       "\n",
       "      wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "5459 -0.019  1.442  0.113  0.508 -0.036  0.243  0.268  0.255  0.020  0.041   \n",
       "7942 -0.019  0.342  0.126  0.444 -0.014  0.242  0.281  0.259  0.020  0.154   \n",
       "8142 -0.019  0.533  0.017  0.630  0.004  0.265  0.160  0.200  0.028  0.337   \n",
       "6599 -0.019  0.131  0.334  0.178  0.177  0.468  0.476  0.473  0.268  0.041   \n",
       "3570 -0.019  0.353  0.124  0.451 -0.017  0.240  0.282  0.258  0.007  0.165   \n",
       "\n",
       "      wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "5459  0.008 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "7942  0.121 -0.042 -0.404 -0.090  0.165 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "8142  0.366 -0.042 -0.404 -0.090  0.343 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "6599  0.009 -0.042 -0.404 -0.090  0.047 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "3570  0.009 -0.042 -0.404 -0.090  0.175 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "\n",
       "      wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "5459 -0.119 -0.084  0.182  0.184 -0.105  0.000 -0.070  0.050  0.001  0.000   \n",
       "7942 -0.037 -0.009  0.122  0.124 -0.014  0.000 -0.003 -0.017 -0.028  0.000   \n",
       "8142 -0.228 -0.268  0.052  0.054 -0.249  0.000 -0.273 -0.303 -0.085  0.000   \n",
       "6599 -0.131 -0.083  0.198  0.198 -0.110  0.000 -0.069  0.156 -1.012  0.000   \n",
       "3570 -0.019 -0.006  0.154  0.157 -0.008  0.000 -0.005 -0.009 -0.030  0.000   \n",
       "\n",
       "      wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "5459  0.000 -0.880  0.185  0.001  0.189  0.179  0.180  0.179  0.194 -0.047   \n",
       "7942  0.000 -0.024  0.125 -0.029  0.133  0.119  0.120  0.119  0.149 -0.002   \n",
       "8142  0.000 -0.251  0.045 -0.080 -0.023  0.014  0.055  0.027 -0.040 -0.255   \n",
       "6599  0.000 -0.131  0.200 -0.178  0.197  0.203  0.196  0.200  0.211 -0.047   \n",
       "3570  0.000 -0.014  0.161 -0.033  0.167  0.157  0.150  0.153  0.201 -0.003   \n",
       "\n",
       "      wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "5459 -0.023  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254 -0.359   \n",
       "7942 -0.002  0.000  0.000  0.000 -0.002  0.000  0.000  0.000 -0.254 -0.359   \n",
       "8142 -0.276  0.000  0.000  0.000 -0.256  0.000  0.000  0.000 -0.254 -0.359   \n",
       "6599 -0.022  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254 -0.359   \n",
       "3570 -0.022  0.000  0.000  0.000 -0.004  0.000  0.000  0.000 -0.254 -0.359   \n",
       "\n",
       "      wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "5459 -0.176 -0.225  0.279  0.328 -0.018 -0.408 -0.258 -0.025 -0.228 -0.352   \n",
       "7942 -0.348 -0.346  0.242  0.290 -0.189 -0.408 -0.372 -0.076 -0.213 -0.352   \n",
       "8142 -0.612 -0.711  0.071  0.128 -0.505 -0.408 -0.769 -0.390 -0.397 -0.352   \n",
       "6599 -0.201 -0.226  0.406  0.443 -0.043 -0.408 -0.258  0.160  2.296 -0.352   \n",
       "3570 -0.359 -0.362  0.273  0.328 -0.202 -0.408 -0.391 -0.086 -0.221 -0.352   \n",
       "\n",
       "      wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "5459 -0.364  2.354  0.264 -0.324  0.579  0.149  0.299  0.185  0.209 -0.309   \n",
       "7942 -0.364 -0.206  0.226 -0.290  0.586  0.115  0.266  0.152  0.200 -0.419   \n",
       "8142 -0.364 -0.488  0.055 -0.473  0.390 -0.075  0.096 -0.025  0.041 -0.872   \n",
       "6599 -0.364 -0.043  0.377 -0.095  0.638  0.284  0.438  0.322  0.295 -0.310   \n",
       "3570 -0.364 -0.217  0.266 -0.298  0.641  0.144  0.291  0.179  0.261 -0.446   \n",
       "\n",
       "      wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  \n",
       "5459 -0.044 -0.261 -0.383 -0.059 -0.334  0.144 -0.258 -0.315  0.183  \n",
       "7942 -0.168 -0.261 -0.383 -0.059 -0.445  0.144 -0.258 -0.315  0.122  \n",
       "8142 -0.730 -0.261 -0.383 -0.059 -0.881  0.144 -0.258 -0.315  0.074  \n",
       "6599 -0.045 -0.261 -0.383 -0.059 -0.335  0.144 -0.258 -0.315  0.190  \n",
       "3570 -0.045 -0.261 -0.383 -0.059 -0.468  0.144 -0.258 -0.315  0.147  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_valid_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T09:34:10.970350Z",
     "start_time": "2021-01-05T09:34:07.411246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>0-target</th>\n",
       "      <th>1-target</th>\n",
       "      <th>2-target</th>\n",
       "      <th>3-target</th>\n",
       "      <th>4-target</th>\n",
       "      <th>5-target</th>\n",
       "      <th>0-lstsq_lambda</th>\n",
       "      <th>1-lstsq_lambda</th>\n",
       "      <th>2-lstsq_lambda</th>\n",
       "      <th>3-lstsq_lambda</th>\n",
       "      <th>4-lstsq_lambda</th>\n",
       "      <th>5-lstsq_lambda</th>\n",
       "      <th>0-lstsq_target</th>\n",
       "      <th>1-lstsq_target</th>\n",
       "      <th>2-lstsq_target</th>\n",
       "      <th>3-lstsq_target</th>\n",
       "      <th>4-lstsq_target</th>\n",
       "      <th>5-lstsq_target</th>\n",
       "      <th>wb_0</th>\n",
       "      <th>wb_1</th>\n",
       "      <th>wb_2</th>\n",
       "      <th>wb_3</th>\n",
       "      <th>wb_4</th>\n",
       "      <th>wb_5</th>\n",
       "      <th>wb_6</th>\n",
       "      <th>wb_7</th>\n",
       "      <th>wb_8</th>\n",
       "      <th>wb_9</th>\n",
       "      <th>wb_10</th>\n",
       "      <th>wb_11</th>\n",
       "      <th>wb_12</th>\n",
       "      <th>wb_13</th>\n",
       "      <th>wb_14</th>\n",
       "      <th>wb_15</th>\n",
       "      <th>wb_16</th>\n",
       "      <th>wb_17</th>\n",
       "      <th>wb_18</th>\n",
       "      <th>wb_19</th>\n",
       "      <th>wb_20</th>\n",
       "      <th>wb_21</th>\n",
       "      <th>wb_22</th>\n",
       "      <th>wb_23</th>\n",
       "      <th>wb_24</th>\n",
       "      <th>wb_25</th>\n",
       "      <th>wb_26</th>\n",
       "      <th>wb_27</th>\n",
       "      <th>wb_28</th>\n",
       "      <th>wb_29</th>\n",
       "      <th>wb_30</th>\n",
       "      <th>wb_31</th>\n",
       "      <th>wb_32</th>\n",
       "      <th>wb_33</th>\n",
       "      <th>wb_34</th>\n",
       "      <th>wb_35</th>\n",
       "      <th>wb_36</th>\n",
       "      <th>wb_37</th>\n",
       "      <th>wb_38</th>\n",
       "      <th>wb_39</th>\n",
       "      <th>wb_40</th>\n",
       "      <th>wb_41</th>\n",
       "      <th>wb_42</th>\n",
       "      <th>wb_43</th>\n",
       "      <th>wb_44</th>\n",
       "      <th>wb_45</th>\n",
       "      <th>wb_46</th>\n",
       "      <th>wb_47</th>\n",
       "      <th>wb_48</th>\n",
       "      <th>wb_49</th>\n",
       "      <th>wb_50</th>\n",
       "      <th>wb_51</th>\n",
       "      <th>wb_52</th>\n",
       "      <th>wb_53</th>\n",
       "      <th>wb_54</th>\n",
       "      <th>wb_55</th>\n",
       "      <th>wb_56</th>\n",
       "      <th>wb_57</th>\n",
       "      <th>wb_58</th>\n",
       "      <th>wb_59</th>\n",
       "      <th>wb_60</th>\n",
       "      <th>wb_61</th>\n",
       "      <th>wb_62</th>\n",
       "      <th>wb_63</th>\n",
       "      <th>wb_64</th>\n",
       "      <th>wb_65</th>\n",
       "      <th>wb_66</th>\n",
       "      <th>wb_67</th>\n",
       "      <th>wb_68</th>\n",
       "      <th>wb_69</th>\n",
       "      <th>wb_70</th>\n",
       "      <th>wb_71</th>\n",
       "      <th>wb_72</th>\n",
       "      <th>wb_73</th>\n",
       "      <th>wb_74</th>\n",
       "      <th>wb_75</th>\n",
       "      <th>wb_76</th>\n",
       "      <th>wb_77</th>\n",
       "      <th>wb_78</th>\n",
       "      <th>wb_79</th>\n",
       "      <th>wb_80</th>\n",
       "      <th>wb_81</th>\n",
       "      <th>wb_82</th>\n",
       "      <th>wb_83</th>\n",
       "      <th>wb_84</th>\n",
       "      <th>wb_85</th>\n",
       "      <th>wb_86</th>\n",
       "      <th>wb_87</th>\n",
       "      <th>wb_88</th>\n",
       "      <th>wb_89</th>\n",
       "      <th>wb_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.329</td>\n",
       "      <td>-0.815</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>-1.045</td>\n",
       "      <td>2.350</td>\n",
       "      <td>-6.134</td>\n",
       "      <td>5.577</td>\n",
       "      <td>-1.735</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.347</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>0.388</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.294</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.086</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.187</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.418</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.133</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.324</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>1.912</td>\n",
       "      <td>-4.323</td>\n",
       "      <td>1.916</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.303</td>\n",
       "      <td>1.063</td>\n",
       "      <td>-3.034</td>\n",
       "      <td>2.487</td>\n",
       "      <td>-1.201</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.458</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.250</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.236</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.199</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.203</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.207</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.437</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>-0.332</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-0.791</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.572</td>\n",
       "      <td>-0.944</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1.926</td>\n",
       "      <td>-5.599</td>\n",
       "      <td>2.460</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.663</td>\n",
       "      <td>1.273</td>\n",
       "      <td>-2.535</td>\n",
       "      <td>0.573</td>\n",
       "      <td>-0.376</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.492</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.085</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.262</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.361</td>\n",
       "      <td>-0.717</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-1.113</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.664</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.202</td>\n",
       "      <td>-1.396</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-1.351</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.497</td>\n",
       "      <td>-0.631</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.132</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>3.376</td>\n",
       "      <td>-7.821</td>\n",
       "      <td>3.986</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-1.766</td>\n",
       "      <td>0.678</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.602</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.568</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.227</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-0.532</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.444</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.504</td>\n",
       "      <td>0.306</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.913</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>1373158606</td>\n",
       "      <td>0.502</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.917</td>\n",
       "      <td>9.580</td>\n",
       "      <td>-30.508</td>\n",
       "      <td>38.591</td>\n",
       "      <td>-15.914</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>2.472</td>\n",
       "      <td>-2.970</td>\n",
       "      <td>2.052</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.227</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.183</td>\n",
       "      <td>1.057</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.410</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.224</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.325</td>\n",
       "      <td>1.983</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.364</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.258</td>\n",
       "      <td>-0.315</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seed  0-target  1-target  2-target  3-target  4-target  5-target  \\\n",
       "5206  1373158606    -0.399    -0.329    -0.815     0.240    -0.306     0.258   \n",
       "2771  1373158606    -0.016     0.467     0.133    -0.457    -0.709     0.212   \n",
       "5928  1373158606     0.298     0.780     0.572    -0.944    -0.869     0.051   \n",
       "103   1373158606     0.612     0.497    -0.631    -0.581    -0.259    -0.031   \n",
       "4367  1373158606     0.502    -0.106     0.268    -0.095     0.221     0.645   \n",
       "\n",
       "      0-lstsq_lambda  1-lstsq_lambda  2-lstsq_lambda  3-lstsq_lambda  \\\n",
       "5206          -0.345          -1.045           2.350          -6.134   \n",
       "2771           0.006           0.324          -0.163           1.912   \n",
       "5928           0.308           0.727           0.111           1.926   \n",
       "103            0.652           0.132          -0.660           3.376   \n",
       "4367           0.496          -0.917           9.580         -30.508   \n",
       "\n",
       "      4-lstsq_lambda  5-lstsq_lambda  0-lstsq_target  1-lstsq_target  \\\n",
       "5206           5.577          -1.735          -0.393          -0.347   \n",
       "2771          -4.323           1.916          -0.005           0.303   \n",
       "5928          -5.599           2.460           0.302           0.663   \n",
       "103           -7.821           3.986           0.627           0.329   \n",
       "4367          38.591         -15.914           0.503           0.003   \n",
       "\n",
       "      2-lstsq_target  3-lstsq_target  4-lstsq_target  5-lstsq_target   wb_0  \\\n",
       "5206          -0.912           0.580          -0.667           0.388 -0.009   \n",
       "2771           1.063          -3.034           2.487          -1.201 -0.009   \n",
       "5928           1.273          -2.535           0.573          -0.376 -0.009   \n",
       "103            0.048          -1.766           0.678          -0.316 -0.009   \n",
       "4367          -0.605           2.472          -2.970           2.052 -0.009   \n",
       "\n",
       "       wb_1  wb_2  wb_3  wb_4  wb_5  wb_6   wb_7  wb_8  wb_9  wb_10  wb_11  \\\n",
       "5206 -0.227 0.336 0.230 0.100 0.080 0.294 -0.297 0.197 0.367  0.461 -0.088   \n",
       "2771 -0.227 0.362 0.277 0.253 0.191 0.351 -0.297 0.261 0.414  0.458 -0.088   \n",
       "5928 -0.227 0.480 0.388 0.350 0.286 0.492 -0.297 0.372 0.355  0.602 -0.088   \n",
       "103  -0.227 0.455 0.367 0.107 0.039 0.448 -0.297 0.336 0.516  0.602 -0.088   \n",
       "4367 -0.227 0.285 0.081 0.246 0.183 1.057 -0.297 0.067 0.261  0.410 -0.088   \n",
       "\n",
       "      wb_12  wb_13  wb_14  wb_15  wb_16  wb_17  wb_18  wb_19  wb_20  wb_21  \\\n",
       "5206 -0.019  0.340  0.078  0.445  0.008  0.623  0.277  0.247  0.033  0.145   \n",
       "2771 -0.019  0.376  0.186  0.446  0.050  0.298  0.340  0.316  0.085  0.232   \n",
       "5928 -0.019  0.499  0.282  0.569  0.144  0.399  0.437  0.415  0.181  0.367   \n",
       "103  -0.019  0.478  0.026  0.568 -0.109  0.144  0.204  0.170 -0.118  0.316   \n",
       "4367 -0.019  0.283  0.179  0.396  0.040  0.296  0.333  0.312  0.085  0.040   \n",
       "\n",
       "      wb_22  wb_23  wb_24  wb_25  wb_26  wb_27  wb_28  wb_29  wb_30  wb_31  \\\n",
       "5206  0.091 -0.042 -0.404 -0.090  0.157 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "2771  0.250 -0.042 -0.404 -0.090  0.236 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "5928  0.009 -0.042 -0.404 -0.090  0.361 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "103   0.012 -0.042 -0.404 -0.090  0.314 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "4367  0.008 -0.042 -0.404 -0.090  0.046 -0.291 -0.209 -0.233  0.000  0.000   \n",
       "\n",
       "      wb_32  wb_33  wb_34  wb_35  wb_36  wb_37  wb_38  wb_39  wb_40  wb_41  \\\n",
       "5206  0.086  0.086 -0.100 -0.080  0.086  0.000  0.086  0.087  0.086  0.000   \n",
       "2771 -0.199 -0.214  0.001  0.001 -0.203  0.000 -0.202 -0.304 -0.209  0.000   \n",
       "5928 -0.246 -0.293  0.085  0.085 -0.317  0.000 -0.298  0.018 -0.277  0.000   \n",
       "103  -0.229 -0.240  0.180  0.182 -0.293  0.000 -0.246 -0.160 -0.240  0.000   \n",
       "4367  0.001 -0.083  0.143  0.143 -0.705  0.000 -0.069  0.072  0.001  0.000   \n",
       "\n",
       "      wb_42  wb_43  wb_44  wb_45  wb_46  wb_47  wb_48  wb_49  wb_50  wb_51  \\\n",
       "5206  0.000  0.086 -0.079  0.086 -0.023 -0.279 -0.002 -0.002 -0.040  0.086   \n",
       "2771  0.000 -0.207  0.001 -0.200  0.002 -0.000  0.001  0.000 -0.001 -0.184   \n",
       "5928  0.000 -0.321  0.087 -0.262  0.086  0.089  0.084  0.087  0.098 -0.299   \n",
       "103   0.000 -0.279  0.188 -0.226  0.184  0.191  0.174  0.183  0.227 -0.235   \n",
       "4367  0.000  0.001  0.144  0.001  0.143  0.146  0.142  0.144  0.152 -0.047   \n",
       "\n",
       "      wb_52  wb_53  wb_54  wb_55  wb_56  wb_57  wb_58  wb_59  wb_60  wb_61  \\\n",
       "5206  0.087  0.000  0.000  0.000  0.086  0.000  0.000  0.000 -0.254 -0.359   \n",
       "2771 -0.198  0.000  0.000  0.000 -0.187  0.000  0.000  0.000 -0.254 -0.359   \n",
       "5928 -0.022  0.000  0.000  0.000 -0.290  0.000  0.000  0.000 -0.254 -0.359   \n",
       "103  -0.020  0.000  0.000  0.000 -0.233  0.000  0.000  0.000 -0.254 -0.359   \n",
       "4367 -0.023  0.000  0.000  0.000 -0.052  0.000  0.000  0.000 -0.254 -0.359   \n",
       "\n",
       "      wb_62  wb_63  wb_64  wb_65  wb_66  wb_67  wb_68  wb_69  wb_70  wb_71  \\\n",
       "5206 -0.357 -0.353  0.121  0.187 -0.197 -0.408 -0.377 -0.085 -0.221 -0.352   \n",
       "2771 -0.550 -0.679  0.229  0.254 -0.437 -0.408 -0.748 -0.351 -0.332 -0.352   \n",
       "5928 -0.746 -0.957  0.331  0.361 -0.717 -0.408 -1.113  0.076 -0.537 -0.352   \n",
       "103  -0.620 -0.716  0.295  0.365 -0.532 -0.408 -0.783 -0.204 -0.444 -0.352   \n",
       "4367 -0.275 -0.224  0.287  0.325  1.983 -0.408 -0.257  0.021 -0.147 -0.352   \n",
       "\n",
       "      wb_72  wb_73  wb_74  wb_75  wb_76  wb_77  wb_78  wb_79  wb_80  wb_81  \\\n",
       "5206 -0.364 -0.215  0.121 -0.298  0.394 -0.686  0.194  0.066  0.049 -0.418   \n",
       "2771 -0.364 -0.414  0.188 -0.416  0.377  0.113  0.272  0.153  0.072 -0.876   \n",
       "5928 -0.364 -0.664  0.295 -0.615  0.544  0.212  0.370  0.252  0.202 -1.396   \n",
       "103  -0.364 -0.504  0.306 -0.519  0.679  0.162  0.293  0.190  0.322 -0.913   \n",
       "4367 -0.364 -0.126  0.260 -0.226  0.533  0.165  0.317  0.202  0.186 -0.309   \n",
       "\n",
       "      wb_82  wb_83  wb_84  wb_85  wb_86  wb_87  wb_88  wb_89  wb_90  \n",
       "5206 -0.142 -0.261 -0.383 -0.059 -0.445  0.144 -0.258 -0.315 -0.086  \n",
       "2771 -0.791 -0.261 -0.383 -0.059 -0.880  0.144 -0.258 -0.315  0.004  \n",
       "5928 -0.045 -0.261 -0.383 -0.059 -1.351  0.144 -0.258 -0.315  0.081  \n",
       "103  -0.046 -0.261 -0.383 -0.059 -0.912  0.144 -0.258 -0.315  0.163  \n",
       "4367 -0.044 -0.261 -0.383 -0.059 -0.334  0.144 -0.258 -0.315  0.139  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_net_test_dataset_list[-1].as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 18s]\n",
      "val_loss: 0.1308407187461853\n",
      "\n",
      "Best val_loss So Far: 0.1308407187461853\n",
      "Total elapsed time: 00h 00m 18s\n",
      "Epoch 1/500\n",
      "26/26 [==============================] - 3s 37ms/step - loss: 0.5342 - val_loss: 0.3626\n",
      "Epoch 2/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.3409 - val_loss: 0.2957\n",
      "Epoch 3/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2732 - val_loss: 0.2456\n",
      "Epoch 4/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2435 - val_loss: 0.2387\n",
      "Epoch 5/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2304 - val_loss: 0.2170\n",
      "Epoch 6/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2186 - val_loss: 0.2428\n",
      "Epoch 7/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2359 - val_loss: 0.2479\n",
      "Epoch 8/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2359 - val_loss: 0.2271\n",
      "Epoch 9/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2266 - val_loss: 0.2356\n",
      "Epoch 10/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.2223 - val_loss: 0.2200\n",
      "Epoch 11/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2211 - val_loss: 0.2229\n",
      "Epoch 12/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2137 - val_loss: 0.2201\n",
      "Epoch 13/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.2213 - val_loss: 0.2172\n",
      "Epoch 14/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.2172 - val_loss: 0.2149\n",
      "Epoch 15/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2129 - val_loss: 0.2123\n",
      "Epoch 16/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2157 - val_loss: 0.2088\n",
      "Epoch 17/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2128 - val_loss: 0.2331\n",
      "Epoch 18/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2205 - val_loss: 0.2110\n",
      "Epoch 19/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2096 - val_loss: 0.2091\n",
      "Epoch 20/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2083 - val_loss: 0.2111\n",
      "Epoch 21/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2060 - val_loss: 0.2055\n",
      "Epoch 22/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2018 - val_loss: 0.2073\n",
      "Epoch 23/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2031 - val_loss: 0.2018\n",
      "Epoch 24/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1985 - val_loss: 0.2018\n",
      "Epoch 25/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1994 - val_loss: 0.1990\n",
      "Epoch 26/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1971 - val_loss: 0.1966\n",
      "Epoch 27/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1911 - val_loss: 0.1994\n",
      "Epoch 28/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1922 - val_loss: 0.1933\n",
      "Epoch 29/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1882 - val_loss: 0.1925\n",
      "Epoch 30/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1837 - val_loss: 0.1969\n",
      "Epoch 31/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1896 - val_loss: 0.1934\n",
      "Epoch 32/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1858 - val_loss: 0.1905\n",
      "Epoch 33/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1840 - val_loss: 0.1910\n",
      "Epoch 34/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1800 - val_loss: 0.1890\n",
      "Epoch 35/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1799 - val_loss: 0.1832\n",
      "Epoch 36/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1778 - val_loss: 0.1841\n",
      "Epoch 37/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1777 - val_loss: 0.1846\n",
      "Epoch 38/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1779 - val_loss: 0.1815\n",
      "Epoch 39/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1772 - val_loss: 0.1787\n",
      "Epoch 40/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1734 - val_loss: 0.1819\n",
      "Epoch 41/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1749 - val_loss: 0.1774\n",
      "Epoch 42/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1747 - val_loss: 0.1794\n",
      "Epoch 43/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1734 - val_loss: 0.1773\n",
      "Epoch 44/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1761 - val_loss: 0.1675\n",
      "Epoch 45/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1698 - val_loss: 0.1760\n",
      "Epoch 46/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1714 - val_loss: 0.1684\n",
      "Epoch 47/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1701 - val_loss: 0.1669\n",
      "Epoch 48/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1689 - val_loss: 0.1686\n",
      "Epoch 49/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1663 - val_loss: 0.1708\n",
      "Epoch 50/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1691 - val_loss: 0.1706\n",
      "Epoch 51/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1705 - val_loss: 0.1703\n",
      "Epoch 52/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1742 - val_loss: 0.1701\n",
      "Epoch 53/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1699 - val_loss: 0.1682\n",
      "Epoch 54/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1699 - val_loss: 0.1691\n",
      "Epoch 55/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1682 - val_loss: 0.1624\n",
      "Epoch 56/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1633 - val_loss: 0.1672\n",
      "Epoch 57/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1664 - val_loss: 0.1767\n",
      "Epoch 58/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1707 - val_loss: 0.1628\n",
      "Epoch 59/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1649 - val_loss: 0.1669\n",
      "Epoch 60/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1669 - val_loss: 0.1668\n",
      "Epoch 61/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1659 - val_loss: 0.1746\n",
      "Epoch 62/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1678 - val_loss: 0.1603\n",
      "Epoch 63/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1627 - val_loss: 0.1653\n",
      "Epoch 64/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1619 - val_loss: 0.1603\n",
      "Epoch 65/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1606 - val_loss: 0.1640\n",
      "Epoch 66/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1600 - val_loss: 0.1643\n",
      "Epoch 67/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1588 - val_loss: 0.1574\n",
      "Epoch 68/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1585 - val_loss: 0.1750\n",
      "Epoch 69/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1641 - val_loss: 0.1649\n",
      "Epoch 70/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1631 - val_loss: 0.1616\n",
      "Epoch 71/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1651 - val_loss: 0.1633\n",
      "Epoch 72/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1661 - val_loss: 0.1690\n",
      "Epoch 73/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1731 - val_loss: 0.1687\n",
      "Epoch 74/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1679 - val_loss: 0.1689\n",
      "Epoch 75/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1718 - val_loss: 0.1669\n",
      "Epoch 76/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1703 - val_loss: 0.1878\n",
      "Epoch 77/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1980 - val_loss: 0.1843\n",
      "Epoch 78/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1933 - val_loss: 0.2248\n",
      "Epoch 79/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2110 - val_loss: 0.1819\n",
      "Epoch 80/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1957 - val_loss: 0.1865\n",
      "Epoch 81/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2006 - val_loss: 0.2132\n",
      "Epoch 82/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2258 - val_loss: 0.2125\n",
      "Epoch 83/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2404 - val_loss: 0.2010\n",
      "Epoch 84/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2203 - val_loss: 0.1902\n",
      "Epoch 85/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.2058 - val_loss: 0.1921\n",
      "Epoch 86/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1939 - val_loss: 0.1775\n",
      "Epoch 87/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1844 - val_loss: 0.1876\n",
      "Epoch 88/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1885 - val_loss: 0.1859\n",
      "Epoch 89/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1978 - val_loss: 0.2056\n",
      "Epoch 90/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1972 - val_loss: 0.1831\n",
      "Epoch 91/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1955 - val_loss: 0.1847\n",
      "Epoch 92/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1988 - val_loss: 0.1912\n",
      "Epoch 93/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1930 - val_loss: 0.1706\n",
      "Epoch 94/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1875 - val_loss: 0.1984\n",
      "Epoch 95/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2001 - val_loss: 0.2054\n",
      "Epoch 96/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1945 - val_loss: 0.2002\n",
      "Epoch 97/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1965 - val_loss: 0.1947\n",
      "Epoch 98/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1874 - val_loss: 0.1794\n",
      "Epoch 99/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1777 - val_loss: 0.1732\n",
      "Epoch 100/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1731 - val_loss: 0.1701\n",
      "Epoch 101/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1725 - val_loss: 0.1702\n",
      "Epoch 102/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1724 - val_loss: 0.1687\n",
      "Epoch 103/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1698 - val_loss: 0.1637\n",
      "Epoch 104/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1641 - val_loss: 0.1534\n",
      "Epoch 105/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1640 - val_loss: 0.1576\n",
      "Epoch 106/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1662 - val_loss: 0.1398\n",
      "Epoch 107/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1429 - val_loss: 0.1484\n",
      "Epoch 108/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1607 - val_loss: 0.1503\n",
      "Epoch 109/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1841 - val_loss: 0.1865\n",
      "Epoch 110/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1940 - val_loss: 0.1926\n",
      "Epoch 111/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1987 - val_loss: 0.2058\n",
      "Epoch 112/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1932 - val_loss: 0.1873\n",
      "Epoch 113/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1759 - val_loss: 0.1745\n",
      "Epoch 114/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1717 - val_loss: 0.1791\n",
      "Epoch 115/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1731 - val_loss: 0.1728\n",
      "Epoch 116/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1697 - val_loss: 0.1723\n",
      "Epoch 117/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1695 - val_loss: 0.1684\n",
      "Epoch 118/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1670 - val_loss: 0.1643\n",
      "Epoch 119/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1737 - val_loss: 0.1776\n",
      "Epoch 120/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1827 - val_loss: 0.1698\n",
      "Epoch 121/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1718 - val_loss: 0.1611\n",
      "Epoch 122/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1640 - val_loss: 0.1682\n",
      "Epoch 123/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1647 - val_loss: 0.1686\n",
      "Epoch 124/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1649 - val_loss: 0.1749\n",
      "Epoch 125/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1671 - val_loss: 0.1648\n",
      "Epoch 126/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1649 - val_loss: 0.1650\n",
      "Epoch 127/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1662 - val_loss: 0.1715\n",
      "Epoch 128/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1655 - val_loss: 0.1687\n",
      "Epoch 129/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1660 - val_loss: 0.1756\n",
      "Epoch 130/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1721 - val_loss: 0.1804\n",
      "Epoch 131/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1723 - val_loss: 0.1720\n",
      "Epoch 132/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1682 - val_loss: 0.1725\n",
      "Epoch 133/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1699 - val_loss: 0.1725\n",
      "Epoch 134/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1696 - val_loss: 0.1768\n",
      "Epoch 135/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1703 - val_loss: 0.1717\n",
      "Epoch 136/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1697 - val_loss: 0.1680\n",
      "Epoch 137/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1732 - val_loss: 0.1724\n",
      "Epoch 138/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1675 - val_loss: 0.1707\n",
      "Epoch 139/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1705 - val_loss: 0.1725\n",
      "Epoch 140/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1790 - val_loss: 0.1721\n",
      "Epoch 141/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1779 - val_loss: 0.1700\n",
      "Epoch 142/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1713 - val_loss: 0.1683\n",
      "Epoch 143/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1684 - val_loss: 0.1700\n",
      "Epoch 144/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1664 - val_loss: 0.1675\n",
      "Epoch 145/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1639 - val_loss: 0.1696\n",
      "Epoch 146/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1679 - val_loss: 0.1648\n",
      "Epoch 147/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1677 - val_loss: 0.1659\n",
      "Epoch 148/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1640 - val_loss: 0.1643\n",
      "Epoch 149/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1631 - val_loss: 0.1623\n",
      "Epoch 150/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1659 - val_loss: 0.1687\n",
      "Epoch 151/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1696 - val_loss: 0.1715\n",
      "Epoch 152/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1661 - val_loss: 0.1650\n",
      "Epoch 153/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1696 - val_loss: 0.1657\n",
      "Epoch 154/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1648 - val_loss: 0.1677\n",
      "Epoch 155/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1687 - val_loss: 0.1684\n",
      "Epoch 156/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1648 - val_loss: 0.1648\n",
      "Epoch 157/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1630 - val_loss: 0.1667\n",
      "Epoch 158/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1663 - val_loss: 0.1634\n",
      "Epoch 159/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1665 - val_loss: 0.1680\n",
      "Epoch 160/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1697 - val_loss: 0.1675\n",
      "Epoch 161/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1681 - val_loss: 0.1654\n",
      "Epoch 162/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1635 - val_loss: 0.1629\n",
      "Epoch 163/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1632 - val_loss: 0.1604\n",
      "Epoch 164/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1647 - val_loss: 0.1640\n",
      "Epoch 165/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1634 - val_loss: 0.1678\n",
      "Epoch 166/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1648 - val_loss: 0.1631\n",
      "Epoch 167/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1640 - val_loss: 0.1685\n",
      "Epoch 168/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1696 - val_loss: 0.1671\n",
      "Epoch 169/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1678 - val_loss: 0.1615\n",
      "Epoch 170/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1680 - val_loss: 0.1630\n",
      "Epoch 171/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1648 - val_loss: 0.1598\n",
      "Epoch 172/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1613 - val_loss: 0.1630\n",
      "Epoch 173/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1632 - val_loss: 0.1640\n",
      "Epoch 174/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1629 - val_loss: 0.1631\n",
      "Epoch 175/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1646 - val_loss: 0.1647\n",
      "Epoch 176/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1636 - val_loss: 0.1632\n",
      "Epoch 177/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1660 - val_loss: 0.1609\n",
      "Epoch 178/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1634 - val_loss: 0.1609\n",
      "Epoch 179/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1608 - val_loss: 0.1622\n",
      "Epoch 180/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1644 - val_loss: 0.1616\n",
      "Epoch 181/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1628 - val_loss: 0.1641\n",
      "Epoch 182/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1638 - val_loss: 0.1615\n",
      "Epoch 183/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1636 - val_loss: 0.1657\n",
      "Epoch 184/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1646 - val_loss: 0.1659\n",
      "Epoch 185/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1695 - val_loss: 0.1718\n",
      "Epoch 186/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1678 - val_loss: 0.1642\n",
      "Epoch 187/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1687 - val_loss: 0.1625\n",
      "Epoch 188/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1691 - val_loss: 0.1618\n",
      "Epoch 189/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1660 - val_loss: 0.1630\n",
      "Epoch 190/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1637 - val_loss: 0.1619\n",
      "Epoch 191/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1627 - val_loss: 0.1606\n",
      "Epoch 192/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1617 - val_loss: 0.1609\n",
      "Epoch 193/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1607 - val_loss: 0.1613\n",
      "Epoch 194/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1609 - val_loss: 0.1587\n",
      "Epoch 195/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1615 - val_loss: 0.1590\n",
      "Epoch 196/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1606 - val_loss: 0.1588\n",
      "Epoch 197/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1618 - val_loss: 0.1608\n",
      "Epoch 198/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1614 - val_loss: 0.1567\n",
      "Epoch 199/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1600 - val_loss: 0.1601\n",
      "Epoch 200/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1619 - val_loss: 0.1574\n",
      "Epoch 201/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1595 - val_loss: 0.1595\n",
      "Epoch 202/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1617 - val_loss: 0.1603\n",
      "Epoch 203/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1598 - val_loss: 0.1591\n",
      "Epoch 204/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1618 - val_loss: 0.1611\n",
      "Epoch 205/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1671 - val_loss: 0.1669\n",
      "Epoch 206/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1695 - val_loss: 0.1618\n",
      "Epoch 207/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1700 - val_loss: 0.1606\n",
      "Epoch 208/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1611 - val_loss: 0.1591\n",
      "Epoch 209/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1619 - val_loss: 0.1573\n",
      "Epoch 210/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1610 - val_loss: 0.1581\n",
      "Epoch 211/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1592 - val_loss: 0.1564\n",
      "Epoch 212/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1589 - val_loss: 0.1592\n",
      "Epoch 213/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1573 - val_loss: 0.1566\n",
      "Epoch 214/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1574 - val_loss: 0.1578\n",
      "Epoch 215/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1584 - val_loss: 0.1561\n",
      "Epoch 216/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1565 - val_loss: 0.1579\n",
      "Epoch 217/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1579 - val_loss: 0.1568\n",
      "Epoch 218/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1580 - val_loss: 0.1583\n",
      "Epoch 219/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1603 - val_loss: 0.1579\n",
      "Epoch 220/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1598 - val_loss: 0.1570\n",
      "Epoch 221/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1573 - val_loss: 0.1607\n",
      "Epoch 222/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1580 - val_loss: 0.1565\n",
      "Epoch 223/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1570 - val_loss: 0.1600\n",
      "Epoch 224/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1565 - val_loss: 0.1562\n",
      "Epoch 225/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1573 - val_loss: 0.1560\n",
      "Epoch 226/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1556 - val_loss: 0.1553\n",
      "Epoch 227/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1564 - val_loss: 0.1566\n",
      "Epoch 228/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1570 - val_loss: 0.1558\n",
      "Epoch 229/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1566 - val_loss: 0.1558\n",
      "Epoch 230/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1579 - val_loss: 0.1560\n",
      "Epoch 231/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1573 - val_loss: 0.1608\n",
      "Epoch 232/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1591 - val_loss: 0.1620\n",
      "Epoch 233/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1589 - val_loss: 0.1600\n",
      "Epoch 234/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1588 - val_loss: 0.1608\n",
      "Epoch 235/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1593 - val_loss: 0.1612\n",
      "Epoch 236/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1580 - val_loss: 0.1640\n",
      "Epoch 237/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1606 - val_loss: 0.1609\n",
      "Epoch 238/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1598 - val_loss: 0.1610\n",
      "Epoch 239/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1592 - val_loss: 0.1618\n",
      "Epoch 240/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1584 - val_loss: 0.1625\n",
      "Epoch 241/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1589 - val_loss: 0.1654\n",
      "Epoch 242/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1587 - val_loss: 0.1610\n",
      "Epoch 243/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1593 - val_loss: 0.1633\n",
      "Epoch 244/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1598 - val_loss: 0.1664\n",
      "Epoch 245/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1645 - val_loss: 0.1658\n",
      "Epoch 246/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1721 - val_loss: 0.1576\n",
      "Epoch 247/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1628 - val_loss: 0.1599\n",
      "Epoch 248/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1624 - val_loss: 0.1613\n",
      "Epoch 249/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1630 - val_loss: 0.1622\n",
      "Epoch 250/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1613 - val_loss: 0.1622\n",
      "Epoch 251/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1614 - val_loss: 0.1616\n",
      "Epoch 252/500\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.1614 - val_loss: 0.1626\n",
      "Epoch 253/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1623 - val_loss: 0.1645\n",
      "Epoch 254/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1645 - val_loss: 0.1646\n",
      "Epoch 255/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1650 - val_loss: 0.1718\n",
      "Epoch 256/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1644 - val_loss: 0.1709\n",
      "Epoch 257/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1612 - val_loss: 0.1648\n",
      "Epoch 258/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1602 - val_loss: 0.1612\n",
      "Epoch 259/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1591 - val_loss: 0.1606\n",
      "Epoch 260/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1582 - val_loss: 0.1611\n",
      "Epoch 261/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1587 - val_loss: 0.1622\n",
      "Epoch 262/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1580 - val_loss: 0.1606\n",
      "Epoch 263/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1588 - val_loss: 0.1616\n",
      "Epoch 264/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1585 - val_loss: 0.1610\n",
      "Epoch 265/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1580 - val_loss: 0.1603\n",
      "Epoch 266/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1575 - val_loss: 0.1625\n",
      "Epoch 267/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1609 - val_loss: 0.1606\n",
      "Epoch 268/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1582 - val_loss: 0.1605\n",
      "Epoch 269/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1580 - val_loss: 0.1606\n",
      "Epoch 270/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1582 - val_loss: 0.1616\n",
      "Epoch 271/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1580 - val_loss: 0.1622\n",
      "Epoch 272/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1582 - val_loss: 0.1651\n",
      "Epoch 273/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1596 - val_loss: 0.1658\n",
      "Epoch 274/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1592 - val_loss: 0.1670\n",
      "Epoch 275/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1602 - val_loss: 0.1675\n",
      "Epoch 276/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1610 - val_loss: 0.1698\n",
      "Epoch 277/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1610 - val_loss: 0.1707\n",
      "Epoch 278/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1659 - val_loss: 0.1770\n",
      "Epoch 279/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1663 - val_loss: 0.1754\n",
      "Epoch 280/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1666 - val_loss: 0.1751\n",
      "Epoch 281/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1687 - val_loss: 0.1770\n",
      "Epoch 282/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1696 - val_loss: 0.1731\n",
      "Epoch 283/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1697 - val_loss: 0.1741\n",
      "Epoch 284/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1736 - val_loss: 0.1718\n",
      "Epoch 285/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1726 - val_loss: 0.1823\n",
      "Epoch 286/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1717 - val_loss: 0.1834\n",
      "Epoch 287/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1711 - val_loss: 0.1827\n",
      "Epoch 288/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1741 - val_loss: 0.1781\n",
      "Epoch 289/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1732 - val_loss: 0.1787\n",
      "Epoch 290/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1712 - val_loss: 0.1771\n",
      "Epoch 291/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1725 - val_loss: 0.1789\n",
      "Epoch 292/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1712 - val_loss: 0.1780\n",
      "Epoch 293/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1703 - val_loss: 0.1745\n",
      "Epoch 294/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1697 - val_loss: 0.1745\n",
      "Epoch 295/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1722 - val_loss: 0.1891\n",
      "Epoch 296/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1751 - val_loss: 0.1803\n",
      "Epoch 297/500\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1798 - val_loss: 0.1824\n",
      "Epoch 298/500\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1771 - val_loss: 0.1858\n",
      "Epoch 299/500\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1787 - val_loss: 0.1875\n",
      "Epoch 300/500\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1782 - val_loss: 0.1856\n",
      "Epoch 301/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1777 - val_loss: 0.1815\n",
      "Epoch 302/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1781 - val_loss: 0.1855\n",
      "Epoch 303/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1787 - val_loss: 0.1895\n",
      "Epoch 304/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1793 - val_loss: 0.1832\n",
      "Epoch 305/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1782 - val_loss: 0.1842\n",
      "Epoch 306/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1764 - val_loss: 0.1851\n",
      "Epoch 307/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1749 - val_loss: 0.1854\n",
      "Epoch 308/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1753 - val_loss: 0.1802\n",
      "Epoch 309/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1748 - val_loss: 0.1813\n",
      "Epoch 310/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1754 - val_loss: 0.1790\n",
      "Epoch 311/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1728 - val_loss: 0.1813\n",
      "Epoch 312/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1727 - val_loss: 0.1790\n",
      "Epoch 313/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1713 - val_loss: 0.1823\n",
      "Epoch 314/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1722 - val_loss: 0.1820\n",
      "Epoch 315/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1718 - val_loss: 0.1798\n",
      "Epoch 316/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1708 - val_loss: 0.1823\n",
      "Epoch 317/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1726 - val_loss: 0.1817\n",
      "Epoch 318/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1723 - val_loss: 0.1812\n",
      "Epoch 319/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1725 - val_loss: 0.1814\n",
      "Epoch 320/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1713 - val_loss: 0.1790\n",
      "Epoch 321/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1701 - val_loss: 0.1805\n",
      "Epoch 322/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1707 - val_loss: 0.1811\n",
      "Epoch 323/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1704 - val_loss: 0.1852\n",
      "Epoch 324/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1722 - val_loss: 0.1787\n",
      "Epoch 325/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1710 - val_loss: 0.1813\n",
      "Epoch 326/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1709 - val_loss: 0.1816\n",
      "Epoch 327/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1731 - val_loss: 0.1802\n",
      "Epoch 328/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1700 - val_loss: 0.1788\n",
      "Epoch 329/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1698 - val_loss: 0.1811\n",
      "Epoch 330/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1717 - val_loss: 0.1803\n",
      "Epoch 331/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1713 - val_loss: 0.1827\n",
      "Epoch 332/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1714 - val_loss: 0.1784\n",
      "Epoch 333/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1726 - val_loss: 0.1821\n",
      "Epoch 334/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1775 - val_loss: 0.1844\n",
      "Epoch 335/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1730 - val_loss: 0.1804\n",
      "Epoch 336/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1710 - val_loss: 0.1875\n",
      "Epoch 337/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1708 - val_loss: 0.1792\n",
      "Epoch 338/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1700 - val_loss: 0.1809\n",
      "Epoch 339/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1699 - val_loss: 0.1787\n",
      "Epoch 340/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1695 - val_loss: 0.1813\n",
      "Epoch 341/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1701 - val_loss: 0.1788\n",
      "Epoch 342/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1687 - val_loss: 0.1789\n",
      "Epoch 343/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1692 - val_loss: 0.1808\n",
      "Epoch 344/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1720 - val_loss: 0.1804\n",
      "Epoch 345/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1706 - val_loss: 0.1781\n",
      "Epoch 346/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1687 - val_loss: 0.1777\n",
      "Epoch 347/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1700 - val_loss: 0.1802\n",
      "Epoch 348/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1693 - val_loss: 0.1790\n",
      "Epoch 349/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1727 - val_loss: 0.1821\n",
      "Epoch 350/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1740 - val_loss: 0.1851\n",
      "Epoch 351/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1746 - val_loss: 0.1844\n",
      "Epoch 352/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1716 - val_loss: 0.1812\n",
      "Epoch 353/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1699 - val_loss: 0.1807\n",
      "Epoch 354/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1699 - val_loss: 0.1801\n",
      "Epoch 355/500\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1690 - val_loss: 0.1789\n",
      "Epoch 356/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1693 - val_loss: 0.1809\n",
      "Epoch 357/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1707 - val_loss: 0.1818\n",
      "Epoch 358/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1719 - val_loss: 0.1842\n",
      "Epoch 359/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1706 - val_loss: 0.1820\n",
      "Epoch 360/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1695 - val_loss: 0.1805\n",
      "Epoch 361/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1686 - val_loss: 0.1796\n",
      "Epoch 362/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1691 - val_loss: 0.1798\n",
      "Epoch 363/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1692 - val_loss: 0.1801\n",
      "Epoch 364/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1699 - val_loss: 0.1791\n",
      "Epoch 365/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1709 - val_loss: 0.1814\n",
      "Epoch 366/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1736 - val_loss: 0.1785\n",
      "Epoch 367/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1737 - val_loss: 0.1834\n",
      "Epoch 368/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1788 - val_loss: 0.1833\n",
      "Epoch 369/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1734 - val_loss: 0.1808\n",
      "Epoch 370/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1706 - val_loss: 0.1804\n",
      "Epoch 371/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1698 - val_loss: 0.1790\n",
      "Epoch 372/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1720 - val_loss: 0.1817\n",
      "Epoch 373/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1746 - val_loss: 0.1803\n",
      "Epoch 374/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1703 - val_loss: 0.1782\n",
      "Epoch 375/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1699 - val_loss: 0.1796\n",
      "Epoch 376/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1701 - val_loss: 0.1816\n",
      "Epoch 377/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1708 - val_loss: 0.1791\n",
      "Epoch 378/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1685 - val_loss: 0.1805\n",
      "Epoch 379/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1705 - val_loss: 0.1820\n",
      "Epoch 380/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1758 - val_loss: 0.1828\n",
      "Epoch 381/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1752 - val_loss: 0.1884\n",
      "Epoch 382/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1787 - val_loss: 0.1885\n",
      "Epoch 383/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1789 - val_loss: 0.1912\n",
      "Epoch 384/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1773 - val_loss: 0.1897\n",
      "Epoch 385/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1775 - val_loss: 0.1893\n",
      "Epoch 386/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1784 - val_loss: 0.1914\n",
      "Epoch 387/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1790 - val_loss: 0.1919\n",
      "Epoch 388/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1794 - val_loss: 0.1926\n",
      "Epoch 389/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1787 - val_loss: 0.1889\n",
      "Epoch 390/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1778 - val_loss: 0.1929\n",
      "Epoch 391/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1770 - val_loss: 0.1907\n",
      "Epoch 392/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1763 - val_loss: 0.1917\n",
      "Epoch 393/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1763 - val_loss: 0.1908\n",
      "Epoch 394/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1763 - val_loss: 0.1919\n",
      "Epoch 395/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1761 - val_loss: 0.1938\n",
      "Epoch 396/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1758 - val_loss: 0.1918\n",
      "Epoch 397/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1744 - val_loss: 0.1918\n",
      "Epoch 398/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1760 - val_loss: 0.1969\n",
      "Epoch 399/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1769 - val_loss: 0.1934\n",
      "Epoch 400/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1754 - val_loss: 0.1904\n",
      "Epoch 401/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1756 - val_loss: 0.1888\n",
      "Epoch 402/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1752 - val_loss: 0.1909\n",
      "Epoch 403/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1749 - val_loss: 0.1889\n",
      "Epoch 404/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1746 - val_loss: 0.1900\n",
      "Epoch 405/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1748 - val_loss: 0.1935\n",
      "Epoch 406/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1736 - val_loss: 0.1946\n",
      "Epoch 407/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1745 - val_loss: 0.1924\n",
      "Epoch 408/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1736 - val_loss: 0.1936\n",
      "Epoch 409/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1737 - val_loss: 0.1896\n",
      "Epoch 410/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1735 - val_loss: 0.1931\n",
      "Epoch 411/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1735 - val_loss: 0.1926\n",
      "Epoch 412/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1734 - val_loss: 0.1931\n",
      "Epoch 413/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1734 - val_loss: 0.1923\n",
      "Epoch 414/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1757 - val_loss: 0.1981\n",
      "Epoch 415/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1770 - val_loss: 0.1941\n",
      "Epoch 416/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1734 - val_loss: 0.1935\n",
      "Epoch 417/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1722 - val_loss: 0.1923\n",
      "Epoch 418/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1731 - val_loss: 0.1957\n",
      "Epoch 419/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1752 - val_loss: 0.1974\n",
      "Epoch 420/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1756 - val_loss: 0.1947\n",
      "Epoch 421/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1737 - val_loss: 0.1924\n",
      "Epoch 422/500\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.1730 - val_loss: 0.1954\n",
      "Epoch 423/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1748 - val_loss: 0.1940\n",
      "Epoch 424/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1778 - val_loss: 0.1898\n",
      "Epoch 425/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1812 - val_loss: 0.1855\n",
      "Epoch 426/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1792 - val_loss: 0.1970\n",
      "Epoch 427/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1833 - val_loss: 0.1864\n",
      "Epoch 428/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1799 - val_loss: 0.1916\n",
      "Epoch 429/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1821 - val_loss: 0.1901\n",
      "Epoch 430/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1838 - val_loss: 0.1946\n",
      "Epoch 431/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1883 - val_loss: 0.1947\n",
      "Epoch 432/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1889 - val_loss: 0.1843\n",
      "Epoch 433/500\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1798 - val_loss: 0.1818\n",
      "Epoch 434/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1767 - val_loss: 0.1816\n",
      "Epoch 435/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1759 - val_loss: 0.1821\n",
      "Epoch 436/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1755 - val_loss: 0.1829\n",
      "Epoch 437/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1772 - val_loss: 0.1815\n",
      "Epoch 438/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1779 - val_loss: 0.1818\n",
      "Epoch 439/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1802 - val_loss: 0.1829\n",
      "Epoch 440/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1781 - val_loss: 0.1817\n",
      "Epoch 441/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1775 - val_loss: 0.1809\n",
      "Epoch 442/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1753 - val_loss: 0.1825\n",
      "Epoch 443/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1756 - val_loss: 0.1825\n",
      "Epoch 444/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1751 - val_loss: 0.1821\n",
      "Epoch 445/500\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1763 - val_loss: 0.1822\n",
      "Epoch 446/500\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1751 - val_loss: 0.1802\n",
      "Epoch 447/500\n",
      "21/26 [=======================>......] - ETA: 0s - loss: 0.1753"
     ]
    }
   ],
   "source": [
    "(history_list, \n",
    "scores_list, \n",
    "\n",
    "function_values_complete_list, \n",
    "function_values_valid_list, \n",
    "function_values_test_list, \n",
    "\n",
    "inet_preds_list, \n",
    "inet_preds_valid_list, \n",
    "inet_preds_test_list, \n",
    " \n",
    "per_network_preds_list,\n",
    "\n",
    "distrib_dict_list,\n",
    "model_list) = calculate_interpretation_net_results(lambda_net_train_dataset_list, \n",
    "                                                   lambda_net_valid_dataset_list, \n",
    "                                                   lambda_net_test_dataset_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Interpretation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize = tf.constant([float(i) for i in range(interpretation_net_output_shape)])\n",
    "\n",
    "if interpretation_net_output_monomials != None:\n",
    "    poly_optimize_coeffs = poly_optimize[:interpretation_net_output_monomials]\n",
    "\n",
    "    poly_optimize_identifiers_list = []\n",
    "    for i in range(interpretation_net_output_monomials):\n",
    "        poly_optimize_identifiers = tf.math.softmax(poly_optimize[sparsity*i+interpretation_net_output_monomials:sparsity*(i+1)+interpretation_net_output_monomials])\n",
    "        poly_optimize_identifiers_list.append(poly_optimize_identifiers)\n",
    "    poly_optimize_identifiers_list = tf.keras.backend.flatten(poly_optimize_identifiers_list)\n",
    "    poly_optimize = tf.concat([poly_optimize_coeffs, poly_optimize_identifiers_list], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nas:\n",
    "    for trial in history_list[-1]: \n",
    "        print(trial.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_list) >= 1:\n",
    "    print(model_list[-1].summary())\n",
    "    print(model_list[-1].get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_with_real_function:\n",
    "    keys = ['inetPoly_VS_targetPoly_test', 'perNetworkPoly_VS_targetPoly_test', 'predLambda_VS_targetPoly_test', 'lstsqLambda_VS_targetPoly_test', 'lstsqTarget_VS_targetPoly_test']\n",
    "else:\n",
    "    keys = ['inetPoly_VS_predLambda_test', 'inetPoly_VS_lstsqLambda_test', 'perNetworkPoly_VS_predLambda_test', 'perNetworkPoly_VS_lstsqLambda_test', 'lstsqLambda_VS_predLambda_test', 'predLambda_VS_targetPoly_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T15:23:49.656430Z",
     "start_time": "2021-01-05T15:23:49.578409Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:55.162513Z",
     "start_time": "2021-01-08T11:56:54.472198Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_list[-1]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:56:56.434915Z",
     "start_time": "2021-01-08T11:56:55.669304Z"
    }
   },
   "outputs": [],
   "source": [
    "distrib_dict_list[-1]['R2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.196958Z",
     "start_time": "2021-01-07T20:33:18.177611Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_list[-1]['R2'].loc['predLambda_VS_lstsqTarget_test']))\n",
    "polynomial_inet = inet_preds_test_list[-1][index_min]\n",
    "\n",
    "print(distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test'][index_min])\n",
    "\n",
    "print_polynomial_from_coefficients(polynomial_inet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.514683Z",
     "start_time": "2021-01-07T20:33:18.506614Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_list[-1]['R2'].loc['predLambda_VS_lstsqTarget_test']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "\n",
    "print(distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test'][index_min])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:18.820457Z",
     "start_time": "2021-01-07T20:33:18.813628Z"
    }
   },
   "outputs": [],
   "source": [
    "index_min = int(np.argmin(distrib_dict_list[-1]['R2'].loc['predLambda_VS_lstsqTarget_test']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "polynomial_target = lambda_net_test_dataset.target_polynomial_list[index_min]\n",
    "print_polynomial_from_coefficients(polynomial_target, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:22.149541Z",
     "start_time": "2021-01-07T20:33:22.141264Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_list[-1]['R2'].loc['predLambda_VS_lstsqTarget_test']))\n",
    "polynomial_inet = inet_preds_test_list[-1][index_max]\n",
    "\n",
    "print(distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test'][index_max])\n",
    "\n",
    "print_polynomial_from_coefficients(polynomial_inet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:22.526765Z",
     "start_time": "2021-01-07T20:33:22.518702Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_list[-1]['R2'].loc['predLambda_VS_lstsqTarget_test']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "\n",
    "print(distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test'][index_max])\n",
    "\n",
    "polynomial_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index_max]\n",
    "print_polynomial_from_coefficients(polynomial_lambda, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T20:33:23.155159Z",
     "start_time": "2021-01-07T20:33:23.146225Z"
    }
   },
   "outputs": [],
   "source": [
    "index_max = int(np.argmax(distrib_dict_list[-1]['R2'].loc['predLambda_VS_lstsqTarget_test']))\n",
    "#polynomial_target = lambda_net_test_dataset.get_lambda_net_by_lambda_index(index_min).target_polynomial\n",
    "polynomial_target = lambda_net_test_dataset.target_polynomial_list[index_max]\n",
    "print_polynomial_from_coefficients(polynomial_target, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.304392Z",
     "start_time": "2021-01-07T15:49:42.291475Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_inet = distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test']\n",
    "print('Mean: ' + str(np.mean(r2_values_inet)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_inet = r2_values_inet[r2_values_inet>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_inet)) + ' (' + str(r2_values_positive_inet.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:42.833577Z",
     "start_time": "2021-01-07T15:49:42.821286Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_values_lstsq_lambda = distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test']\n",
    "print('Mean: ' + str(np.mean(r2_values_lstsq_lambda)) + ' (' + str(r2_values_inet.shape[0]) + ' Samples)')\n",
    "\n",
    "r2_values_positive_lstsq_lambda = r2_values_lstsq_lambda[r2_values_lstsq_lambda>0]\n",
    "print('Mean (only positive): ' + str(np.mean(r2_values_positive_lstsq_lambda)) + ' (' + str(r2_values_positive_lstsq_lambda.shape[0]) + ' Samples)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.179590Z",
     "start_time": "2021-01-07T15:49:43.001746Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(distrib_dict_list[-1]['MAE'].loc['inetPoly_VS_targetPoly_test'][distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test'] > -50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.417509Z",
     "start_time": "2021-01-07T15:49:44.181928Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(distrib_dict_list[-1]['MAE'].loc['lstsqLambda_VS_targetPoly_test'][distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test'] > -50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:44.935810Z",
     "start_time": "2021-01-07T15:49:44.419772Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test'][distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test'] > -50], binwidth=0.2)\n",
    "p.set(xlim=(-30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:45.795559Z",
     "start_time": "2021-01-07T15:49:44.938329Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test'][distrib_dict_list[-1]['R2'].loc['inetPoly_VS_targetPoly_test'] > -50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:47.784878Z",
     "start_time": "2021-01-07T15:49:45.797362Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test'][distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test'] > -50], binwidth=0.2)\n",
    "p.set(xlim=(-10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.252121Z",
     "start_time": "2021-01-07T15:49:47.786575Z"
    }
   },
   "outputs": [],
   "source": [
    "p = sns.histplot(distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test'][distrib_dict_list[-1]['R2'].loc['lstsqLambda_VS_targetPoly_test'] > -50], binwidth=0.1)\n",
    "p.set(xlim=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.410283Z",
     "start_time": "2021-01-07T15:49:48.254228Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history[list(history.keys())[1]])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history[list(history.keys())[len(history.keys())//2+1]]) \n",
    "    plt.title('model ' + list(history.keys())[1])\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/metric_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:48.567983Z",
     "start_time": "2021-01-07T15:49:48.413234Z"
    }
   },
   "outputs": [],
   "source": [
    "if not nas:\n",
    "    history = history_list[-1]\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    if consider_labels_training or evaluate_with_real_function:\n",
    "        plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.savefig('./data/results/' + path_identifier_interpretation_net_data + '/loss_' + '_epoch_' + str(epochs_lambda).zfill(3) + '.png')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Epoch/Sampes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_list) > 1:\n",
    "    plot_metric_list = ['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_list) > 1:\n",
    "    plot_metric_list = ['MAE FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_list, plot_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(scores_list) > 1:\n",
    "    plot_metric_list = ['R2 FV']\n",
    "\n",
    "    generate_inet_comparison_plot(scores_list, plot_metric_list, ylim=(-5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Analyze Predictions for Random Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "\n",
    "polynomial_target = lambda_net_test_dataset.target_polynomial_list[index]\n",
    "polynomial_lstsq_target = lambda_net_test_dataset.lstsq_target_polynomial_list[index]\n",
    "polynomial_lstsq_lambda = lambda_net_test_dataset.lstsq_lambda_pred_polynomial_list[index]\n",
    "polynomial_inet = inet_preds_test_list[-1][index]\n",
    "per_network_poly = per_network_preds_list[-1][index]\n",
    "\n",
    "print('Target Poly:')\n",
    "print_polynomial_from_coefficients(polynomial_target, force_complete_poly_representation=True, round_digits=4)\n",
    "print('LSTSQ Target Poly:')\n",
    "print_polynomial_from_coefficients(polynomial_lstsq_target, force_complete_poly_representation=True, round_digits=4)\n",
    "print('LSTSQ Lambda Poly:')\n",
    "print_polynomial_from_coefficients(polynomial_lstsq_lambda, force_complete_poly_representation=True, round_digits=4)\n",
    "print('I-Net Poly:')\n",
    "print_polynomial_from_coefficients(polynomial_inet, round_digits=4)\n",
    "print('Per Network Optimization Poly:')\n",
    "print_polynomial_from_coefficients(per_network_poly, round_digits=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset_list[0].X_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_net_test_dataset_list[0].y_test_data_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:52.425282Z",
     "start_time": "2021-01-07T15:49:51.529992Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      inet_preds_test_list,\n",
    "                                                      per_network_preds_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:57.631017Z",
     "start_time": "2021-01-07T15:49:52.427326Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      inet_preds_test_list,\n",
    "                                                      per_network_preds_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:49:59.108419Z",
     "start_time": "2021-01-07T15:49:57.634294Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_and_save_single_polynomial_prediction_evaluation(lambda_net_test_dataset_list, \n",
    "                                                      function_values_test_list, \n",
    "                                                      inet_preds_test_list,\n",
    "                                                      per_network_preds_list,\n",
    "                                                      rand_index=index, \n",
    "                                                      plot_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (RANDOM GUESS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T15:50:04.140254Z",
     "start_time": "2021-01-07T15:50:03.647192Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_random_polynomials = np.random.uniform(low=-10, high=10, size=(len(lambda_net_test_dataset_list[-1]), sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.030192Z",
     "start_time": "2021-01-07T15:50:04.141837Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_test = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "random_fv_test = parallel_fv_calculation_from_polynomial(list_of_random_polynomials, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.064612Z",
     "start_time": "2021-01-07T16:08:23.032372Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error Coefficients: ' + str(np.round(mean_absolute_error(lambda_net_test_dataset_list[-1].target_polynomial_list, list_of_random_polynomials), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:08:23.204426Z",
     "start_time": "2021-01-07T16:08:23.066205Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Guess Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, random_fv_test), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK (EDUCATED GUESS/MEAN PREDICTION) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:31.911007Z",
     "start_time": "2021-01-07T16:08:23.205879Z"
    }
   },
   "outputs": [],
   "source": [
    "true_fv_train = parallel_fv_calculation_from_polynomial(lambda_net_test_dataset_list[-1].target_polynomial_list, lambda_net_test_dataset_list[-1].X_test_data_list, force_complete_poly_representation=True)\n",
    "\n",
    "mean_fv = np.mean(true_fv_train)\n",
    "mean_fv_pred_test = [mean_fv for _ in range(true_fv_test.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.029945Z",
     "start_time": "2021-01-07T16:17:31.912980Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Educated Guess/Mean Prediction Error FVs: ' + str(np.round(mean_absolute_error_function_values(true_fv_test, mean_fv_pred_test), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T16:17:32.508984Z",
     "start_time": "2021-01-07T16:17:32.031355Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "base_model = generate_base_model()\n",
    "random_evaluation_dataset = np.random.uniform(low=x_min, high=x_max, size=(random_evaluation_dataset_size, n))\n",
    "#random_evaluation_dataset = lambda_train_input_train_split[0]#lambda_train_input[0] #JUST [0] HERE BECAUSE EVALUATION ALWAYS ON THE SAME DATASET FOR ALL!!\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(tf.float32)\n",
    "\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "#X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "\n",
    "seed_in_inet_training = False\n",
    "\n",
    "loss_function = mean_absolute_error_tf_fv_lambda_extended_wrapper(random_evaluation_dataset, list_of_monomial_identifiers_numbers, base_model)      \n",
    "\n",
    "X_train = X_train_list[-1].values[:,1:]\n",
    "y_train = y_train_list[-1].values[:,2:]\n",
    "\n",
    "X_train = X_train[:,1:]\n",
    "y_train_model = np.hstack((y_train, X_train))\n",
    "\n",
    "print('seed_in_inet_training = ' + str(seed_in_inet_training), loss_function(y_train_model, y_train))\n",
    "\n",
    "seed_in_inet_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  'Powell',\n",
    "    'max_steps': 5000,#100,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 500,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_scipy(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 10\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer':  ['Nelder-Mead', 'Powell', 'CG', 'BFGS', 'Newton-CG', 'L-BFGS-B', 'TNC', 'COBYLA', 'SLSQP', 'trust-constr', 'dogleg', 'trust-ncg', 'trust-exact', 'trust-krylov'],\n",
    "    'max_steps': [5000],#100,\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [500],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_scipy)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = RANDOM_SEED\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': tf.keras.optimizers.RMSprop,\n",
    "    'lr': 0.02,\n",
    "    'max_steps': 500,\n",
    "    'early_stopping': 10,\n",
    "    'restarts': 3,\n",
    "    'per_network_dataset_size': 5000,\n",
    "}\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "lambda_network_weights = lambda_network_weights_list[random_index]\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "\n",
    "printing = True\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }\n",
    "\n",
    "\n",
    "per_network_optimization_error, per_network_optimization_polynomial = per_network_poly_optimization_tf(per_network_hyperparams['per_network_dataset_size'], \n",
    "                                                                                                      lambda_network_weights, \n",
    "                                                                                                      list_of_monomial_identifiers_numbers, \n",
    "                                                                                                      config,\n",
    "                                                                                                      optimizer = per_network_hyperparams['optimizer'],\n",
    "                                                                                                      lr = per_network_hyperparams['lr'], \n",
    "                                                                                                      max_steps = per_network_hyperparams['max_steps'], \n",
    "                                                                                                      early_stopping = per_network_hyperparams['early_stopping'], \n",
    "                                                                                                      restarts = per_network_hyperparams['restarts'],\n",
    "                                                                                                      printing = True,\n",
    "                                                                                                      return_error = True)\n",
    "\n",
    "print('\\n\\nError: ' + str(per_network_optimization_error.numpy()))\n",
    "print_polynomial_from_coefficients(per_network_optimization_polynomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "evaluation_size = 100\n",
    "\n",
    "per_network_hyperparams = {\n",
    "    'optimizer': [tf.keras.optimizers.RMSprop], #[tf.keras.optimizers.SGD, tf.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.Adadelta]\n",
    "    'lr': [0.02], #[0.5, 0.25, 0.1, 0.05, 0.025]\n",
    "    'max_steps': [500],#100,\n",
    "    'early_stopping': [10],\n",
    "    'restarts': [3],\n",
    "    'per_network_dataset_size': [5000, 10000, 20000],\n",
    "}\n",
    "\n",
    "#param_iterator = ParameterSampler(per_network_hyperparams, n_iter=60, random_state=RANDOM_SEED)\n",
    "param_iterator = ParameterGrid(per_network_hyperparams)\n",
    "\n",
    "\n",
    "lambda_net_test_dataset = lambda_net_test_dataset_list[-1]\n",
    "lambda_network_weights_list = np.array(lambda_net_test_dataset.weight_list)\n",
    "\n",
    "list_of_monomial_identifiers_numbers = np.array([list(monomial_identifiers) for monomial_identifiers in list_of_monomial_identifiers]).astype(float)  \n",
    "printing = True if n_jobs == 1 else False\n",
    "\n",
    "config = {\n",
    "         'n': n,\n",
    "         'inet_loss': inet_loss,\n",
    "         'sparsity': sparsity,\n",
    "         'lambda_network_layers': lambda_network_layers,\n",
    "         'interpretation_net_output_shape': interpretation_net_output_shape,\n",
    "         'RANDOM_SEED': RANDOM_SEED,\n",
    "         'nas': nas,\n",
    "         'number_of_lambda_weights': number_of_lambda_weights,\n",
    "         'interpretation_net_output_monomials': interpretation_net_output_monomials,\n",
    "         'x_min': x_min,\n",
    "         'x_max': x_max,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_error_list = []\n",
    "for params in tqdm(param_iterator):\n",
    "    parallel_per_network = Parallel(n_jobs=n_jobs, verbose=0, backend='loky')\n",
    "\n",
    "    result_list = parallel_per_network(delayed(per_network_poly_optimization_tf)(params['per_network_dataset_size'], \n",
    "                                                                                  lambda_network_weights, \n",
    "                                                                                  list_of_monomial_identifiers_numbers, \n",
    "                                                                                  config,\n",
    "                                                                                  optimizer = params['optimizer'],\n",
    "                                                                                  lr = params['lr'], \n",
    "                                                                                  max_steps = params['max_steps'], \n",
    "                                                                                  early_stopping = params['early_stopping'], \n",
    "                                                                                  restarts = params['restarts'],\n",
    "                                                                                  printing = printing,\n",
    "                                                                                  return_error = True) for lambda_network_weights in lambda_network_weights_list[:evaluation_size])  \n",
    "    \n",
    "    \n",
    "    per_network_optimization_errors = [result[0] for result in result_list]\n",
    "    per_network_optimization_polynomials = [result[1] for result in result_list]\n",
    "        \n",
    "    params_score = np.mean(per_network_optimization_errors)\n",
    "    \n",
    "    evaluation_result = list(params.values())\n",
    "    evaluation_result.append(params_score)\n",
    "    \n",
    "    params_error_list.append(evaluation_result)\n",
    "        \n",
    "    del parallel_per_network\n",
    "\n",
    "columns = list(params.keys())\n",
    "columns.append('score')\n",
    "params_error_df = pd.DataFrame(data=params_error_list, columns=columns).sort_values(by='score')\n",
    "params_error_df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
