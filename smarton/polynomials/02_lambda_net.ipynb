{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:33.918536Z",
     "start_time": "2020-12-21T19:43:33.910881Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:33.986105Z",
     "start_time": "2020-12-21T19:43:33.922487Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 50000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 5000\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:34.037368Z",
     "start_time": "2020-12-21T19:43:33.989031Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:42.750881Z",
     "start_time": "2020-12-21T19:43:34.040435Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:42.759503Z",
     "start_time": "2020-12-21T19:43:42.753051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:42.781729Z",
     "start_time": "2020-12-21T19:43:42.762082Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:42.790291Z",
     "start_time": "2020-12-21T19:43:42.783479Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:42.832731Z",
     "start_time": "2020-12-21T19:43:42.794381Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:42.912900Z",
     "start_time": "2020-12-21T19:43:42.836490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b3e147893d498c9aca3202e2c24cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c595b75558044309abdc63caa04ee91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.459096Z",
     "start_time": "2020-12-21T19:43:42.914637Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.489837Z",
     "start_time": "2020-12-21T19:43:54.461382Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.525379Z",
     "start_time": "2020-12-21T19:43:54.491553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.620 -0.950  0.340 -0.760\n",
       "1 -0.300  0.950 -0.770  0.310\n",
       "2 -0.300  0.280  0.480  0.080\n",
       "3 -0.520  0.980  0.230  0.170\n",
       "4  0.710 -0.920 -0.310  0.260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.535743Z",
     "start_time": "2020-12-21T19:43:54.527241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1      2      3\n",
       "0  0.660 0.100 -0.040 -1.000\n",
       "1 -0.970 0.920  0.460  0.990\n",
       "2 -0.280 0.760  0.910 -0.230\n",
       "3 -0.250 0.210 -0.070 -0.300\n",
       "4 -0.330 0.770 -0.360  0.110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.542588Z",
     "start_time": "2020-12-21T19:43:54.537673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.549050Z",
     "start_time": "2020-12-21T19:43:54.544213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.584481Z",
     "start_time": "2020-12-21T19:43:54.550814Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.590774Z",
     "start_time": "2020-12-21T19:43:54.586382Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.599027Z",
     "start_time": "2020-12-21T19:43:54.592502Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T19:43:54.674618Z",
     "start_time": "2020-12-21T19:43:54.600878Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(lambda_index, X_data_lambda, y_data_real_lambda, polynomial, seed_list, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    current_seed = seed_list[lambda_index%number_different_lambda_trainings]\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(current_seed)\n",
    "        else:\n",
    "            tf.set_random_seed(current_seed) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=current_seed)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=current_seed)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    #kerase defaults: kernel_initializer='glorot_uniform', bias_initializer='zeros'               \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros')) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if fixed_initialization_lambda_training:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (lambda_index,\n",
    "                     y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list = [lambda_index,\n",
    "                     scores_train,\n",
    "                     scores_valid,\n",
    "                     scores_test,\n",
    "                     scores_std,\n",
    "                     scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((lambda_index,\n",
    "                              y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [lambda_index,\n",
    "                                         scores_train,\n",
    "                                          scores_valid,\n",
    "                                          scores_test,\n",
    "                                          scores_std,\n",
    "                                          scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                text_file.write(str(lambda_index))\n",
    "                text_file.write(', ' + str(current_seed))\n",
    "                for i, value in enumerate(polynomial.values): \n",
    "                    text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "        text_file.close() \n",
    "\n",
    "        directory = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "        path = directory + 'lambda_' + str(lambda_index) + '_test_data'\n",
    "        np.save(path, X_test_lambda)\n",
    "            \n",
    "    if return_model:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:13:57.766479Z",
     "start_time": "2020-12-21T19:43:54.676452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    4.710, max:    9.420, cur:    4.710)\n",
      "\tvalidation       \t (min:    5.200, max:    9.216, cur:    5.200)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    1.046, max:    3.505, cur:    3.261)\n",
      "\tvalidation       \t (min:    1.097, max:    2.386, cur:    1.942)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    6.750, max:   12.203, cur:    6.758)\n",
      "\tvalidation       \t (min:    7.249, max:   11.883, cur:    7.249)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2**32 - 1\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:14:09.493874Z",
     "start_time": "2020-12-22T00:13:57.768601Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][4] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][5] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[5] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:14:09.518079Z",
     "start_time": "2020-12-22T00:14:09.496219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.189</td>\n",
       "      <td>10.189</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.695</td>\n",
       "      <td>9.696</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.932</td>\n",
       "      <td>8.933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.397</td>\n",
       "      <td>4.433</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.310</td>\n",
       "      <td>4.347</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.986</td>\n",
       "      <td>12.986</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.477</td>\n",
       "      <td>12.477</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.687</td>\n",
       "      <td>11.687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.200</td>\n",
       "      <td>6.177</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.090</td>\n",
       "      <td>6.064</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.922</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.478</td>\n",
       "      <td>3.478</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.450</td>\n",
       "      <td>3.450</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.393</td>\n",
       "      <td>2.341</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.364</td>\n",
       "      <td>2.307</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.189         10.189               0.013           0.000   \n",
       "RMSE FV         12.986         12.986               0.016           0.000   \n",
       "MAPE FV            inf            inf               0.349           0.000   \n",
       "R2 FV           -0.416         -0.416               0.999           1.000   \n",
       "RAAE FV          0.922          0.922               0.022           0.000   \n",
       "RMAE FV          3.478          3.478               0.094           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.695           9.696                0.015            0.000   \n",
       "RMSE FV          12.477          12.477                0.020            0.000   \n",
       "MAPE FV             inf             inf                0.217            0.000   \n",
       "R2 FV            -0.293          -0.293                0.999            1.000   \n",
       "RAAE FV           0.876           0.876                0.023            0.000   \n",
       "RMAE FV           3.450           3.450                0.116            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            8.932           8.933  ...                 0.365   \n",
       "RMSE FV          11.687          11.687  ...                 0.469   \n",
       "MAPE FV             inf             inf  ...                 0.512   \n",
       "R2 FV            -0.117          -0.117  ...                 0.996   \n",
       "RAAE FV           0.804           0.804  ...                 0.049   \n",
       "RMAE FV           3.401           3.401  ...                 0.246   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            4.397            4.433   \n",
       "RMSE FV             0.000            6.200            6.177   \n",
       "MAPE FV             0.000              inf              inf   \n",
       "R2 FV               1.000            0.659            0.661   \n",
       "RAAE FV             0.000            0.406            0.410   \n",
       "RMAE FV             0.000            2.393            2.341   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.388             0.000            4.310   \n",
       "RMSE FV                 0.497             0.000            6.090   \n",
       "MAPE FV                 0.715             0.000              inf   \n",
       "R2 FV                   0.995             1.000            0.670   \n",
       "RAAE FV                 0.051             0.000            0.398   \n",
       "RMAE FV                 0.254             0.000            2.364   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             4.347                 0.411             0.000  \n",
       "RMSE FV            6.064                 0.526             0.000  \n",
       "MAPE FV              inf                 0.539             0.000  \n",
       "R2 FV              0.672                 0.995             1.000  \n",
       "RAAE FV            0.402                 0.054             0.000  \n",
       "RMAE FV            2.307                 0.263             0.000  \n",
       "\n",
       "[6 rows x 84 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:14:09.568225Z",
     "start_time": "2020-12-22T00:14:09.522202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.193</td>\n",
       "      <td>10.193</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.705</td>\n",
       "      <td>9.705</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.952</td>\n",
       "      <td>8.953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.525</td>\n",
       "      <td>4.540</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.442</td>\n",
       "      <td>4.457</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.974</td>\n",
       "      <td>12.974</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.469</td>\n",
       "      <td>12.469</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.687</td>\n",
       "      <td>11.686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.302</td>\n",
       "      <td>6.261</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.196</td>\n",
       "      <td>6.151</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.421</td>\n",
       "      <td>1.422</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.573</td>\n",
       "      <td>1.575</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.156</td>\n",
       "      <td>2.156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.697</td>\n",
       "      <td>3.673</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.712</td>\n",
       "      <td>3.657</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.425</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.927</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.046</td>\n",
       "      <td>3.046</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.020</td>\n",
       "      <td>3.020</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.972</td>\n",
       "      <td>2.972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.021</td>\n",
       "      <td>1.976</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.995</td>\n",
       "      <td>1.946</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.193         10.193               0.014           0.000   \n",
       "RMSE FV         12.974         12.974               0.017           0.000   \n",
       "MAPE FV          1.421          1.422               0.245           0.000   \n",
       "R2 FV           -0.425         -0.425               0.999           1.000   \n",
       "RAAE FV          0.927          0.926               0.024           0.000   \n",
       "RMAE FV          3.046          3.046               0.093           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.705           9.705                0.016            0.000   \n",
       "RMSE FV          12.469          12.469                0.022            0.000   \n",
       "MAPE FV           1.573           1.575                0.242            0.000   \n",
       "R2 FV            -0.302          -0.302                0.999            1.000   \n",
       "RAAE FV           0.880           0.880                0.025            0.000   \n",
       "RMAE FV           3.020           3.020                0.109            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            8.952           8.953  ...                 0.389   \n",
       "RMSE FV          11.687          11.686  ...                 0.507   \n",
       "MAPE FV           2.156           2.156  ...                 0.427   \n",
       "R2 FV            -0.125          -0.125  ...                 0.995   \n",
       "RAAE FV           0.809           0.809  ...                 0.052   \n",
       "RMAE FV           2.972           2.972  ...                 0.227   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            4.525            4.540   \n",
       "RMSE FV             0.000            6.302            6.261   \n",
       "MAPE FV             0.000            3.697            3.673   \n",
       "R2 FV               1.000            0.645            0.649   \n",
       "RAAE FV             0.000            0.419            0.421   \n",
       "RMAE FV             0.000            2.021            1.976   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.413             0.000            4.442   \n",
       "RMSE FV                 0.537             0.000            6.196   \n",
       "MAPE FV                 0.502             0.000            3.712   \n",
       "R2 FV                   0.994             1.000            0.656   \n",
       "RAAE FV                 0.055             0.000            0.412   \n",
       "RMAE FV                 0.236             0.000            1.995   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             4.457                 0.437             0.000  \n",
       "RMSE FV            6.151                 0.567             0.000  \n",
       "MAPE FV            3.657                 0.944             0.000  \n",
       "R2 FV              0.661                 0.994             1.000  \n",
       "RAAE FV            0.413                 0.058             0.000  \n",
       "RMAE FV            1.946                 0.246             0.000  \n",
       "\n",
       "[6 rows x 84 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:14:09.606200Z",
     "start_time": "2020-12-22T00:14:09.570252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.196</td>\n",
       "      <td>10.196</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.708</td>\n",
       "      <td>9.708</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.955</td>\n",
       "      <td>8.956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.531</td>\n",
       "      <td>4.545</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.448</td>\n",
       "      <td>4.462</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.979</td>\n",
       "      <td>12.978</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.474</td>\n",
       "      <td>12.473</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.691</td>\n",
       "      <td>11.691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.314</td>\n",
       "      <td>6.273</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.208</td>\n",
       "      <td>6.162</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.609</td>\n",
       "      <td>1.587</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.945</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.225</td>\n",
       "      <td>3.210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.487</td>\n",
       "      <td>7.222</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.445</td>\n",
       "      <td>7.203</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.423</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.156</td>\n",
       "      <td>3.156</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>3.130</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.082</td>\n",
       "      <td>3.082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.126</td>\n",
       "      <td>2.077</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.100</td>\n",
       "      <td>2.047</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.196        10.196              0.014          0.000   \n",
       "RMSE FV        12.979        12.978              0.017          0.000   \n",
       "MAPE FV         1.609         1.587              0.192          0.000   \n",
       "R2 FV          -0.423        -0.423              0.999          1.000   \n",
       "RAAE FV         0.925         0.925              0.024          0.000   \n",
       "RMAE FV         3.156         3.156              0.099          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.708          9.708               0.016           0.000   \n",
       "RMSE FV         12.474         12.473               0.022           0.000   \n",
       "MAPE FV          1.945          1.940               0.369           0.000   \n",
       "R2 FV           -0.300         -0.300               0.999           1.000   \n",
       "RAAE FV          0.879          0.879               0.025           0.000   \n",
       "RMAE FV          3.130          3.130               0.117           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           8.955          8.956  ...                0.388   \n",
       "RMSE FV         11.691         11.691  ...                0.506   \n",
       "MAPE FV          3.225          3.210  ...                0.971   \n",
       "R2 FV           -0.123         -0.123  ...                0.995   \n",
       "RAAE FV          0.808          0.808  ...                0.052   \n",
       "RMAE FV          3.082          3.082  ...                0.242   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           4.531           4.545                0.413   \n",
       "RMSE FV            0.000           6.314           6.273                0.536   \n",
       "MAPE FV            0.000           7.487           7.222                0.609   \n",
       "R2 FV              1.000           0.645           0.649                0.994   \n",
       "RAAE FV            0.000           0.419           0.421                0.055   \n",
       "RMAE FV            0.000           2.126           2.077                0.251   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           4.448           4.462                0.437   \n",
       "RMSE FV            0.000           6.208           6.162                0.567   \n",
       "MAPE FV            0.000           7.445           7.203                0.569   \n",
       "R2 FV              1.000           0.656           0.661                0.994   \n",
       "RAAE FV            0.000           0.412           0.413                0.058   \n",
       "RMAE FV            0.000           2.100           2.047                0.261   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "\n",
       "[6 rows x 84 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:14:09.650159Z",
     "start_time": "2020-12-22T00:14:09.607743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>...</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.573</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.703</td>\n",
       "      <td>2.610</td>\n",
       "      <td>3.579</td>\n",
       "      <td>4.476</td>\n",
       "      <td>5.237</td>\n",
       "      <td>5.847</td>\n",
       "      <td>6.321</td>\n",
       "      <td>...</td>\n",
       "      <td>6.960</td>\n",
       "      <td>7.173</td>\n",
       "      <td>7.340</td>\n",
       "      <td>7.477</td>\n",
       "      <td>7.593</td>\n",
       "      <td>7.695</td>\n",
       "      <td>7.787</td>\n",
       "      <td>7.872</td>\n",
       "      <td>7.953</td>\n",
       "      <td>8.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.572</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.703</td>\n",
       "      <td>2.609</td>\n",
       "      <td>3.577</td>\n",
       "      <td>4.474</td>\n",
       "      <td>5.234</td>\n",
       "      <td>5.843</td>\n",
       "      <td>6.316</td>\n",
       "      <td>...</td>\n",
       "      <td>6.954</td>\n",
       "      <td>7.165</td>\n",
       "      <td>7.332</td>\n",
       "      <td>7.467</td>\n",
       "      <td>7.582</td>\n",
       "      <td>7.682</td>\n",
       "      <td>7.773</td>\n",
       "      <td>7.856</td>\n",
       "      <td>7.935</td>\n",
       "      <td>8.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>...</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "      <td>11.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>...</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.570</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.692</td>\n",
       "      <td>2.592</td>\n",
       "      <td>3.555</td>\n",
       "      <td>4.447</td>\n",
       "      <td>5.205</td>\n",
       "      <td>5.813</td>\n",
       "      <td>6.285</td>\n",
       "      <td>...</td>\n",
       "      <td>6.922</td>\n",
       "      <td>7.134</td>\n",
       "      <td>7.300</td>\n",
       "      <td>7.436</td>\n",
       "      <td>7.550</td>\n",
       "      <td>7.650</td>\n",
       "      <td>7.741</td>\n",
       "      <td>7.825</td>\n",
       "      <td>7.904</td>\n",
       "      <td>7.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.570</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.691</td>\n",
       "      <td>2.591</td>\n",
       "      <td>3.553</td>\n",
       "      <td>4.445</td>\n",
       "      <td>5.202</td>\n",
       "      <td>5.809</td>\n",
       "      <td>6.281</td>\n",
       "      <td>...</td>\n",
       "      <td>6.917</td>\n",
       "      <td>7.128</td>\n",
       "      <td>7.293</td>\n",
       "      <td>7.428</td>\n",
       "      <td>7.542</td>\n",
       "      <td>7.641</td>\n",
       "      <td>7.731</td>\n",
       "      <td>7.814</td>\n",
       "      <td>7.892</td>\n",
       "      <td>7.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>...</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "      <td>11.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>...</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.693</td>\n",
       "      <td>2.593</td>\n",
       "      <td>3.556</td>\n",
       "      <td>4.449</td>\n",
       "      <td>5.207</td>\n",
       "      <td>5.815</td>\n",
       "      <td>6.287</td>\n",
       "      <td>...</td>\n",
       "      <td>6.924</td>\n",
       "      <td>7.136</td>\n",
       "      <td>7.302</td>\n",
       "      <td>7.437</td>\n",
       "      <td>7.551</td>\n",
       "      <td>7.651</td>\n",
       "      <td>7.741</td>\n",
       "      <td>7.825</td>\n",
       "      <td>7.904</td>\n",
       "      <td>7.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.571</td>\n",
       "      <td>0.668</td>\n",
       "      <td>1.026</td>\n",
       "      <td>1.692</td>\n",
       "      <td>2.592</td>\n",
       "      <td>3.554</td>\n",
       "      <td>4.447</td>\n",
       "      <td>5.205</td>\n",
       "      <td>5.812</td>\n",
       "      <td>6.283</td>\n",
       "      <td>...</td>\n",
       "      <td>6.919</td>\n",
       "      <td>7.129</td>\n",
       "      <td>7.294</td>\n",
       "      <td>7.429</td>\n",
       "      <td>7.542</td>\n",
       "      <td>7.641</td>\n",
       "      <td>7.730</td>\n",
       "      <td>7.812</td>\n",
       "      <td>7.890</td>\n",
       "      <td>7.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>...</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "      <td>11.135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.157 11.157 11.157 11.157 11.157 11.157   \n",
       "STD FV TRAIN PRED LAMBDA             0.573  0.671  1.032  1.703  2.610  3.579   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.572  0.671  1.032  1.703  2.609  3.577   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.157 11.157 11.157 11.157 11.157 11.157   \n",
       "STD FV VALID REAL LAMBDA            11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV VALID PRED LAMBDA             0.570  0.667  1.025  1.692  2.592  3.555   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.570  0.667  1.025  1.691  2.591  3.553   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV TEST REAL LAMBDA             11.135 11.135 11.135 11.135 11.135 11.135   \n",
       "STD FV TEST PRED LAMBDA              0.571  0.669  1.027  1.693  2.593  3.556   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.571  0.668  1.026  1.692  2.592  3.554   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.135 11.135 11.135 11.135 11.135 11.135   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.157 11.157 11.157 11.157  ... 11.157   \n",
       "STD FV TRAIN PRED LAMBDA             4.476  5.237  5.847  6.321  ...  6.960   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  4.474  5.234  5.843  6.316  ...  6.954   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.157 11.157 11.157 11.157  ... 11.157   \n",
       "STD FV VALID REAL LAMBDA            11.124 11.124 11.124 11.124  ... 11.124   \n",
       "STD FV VALID PRED LAMBDA             4.447  5.205  5.813  6.285  ...  6.922   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  4.445  5.202  5.809  6.281  ...  6.917   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.124 11.124 11.124 11.124  ... 11.124   \n",
       "STD FV TEST REAL LAMBDA             11.135 11.135 11.135 11.135  ... 11.135   \n",
       "STD FV TEST PRED LAMBDA              4.449  5.207  5.815  6.287  ...  6.924   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   4.447  5.205  5.812  6.283  ...  6.919   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.135 11.135 11.135 11.135  ... 11.135   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.157 11.157 11.157 11.157 11.157 11.157   \n",
       "STD FV TRAIN PRED LAMBDA             7.173  7.340  7.477  7.593  7.695  7.787   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.165  7.332  7.467  7.582  7.682  7.773   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.157 11.157 11.157 11.157 11.157 11.157   \n",
       "STD FV VALID REAL LAMBDA            11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV VALID PRED LAMBDA             7.134  7.300  7.436  7.550  7.650  7.741   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.128  7.293  7.428  7.542  7.641  7.731   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.124 11.124 11.124 11.124 11.124 11.124   \n",
       "STD FV TEST REAL LAMBDA             11.135 11.135 11.135 11.135 11.135 11.135   \n",
       "STD FV TEST PRED LAMBDA              7.136  7.302  7.437  7.551  7.651  7.741   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.129  7.294  7.429  7.542  7.641  7.730   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.135 11.135 11.135 11.135 11.135 11.135   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.157 11.157 11.157  \n",
       "STD FV TRAIN PRED LAMBDA             7.872  7.953  8.031  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.856  7.935  8.012  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.157 11.157 11.157  \n",
       "STD FV VALID REAL LAMBDA            11.124 11.124 11.124  \n",
       "STD FV VALID PRED LAMBDA             7.825  7.904  7.981  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.814  7.892  7.968  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.124 11.124 11.124  \n",
       "STD FV TEST REAL LAMBDA             11.135 11.135 11.135  \n",
       "STD FV TEST PRED LAMBDA              7.825  7.904  7.980  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.812  7.890  7.966  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.135 11.135 11.135  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:14:09.705804Z",
     "start_time": "2020-12-22T00:14:09.651605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.001 -0.010 -0.027 -0.046 -0.063   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.001 -0.010 -0.027 -0.046 -0.063   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV VALID REAL LAMBDA            -0.062 -0.062 -0.062 -0.062 -0.062   \n",
       "MEAN FV VALID PRED LAMBDA            -0.002 -0.011 -0.027 -0.046 -0.061   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.002 -0.011 -0.027 -0.047 -0.061   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.062 -0.062 -0.062 -0.062 -0.062   \n",
       "MEAN FV TEST REAL LAMBDA             -0.090 -0.090 -0.090 -0.090 -0.090   \n",
       "MEAN FV TEST PRED LAMBDA             -0.003 -0.012 -0.030 -0.052 -0.071   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.003 -0.012 -0.030 -0.052 -0.071   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.090 -0.090 -0.090 -0.090 -0.090   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.068 -0.065 -0.064 -0.064 -0.064  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.068 -0.065 -0.064 -0.064 -0.064  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.062 -0.062 -0.062 -0.062 -0.062  ...   \n",
       "MEAN FV VALID PRED LAMBDA            -0.064 -0.060 -0.057 -0.056 -0.055  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.064 -0.060 -0.057 -0.057 -0.056  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.062 -0.062 -0.062 -0.062 -0.062  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.090 -0.090 -0.090 -0.090 -0.090  ...   \n",
       "MEAN FV TEST PRED LAMBDA             -0.077 -0.076 -0.075 -0.075 -0.075  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.077 -0.076 -0.075 -0.075 -0.075  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.090 -0.090 -0.090 -0.090 -0.090  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.063 -0.063 -0.063 -0.063 -0.064   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.063 -0.063 -0.063 -0.063 -0.064   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV VALID REAL LAMBDA            -0.062 -0.062 -0.062 -0.062 -0.062   \n",
       "MEAN FV VALID PRED LAMBDA            -0.054 -0.053 -0.052 -0.052 -0.052   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.054 -0.053 -0.052 -0.052 -0.052   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.062 -0.062 -0.062 -0.062 -0.062   \n",
       "MEAN FV TEST REAL LAMBDA             -0.090 -0.090 -0.090 -0.090 -0.090   \n",
       "MEAN FV TEST PRED LAMBDA             -0.074 -0.073 -0.073 -0.073 -0.074   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.073 -0.073 -0.072 -0.072 -0.073   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.090 -0.090 -0.090 -0.090 -0.090   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.064 -0.064 -0.065 -0.065 -0.065  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.064 -0.064 -0.065 -0.065 -0.065  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079  \n",
       "MEAN FV VALID REAL LAMBDA            -0.062 -0.062 -0.062 -0.062 -0.062  \n",
       "MEAN FV VALID PRED LAMBDA            -0.052 -0.051 -0.051 -0.051 -0.051  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.052 -0.052 -0.052 -0.051 -0.051  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.062 -0.062 -0.062 -0.062 -0.062  \n",
       "MEAN FV TEST REAL LAMBDA             -0.090 -0.090 -0.090 -0.090 -0.090  \n",
       "MEAN FV TEST PRED LAMBDA             -0.074 -0.074 -0.074 -0.074 -0.074  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.073 -0.073 -0.073 -0.073 -0.073  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.090 -0.090 -0.090 -0.090 -0.090  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:14:09.802397Z",
     "start_time": "2020-12-22T00:14:09.707384Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_lambda_preds(i, \n",
    "                      lambda_indices,\n",
    "                      y_train_real_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_train_lambda_by_epoch, \n",
    "                      y_valid_real_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_valid_lambda_by_epoch, \n",
    "                      y_test_real_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_test_lambda_by_epoch):\n",
    "    \n",
    "    \n",
    "    index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "\n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n",
    "\n",
    "    return y_train_real_lambda_df, y_valid_real_lambda_df, y_test_real_lambda_df, y_train_pred_lambda_df, y_valid_pred_lambda_df, y_test_pred_lambda_df, y_train_pred_lambda_poly_lstsq_df, y_valid_pred_lambda_poly_lstsq_df, y_test_pred_lambda_poly_lstsq_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:35.207885Z",
     "start_time": "2020-12-22T00:14:09.804419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e54cf9e27c4caabbb08b3482fd799f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend MultiprocessingBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of  21 | elapsed:  7.0min remaining: 66.8min\n",
      "[Parallel(n_jobs=-3)]: Done  10 out of  21 | elapsed:  7.5min remaining:  8.3min\n",
      "[Parallel(n_jobs=-3)]: Done  18 out of  21 | elapsed:  7.9min remaining:  1.3min\n",
      "[Parallel(n_jobs=-3)]: Done  21 out of  21 | elapsed:  8.1min finished\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    lambda_index_list = [clf[0][0] for clf in clf_list]\n",
    "    lambda_seed_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_real_list = [clf[0][2] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][3] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][4] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    lambda_indices_list = np.zeros((len(clf_list), 1))\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][3])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][4].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][7])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][8].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][11])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][12].shape)]][0])\n",
    "\n",
    "    for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        lambda_indices_list[index] = lambda_indices\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    lambda_indices_list = [np.zeros((len(clf_list), 1)) for i in epochs_save_range]\n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][3]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][4].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][7]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][8].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][12]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][12].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            lambda_indices_list[index][i] = lambda_indices\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    y_data_lambda_list = parallel(delayed(save_lambda_preds)(i, \n",
    "                                                           lambda_indices,\n",
    "                                                           y_train_real_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_train_lambda_by_epoch, \n",
    "                                                           y_valid_real_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_valid_lambda_by_epoch, \n",
    "                                                           y_test_real_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_test_lambda_by_epoch) for i, \n",
    "                                                                                        (lambda_indices,\n",
    "                                                                                         y_train_real_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_train_lambda_by_epoch, \n",
    "                                                                                         y_valid_real_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_valid_lambda_by_epoch, \n",
    "                                                                                         y_test_real_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_test_lambda_by_epoch) in enumerate(zip(lambda_indices_list,\n",
    "                                                                                                                                  y_train_real_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_train_lambda_list, \n",
    "                                                                                                                                  y_valid_real_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_valid_lambda_list, \n",
    "                                                                                                                                  y_test_real_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_test_lambda_list)))  \n",
    "    y_test_real_lambda_df = y_data_lambda_list[-1][2]\n",
    "    y_test_pred_lambda_df = y_data_lambda_list[-1][5]\n",
    "    y_test_pred_lambda_poly_lstsq_df = y_data_lambda_list[-1][8]\n",
    "    del parallel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:35.259607Z",
     "start_time": "2020-12-22T00:22:35.210116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>d_1</th>\n",
       "      <th>FV_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>c_2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>7.865</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-7.580</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>10.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-16.436</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-11.955</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-19.299</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-19.621</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-11.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-4.939</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-2.020</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>20.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.980</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.320</td>\n",
       "      <td>12.639</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>25.836</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index  0    a_1    b_1    c_1    d_1    FV_1    a_2    b_2    c_2  \\\n",
       "0         0.000  0 -0.640  0.820 -0.190 -0.560   7.865 -0.080  0.990 -0.510   \n",
       "1         1.000  1  0.530  0.930  0.980  0.120 -16.436  0.460  0.990  0.970   \n",
       "2         2.000  2 -0.840 -0.340  0.050  0.260 -19.299 -0.370 -0.820  0.820   \n",
       "3         3.000  3  0.910  0.670 -0.960 -0.230  -4.939 -0.090  0.830 -0.400   \n",
       "4         4.000  4  0.980 -0.040  0.280  0.320  12.639 -0.840  0.900 -0.520   \n",
       "\n",
       "   ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  ... -0.060  0.560  0.770  0.060  -7.580 -0.710  0.550  0.110 -0.450  10.155  \n",
       "1  ...  0.450  0.510 -0.530 -0.220 -11.955  0.430 -0.500  0.260  0.430  -0.622  \n",
       "2  ... -0.660  0.140  0.150 -0.680 -19.621  0.110  0.200 -0.850 -0.150 -11.502  \n",
       "3  ... -0.940 -0.850  0.080 -0.940  -2.020  0.940 -0.850  0.280  0.900  20.981  \n",
       "4  ...  0.570  0.600  0.960 -0.920  25.836 -0.600  0.350  0.940  0.810  -7.972  \n",
       "\n",
       "[5 rows x 1252 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:35.827282Z",
     "start_time": "2020-12-22T00:22:35.261252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>d_1</th>\n",
       "      <th>FV_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>c_2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>6.581</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-4.458</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>3.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-12.990</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-17.039</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-16.919</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.316</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-8.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>9.726</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.980</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>4.995</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index  0    a_1    b_1    c_1    d_1    FV_1    a_2    b_2    c_2  \\\n",
       "0         0.000  0 -0.640  0.820 -0.190 -0.560   6.581 -0.080  0.990 -0.510   \n",
       "1         1.000  1  0.530  0.930  0.980  0.120 -12.990  0.460  0.990  0.970   \n",
       "2         2.000  2 -0.840 -0.340  0.050  0.260 -16.919 -0.370 -0.820  0.820   \n",
       "3         3.000  3  0.910  0.670 -0.960 -0.230  -0.190 -0.090  0.830 -0.400   \n",
       "4         4.000  4  0.980 -0.040  0.280  0.320   3.128 -0.840  0.900 -0.520   \n",
       "\n",
       "   ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  ... -0.060  0.560  0.770  0.060  -4.458 -0.710  0.550  0.110 -0.450   3.025  \n",
       "1  ...  0.450  0.510 -0.530 -0.220 -17.039  0.430 -0.500  0.260  0.430  -1.840  \n",
       "2  ... -0.660  0.140  0.150 -0.680 -24.316  0.110  0.200 -0.850 -0.150  -8.380  \n",
       "3  ... -0.940 -0.850  0.080 -0.940   9.726  0.940 -0.850  0.280  0.900   6.087  \n",
       "4  ...  0.570  0.600  0.960 -0.920   4.995 -0.600  0.350  0.940  0.810  -7.059  \n",
       "\n",
       "[5 rows x 1252 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:35.875769Z",
     "start_time": "2020-12-22T00:22:35.829004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>d_1</th>\n",
       "      <th>FV_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>c_2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.820</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>6.862</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.990</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-3.996</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>4.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.120</td>\n",
       "      <td>-12.489</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-17.320</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.260</td>\n",
       "      <td>-16.251</td>\n",
       "      <td>-0.370</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-25.038</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-7.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>8.884</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.980</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.320</td>\n",
       "      <td>2.680</td>\n",
       "      <td>-0.840</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>3.905</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-6.412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index  0    a_1    b_1    c_1    d_1    FV_1    a_2    b_2    c_2  \\\n",
       "0         0.000  0 -0.640  0.820 -0.190 -0.560   6.862 -0.080  0.990 -0.510   \n",
       "1         1.000  1  0.530  0.930  0.980  0.120 -12.489  0.460  0.990  0.970   \n",
       "2         2.000  2 -0.840 -0.340  0.050  0.260 -16.251 -0.370 -0.820  0.820   \n",
       "3         3.000  3  0.910  0.670 -0.960 -0.230  -0.657 -0.090  0.830 -0.400   \n",
       "4         4.000  4  0.980 -0.040  0.280  0.320   2.680 -0.840  0.900 -0.520   \n",
       "\n",
       "   ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  ... -0.060  0.560  0.770  0.060  -3.996 -0.710  0.550  0.110 -0.450   4.058  \n",
       "1  ...  0.450  0.510 -0.530 -0.220 -17.320  0.430 -0.500  0.260  0.430  -1.936  \n",
       "2  ... -0.660  0.140  0.150 -0.680 -25.038  0.110  0.200 -0.850 -0.150  -7.937  \n",
       "3  ... -0.940 -0.850  0.080 -0.940   8.884  0.940 -0.850  0.280  0.900   6.374  \n",
       "4  ...  0.570  0.600  0.960 -0.920   3.905 -0.600  0.350  0.940  0.810  -6.412  \n",
       "\n",
       "[5 rows x 1252 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:39.012594Z",
     "start_time": "2020-12-22T00:22:35.877455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0da237cfbc946a19ebb473972d795af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:55.161545Z",
     "start_time": "2020-12-22T00:22:39.014553Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:55.773870Z",
     "start_time": "2020-12-22T00:22:55.163487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.220</td>\n",
       "      <td>10.167</td>\n",
       "      <td>10.114</td>\n",
       "      <td>10.061</td>\n",
       "      <td>10.008</td>\n",
       "      <td>9.955</td>\n",
       "      <td>9.900</td>\n",
       "      <td>9.845</td>\n",
       "      <td>9.788</td>\n",
       "      <td>9.730</td>\n",
       "      <td>...</td>\n",
       "      <td>4.397</td>\n",
       "      <td>4.388</td>\n",
       "      <td>4.379</td>\n",
       "      <td>4.371</td>\n",
       "      <td>4.362</td>\n",
       "      <td>4.353</td>\n",
       "      <td>4.344</td>\n",
       "      <td>4.336</td>\n",
       "      <td>4.327</td>\n",
       "      <td>4.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.342</td>\n",
       "      <td>2.319</td>\n",
       "      <td>2.296</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.227</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.154</td>\n",
       "      <td>2.128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.543</td>\n",
       "      <td>4.537</td>\n",
       "      <td>4.530</td>\n",
       "      <td>4.524</td>\n",
       "      <td>4.517</td>\n",
       "      <td>4.511</td>\n",
       "      <td>4.505</td>\n",
       "      <td>4.499</td>\n",
       "      <td>4.492</td>\n",
       "      <td>4.486</td>\n",
       "      <td>...</td>\n",
       "      <td>2.493</td>\n",
       "      <td>2.490</td>\n",
       "      <td>2.488</td>\n",
       "      <td>2.483</td>\n",
       "      <td>2.479</td>\n",
       "      <td>2.474</td>\n",
       "      <td>2.470</td>\n",
       "      <td>2.467</td>\n",
       "      <td>2.463</td>\n",
       "      <td>2.459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.519</td>\n",
       "      <td>8.482</td>\n",
       "      <td>8.449</td>\n",
       "      <td>8.409</td>\n",
       "      <td>8.373</td>\n",
       "      <td>8.331</td>\n",
       "      <td>8.293</td>\n",
       "      <td>8.249</td>\n",
       "      <td>8.213</td>\n",
       "      <td>8.168</td>\n",
       "      <td>...</td>\n",
       "      <td>4.002</td>\n",
       "      <td>3.994</td>\n",
       "      <td>3.986</td>\n",
       "      <td>3.978</td>\n",
       "      <td>3.970</td>\n",
       "      <td>3.960</td>\n",
       "      <td>3.952</td>\n",
       "      <td>3.943</td>\n",
       "      <td>3.935</td>\n",
       "      <td>3.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.953</td>\n",
       "      <td>9.907</td>\n",
       "      <td>9.862</td>\n",
       "      <td>9.814</td>\n",
       "      <td>9.765</td>\n",
       "      <td>9.713</td>\n",
       "      <td>9.665</td>\n",
       "      <td>9.615</td>\n",
       "      <td>9.565</td>\n",
       "      <td>9.509</td>\n",
       "      <td>...</td>\n",
       "      <td>4.377</td>\n",
       "      <td>4.367</td>\n",
       "      <td>4.359</td>\n",
       "      <td>4.350</td>\n",
       "      <td>4.343</td>\n",
       "      <td>4.333</td>\n",
       "      <td>4.324</td>\n",
       "      <td>4.316</td>\n",
       "      <td>4.307</td>\n",
       "      <td>4.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.652</td>\n",
       "      <td>11.575</td>\n",
       "      <td>11.512</td>\n",
       "      <td>11.447</td>\n",
       "      <td>11.383</td>\n",
       "      <td>11.315</td>\n",
       "      <td>11.252</td>\n",
       "      <td>11.181</td>\n",
       "      <td>11.116</td>\n",
       "      <td>11.048</td>\n",
       "      <td>...</td>\n",
       "      <td>4.765</td>\n",
       "      <td>4.756</td>\n",
       "      <td>4.747</td>\n",
       "      <td>4.738</td>\n",
       "      <td>4.730</td>\n",
       "      <td>4.720</td>\n",
       "      <td>4.711</td>\n",
       "      <td>4.701</td>\n",
       "      <td>4.692</td>\n",
       "      <td>4.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.716</td>\n",
       "      <td>20.558</td>\n",
       "      <td>20.402</td>\n",
       "      <td>20.246</td>\n",
       "      <td>20.090</td>\n",
       "      <td>19.932</td>\n",
       "      <td>19.769</td>\n",
       "      <td>19.599</td>\n",
       "      <td>19.422</td>\n",
       "      <td>19.237</td>\n",
       "      <td>...</td>\n",
       "      <td>7.347</td>\n",
       "      <td>7.331</td>\n",
       "      <td>7.319</td>\n",
       "      <td>7.303</td>\n",
       "      <td>7.289</td>\n",
       "      <td>7.274</td>\n",
       "      <td>7.263</td>\n",
       "      <td>7.247</td>\n",
       "      <td>7.231</td>\n",
       "      <td>7.218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         10.220        10.167        10.114        10.061        10.008   \n",
       "std           2.342         2.319         2.296         2.273         2.250   \n",
       "min           4.543         4.537         4.530         4.524         4.517   \n",
       "25%           8.519         8.482         8.449         8.409         8.373   \n",
       "50%           9.953         9.907         9.862         9.814         9.765   \n",
       "75%          11.652        11.575        11.512        11.447        11.383   \n",
       "max          20.716        20.558        20.402        20.246        20.090   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean          9.955         9.900         9.845         9.788          9.730   \n",
       "std           2.227         2.203         2.179         2.154          2.128   \n",
       "min           4.511         4.505         4.499         4.492          4.486   \n",
       "25%           8.331         8.293         8.249         8.213          8.168   \n",
       "50%           9.713         9.665         9.615         9.565          9.509   \n",
       "75%          11.315        11.252        11.181        11.116         11.048   \n",
       "max          19.932        19.769        19.599        19.422         19.237   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           4.397           4.388           4.379           4.371   \n",
       "std    ...           0.583           0.582           0.581           0.580   \n",
       "min    ...           2.493           2.490           2.488           2.483   \n",
       "25%    ...           4.002           3.994           3.986           3.978   \n",
       "50%    ...           4.377           4.367           4.359           4.350   \n",
       "75%    ...           4.765           4.756           4.747           4.738   \n",
       "max    ...           7.347           7.331           7.319           7.303   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            4.362           4.353           4.344           4.336   \n",
       "std             0.580           0.579           0.578           0.577   \n",
       "min             2.479           2.474           2.470           2.467   \n",
       "25%             3.970           3.960           3.952           3.943   \n",
       "50%             4.343           4.333           4.324           4.316   \n",
       "75%             4.730           4.720           4.711           4.701   \n",
       "max             7.289           7.274           7.263           7.247   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            4.327           4.318  \n",
       "std             0.577           0.576  \n",
       "min             2.463           2.459  \n",
       "25%             3.935           3.926  \n",
       "50%             4.307           4.297  \n",
       "75%             4.692           4.683  \n",
       "max             7.231           7.218  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:56.381714Z",
     "start_time": "2020-12-22T00:22:55.775565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.193</td>\n",
       "      <td>10.140</td>\n",
       "      <td>10.088</td>\n",
       "      <td>10.036</td>\n",
       "      <td>9.984</td>\n",
       "      <td>9.930</td>\n",
       "      <td>9.876</td>\n",
       "      <td>9.821</td>\n",
       "      <td>9.764</td>\n",
       "      <td>9.705</td>\n",
       "      <td>...</td>\n",
       "      <td>4.517</td>\n",
       "      <td>4.508</td>\n",
       "      <td>4.500</td>\n",
       "      <td>4.491</td>\n",
       "      <td>4.483</td>\n",
       "      <td>4.475</td>\n",
       "      <td>4.467</td>\n",
       "      <td>4.458</td>\n",
       "      <td>4.450</td>\n",
       "      <td>4.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.381</td>\n",
       "      <td>2.359</td>\n",
       "      <td>2.336</td>\n",
       "      <td>2.313</td>\n",
       "      <td>2.291</td>\n",
       "      <td>2.268</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.220</td>\n",
       "      <td>2.195</td>\n",
       "      <td>2.169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.716</td>\n",
       "      <td>4.711</td>\n",
       "      <td>4.706</td>\n",
       "      <td>4.701</td>\n",
       "      <td>4.696</td>\n",
       "      <td>4.691</td>\n",
       "      <td>4.686</td>\n",
       "      <td>4.682</td>\n",
       "      <td>4.677</td>\n",
       "      <td>4.673</td>\n",
       "      <td>...</td>\n",
       "      <td>2.377</td>\n",
       "      <td>2.373</td>\n",
       "      <td>2.368</td>\n",
       "      <td>2.364</td>\n",
       "      <td>2.358</td>\n",
       "      <td>2.354</td>\n",
       "      <td>2.349</td>\n",
       "      <td>2.343</td>\n",
       "      <td>2.339</td>\n",
       "      <td>2.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.433</td>\n",
       "      <td>8.393</td>\n",
       "      <td>8.355</td>\n",
       "      <td>8.322</td>\n",
       "      <td>8.286</td>\n",
       "      <td>8.244</td>\n",
       "      <td>8.201</td>\n",
       "      <td>8.166</td>\n",
       "      <td>8.133</td>\n",
       "      <td>8.095</td>\n",
       "      <td>...</td>\n",
       "      <td>4.057</td>\n",
       "      <td>4.049</td>\n",
       "      <td>4.042</td>\n",
       "      <td>4.035</td>\n",
       "      <td>4.027</td>\n",
       "      <td>4.020</td>\n",
       "      <td>4.011</td>\n",
       "      <td>4.004</td>\n",
       "      <td>3.994</td>\n",
       "      <td>3.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.924</td>\n",
       "      <td>9.877</td>\n",
       "      <td>9.824</td>\n",
       "      <td>9.778</td>\n",
       "      <td>9.730</td>\n",
       "      <td>9.681</td>\n",
       "      <td>9.635</td>\n",
       "      <td>9.581</td>\n",
       "      <td>9.532</td>\n",
       "      <td>9.475</td>\n",
       "      <td>...</td>\n",
       "      <td>4.488</td>\n",
       "      <td>4.480</td>\n",
       "      <td>4.470</td>\n",
       "      <td>4.461</td>\n",
       "      <td>4.454</td>\n",
       "      <td>4.446</td>\n",
       "      <td>4.439</td>\n",
       "      <td>4.431</td>\n",
       "      <td>4.423</td>\n",
       "      <td>4.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.630</td>\n",
       "      <td>11.563</td>\n",
       "      <td>11.492</td>\n",
       "      <td>11.427</td>\n",
       "      <td>11.361</td>\n",
       "      <td>11.300</td>\n",
       "      <td>11.242</td>\n",
       "      <td>11.179</td>\n",
       "      <td>11.115</td>\n",
       "      <td>11.049</td>\n",
       "      <td>...</td>\n",
       "      <td>4.939</td>\n",
       "      <td>4.929</td>\n",
       "      <td>4.919</td>\n",
       "      <td>4.910</td>\n",
       "      <td>4.901</td>\n",
       "      <td>4.893</td>\n",
       "      <td>4.883</td>\n",
       "      <td>4.874</td>\n",
       "      <td>4.864</td>\n",
       "      <td>4.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.103</td>\n",
       "      <td>21.960</td>\n",
       "      <td>21.819</td>\n",
       "      <td>21.677</td>\n",
       "      <td>21.532</td>\n",
       "      <td>21.384</td>\n",
       "      <td>21.234</td>\n",
       "      <td>21.077</td>\n",
       "      <td>20.913</td>\n",
       "      <td>20.740</td>\n",
       "      <td>...</td>\n",
       "      <td>8.082</td>\n",
       "      <td>8.072</td>\n",
       "      <td>8.059</td>\n",
       "      <td>8.047</td>\n",
       "      <td>8.036</td>\n",
       "      <td>8.024</td>\n",
       "      <td>8.009</td>\n",
       "      <td>7.997</td>\n",
       "      <td>7.985</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.193            10.140            10.088            10.036   \n",
       "std               2.381             2.359             2.336             2.313   \n",
       "min               4.716             4.711             4.706             4.701   \n",
       "25%               8.433             8.393             8.355             8.322   \n",
       "50%               9.924             9.877             9.824             9.778   \n",
       "75%              11.630            11.563            11.492            11.427   \n",
       "max              22.103            21.960            21.819            21.677   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              9.984             9.930             9.876             9.821   \n",
       "std               2.291             2.268             2.244             2.220   \n",
       "min               4.696             4.691             4.686             4.682   \n",
       "25%               8.286             8.244             8.201             8.166   \n",
       "50%               9.730             9.681             9.635             9.581   \n",
       "75%              11.361            11.300            11.242            11.179   \n",
       "max              21.532            21.384            21.234            21.077   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean              9.764              9.705  ...               4.517   \n",
       "std               2.195              2.169  ...               0.656   \n",
       "min               4.677              4.673  ...               2.377   \n",
       "25%               8.133              8.095  ...               4.057   \n",
       "50%               9.532              9.475  ...               4.488   \n",
       "75%              11.115             11.049  ...               4.939   \n",
       "max              20.913             20.740  ...               8.082   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.508               4.500               4.491   \n",
       "std                 0.655               0.655               0.654   \n",
       "min                 2.373               2.368               2.364   \n",
       "25%                 4.049               4.042               4.035   \n",
       "50%                 4.480               4.470               4.461   \n",
       "75%                 4.929               4.919               4.910   \n",
       "max                 8.072               8.059               8.047   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.483               4.475               4.467   \n",
       "std                 0.653               0.652               0.652   \n",
       "min                 2.358               2.354               2.349   \n",
       "25%                 4.027               4.020               4.011   \n",
       "50%                 4.454               4.446               4.439   \n",
       "75%                 4.901               4.893               4.883   \n",
       "max                 8.036               8.024               8.009   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                4.458               4.450               4.442  \n",
       "std                 0.651               0.650               0.649  \n",
       "min                 2.343               2.339               2.334  \n",
       "25%                 4.004               3.994               3.988  \n",
       "50%                 4.431               4.423               4.414  \n",
       "75%                 4.874               4.864               4.855  \n",
       "max                 7.997               7.985               7.972  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:57.016253Z",
     "start_time": "2020-12-22T00:22:56.383433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.967</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.052</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.038</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.046</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214</td>\n",
       "      <td>1.211</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.203</td>\n",
       "      <td>1.198</td>\n",
       "      <td>1.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.104</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.098</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.099</td>\n",
       "      <td>1.102</td>\n",
       "      <td>1.108</td>\n",
       "      <td>1.114</td>\n",
       "      <td>1.120</td>\n",
       "      <td>...</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.627</td>\n",
       "      <td>1.627</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.622</td>\n",
       "      <td>1.623</td>\n",
       "      <td>1.618</td>\n",
       "      <td>1.613</td>\n",
       "      <td>1.613</td>\n",
       "      <td>1.609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.208</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.203</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.214</td>\n",
       "      <td>1.223</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.251</td>\n",
       "      <td>1.266</td>\n",
       "      <td>...</td>\n",
       "      <td>2.347</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.338</td>\n",
       "      <td>2.329</td>\n",
       "      <td>2.321</td>\n",
       "      <td>2.324</td>\n",
       "      <td>2.320</td>\n",
       "      <td>2.318</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.967           0.952           0.940           0.921   \n",
       "25%             1.052           1.046           1.042           1.039   \n",
       "50%             1.104           1.100           1.098           1.096   \n",
       "75%             1.208           1.202           1.202           1.203   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.900           0.884           0.866           0.848   \n",
       "25%             1.037           1.037           1.038           1.040   \n",
       "50%             1.097           1.099           1.102           1.108   \n",
       "75%             1.208           1.214           1.223           1.234   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean              inf              inf  ...               inf   \n",
       "std               nan              nan  ...               nan   \n",
       "min             0.827            0.805  ...             0.203   \n",
       "25%             1.042            1.046  ...             1.214   \n",
       "50%             1.114            1.120  ...             1.632   \n",
       "75%             1.251            1.266  ...             2.347   \n",
       "max               inf              inf  ...               inf   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.202             0.202             0.200             0.200   \n",
       "25%               1.211             1.212             1.209             1.207   \n",
       "50%               1.627             1.627             1.621             1.622   \n",
       "75%               2.334             2.338             2.329             2.321   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.199             0.199             0.200             0.198   \n",
       "25%               1.205             1.204             1.203             1.198   \n",
       "50%               1.623             1.618             1.613             1.613   \n",
       "75%               2.324             2.320             2.318             2.314   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean                inf  \n",
       "std                 nan  \n",
       "min               0.197  \n",
       "25%               1.198  \n",
       "50%               1.609  \n",
       "75%               2.304  \n",
       "max                 inf  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:22:57.622136Z",
     "start_time": "2020-12-22T00:22:57.017988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.421</td>\n",
       "      <td>1.422</td>\n",
       "      <td>1.427</td>\n",
       "      <td>1.437</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.468</td>\n",
       "      <td>1.488</td>\n",
       "      <td>1.511</td>\n",
       "      <td>1.538</td>\n",
       "      <td>1.570</td>\n",
       "      <td>...</td>\n",
       "      <td>3.710</td>\n",
       "      <td>3.714</td>\n",
       "      <td>3.715</td>\n",
       "      <td>3.716</td>\n",
       "      <td>3.716</td>\n",
       "      <td>3.719</td>\n",
       "      <td>3.722</td>\n",
       "      <td>3.714</td>\n",
       "      <td>3.715</td>\n",
       "      <td>3.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.740</td>\n",
       "      <td>6.949</td>\n",
       "      <td>7.241</td>\n",
       "      <td>7.605</td>\n",
       "      <td>8.043</td>\n",
       "      <td>8.528</td>\n",
       "      <td>9.076</td>\n",
       "      <td>9.672</td>\n",
       "      <td>10.319</td>\n",
       "      <td>11.013</td>\n",
       "      <td>...</td>\n",
       "      <td>40.582</td>\n",
       "      <td>40.870</td>\n",
       "      <td>41.043</td>\n",
       "      <td>41.270</td>\n",
       "      <td>41.399</td>\n",
       "      <td>41.835</td>\n",
       "      <td>42.244</td>\n",
       "      <td>42.092</td>\n",
       "      <td>42.346</td>\n",
       "      <td>42.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.922</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.021</td>\n",
       "      <td>1.015</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.044</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.067</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.061</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.062</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.073</td>\n",
       "      <td>1.078</td>\n",
       "      <td>...</td>\n",
       "      <td>1.439</td>\n",
       "      <td>1.438</td>\n",
       "      <td>1.437</td>\n",
       "      <td>1.435</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.430</td>\n",
       "      <td>1.429</td>\n",
       "      <td>1.425</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.164</td>\n",
       "      <td>1.163</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.163</td>\n",
       "      <td>1.170</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.197</td>\n",
       "      <td>1.211</td>\n",
       "      <td>1.225</td>\n",
       "      <td>...</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.138</td>\n",
       "      <td>2.137</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.121</td>\n",
       "      <td>2.118</td>\n",
       "      <td>2.119</td>\n",
       "      <td>2.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>467.201</td>\n",
       "      <td>523.798</td>\n",
       "      <td>581.178</td>\n",
       "      <td>639.612</td>\n",
       "      <td>699.802</td>\n",
       "      <td>760.310</td>\n",
       "      <td>823.188</td>\n",
       "      <td>887.659</td>\n",
       "      <td>954.545</td>\n",
       "      <td>1023.556</td>\n",
       "      <td>...</td>\n",
       "      <td>3115.015</td>\n",
       "      <td>3127.487</td>\n",
       "      <td>3140.967</td>\n",
       "      <td>3155.816</td>\n",
       "      <td>3159.581</td>\n",
       "      <td>3190.271</td>\n",
       "      <td>3235.962</td>\n",
       "      <td>3245.711</td>\n",
       "      <td>3279.664</td>\n",
       "      <td>3307.615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.421               1.422               1.427   \n",
       "std                 6.740               6.949               7.241   \n",
       "min                 0.922               0.918               0.901   \n",
       "25%                 1.021               1.015               1.011   \n",
       "50%                 1.067               1.063               1.061   \n",
       "75%                 1.164               1.163               1.162   \n",
       "max               467.201             523.798             581.178   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.437               1.450               1.468   \n",
       "std                 7.605               8.043               8.528   \n",
       "min                 0.884               0.867               0.850   \n",
       "25%                 1.008               1.005               1.004   \n",
       "50%                 1.060               1.060               1.062   \n",
       "75%                 1.163               1.170               1.178   \n",
       "max               639.612             699.802             760.310   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.488               1.511               1.538   \n",
       "std                 9.076               9.672              10.319   \n",
       "min                 0.836               0.822               0.808   \n",
       "25%                 1.004               1.004               1.005   \n",
       "50%                 1.065               1.068               1.073   \n",
       "75%                 1.187               1.197               1.211   \n",
       "max               823.188             887.659             954.545   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                 1.570  ...                 3.710                 3.714   \n",
       "std                 11.013  ...                40.582                40.870   \n",
       "min                  0.795  ...                 0.230                 0.229   \n",
       "25%                  1.006  ...                 1.047                 1.046   \n",
       "50%                  1.078  ...                 1.439                 1.438   \n",
       "75%                  1.225  ...                 2.142                 2.138   \n",
       "max               1023.556  ...              3115.015              3127.487   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.715                 3.716                 3.716   \n",
       "std                  41.043                41.270                41.399   \n",
       "min                   0.229                 0.228                 0.228   \n",
       "25%                   1.044                 1.042                 1.041   \n",
       "50%                   1.437                 1.435                 1.432   \n",
       "75%                   2.137                 2.133                 2.129   \n",
       "max                3140.967              3155.816              3159.581   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.719                 3.722                 3.714   \n",
       "std                  41.835                42.244                42.092   \n",
       "min                   0.227                 0.227                 0.226   \n",
       "25%                   1.039                 1.037                 1.036   \n",
       "50%                   1.430                 1.429                 1.425   \n",
       "75%                   2.125                 2.121                 2.118   \n",
       "max                3190.271              3235.962              3245.711   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  3.715                 3.720  \n",
       "std                  42.346                42.777  \n",
       "min                   0.226                 0.225  \n",
       "25%                   1.034                 1.032  \n",
       "50%                   1.421                 1.420  \n",
       "75%                   2.119                 2.118  \n",
       "max                3279.664              3307.615  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:23:00.009041Z",
     "start_time": "2020-12-22T00:22:57.623857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T00:23:01.718901Z",
     "start_time": "2020-12-22T00:23:00.011347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['trainover', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
