{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:22.968047Z",
     "start_time": "2020-12-15T14:36:22.961150Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:22.982543Z",
     "start_time": "2020-12-15T14:36:22.971346Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 10000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 10000\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:22.996155Z",
     "start_time": "2020-12-15T14:36:22.984758Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:30.904021Z",
     "start_time": "2020-12-15T14:36:23.088104Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:30.914789Z",
     "start_time": "2020-12-15T14:36:30.907548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:30.935046Z",
     "start_time": "2020-12-15T14:36:30.917451Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:30.943869Z",
     "start_time": "2020-12-15T14:36:30.937932Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:30.983499Z",
     "start_time": "2020-12-15T14:36:30.946988Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:31.058145Z",
     "start_time": "2020-12-15T14:36:30.985835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf026e7f1f534d3683eca0a19fcab1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107b95e1871445089fb335559cb2aa1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:40.931016Z",
     "start_time": "2020-12-15T14:36:31.060136Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:40.959611Z",
     "start_time": "2020-12-15T14:36:40.933682Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:40.980941Z",
     "start_time": "2020-12-15T14:36:40.961562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.620 -0.950  0.340 -0.760\n",
       "1 -0.300  0.950 -0.770  0.310\n",
       "2 -0.300  0.280  0.480  0.080\n",
       "3 -0.520  0.980  0.230  0.170\n",
       "4  0.710 -0.920 -0.310  0.260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:40.991491Z",
     "start_time": "2020-12-15T14:36:40.982641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1      2      3\n",
       "0  0.660 0.100 -0.040 -1.000\n",
       "1 -0.970 0.920  0.460  0.990\n",
       "2 -0.280 0.760  0.910 -0.230\n",
       "3 -0.250 0.210 -0.070 -0.300\n",
       "4 -0.330 0.770 -0.360  0.110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:40.999218Z",
     "start_time": "2020-12-15T14:36:40.993408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:41.007602Z",
     "start_time": "2020-12-15T14:36:41.001242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:41.051352Z",
     "start_time": "2020-12-15T14:36:41.011711Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    #fd_real_VS_predLambda = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    #fd_real_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "    #fd_predLambda_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    #fd_real_VS_realPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "    #dtw_real_VS_predLambda, dtw_complete_real_VS_predLambda = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    #dtw_real_VS_predLambda = np.round(dtw_real_VS_predLambda, 4)\n",
    "    #dtw_real_VS_predPolyLstsq, dtw_complete_real_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    #dtw_real_VS_predPolyLstsq = np.round(dtw_real_VS_predPolyLstsq, 4)\n",
    "    #dtw_predLambda_VS_predPolyLstsq, dtw_complete_predLambda_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    #dtw_predLambda_VS_predPolyLstsq = np.round(dtw_predLambda_VS_predPolyLstsq, 4)    \n",
    "    #dtw_real_VS_realPolyLstsq, dtw_complete_real_VS_realPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    #dtw_real_VS_realPolyLstsq = np.round(dtw_real_VS_realPolyLstsq, 4) \n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "             #'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': fd_real_VS_predLambda,   \n",
    "             #'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_real_VS_predPolyLstsq,   \n",
    "             #'FD FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_predLambda_VS_predPolyLstsq,   \n",
    "             #'FD FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': fd_real_VS_realPolyLstsq,   \n",
    "             #'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': dtw_real_VS_predLambda, \n",
    "             #'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_real_VS_predPolyLstsq, \n",
    "             #'DTW FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_predLambda_VS_predPolyLstsq, \n",
    "             #'DTW FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': dtw_real_VS_realPolyLstsq, \n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:41.059106Z",
     "start_time": "2020-12-15T14:36:41.054632Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:41.069490Z",
     "start_time": "2020-12-15T14:36:41.060818Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T14:36:41.133783Z",
     "start_time": "2020-12-15T14:36:41.071587Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(lambda_index, X_data_lambda, y_data_real_lambda, polynomial, seed_list, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    current_seed = seed_list[lambda_index%number_different_lambda_trainings]\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(current_seed)\n",
    "        else:\n",
    "            tf.set_random_seed(current_seed) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=current_seed)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=current_seed)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    #kerase defaults: kernel_initializer='glorot_uniform', bias_initializer='zeros'               \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros')) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if fixed_initialization_lambda_training:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (lambda_index,\n",
    "                     y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list = [lambda_index,\n",
    "                     scores_train,\n",
    "                     scores_valid,\n",
    "                     scores_test,\n",
    "                     scores_std,\n",
    "                     scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((lambda_index,\n",
    "                              y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [lambda_index,\n",
    "                                         scores_train,\n",
    "                                          scores_valid,\n",
    "                                          scores_test,\n",
    "                                          scores_std,\n",
    "                                          scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                text_file.write(str(lambda_index))\n",
    "                text_file.write(', ' + str(current_seed))\n",
    "                for i, value in enumerate(polynomial.values): \n",
    "                    text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:42.614594Z",
     "start_time": "2020-12-15T14:36:41.135935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    4.760, max:    9.031, cur:    4.760)\n",
      "\tvalidation       \t (min:    5.555, max:    9.823, cur:    5.555)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    0.949, max:    1.419, cur:    1.260)\n",
      "\tvalidation       \t (min:    0.933, max:    1.737, cur:    1.691)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    6.439, max:   11.541, cur:    6.506)\n",
      "\tvalidation       \t (min:    8.075, max:   13.167, cur:    8.075)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2**32 - 1\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:52.866882Z",
     "start_time": "2020-12-15T19:19:42.617717Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][4] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][5] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[5] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:52.891121Z",
     "start_time": "2020-12-15T19:19:52.869207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.195</td>\n",
       "      <td>10.195</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.700</td>\n",
       "      <td>9.700</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.935</td>\n",
       "      <td>8.936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.397</td>\n",
       "      <td>4.432</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.310</td>\n",
       "      <td>4.347</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.988</td>\n",
       "      <td>12.988</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.477</td>\n",
       "      <td>12.477</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.686</td>\n",
       "      <td>11.686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.196</td>\n",
       "      <td>6.172</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.086</td>\n",
       "      <td>6.060</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.416</td>\n",
       "      <td>-0.416</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.472</td>\n",
       "      <td>3.472</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.444</td>\n",
       "      <td>3.444</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.395</td>\n",
       "      <td>3.395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.381</td>\n",
       "      <td>2.329</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.353</td>\n",
       "      <td>2.296</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.450</td>\n",
       "      <td>24.450</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.718</td>\n",
       "      <td>23.717</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.382</td>\n",
       "      <td>22.380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.521</td>\n",
       "      <td>11.434</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.344</td>\n",
       "      <td>11.248</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.435</td>\n",
       "      <td>102.435</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.482</td>\n",
       "      <td>97.488</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.563</td>\n",
       "      <td>89.579</td>\n",
       "      <td>...</td>\n",
       "      <td>3.652</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.305</td>\n",
       "      <td>42.652</td>\n",
       "      <td>3.882</td>\n",
       "      <td>0.000</td>\n",
       "      <td>41.519</td>\n",
       "      <td>41.890</td>\n",
       "      <td>4.112</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.195         10.195               0.013           0.000   \n",
       "RMSE FV         12.988         12.988               0.016           0.000   \n",
       "MAPE FV            inf            inf               0.218           0.000   \n",
       "R2 FV           -0.416         -0.416               0.999           1.000   \n",
       "RAAE FV          0.923          0.923               0.022           0.000   \n",
       "RMAE FV          3.472          3.472               0.093           0.000   \n",
       "FD FV           24.450         24.450               0.030           0.000   \n",
       "DTW FV         102.435        102.435               0.127           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.700           9.700                0.015            0.000   \n",
       "RMSE FV          12.477          12.477                0.020            0.000   \n",
       "MAPE FV             inf             inf                0.188            0.000   \n",
       "R2 FV            -0.293          -0.293                0.999            1.000   \n",
       "RAAE FV           0.876           0.876                0.023            0.000   \n",
       "RMAE FV           3.444           3.444                0.115            0.000   \n",
       "FD FV            23.718          23.717                0.039            0.000   \n",
       "DTW FV           97.482          97.488                0.153            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            8.935           8.936  ...                 0.365   \n",
       "RMSE FV          11.686          11.686  ...                 0.469   \n",
       "MAPE FV             inf             inf  ...                 0.455   \n",
       "R2 FV            -0.116          -0.116  ...                 0.996   \n",
       "RAAE FV           0.804           0.804  ...                 0.049   \n",
       "RMAE FV           3.395           3.395  ...                 0.245   \n",
       "FD FV            22.382          22.380  ...                 0.898   \n",
       "DTW FV           89.563          89.579  ...                 3.652   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            4.397            4.432   \n",
       "RMSE FV             0.000            6.196            6.172   \n",
       "MAPE FV             0.000              inf              inf   \n",
       "R2 FV               1.000            0.659            0.661   \n",
       "RAAE FV             0.000            0.406            0.409   \n",
       "RMAE FV             0.000            2.381            2.329   \n",
       "FD FV               0.000           11.521           11.434   \n",
       "DTW FV              0.000           42.305           42.652   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.388             0.000            4.310   \n",
       "RMSE FV                 0.497             0.000            6.086   \n",
       "MAPE FV                 0.652             0.000              inf   \n",
       "R2 FV                   0.995             1.000            0.670   \n",
       "RAAE FV                 0.051             0.000            0.398   \n",
       "RMAE FV                 0.253             0.000            2.353   \n",
       "FD FV                   0.950             0.000           11.344   \n",
       "DTW FV                  3.882             0.000           41.519   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             4.347                 0.411             0.000  \n",
       "RMSE FV            6.060                 0.525             0.000  \n",
       "MAPE FV              inf                 0.470             0.000  \n",
       "R2 FV              0.673                 0.995             1.000  \n",
       "RAAE FV            0.402                 0.054             0.000  \n",
       "RMAE FV            2.296                 0.261             0.000  \n",
       "FD FV             11.248                 1.003             0.000  \n",
       "DTW FV            41.890                 4.112             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:52.919197Z",
     "start_time": "2020-12-15T19:19:52.892671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.185</td>\n",
       "      <td>10.184</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.696</td>\n",
       "      <td>9.696</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.942</td>\n",
       "      <td>8.943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.530</td>\n",
       "      <td>4.545</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.447</td>\n",
       "      <td>4.462</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.966</td>\n",
       "      <td>12.965</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.460</td>\n",
       "      <td>12.459</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.676</td>\n",
       "      <td>11.676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.309</td>\n",
       "      <td>6.267</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.204</td>\n",
       "      <td>6.157</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.374</td>\n",
       "      <td>1.380</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.781</td>\n",
       "      <td>1.779</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.490</td>\n",
       "      <td>2.480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.214</td>\n",
       "      <td>4.095</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.171</td>\n",
       "      <td>4.079</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.426</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>-0.302</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.926</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.054</td>\n",
       "      <td>3.054</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.028</td>\n",
       "      <td>3.028</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.981</td>\n",
       "      <td>2.981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.033</td>\n",
       "      <td>1.989</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.007</td>\n",
       "      <td>1.959</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.413</td>\n",
       "      <td>24.413</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.697</td>\n",
       "      <td>23.696</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.390</td>\n",
       "      <td>22.388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.792</td>\n",
       "      <td>11.681</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.623</td>\n",
       "      <td>11.504</td>\n",
       "      <td>1.095</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.173</td>\n",
       "      <td>102.173</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.335</td>\n",
       "      <td>97.336</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.605</td>\n",
       "      <td>89.615</td>\n",
       "      <td>...</td>\n",
       "      <td>3.899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.933</td>\n",
       "      <td>44.078</td>\n",
       "      <td>4.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.174</td>\n",
       "      <td>43.318</td>\n",
       "      <td>4.389</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.185         10.184               0.014           0.000   \n",
       "RMSE FV         12.966         12.965               0.017           0.000   \n",
       "MAPE FV          1.374          1.380               0.189           0.000   \n",
       "R2 FV           -0.426         -0.426               0.999           1.000   \n",
       "RAAE FV          0.926          0.926               0.024           0.000   \n",
       "RMAE FV          3.054          3.054               0.093           0.000   \n",
       "FD FV           24.413         24.413               0.034           0.000   \n",
       "DTW FV         102.173        102.173               0.137           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.696           9.696                0.016            0.000   \n",
       "RMSE FV          12.460          12.459                0.022            0.000   \n",
       "MAPE FV           1.781           1.779                0.394            0.000   \n",
       "R2 FV            -0.302          -0.302                0.999            1.000   \n",
       "RAAE FV           0.880           0.880                0.025            0.000   \n",
       "RMAE FV           3.028           3.028                0.110            0.000   \n",
       "FD FV            23.697          23.696                0.043            0.000   \n",
       "DTW FV           97.335          97.336                0.164            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            8.942           8.943  ...                 0.389   \n",
       "RMSE FV          11.676          11.676  ...                 0.508   \n",
       "MAPE FV           2.490           2.480  ...                 0.414   \n",
       "R2 FV            -0.125          -0.124  ...                 0.995   \n",
       "RAAE FV           0.808           0.808  ...                 0.052   \n",
       "RMAE FV           2.981           2.981  ...                 0.228   \n",
       "FD FV            22.390          22.388  ...                 0.980   \n",
       "DTW FV           89.605          89.615  ...                 3.899   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            4.530            4.545   \n",
       "RMSE FV             0.000            6.309            6.267   \n",
       "MAPE FV             0.000            4.214            4.095   \n",
       "R2 FV               1.000            0.644            0.648   \n",
       "RAAE FV             0.000            0.420            0.421   \n",
       "RMAE FV             0.000            2.033            1.989   \n",
       "FD FV               0.000           11.792           11.681   \n",
       "DTW FV              0.000           43.933           44.078   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.414             0.000            4.447   \n",
       "RMSE FV                 0.538             0.000            6.204   \n",
       "MAPE FV                 0.575             0.000            4.171   \n",
       "R2 FV                   0.994             1.000            0.655   \n",
       "RAAE FV                 0.055             0.000            0.412   \n",
       "RMAE FV                 0.238             0.000            2.007   \n",
       "FD FV                   1.038             0.000           11.623   \n",
       "DTW FV                  4.145             0.000           43.174   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             4.462                 0.438             0.000  \n",
       "RMSE FV            6.157                 0.569             0.000  \n",
       "MAPE FV            4.079                 0.471             0.000  \n",
       "R2 FV              0.660                 0.994             1.000  \n",
       "RAAE FV            0.414                 0.058             0.000  \n",
       "RMAE FV            1.959                 0.247             0.000  \n",
       "FD FV             11.504                 1.095             0.000  \n",
       "DTW FV            43.318                 4.389             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:52.940571Z",
     "start_time": "2020-12-15T19:19:52.920730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.192</td>\n",
       "      <td>10.192</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.703</td>\n",
       "      <td>9.704</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.949</td>\n",
       "      <td>8.950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.528</td>\n",
       "      <td>4.543</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.445</td>\n",
       "      <td>4.460</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.983</td>\n",
       "      <td>12.983</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.477</td>\n",
       "      <td>12.476</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.693</td>\n",
       "      <td>11.693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.316</td>\n",
       "      <td>6.275</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.211</td>\n",
       "      <td>6.165</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.419</td>\n",
       "      <td>1.417</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.603</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.193</td>\n",
       "      <td>2.190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.658</td>\n",
       "      <td>3.665</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.629</td>\n",
       "      <td>3.626</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.421</td>\n",
       "      <td>-0.421</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.162</td>\n",
       "      <td>3.162</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.135</td>\n",
       "      <td>3.135</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.088</td>\n",
       "      <td>3.087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.126</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.100</td>\n",
       "      <td>2.048</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.413</td>\n",
       "      <td>24.412</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.680</td>\n",
       "      <td>23.680</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.358</td>\n",
       "      <td>22.356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.712</td>\n",
       "      <td>11.596</td>\n",
       "      <td>1.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.546</td>\n",
       "      <td>11.420</td>\n",
       "      <td>1.086</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>101.707</td>\n",
       "      <td>101.708</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.858</td>\n",
       "      <td>96.861</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.127</td>\n",
       "      <td>89.141</td>\n",
       "      <td>...</td>\n",
       "      <td>3.881</td>\n",
       "      <td>0.000</td>\n",
       "      <td>43.633</td>\n",
       "      <td>43.782</td>\n",
       "      <td>4.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.891</td>\n",
       "      <td>43.042</td>\n",
       "      <td>4.369</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.192        10.192              0.014          0.000   \n",
       "RMSE FV        12.983        12.983              0.017          0.000   \n",
       "MAPE FV         1.419         1.417              0.201          0.000   \n",
       "R2 FV          -0.421        -0.421              0.999          1.000   \n",
       "RAAE FV         0.925         0.925              0.024          0.000   \n",
       "RMAE FV         3.162         3.162              0.098          0.000   \n",
       "FD FV          24.413        24.412              0.033          0.000   \n",
       "DTW FV        101.707       101.708              0.136          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.703          9.704               0.016           0.000   \n",
       "RMSE FV         12.477         12.476               0.022           0.000   \n",
       "MAPE FV          1.603          1.599               0.206           0.000   \n",
       "R2 FV           -0.299         -0.299               0.999           1.000   \n",
       "RAAE FV          0.878          0.878               0.025           0.000   \n",
       "RMAE FV          3.135          3.135               0.116           0.000   \n",
       "FD FV           23.680         23.680               0.043           0.000   \n",
       "DTW FV          96.858         96.861               0.163           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           8.949          8.950  ...                0.389   \n",
       "RMSE FV         11.693         11.693  ...                0.508   \n",
       "MAPE FV          2.193          2.190  ...                0.761   \n",
       "R2 FV           -0.122         -0.122  ...                0.995   \n",
       "RAAE FV          0.807          0.807  ...                0.052   \n",
       "RMAE FV          3.088          3.087  ...                0.243   \n",
       "FD FV           22.358         22.356  ...                0.972   \n",
       "DTW FV          89.127         89.141  ...                3.881   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           4.528           4.543                0.414   \n",
       "RMSE FV            0.000           6.316           6.275                0.538   \n",
       "MAPE FV            0.000           3.658           3.665                0.482   \n",
       "R2 FV              1.000           0.645           0.649                0.994   \n",
       "RAAE FV            0.000           0.419           0.420                0.055   \n",
       "RMAE FV            0.000           2.126           2.078                0.252   \n",
       "FD FV              0.000          11.712          11.596                1.029   \n",
       "DTW FV             0.000          43.633          43.782                4.126   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           4.445           4.460                0.438   \n",
       "RMSE FV            0.000           6.211           6.165                0.569   \n",
       "MAPE FV            0.000           3.629           3.626                0.454   \n",
       "R2 FV              1.000           0.656           0.660                0.994   \n",
       "RAAE FV            0.000           0.411           0.413                0.058   \n",
       "RMAE FV            0.000           2.100           2.048                0.262   \n",
       "FD FV              0.000          11.546          11.420                1.086   \n",
       "DTW FV             0.000          42.891          43.042                4.369   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:52.964209Z",
     "start_time": "2020-12-15T19:19:52.941995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>...</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.575</td>\n",
       "      <td>0.673</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.709</td>\n",
       "      <td>2.619</td>\n",
       "      <td>3.590</td>\n",
       "      <td>4.487</td>\n",
       "      <td>5.247</td>\n",
       "      <td>5.856</td>\n",
       "      <td>6.330</td>\n",
       "      <td>...</td>\n",
       "      <td>6.969</td>\n",
       "      <td>7.181</td>\n",
       "      <td>7.349</td>\n",
       "      <td>7.486</td>\n",
       "      <td>7.601</td>\n",
       "      <td>7.702</td>\n",
       "      <td>7.794</td>\n",
       "      <td>7.879</td>\n",
       "      <td>7.960</td>\n",
       "      <td>8.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.673</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.708</td>\n",
       "      <td>2.618</td>\n",
       "      <td>3.588</td>\n",
       "      <td>4.485</td>\n",
       "      <td>5.244</td>\n",
       "      <td>5.853</td>\n",
       "      <td>6.325</td>\n",
       "      <td>...</td>\n",
       "      <td>6.962</td>\n",
       "      <td>7.174</td>\n",
       "      <td>7.341</td>\n",
       "      <td>7.476</td>\n",
       "      <td>7.590</td>\n",
       "      <td>7.690</td>\n",
       "      <td>7.780</td>\n",
       "      <td>7.864</td>\n",
       "      <td>7.943</td>\n",
       "      <td>8.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>...</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "      <td>11.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>...</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.573</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.697</td>\n",
       "      <td>2.600</td>\n",
       "      <td>3.564</td>\n",
       "      <td>4.456</td>\n",
       "      <td>5.212</td>\n",
       "      <td>5.818</td>\n",
       "      <td>6.289</td>\n",
       "      <td>...</td>\n",
       "      <td>6.924</td>\n",
       "      <td>7.136</td>\n",
       "      <td>7.302</td>\n",
       "      <td>7.437</td>\n",
       "      <td>7.550</td>\n",
       "      <td>7.649</td>\n",
       "      <td>7.739</td>\n",
       "      <td>7.822</td>\n",
       "      <td>7.902</td>\n",
       "      <td>7.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.573</td>\n",
       "      <td>0.670</td>\n",
       "      <td>1.028</td>\n",
       "      <td>1.696</td>\n",
       "      <td>2.599</td>\n",
       "      <td>3.562</td>\n",
       "      <td>4.454</td>\n",
       "      <td>5.209</td>\n",
       "      <td>5.814</td>\n",
       "      <td>6.285</td>\n",
       "      <td>...</td>\n",
       "      <td>6.919</td>\n",
       "      <td>7.129</td>\n",
       "      <td>7.295</td>\n",
       "      <td>7.429</td>\n",
       "      <td>7.541</td>\n",
       "      <td>7.640</td>\n",
       "      <td>7.728</td>\n",
       "      <td>7.811</td>\n",
       "      <td>7.889</td>\n",
       "      <td>7.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>...</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "      <td>11.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>...</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.672</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.700</td>\n",
       "      <td>2.603</td>\n",
       "      <td>3.567</td>\n",
       "      <td>4.459</td>\n",
       "      <td>5.216</td>\n",
       "      <td>5.823</td>\n",
       "      <td>6.294</td>\n",
       "      <td>...</td>\n",
       "      <td>6.931</td>\n",
       "      <td>7.142</td>\n",
       "      <td>7.309</td>\n",
       "      <td>7.444</td>\n",
       "      <td>7.558</td>\n",
       "      <td>7.657</td>\n",
       "      <td>7.747</td>\n",
       "      <td>7.831</td>\n",
       "      <td>7.910</td>\n",
       "      <td>7.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.671</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.699</td>\n",
       "      <td>2.602</td>\n",
       "      <td>3.565</td>\n",
       "      <td>4.457</td>\n",
       "      <td>5.213</td>\n",
       "      <td>5.819</td>\n",
       "      <td>6.290</td>\n",
       "      <td>...</td>\n",
       "      <td>6.925</td>\n",
       "      <td>7.136</td>\n",
       "      <td>7.302</td>\n",
       "      <td>7.436</td>\n",
       "      <td>7.549</td>\n",
       "      <td>7.648</td>\n",
       "      <td>7.737</td>\n",
       "      <td>7.819</td>\n",
       "      <td>7.898</td>\n",
       "      <td>7.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>...</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "      <td>11.140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.158 11.158 11.158 11.158 11.158 11.158   \n",
       "STD FV TRAIN PRED LAMBDA             0.575  0.673  1.035  1.709  2.619  3.590   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.574  0.673  1.035  1.708  2.618  3.588   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.158 11.158 11.158 11.158 11.158 11.158   \n",
       "STD FV VALID REAL LAMBDA            11.117 11.117 11.117 11.117 11.117 11.117   \n",
       "STD FV VALID PRED LAMBDA             0.573  0.671  1.029  1.697  2.600  3.564   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.573  0.670  1.028  1.696  2.599  3.562   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.117 11.117 11.117 11.117 11.117 11.117   \n",
       "STD FV TEST REAL LAMBDA             11.140 11.140 11.140 11.140 11.140 11.140   \n",
       "STD FV TEST PRED LAMBDA              0.574  0.672  1.031  1.700  2.603  3.567   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.574  0.671  1.030  1.699  2.602  3.565   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.140 11.140 11.140 11.140 11.140 11.140   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.158 11.158 11.158 11.158  ... 11.158   \n",
       "STD FV TRAIN PRED LAMBDA             4.487  5.247  5.856  6.330  ...  6.969   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  4.485  5.244  5.853  6.325  ...  6.962   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.158 11.158 11.158 11.158  ... 11.158   \n",
       "STD FV VALID REAL LAMBDA            11.117 11.117 11.117 11.117  ... 11.117   \n",
       "STD FV VALID PRED LAMBDA             4.456  5.212  5.818  6.289  ...  6.924   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  4.454  5.209  5.814  6.285  ...  6.919   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.117 11.117 11.117 11.117  ... 11.117   \n",
       "STD FV TEST REAL LAMBDA             11.140 11.140 11.140 11.140  ... 11.140   \n",
       "STD FV TEST PRED LAMBDA              4.459  5.216  5.823  6.294  ...  6.931   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   4.457  5.213  5.819  6.290  ...  6.925   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.140 11.140 11.140 11.140  ... 11.140   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.158 11.158 11.158 11.158 11.158 11.158   \n",
       "STD FV TRAIN PRED LAMBDA             7.181  7.349  7.486  7.601  7.702  7.794   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.174  7.341  7.476  7.590  7.690  7.780   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.158 11.158 11.158 11.158 11.158 11.158   \n",
       "STD FV VALID REAL LAMBDA            11.117 11.117 11.117 11.117 11.117 11.117   \n",
       "STD FV VALID PRED LAMBDA             7.136  7.302  7.437  7.550  7.649  7.739   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.129  7.295  7.429  7.541  7.640  7.728   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.117 11.117 11.117 11.117 11.117 11.117   \n",
       "STD FV TEST REAL LAMBDA             11.140 11.140 11.140 11.140 11.140 11.140   \n",
       "STD FV TEST PRED LAMBDA              7.142  7.309  7.444  7.558  7.657  7.747   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.136  7.302  7.436  7.549  7.648  7.737   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.140 11.140 11.140 11.140 11.140 11.140   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.158 11.158 11.158  \n",
       "STD FV TRAIN PRED LAMBDA             7.879  7.960  8.038  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.864  7.943  8.019  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.158 11.158 11.158  \n",
       "STD FV VALID REAL LAMBDA            11.117 11.117 11.117  \n",
       "STD FV VALID PRED LAMBDA             7.822  7.902  7.977  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.811  7.889  7.963  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.117 11.117 11.117  \n",
       "STD FV TEST REAL LAMBDA             11.140 11.140 11.140  \n",
       "STD FV TEST PRED LAMBDA              7.831  7.910  7.986  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.819  7.898  7.972  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.140 11.140 11.140  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:52.991657Z",
     "start_time": "2020-12-15T19:19:52.965594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.004 -0.014 -0.034 -0.056 -0.069   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.004 -0.014 -0.034 -0.056 -0.069   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV VALID PRED LAMBDA            -0.005 -0.015 -0.035 -0.058 -0.072   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.005 -0.015 -0.035 -0.058 -0.072   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV TEST REAL LAMBDA             -0.084 -0.084 -0.084 -0.084 -0.084   \n",
       "MEAN FV TEST PRED LAMBDA             -0.005 -0.014 -0.034 -0.057 -0.070   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.005 -0.014 -0.034 -0.057 -0.070   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.084 -0.084 -0.084 -0.084 -0.084   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.077 -0.077 -0.077 -0.077 -0.077  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.074 -0.071 -0.069 -0.067 -0.066  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.074 -0.071 -0.069 -0.067 -0.066  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.077 -0.077 -0.077 -0.077 -0.077  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075  ...   \n",
       "MEAN FV VALID PRED LAMBDA            -0.077 -0.074 -0.071 -0.068 -0.067  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.077 -0.074 -0.071 -0.069 -0.067  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.084 -0.084 -0.084 -0.084 -0.084  ...   \n",
       "MEAN FV TEST PRED LAMBDA             -0.075 -0.073 -0.071 -0.069 -0.068  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.075 -0.073 -0.071 -0.069 -0.068  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.084 -0.084 -0.084 -0.084 -0.084  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.065 -0.063 -0.062 -0.061 -0.062   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.065 -0.063 -0.062 -0.061 -0.062   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.077 -0.077 -0.077 -0.077 -0.077   \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV VALID PRED LAMBDA            -0.067 -0.065 -0.064 -0.064 -0.065   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.067 -0.065 -0.065 -0.064 -0.065   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075   \n",
       "MEAN FV TEST REAL LAMBDA             -0.084 -0.084 -0.084 -0.084 -0.084   \n",
       "MEAN FV TEST PRED LAMBDA             -0.067 -0.066 -0.065 -0.064 -0.064   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.068 -0.066 -0.065 -0.064 -0.065   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.084 -0.084 -0.084 -0.084 -0.084   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.077 -0.077 -0.077 -0.077 -0.077  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.061 -0.062 -0.062 -0.063 -0.064  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.061 -0.062 -0.062 -0.063 -0.064  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.077 -0.077 -0.077 -0.077 -0.077  \n",
       "MEAN FV VALID REAL LAMBDA            -0.075 -0.075 -0.075 -0.075 -0.075  \n",
       "MEAN FV VALID PRED LAMBDA            -0.065 -0.065 -0.066 -0.067 -0.068  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.065 -0.065 -0.066 -0.067 -0.068  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.075 -0.075 -0.075 -0.075 -0.075  \n",
       "MEAN FV TEST REAL LAMBDA             -0.084 -0.084 -0.084 -0.084 -0.084  \n",
       "MEAN FV TEST PRED LAMBDA             -0.064 -0.065 -0.065 -0.066 -0.067  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.064 -0.065 -0.065 -0.067 -0.067  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.084 -0.084 -0.084 -0.084 -0.084  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:19:53.087397Z",
     "start_time": "2020-12-15T19:19:52.993294Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_lambda_preds(i, \n",
    "                      lambda_indices,\n",
    "                      y_train_real_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_train_lambda_by_epoch, \n",
    "                      y_valid_real_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_valid_lambda_by_epoch, \n",
    "                      y_test_real_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_test_lambda_by_epoch):\n",
    "    \n",
    "    \n",
    "    index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "\n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n",
    "\n",
    "    return y_train_real_lambda_df, y_valid_real_lambda_df, y_test_real_lambda_df, y_train_pred_lambda_df, y_valid_pred_lambda_df, y_test_pred_lambda_df, y_train_pred_lambda_poly_lstsq_df, y_valid_pred_lambda_poly_lstsq_df, y_test_pred_lambda_poly_lstsq_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:38.342781Z",
     "start_time": "2020-12-15T19:19:53.089405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02da36beb804be993836fe5a625c8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend MultiprocessingBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of  21 | elapsed:  6.9min remaining: 65.7min\n",
      "[Parallel(n_jobs=-3)]: Done  10 out of  21 | elapsed:  7.8min remaining:  8.6min\n",
      "[Parallel(n_jobs=-3)]: Done  18 out of  21 | elapsed:  8.3min remaining:  1.4min\n",
      "[Parallel(n_jobs=-3)]: Done  21 out of  21 | elapsed:  8.4min finished\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    lambda_index_list = [clf[0][0] for clf in clf_list]\n",
    "    lambda_seed_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_real_list = [clf[0][2] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][3] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][4] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    lambda_indices_list = np.zeros((len(clf_list), 1))\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][3])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][4].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][7])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][8].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][11])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][12].shape)]][0])\n",
    "\n",
    "    for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        lambda_indices_list[index] = lambda_indices\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    lambda_indices_list = [np.zeros((len(clf_list), 1)) for i in epochs_save_range]\n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][3]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][4].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][7]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][8].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][12]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][12].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            lambda_indices_list[index][i] = lambda_indices\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    y_data_lambda_list = parallel(delayed(save_lambda_preds)(i, \n",
    "                                                           lambda_indices,\n",
    "                                                           y_train_real_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_train_lambda_by_epoch, \n",
    "                                                           y_valid_real_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_valid_lambda_by_epoch, \n",
    "                                                           y_test_real_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_test_lambda_by_epoch) for i, \n",
    "                                                                                        (lambda_indices,\n",
    "                                                                                         y_train_real_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_train_lambda_by_epoch, \n",
    "                                                                                         y_valid_real_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_valid_lambda_by_epoch, \n",
    "                                                                                         y_test_real_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_test_lambda_by_epoch) in enumerate(zip(lambda_indices_list,\n",
    "                                                                                                                                  y_train_real_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_train_lambda_list, \n",
    "                                                                                                                                  y_valid_real_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_valid_lambda_list, \n",
    "                                                                                                                                  y_test_real_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_test_lambda_list)))  \n",
    "    y_test_real_lambda_df = y_data_lambda_list[-1][2]\n",
    "    y_test_pred_lambda_df = y_data_lambda_list[-1][5]\n",
    "    y_test_pred_lambda_poly_lstsq_df = y_data_lambda_list[-1][8]\n",
    "    del parallel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:38.366580Z",
     "start_time": "2020-12-15T19:28:38.345003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-7.580</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>10.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-11.955</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-19.621</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-11.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-2.020</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>20.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>25.836</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -7.580 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -11.955  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -19.621  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940  -2.020  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920  25.836 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0  10.155  \n",
       "1  -0.622  \n",
       "2 -11.502  \n",
       "3  20.981  \n",
       "4  -7.972  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:38.385990Z",
     "start_time": "2020-12-15T19:28:38.368133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-4.452</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>3.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-17.056</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.238</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-8.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>9.805</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>4.999</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -4.452 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -17.056  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.238  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   9.805  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   4.999 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   3.041  \n",
       "1  -1.833  \n",
       "2  -8.347  \n",
       "3   6.104  \n",
       "4  -7.074  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:38.405714Z",
     "start_time": "2020-12-15T19:28:38.387617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-3.989</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>4.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-17.348</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-1.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.962</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-7.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>8.962</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>6.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>3.913</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-6.428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -3.989 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -17.348  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.962  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   8.962  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   3.913 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   4.071  \n",
       "1  -1.945  \n",
       "2  -7.902  \n",
       "3   6.402  \n",
       "4  -6.428  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:41.432868Z",
     "start_time": "2020-12-15T19:28:38.407420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a01caca62b74154974d2b9d42928c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:56.131118Z",
     "start_time": "2020-12-15T19:28:41.434841Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:56.743962Z",
     "start_time": "2020-12-15T19:28:56.133887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.225</td>\n",
       "      <td>10.172</td>\n",
       "      <td>10.119</td>\n",
       "      <td>10.066</td>\n",
       "      <td>10.013</td>\n",
       "      <td>9.959</td>\n",
       "      <td>9.905</td>\n",
       "      <td>9.850</td>\n",
       "      <td>9.793</td>\n",
       "      <td>9.734</td>\n",
       "      <td>...</td>\n",
       "      <td>4.396</td>\n",
       "      <td>4.388</td>\n",
       "      <td>4.379</td>\n",
       "      <td>4.370</td>\n",
       "      <td>4.362</td>\n",
       "      <td>4.353</td>\n",
       "      <td>4.344</td>\n",
       "      <td>4.336</td>\n",
       "      <td>4.327</td>\n",
       "      <td>4.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.345</td>\n",
       "      <td>2.322</td>\n",
       "      <td>2.299</td>\n",
       "      <td>2.276</td>\n",
       "      <td>2.253</td>\n",
       "      <td>2.229</td>\n",
       "      <td>2.206</td>\n",
       "      <td>2.181</td>\n",
       "      <td>2.156</td>\n",
       "      <td>2.130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.626</td>\n",
       "      <td>4.619</td>\n",
       "      <td>4.612</td>\n",
       "      <td>4.605</td>\n",
       "      <td>4.599</td>\n",
       "      <td>4.592</td>\n",
       "      <td>4.586</td>\n",
       "      <td>4.580</td>\n",
       "      <td>4.574</td>\n",
       "      <td>4.568</td>\n",
       "      <td>...</td>\n",
       "      <td>2.376</td>\n",
       "      <td>2.370</td>\n",
       "      <td>2.365</td>\n",
       "      <td>2.359</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.348</td>\n",
       "      <td>2.344</td>\n",
       "      <td>2.338</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.503</td>\n",
       "      <td>8.464</td>\n",
       "      <td>8.427</td>\n",
       "      <td>8.392</td>\n",
       "      <td>8.356</td>\n",
       "      <td>8.320</td>\n",
       "      <td>8.286</td>\n",
       "      <td>8.244</td>\n",
       "      <td>8.208</td>\n",
       "      <td>8.166</td>\n",
       "      <td>...</td>\n",
       "      <td>4.001</td>\n",
       "      <td>3.993</td>\n",
       "      <td>3.985</td>\n",
       "      <td>3.976</td>\n",
       "      <td>3.967</td>\n",
       "      <td>3.960</td>\n",
       "      <td>3.952</td>\n",
       "      <td>3.945</td>\n",
       "      <td>3.936</td>\n",
       "      <td>3.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.951</td>\n",
       "      <td>9.901</td>\n",
       "      <td>9.856</td>\n",
       "      <td>9.808</td>\n",
       "      <td>9.763</td>\n",
       "      <td>9.719</td>\n",
       "      <td>9.671</td>\n",
       "      <td>9.621</td>\n",
       "      <td>9.571</td>\n",
       "      <td>9.522</td>\n",
       "      <td>...</td>\n",
       "      <td>4.370</td>\n",
       "      <td>4.362</td>\n",
       "      <td>4.353</td>\n",
       "      <td>4.345</td>\n",
       "      <td>4.337</td>\n",
       "      <td>4.328</td>\n",
       "      <td>4.318</td>\n",
       "      <td>4.309</td>\n",
       "      <td>4.300</td>\n",
       "      <td>4.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.646</td>\n",
       "      <td>11.587</td>\n",
       "      <td>11.526</td>\n",
       "      <td>11.460</td>\n",
       "      <td>11.389</td>\n",
       "      <td>11.330</td>\n",
       "      <td>11.268</td>\n",
       "      <td>11.200</td>\n",
       "      <td>11.129</td>\n",
       "      <td>11.049</td>\n",
       "      <td>...</td>\n",
       "      <td>4.765</td>\n",
       "      <td>4.756</td>\n",
       "      <td>4.748</td>\n",
       "      <td>4.739</td>\n",
       "      <td>4.730</td>\n",
       "      <td>4.721</td>\n",
       "      <td>4.710</td>\n",
       "      <td>4.701</td>\n",
       "      <td>4.691</td>\n",
       "      <td>4.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.013</td>\n",
       "      <td>20.833</td>\n",
       "      <td>20.654</td>\n",
       "      <td>20.473</td>\n",
       "      <td>20.287</td>\n",
       "      <td>20.096</td>\n",
       "      <td>19.898</td>\n",
       "      <td>19.690</td>\n",
       "      <td>19.468</td>\n",
       "      <td>19.231</td>\n",
       "      <td>...</td>\n",
       "      <td>7.407</td>\n",
       "      <td>7.399</td>\n",
       "      <td>7.392</td>\n",
       "      <td>7.384</td>\n",
       "      <td>7.378</td>\n",
       "      <td>7.370</td>\n",
       "      <td>7.363</td>\n",
       "      <td>7.355</td>\n",
       "      <td>7.348</td>\n",
       "      <td>7.341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         10.225        10.172        10.119        10.066        10.013   \n",
       "std           2.345         2.322         2.299         2.276         2.253   \n",
       "min           4.626         4.619         4.612         4.605         4.599   \n",
       "25%           8.503         8.464         8.427         8.392         8.356   \n",
       "50%           9.951         9.901         9.856         9.808         9.763   \n",
       "75%          11.646        11.587        11.526        11.460        11.389   \n",
       "max          21.013        20.833        20.654        20.473        20.287   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean          9.959         9.905         9.850         9.793          9.734   \n",
       "std           2.229         2.206         2.181         2.156          2.130   \n",
       "min           4.592         4.586         4.580         4.574          4.568   \n",
       "25%           8.320         8.286         8.244         8.208          8.166   \n",
       "50%           9.719         9.671         9.621         9.571          9.522   \n",
       "75%          11.330        11.268        11.200        11.129         11.049   \n",
       "max          20.096        19.898        19.690        19.468         19.231   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           4.396           4.388           4.379           4.370   \n",
       "std    ...           0.582           0.582           0.581           0.580   \n",
       "min    ...           2.376           2.370           2.365           2.359   \n",
       "25%    ...           4.001           3.993           3.985           3.976   \n",
       "50%    ...           4.370           4.362           4.353           4.345   \n",
       "75%    ...           4.765           4.756           4.748           4.739   \n",
       "max    ...           7.407           7.399           7.392           7.384   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            4.362           4.353           4.344           4.336   \n",
       "std             0.580           0.579           0.578           0.578   \n",
       "min             2.355           2.348           2.344           2.338   \n",
       "25%             3.967           3.960           3.952           3.945   \n",
       "50%             4.337           4.328           4.318           4.309   \n",
       "75%             4.730           4.721           4.710           4.701   \n",
       "max             7.378           7.370           7.363           7.355   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            4.327           4.318  \n",
       "std             0.577           0.576  \n",
       "min             2.333           2.329  \n",
       "25%             3.936           3.927  \n",
       "50%             4.300           4.292  \n",
       "75%             4.691           4.683  \n",
       "max             7.348           7.341  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:57.351775Z",
     "start_time": "2020-12-15T19:28:56.745606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.185</td>\n",
       "      <td>10.132</td>\n",
       "      <td>10.080</td>\n",
       "      <td>10.028</td>\n",
       "      <td>9.975</td>\n",
       "      <td>9.922</td>\n",
       "      <td>9.868</td>\n",
       "      <td>9.812</td>\n",
       "      <td>9.755</td>\n",
       "      <td>9.696</td>\n",
       "      <td>...</td>\n",
       "      <td>4.522</td>\n",
       "      <td>4.514</td>\n",
       "      <td>4.505</td>\n",
       "      <td>4.497</td>\n",
       "      <td>4.489</td>\n",
       "      <td>4.480</td>\n",
       "      <td>4.472</td>\n",
       "      <td>4.464</td>\n",
       "      <td>4.456</td>\n",
       "      <td>4.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.369</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.323</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.277</td>\n",
       "      <td>2.254</td>\n",
       "      <td>2.230</td>\n",
       "      <td>2.206</td>\n",
       "      <td>2.181</td>\n",
       "      <td>2.154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.860</td>\n",
       "      <td>4.855</td>\n",
       "      <td>4.850</td>\n",
       "      <td>4.845</td>\n",
       "      <td>4.840</td>\n",
       "      <td>4.832</td>\n",
       "      <td>4.814</td>\n",
       "      <td>4.795</td>\n",
       "      <td>4.777</td>\n",
       "      <td>4.760</td>\n",
       "      <td>...</td>\n",
       "      <td>2.523</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2.505</td>\n",
       "      <td>2.496</td>\n",
       "      <td>2.489</td>\n",
       "      <td>2.482</td>\n",
       "      <td>2.477</td>\n",
       "      <td>2.470</td>\n",
       "      <td>2.468</td>\n",
       "      <td>2.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.432</td>\n",
       "      <td>8.399</td>\n",
       "      <td>8.368</td>\n",
       "      <td>8.330</td>\n",
       "      <td>8.291</td>\n",
       "      <td>8.257</td>\n",
       "      <td>8.218</td>\n",
       "      <td>8.180</td>\n",
       "      <td>8.149</td>\n",
       "      <td>8.110</td>\n",
       "      <td>...</td>\n",
       "      <td>4.070</td>\n",
       "      <td>4.063</td>\n",
       "      <td>4.055</td>\n",
       "      <td>4.046</td>\n",
       "      <td>4.039</td>\n",
       "      <td>4.032</td>\n",
       "      <td>4.023</td>\n",
       "      <td>4.015</td>\n",
       "      <td>4.007</td>\n",
       "      <td>3.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.927</td>\n",
       "      <td>9.885</td>\n",
       "      <td>9.833</td>\n",
       "      <td>9.784</td>\n",
       "      <td>9.736</td>\n",
       "      <td>9.688</td>\n",
       "      <td>9.641</td>\n",
       "      <td>9.588</td>\n",
       "      <td>9.539</td>\n",
       "      <td>9.483</td>\n",
       "      <td>...</td>\n",
       "      <td>4.492</td>\n",
       "      <td>4.484</td>\n",
       "      <td>4.474</td>\n",
       "      <td>4.465</td>\n",
       "      <td>4.456</td>\n",
       "      <td>4.449</td>\n",
       "      <td>4.441</td>\n",
       "      <td>4.433</td>\n",
       "      <td>4.423</td>\n",
       "      <td>4.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.598</td>\n",
       "      <td>11.533</td>\n",
       "      <td>11.463</td>\n",
       "      <td>11.397</td>\n",
       "      <td>11.336</td>\n",
       "      <td>11.261</td>\n",
       "      <td>11.195</td>\n",
       "      <td>11.128</td>\n",
       "      <td>11.059</td>\n",
       "      <td>10.992</td>\n",
       "      <td>...</td>\n",
       "      <td>4.937</td>\n",
       "      <td>4.929</td>\n",
       "      <td>4.920</td>\n",
       "      <td>4.910</td>\n",
       "      <td>4.902</td>\n",
       "      <td>4.893</td>\n",
       "      <td>4.884</td>\n",
       "      <td>4.875</td>\n",
       "      <td>4.866</td>\n",
       "      <td>4.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.757</td>\n",
       "      <td>20.600</td>\n",
       "      <td>20.440</td>\n",
       "      <td>20.275</td>\n",
       "      <td>20.105</td>\n",
       "      <td>19.927</td>\n",
       "      <td>19.743</td>\n",
       "      <td>19.624</td>\n",
       "      <td>19.500</td>\n",
       "      <td>19.369</td>\n",
       "      <td>...</td>\n",
       "      <td>7.695</td>\n",
       "      <td>7.683</td>\n",
       "      <td>7.671</td>\n",
       "      <td>7.659</td>\n",
       "      <td>7.647</td>\n",
       "      <td>7.635</td>\n",
       "      <td>7.624</td>\n",
       "      <td>7.613</td>\n",
       "      <td>7.600</td>\n",
       "      <td>7.588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.185            10.132            10.080            10.028   \n",
       "std               2.369             2.346             2.323             2.300   \n",
       "min               4.860             4.855             4.850             4.845   \n",
       "25%               8.432             8.399             8.368             8.330   \n",
       "50%               9.927             9.885             9.833             9.784   \n",
       "75%              11.598            11.533            11.463            11.397   \n",
       "max              20.757            20.600            20.440            20.275   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              9.975             9.922             9.868             9.812   \n",
       "std               2.277             2.254             2.230             2.206   \n",
       "min               4.840             4.832             4.814             4.795   \n",
       "25%               8.291             8.257             8.218             8.180   \n",
       "50%               9.736             9.688             9.641             9.588   \n",
       "75%              11.336            11.261            11.195            11.128   \n",
       "max              20.105            19.927            19.743            19.624   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean              9.755              9.696  ...               4.522   \n",
       "std               2.181              2.154  ...               0.656   \n",
       "min               4.777              4.760  ...               2.523   \n",
       "25%               8.149              8.110  ...               4.070   \n",
       "50%               9.539              9.483  ...               4.492   \n",
       "75%              11.059             10.992  ...               4.937   \n",
       "max              19.500             19.369  ...               7.695   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.514               4.505               4.497   \n",
       "std                 0.655               0.655               0.654   \n",
       "min                 2.515               2.505               2.496   \n",
       "25%                 4.063               4.055               4.046   \n",
       "50%                 4.484               4.474               4.465   \n",
       "75%                 4.929               4.920               4.910   \n",
       "max                 7.683               7.671               7.659   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                4.489               4.480               4.472   \n",
       "std                 0.653               0.652               0.652   \n",
       "min                 2.489               2.482               2.477   \n",
       "25%                 4.039               4.032               4.023   \n",
       "50%                 4.456               4.449               4.441   \n",
       "75%                 4.902               4.893               4.884   \n",
       "max                 7.647               7.635               7.624   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                4.464               4.456               4.447  \n",
       "std                 0.651               0.650               0.650  \n",
       "min                 2.470               2.468               2.458  \n",
       "25%                 4.015               4.007               3.999  \n",
       "50%                 4.433               4.423               4.416  \n",
       "75%                 4.875               4.866               4.858  \n",
       "max                 7.613               7.600               7.588  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:57.987826Z",
     "start_time": "2020-12-15T19:28:57.353424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.957</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.051</td>\n",
       "      <td>1.046</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.038</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.047</td>\n",
       "      <td>...</td>\n",
       "      <td>1.223</td>\n",
       "      <td>1.220</td>\n",
       "      <td>1.219</td>\n",
       "      <td>1.215</td>\n",
       "      <td>1.215</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.106</td>\n",
       "      <td>1.100</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.098</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.106</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.119</td>\n",
       "      <td>...</td>\n",
       "      <td>1.639</td>\n",
       "      <td>1.630</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.628</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.621</td>\n",
       "      <td>1.616</td>\n",
       "      <td>1.617</td>\n",
       "      <td>1.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.210</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.210</td>\n",
       "      <td>1.216</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.235</td>\n",
       "      <td>1.248</td>\n",
       "      <td>1.265</td>\n",
       "      <td>...</td>\n",
       "      <td>2.342</td>\n",
       "      <td>2.335</td>\n",
       "      <td>2.335</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.331</td>\n",
       "      <td>2.329</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.316</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.957           0.951           0.935           0.913   \n",
       "25%             1.051           1.046           1.041           1.038   \n",
       "50%             1.106           1.100           1.097           1.096   \n",
       "75%             1.210           1.205           1.205           1.204   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean              inf             inf             inf             inf   \n",
       "std               nan             nan             nan             nan   \n",
       "min             0.894           0.872           0.853           0.832   \n",
       "25%             1.036           1.035           1.037           1.040   \n",
       "50%             1.096           1.098           1.101           1.106   \n",
       "75%             1.210           1.216           1.225           1.235   \n",
       "max               inf             inf             inf             inf   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean              inf              inf  ...               inf   \n",
       "std               nan              nan  ...               nan   \n",
       "min             0.814            0.791  ...             0.196   \n",
       "25%             1.043            1.047  ...             1.223   \n",
       "50%             1.112            1.119  ...             1.639   \n",
       "75%             1.248            1.265  ...             2.342   \n",
       "max               inf              inf  ...               inf   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.197             0.197             0.195             0.195   \n",
       "25%               1.220             1.219             1.215             1.215   \n",
       "50%               1.630             1.632             1.628             1.625   \n",
       "75%               2.335             2.335             2.332             2.331   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean                inf               inf               inf               inf   \n",
       "std                 nan               nan               nan               nan   \n",
       "min               0.195             0.196             0.193             0.194   \n",
       "25%               1.212             1.212             1.209             1.206   \n",
       "50%               1.621             1.621             1.616             1.617   \n",
       "75%               2.329             2.325             2.316             2.314   \n",
       "max                 inf               inf               inf               inf   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean                inf  \n",
       "std                 nan  \n",
       "min               0.194  \n",
       "25%               1.205  \n",
       "50%               1.616  \n",
       "75%               2.310  \n",
       "max                 inf  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:28:58.593147Z",
     "start_time": "2020-12-15T19:28:57.989466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.375</td>\n",
       "      <td>1.407</td>\n",
       "      <td>1.443</td>\n",
       "      <td>1.481</td>\n",
       "      <td>1.523</td>\n",
       "      <td>1.567</td>\n",
       "      <td>1.615</td>\n",
       "      <td>1.667</td>\n",
       "      <td>1.722</td>\n",
       "      <td>1.779</td>\n",
       "      <td>...</td>\n",
       "      <td>4.190</td>\n",
       "      <td>4.193</td>\n",
       "      <td>4.186</td>\n",
       "      <td>4.175</td>\n",
       "      <td>4.173</td>\n",
       "      <td>4.164</td>\n",
       "      <td>4.159</td>\n",
       "      <td>4.151</td>\n",
       "      <td>4.151</td>\n",
       "      <td>4.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.894</td>\n",
       "      <td>10.767</td>\n",
       "      <td>13.654</td>\n",
       "      <td>16.520</td>\n",
       "      <td>19.303</td>\n",
       "      <td>22.091</td>\n",
       "      <td>24.896</td>\n",
       "      <td>27.732</td>\n",
       "      <td>30.585</td>\n",
       "      <td>33.443</td>\n",
       "      <td>...</td>\n",
       "      <td>124.279</td>\n",
       "      <td>125.210</td>\n",
       "      <td>124.959</td>\n",
       "      <td>124.512</td>\n",
       "      <td>124.675</td>\n",
       "      <td>124.244</td>\n",
       "      <td>124.292</td>\n",
       "      <td>124.112</td>\n",
       "      <td>124.633</td>\n",
       "      <td>125.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.022</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.005</td>\n",
       "      <td>...</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.055</td>\n",
       "      <td>1.053</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.048</td>\n",
       "      <td>1.045</td>\n",
       "      <td>1.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.068</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.061</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.066</td>\n",
       "      <td>1.070</td>\n",
       "      <td>1.075</td>\n",
       "      <td>...</td>\n",
       "      <td>1.438</td>\n",
       "      <td>1.437</td>\n",
       "      <td>1.435</td>\n",
       "      <td>1.432</td>\n",
       "      <td>1.431</td>\n",
       "      <td>1.427</td>\n",
       "      <td>1.425</td>\n",
       "      <td>1.424</td>\n",
       "      <td>1.420</td>\n",
       "      <td>1.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.163</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.161</td>\n",
       "      <td>1.160</td>\n",
       "      <td>1.164</td>\n",
       "      <td>1.171</td>\n",
       "      <td>1.179</td>\n",
       "      <td>1.189</td>\n",
       "      <td>1.202</td>\n",
       "      <td>1.216</td>\n",
       "      <td>...</td>\n",
       "      <td>2.126</td>\n",
       "      <td>2.124</td>\n",
       "      <td>2.121</td>\n",
       "      <td>2.119</td>\n",
       "      <td>2.115</td>\n",
       "      <td>2.112</td>\n",
       "      <td>2.109</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>691.515</td>\n",
       "      <td>990.013</td>\n",
       "      <td>1282.436</td>\n",
       "      <td>1568.726</td>\n",
       "      <td>1844.467</td>\n",
       "      <td>2119.339</td>\n",
       "      <td>2395.289</td>\n",
       "      <td>2673.559</td>\n",
       "      <td>2952.822</td>\n",
       "      <td>3232.064</td>\n",
       "      <td>...</td>\n",
       "      <td>12291.996</td>\n",
       "      <td>12387.417</td>\n",
       "      <td>12362.495</td>\n",
       "      <td>12318.910</td>\n",
       "      <td>12336.058</td>\n",
       "      <td>12292.589</td>\n",
       "      <td>12298.102</td>\n",
       "      <td>12280.906</td>\n",
       "      <td>12335.144</td>\n",
       "      <td>12403.379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.375               1.407               1.443   \n",
       "std                 7.894              10.767              13.654   \n",
       "min                 0.920               0.905               0.891   \n",
       "25%                 1.022               1.016               1.012   \n",
       "50%                 1.068               1.064               1.061   \n",
       "75%                 1.163               1.162               1.161   \n",
       "max               691.515             990.013            1282.436   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.481               1.523               1.567   \n",
       "std                16.520              19.303              22.091   \n",
       "min                 0.869               0.842               0.823   \n",
       "25%                 1.008               1.005               1.004   \n",
       "50%                 1.060               1.060               1.060   \n",
       "75%                 1.160               1.164               1.171   \n",
       "max              1568.726            1844.467            2119.339   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.615               1.667               1.722   \n",
       "std                24.896              27.732              30.585   \n",
       "min                 0.810               0.798               0.794   \n",
       "25%                 1.004               1.004               1.004   \n",
       "50%                 1.063               1.066               1.070   \n",
       "75%                 1.179               1.189               1.202   \n",
       "max              2395.289            2673.559            2952.822   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                 1.779  ...                 4.190                 4.193   \n",
       "std                 33.443  ...               124.279               125.210   \n",
       "min                  0.782  ...                 0.195                 0.194   \n",
       "25%                  1.005  ...                 1.060                 1.058   \n",
       "50%                  1.075  ...                 1.438                 1.437   \n",
       "75%                  1.216  ...                 2.126                 2.124   \n",
       "max               3232.064  ...             12291.996             12387.417   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  4.186                 4.175                 4.173   \n",
       "std                 124.959               124.512               124.675   \n",
       "min                   0.194                 0.194                 0.194   \n",
       "25%                   1.056                 1.055                 1.053   \n",
       "50%                   1.435                 1.432                 1.431   \n",
       "75%                   2.121                 2.119                 2.115   \n",
       "max               12362.495             12318.910             12336.058   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  4.164                 4.159                 4.151   \n",
       "std                 124.244               124.292               124.112   \n",
       "min                   0.194                 0.194                 0.193   \n",
       "25%                   1.051                 1.049                 1.048   \n",
       "50%                   1.427                 1.425                 1.424   \n",
       "75%                   2.112                 2.109                 2.107   \n",
       "max               12292.589             12298.102             12280.906   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  4.151                 4.153  \n",
       "std                 124.633               125.299  \n",
       "min                   0.193                 0.192  \n",
       "25%                   1.045                 1.044  \n",
       "50%                   1.420                 1.418  \n",
       "75%                   2.104                 2.101  \n",
       "max               12335.144             12403.379  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:29:00.130760Z",
     "start_time": "2020-12-15T19:28:58.594794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T19:29:01.741918Z",
     "start_time": "2020-12-15T19:29:00.133097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['trainover', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
