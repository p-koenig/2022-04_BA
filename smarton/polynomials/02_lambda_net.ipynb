{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:35.331300Z",
     "start_time": "2020-12-09T17:36:35.323851Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:35.348239Z",
     "start_time": "2020-12-09T17:36:35.340573Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 10000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 20\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.244Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.247Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.251Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:42.584805Z",
     "start_time": "2020-12-09T17:36:42.567698Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:42.595313Z",
     "start_time": "2020-12-09T17:36:42.586716Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:42.634495Z",
     "start_time": "2020-12-09T17:36:42.597718Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:42.711050Z",
     "start_time": "2020-12-09T17:36:42.636958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828ec1e54ac447d6bd5b8f0d379ec45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16723eb2ecf5422ea56d2899090e142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:52.406860Z",
     "start_time": "2020-12-09T17:36:42.713120Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:53.608270Z",
     "start_time": "2020-12-09T17:36:52.409418Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:53.635971Z",
     "start_time": "2020-12-09T17:36:53.612011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.340</td>\n",
       "      <td>-0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.950</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.310</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.620 -0.950  0.340 -0.760\n",
       "1 -0.300  0.950 -0.770  0.310\n",
       "2 -0.300  0.280  0.480  0.080\n",
       "3 -0.520  0.980  0.230  0.170\n",
       "4  0.710 -0.920 -0.310  0.260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:53.647585Z",
     "start_time": "2020-12-09T17:36:53.638154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.210</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1      2      3\n",
       "0  0.660 0.100 -0.040 -1.000\n",
       "1 -0.970 0.920  0.460  0.990\n",
       "2 -0.280 0.760  0.910 -0.230\n",
       "3 -0.250 0.210 -0.070 -0.300\n",
       "4 -0.330 0.770 -0.360  0.110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:36:53.656721Z",
     "start_time": "2020-12-09T17:36:53.651697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.282Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    fd_real_VS_predLambda = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_predLambda_VS_predPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size]))), 4)\n",
    "    fd_real_VS_realPolyLstsq = np.round(frechet_dist(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size]))), 4)\n",
    "\n",
    "    dtw_real_VS_predLambda, dtw_complete_real_VS_predLambda = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predLambda = np.round(dtw_real_VS_predLambda, 4)\n",
    "    dtw_real_VS_predPolyLstsq, dtw_complete_real_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_predPolyLstsq = np.round(dtw_real_VS_predPolyLstsq, 4)\n",
    "    dtw_predLambda_VS_predPolyLstsq, dtw_complete_predLambda_VS_predPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda_poly_lstsq[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_pred_lambda[:advanced_metric_dataset_size])))\n",
    "    dtw_predLambda_VS_predPolyLstsq = np.round(dtw_predLambda_VS_predPolyLstsq, 4)    \n",
    "    dtw_real_VS_realPolyLstsq, dtw_complete_real_VS_realPolyLstsq = dtw(np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda[:advanced_metric_dataset_size])), np.column_stack((X_data_real_lambda[:advanced_metric_dataset_size], y_data_real_lambda_poly_lstsq[:advanced_metric_dataset_size])))\n",
    "    dtw_real_VS_realPolyLstsq = np.round(dtw_real_VS_realPolyLstsq, 4) \n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': fd_real_VS_predLambda,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_real_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': fd_predLambda_VS_predPolyLstsq,   \n",
    "             'FD FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': fd_real_VS_realPolyLstsq,   \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': dtw_real_VS_predLambda, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_real_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': dtw_predLambda_VS_predPolyLstsq, \n",
    "             'DTW FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': dtw_real_VS_realPolyLstsq, \n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.285Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.287Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.290Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(lambda_index, X_data_lambda, y_data_real_lambda, polynomial, seed_list, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    current_seed = seed_list[lambda_index%number_different_lambda_trainings]\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(current_seed)\n",
    "        else:\n",
    "            tf.set_random_seed(current_seed) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=current_seed)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=current_seed)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.RandomUniform(seed=current_seed), bias_initializer=tf.keras.initializers.RandomUniform(seed=current_seed))) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if fixed_initialization_lambda_training:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.RandomUniform(seed=current_seed), bias_initializer=tf.keras.initializers.RandomUniform(seed=current_seed)))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.RandomUniform(seed=current_seed), bias_initializer=tf.keras.initializers.RandomUniform(seed=current_seed)))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (lambda_index,\n",
    "                     y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list = [lambda_index,\n",
    "                     scores_train,\n",
    "                     scores_valid,\n",
    "                     scores_test,\n",
    "                     scores_std,\n",
    "                     scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "                #for key_1 in history.keys():\n",
    "                #    for key_2 in model_history.history.keys():\n",
    "                #        if key_1 == key_2:\n",
    "                #            history[key_1] += model_history.history[key_2]  \n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((lambda_index,\n",
    "                              y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [lambda_index,\n",
    "                                         scores_train,\n",
    "                                          scores_valid,\n",
    "                                          scores_test,\n",
    "                                          scores_std,\n",
    "                                          scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                text_file.write(str(lambda_index))\n",
    "                text_file.write(', ' + str(current_seed))\n",
    "                for i, value in enumerate(polynomial.values): \n",
    "                    text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "            text_file.close() \n",
    "\n",
    "            \n",
    "    if return_model:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T17:36:35.292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c8cb1a3426499aa6502d4847e4df4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 28.1min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 28.2min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 28.3min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 28.2min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 27.9min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.1min\n",
      "/home/smarton/anaconda3/envs/masterthesos/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 27.9min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 27.8min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 27.7min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  84 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-3)]: Done 244 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-3)]: Done 468 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-3)]: Done 756 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-3)]: Done 1000 out of 1000 | elapsed: 27.7min finished\n",
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2**32 - 1\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:19:36.087364Z",
     "start_time": "2020-12-10T08:19:28.677305Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][4] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][5] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[5] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV', 'FD FV', 'DTW FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:19:36.111649Z",
     "start_time": "2020-12-10T08:19:36.089814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.204</td>\n",
       "      <td>10.204</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.979</td>\n",
       "      <td>9.979</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.690</td>\n",
       "      <td>9.690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.227</td>\n",
       "      <td>5.223</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.169</td>\n",
       "      <td>5.167</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.999</td>\n",
       "      <td>12.999</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.777</td>\n",
       "      <td>12.777</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.490</td>\n",
       "      <td>12.490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.250</td>\n",
       "      <td>7.237</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.172</td>\n",
       "      <td>7.159</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.078</td>\n",
       "      <td>1.078</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.409</td>\n",
       "      <td>1.409</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.881</td>\n",
       "      <td>1.881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.280</td>\n",
       "      <td>4.265</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.390</td>\n",
       "      <td>4.359</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.419</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>-0.362</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.462</td>\n",
       "      <td>3.462</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.458</td>\n",
       "      <td>3.458</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.442</td>\n",
       "      <td>3.442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.637</td>\n",
       "      <td>2.633</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.619</td>\n",
       "      <td>2.612</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.907</td>\n",
       "      <td>24.907</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.595</td>\n",
       "      <td>24.595</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.171</td>\n",
       "      <td>24.171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.330</td>\n",
       "      <td>13.269</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.161</td>\n",
       "      <td>13.099</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.034</td>\n",
       "      <td>102.034</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.783</td>\n",
       "      <td>99.783</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>96.901</td>\n",
       "      <td>96.902</td>\n",
       "      <td>...</td>\n",
       "      <td>2.711</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.954</td>\n",
       "      <td>49.903</td>\n",
       "      <td>2.798</td>\n",
       "      <td>0.000</td>\n",
       "      <td>49.388</td>\n",
       "      <td>49.356</td>\n",
       "      <td>2.888</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.204         10.204               0.001           0.000   \n",
       "RMSE FV         12.999         12.999               0.001           0.000   \n",
       "MAPE FV          1.078          1.078               0.055           0.000   \n",
       "R2 FV           -0.419         -0.419               1.000           1.000   \n",
       "RAAE FV          0.924          0.924               0.013           0.000   \n",
       "RMAE FV          3.462          3.462               0.061           0.000   \n",
       "FD FV           24.907         24.907               0.001           0.000   \n",
       "DTW FV         102.034        102.034               0.005           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.979           9.979                0.001            0.000   \n",
       "RMSE FV          12.777          12.777                0.001            0.000   \n",
       "MAPE FV           1.409           1.409                0.013            0.000   \n",
       "R2 FV            -0.362          -0.362                0.999            1.000   \n",
       "RAAE FV           0.902           0.902                0.017            0.000   \n",
       "RMAE FV           3.458           3.458                0.081            0.000   \n",
       "FD FV            24.595          24.595                0.002            0.000   \n",
       "DTW FV           99.783          99.783                0.007            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            9.690           9.690  ...                 0.271   \n",
       "RMSE FV          12.490          12.490  ...                 0.362   \n",
       "MAPE FV           1.881           1.881  ...                 0.495   \n",
       "R2 FV            -0.290          -0.290  ...                 0.997   \n",
       "RAAE FV           0.874           0.874  ...                 0.038   \n",
       "RMAE FV           3.442           3.442  ...                 0.243   \n",
       "FD FV            24.171          24.171  ...                 0.704   \n",
       "DTW FV           96.901          96.902  ...                 2.711   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            5.227            5.223   \n",
       "RMSE FV             0.000            7.250            7.237   \n",
       "MAPE FV             0.000            4.280            4.265   \n",
       "R2 FV               1.000            0.539            0.540   \n",
       "RAAE FV             0.000            0.482            0.482   \n",
       "RMAE FV             0.000            2.637            2.633   \n",
       "FD FV               0.000           13.330           13.269   \n",
       "DTW FV              0.000           49.954           49.903   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.280             0.000            5.169   \n",
       "RMSE FV                 0.371             0.000            7.172   \n",
       "MAPE FV                 0.548             0.000            4.390   \n",
       "R2 FV                   0.997             1.000            0.548   \n",
       "RAAE FV                 0.039             0.000            0.477   \n",
       "RMAE FV                 0.241             0.000            2.619   \n",
       "FD FV                   0.719             0.000           13.161   \n",
       "DTW FV                  2.798             0.000           49.388   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             5.167                 0.289             0.000  \n",
       "RMSE FV            7.159                 0.381             0.000  \n",
       "MAPE FV            4.359                 0.536             0.000  \n",
       "R2 FV              0.549                 0.997             1.000  \n",
       "RAAE FV            0.477                 0.040             0.000  \n",
       "RMAE FV            2.612                 0.240             0.000  \n",
       "FD FV             13.099                 0.734             0.000  \n",
       "DTW FV            49.356                 2.888             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:19:36.133486Z",
     "start_time": "2020-12-10T08:19:36.113316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.210</td>\n",
       "      <td>10.210</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.986</td>\n",
       "      <td>9.986</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.699</td>\n",
       "      <td>9.699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.325</td>\n",
       "      <td>5.314</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.270</td>\n",
       "      <td>5.260</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.990</td>\n",
       "      <td>12.990</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.769</td>\n",
       "      <td>12.769</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.483</td>\n",
       "      <td>12.483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.331</td>\n",
       "      <td>7.313</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.257</td>\n",
       "      <td>7.237</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.055</td>\n",
       "      <td>1.055</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.263</td>\n",
       "      <td>1.263</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.571</td>\n",
       "      <td>1.571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.760</td>\n",
       "      <td>3.840</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.753</td>\n",
       "      <td>3.821</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.431</td>\n",
       "      <td>-0.431</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.928</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.036</td>\n",
       "      <td>3.036</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.032</td>\n",
       "      <td>3.032</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.015</td>\n",
       "      <td>3.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.260</td>\n",
       "      <td>2.255</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.236</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>25.028</td>\n",
       "      <td>25.028</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.716</td>\n",
       "      <td>24.716</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.294</td>\n",
       "      <td>24.294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.471</td>\n",
       "      <td>13.405</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.308</td>\n",
       "      <td>13.239</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.168</td>\n",
       "      <td>102.168</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>99.919</td>\n",
       "      <td>99.919</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.053</td>\n",
       "      <td>97.053</td>\n",
       "      <td>...</td>\n",
       "      <td>2.922</td>\n",
       "      <td>0.000</td>\n",
       "      <td>51.033</td>\n",
       "      <td>50.914</td>\n",
       "      <td>3.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.501</td>\n",
       "      <td>50.389</td>\n",
       "      <td>3.111</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.210         10.210               0.001           0.000   \n",
       "RMSE FV         12.990         12.990               0.001           0.000   \n",
       "MAPE FV          1.055          1.055               0.065           0.000   \n",
       "R2 FV           -0.431         -0.431               1.000           1.000   \n",
       "RAAE FV          0.928          0.928               0.014           0.000   \n",
       "RMAE FV          3.036          3.036               0.060           0.000   \n",
       "FD FV           25.028         25.028               0.002           0.000   \n",
       "DTW FV         102.168        102.168               0.006           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.986           9.986                0.001            0.000   \n",
       "RMSE FV          12.769          12.769                0.001            0.000   \n",
       "MAPE FV           1.263           1.263                0.011            0.000   \n",
       "R2 FV            -0.373          -0.373                0.999            1.000   \n",
       "RAAE FV           0.907           0.907                0.018            0.000   \n",
       "RMAE FV           3.032           3.032                0.076            0.000   \n",
       "FD FV            24.716          24.716                0.002            0.000   \n",
       "DTW FV           99.919          99.919                0.008            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            9.699           9.699  ...                 0.291   \n",
       "RMSE FV          12.483          12.483  ...                 0.395   \n",
       "MAPE FV           1.571           1.571  ...                 0.556   \n",
       "R2 FV            -0.300          -0.300  ...                 0.996   \n",
       "RAAE FV           0.879           0.879  ...                 0.041   \n",
       "RMAE FV           3.015           3.015  ...                 0.208   \n",
       "FD FV            24.294          24.294  ...                 0.776   \n",
       "DTW FV           97.053          97.053  ...                 2.922   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            5.325            5.314   \n",
       "RMSE FV             0.000            7.331            7.313   \n",
       "MAPE FV             0.000            3.760            3.840   \n",
       "R2 FV               1.000            0.526            0.527   \n",
       "RAAE FV             0.000            0.492            0.492   \n",
       "RMAE FV             0.000            2.260            2.255   \n",
       "FD FV               0.000           13.471           13.405   \n",
       "DTW FV              0.000           51.033           50.914   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.301             0.000            5.270   \n",
       "RMSE FV                 0.405             0.000            7.257   \n",
       "MAPE FV                 0.477             0.000            3.753   \n",
       "R2 FV                   0.996             1.000            0.534   \n",
       "RAAE FV                 0.042             0.000            0.488   \n",
       "RMAE FV                 0.208             0.000            2.244   \n",
       "FD FV                   0.793             0.000           13.308   \n",
       "DTW FV                  3.015             0.000           50.501   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             5.260                 0.310             0.000  \n",
       "RMSE FV            7.237                 0.416             0.000  \n",
       "MAPE FV            3.821                 0.588             0.000  \n",
       "R2 FV              0.536                 0.996             1.000  \n",
       "RAAE FV            0.487                 0.043             0.000  \n",
       "RMAE FV            2.236                 0.208             0.000  \n",
       "FD FV             13.239                 0.812             0.000  \n",
       "DTW FV            50.389                 3.111             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:19:36.159162Z",
     "start_time": "2020-12-10T08:19:36.135253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.222</td>\n",
       "      <td>10.222</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.999</td>\n",
       "      <td>9.999</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.712</td>\n",
       "      <td>9.712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.331</td>\n",
       "      <td>5.319</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.276</td>\n",
       "      <td>5.265</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>13.008</td>\n",
       "      <td>13.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.787</td>\n",
       "      <td>12.787</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.501</td>\n",
       "      <td>12.501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.339</td>\n",
       "      <td>7.321</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.264</td>\n",
       "      <td>7.245</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.427</td>\n",
       "      <td>-0.427</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.927</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.160</td>\n",
       "      <td>3.160</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.156</td>\n",
       "      <td>3.156</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.139</td>\n",
       "      <td>3.139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.370</td>\n",
       "      <td>2.365</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.354</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD FV</th>\n",
       "      <td>24.987</td>\n",
       "      <td>24.987</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.676</td>\n",
       "      <td>24.676</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.253</td>\n",
       "      <td>24.253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.379</td>\n",
       "      <td>13.320</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.214</td>\n",
       "      <td>13.152</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTW FV</th>\n",
       "      <td>102.285</td>\n",
       "      <td>102.285</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.051</td>\n",
       "      <td>100.051</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>97.184</td>\n",
       "      <td>97.184</td>\n",
       "      <td>...</td>\n",
       "      <td>2.938</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.868</td>\n",
       "      <td>50.744</td>\n",
       "      <td>3.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.333</td>\n",
       "      <td>50.216</td>\n",
       "      <td>3.128</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.222        10.222              0.001          0.000   \n",
       "RMSE FV        13.008        13.008              0.001          0.000   \n",
       "MAPE FV           inf           inf              0.093          0.000   \n",
       "R2 FV          -0.427        -0.427              1.000          1.000   \n",
       "RAAE FV         0.927         0.927              0.014          0.000   \n",
       "RMAE FV         3.160         3.160              0.063          0.000   \n",
       "FD FV          24.987        24.987              0.001          0.000   \n",
       "DTW FV        102.285       102.285              0.006          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.999          9.999               0.001           0.000   \n",
       "RMSE FV         12.787         12.787               0.001           0.000   \n",
       "MAPE FV            inf            inf               0.013           0.000   \n",
       "R2 FV           -0.369         -0.369               0.999           1.000   \n",
       "RAAE FV          0.906          0.906               0.018           0.000   \n",
       "RMAE FV          3.156          3.156               0.081           0.000   \n",
       "FD FV           24.676         24.676               0.002           0.000   \n",
       "DTW FV         100.051        100.051               0.008           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           9.712          9.712  ...                0.292   \n",
       "RMSE FV         12.501         12.501  ...                0.397   \n",
       "MAPE FV            inf            inf  ...                0.432   \n",
       "R2 FV           -0.297         -0.297  ...                0.996   \n",
       "RAAE FV          0.878          0.878  ...                0.041   \n",
       "RMAE FV          3.139          3.139  ...                0.226   \n",
       "FD FV           24.253         24.253  ...                0.784   \n",
       "DTW FV          97.184         97.184  ...                2.938   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           5.331           5.319                0.301   \n",
       "RMSE FV            0.000           7.339           7.321                0.407   \n",
       "MAPE FV            0.000             inf             inf                0.691   \n",
       "R2 FV              1.000           0.527           0.528                0.996   \n",
       "RAAE FV            0.000           0.492           0.491                0.042   \n",
       "RMAE FV            0.000           2.370           2.365                0.226   \n",
       "FD FV              0.000          13.379          13.320                0.802   \n",
       "DTW FV             0.000          50.868          50.744                3.032   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           5.276           5.265                0.310   \n",
       "RMSE FV            0.000           7.264           7.245                0.418   \n",
       "MAPE FV            0.000             inf             inf                1.020   \n",
       "R2 FV              1.000           0.535           0.537                0.996   \n",
       "RAAE FV            0.000           0.487           0.486                0.043   \n",
       "RMAE FV            0.000           2.354           2.345                0.226   \n",
       "FD FV              0.000          13.214          13.152                0.818   \n",
       "DTW FV             0.000          50.333          50.216                3.128   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "FD FV              0.000  \n",
       "DTW FV             0.000  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:19:36.187943Z",
     "start_time": "2020-12-10T08:19:36.160644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>...</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.871</td>\n",
       "      <td>1.419</td>\n",
       "      <td>2.059</td>\n",
       "      <td>2.751</td>\n",
       "      <td>3.439</td>\n",
       "      <td>...</td>\n",
       "      <td>4.717</td>\n",
       "      <td>5.278</td>\n",
       "      <td>5.763</td>\n",
       "      <td>6.162</td>\n",
       "      <td>6.480</td>\n",
       "      <td>6.729</td>\n",
       "      <td>6.925</td>\n",
       "      <td>7.082</td>\n",
       "      <td>7.210</td>\n",
       "      <td>7.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.871</td>\n",
       "      <td>1.418</td>\n",
       "      <td>2.057</td>\n",
       "      <td>2.749</td>\n",
       "      <td>3.436</td>\n",
       "      <td>...</td>\n",
       "      <td>4.711</td>\n",
       "      <td>5.271</td>\n",
       "      <td>5.754</td>\n",
       "      <td>6.153</td>\n",
       "      <td>6.471</td>\n",
       "      <td>6.719</td>\n",
       "      <td>6.915</td>\n",
       "      <td>7.071</td>\n",
       "      <td>7.199</td>\n",
       "      <td>7.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>...</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "      <td>11.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>...</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.865</td>\n",
       "      <td>1.409</td>\n",
       "      <td>2.045</td>\n",
       "      <td>2.732</td>\n",
       "      <td>3.416</td>\n",
       "      <td>...</td>\n",
       "      <td>4.686</td>\n",
       "      <td>5.245</td>\n",
       "      <td>5.727</td>\n",
       "      <td>6.125</td>\n",
       "      <td>6.442</td>\n",
       "      <td>6.690</td>\n",
       "      <td>6.885</td>\n",
       "      <td>7.041</td>\n",
       "      <td>7.168</td>\n",
       "      <td>7.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.865</td>\n",
       "      <td>1.409</td>\n",
       "      <td>2.043</td>\n",
       "      <td>2.730</td>\n",
       "      <td>3.412</td>\n",
       "      <td>...</td>\n",
       "      <td>4.680</td>\n",
       "      <td>5.238</td>\n",
       "      <td>5.719</td>\n",
       "      <td>6.116</td>\n",
       "      <td>6.433</td>\n",
       "      <td>6.680</td>\n",
       "      <td>6.875</td>\n",
       "      <td>7.031</td>\n",
       "      <td>7.158</td>\n",
       "      <td>7.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>...</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "      <td>11.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>...</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.866</td>\n",
       "      <td>1.411</td>\n",
       "      <td>2.047</td>\n",
       "      <td>2.736</td>\n",
       "      <td>3.421</td>\n",
       "      <td>...</td>\n",
       "      <td>4.693</td>\n",
       "      <td>5.253</td>\n",
       "      <td>5.736</td>\n",
       "      <td>6.135</td>\n",
       "      <td>6.452</td>\n",
       "      <td>6.701</td>\n",
       "      <td>6.897</td>\n",
       "      <td>7.053</td>\n",
       "      <td>7.181</td>\n",
       "      <td>7.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.866</td>\n",
       "      <td>1.410</td>\n",
       "      <td>2.046</td>\n",
       "      <td>2.734</td>\n",
       "      <td>3.417</td>\n",
       "      <td>...</td>\n",
       "      <td>4.687</td>\n",
       "      <td>5.246</td>\n",
       "      <td>5.728</td>\n",
       "      <td>6.126</td>\n",
       "      <td>6.443</td>\n",
       "      <td>6.692</td>\n",
       "      <td>6.887</td>\n",
       "      <td>7.043</td>\n",
       "      <td>7.170</td>\n",
       "      <td>7.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>...</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "      <td>11.146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.154 11.154 11.154 11.154 11.154 11.154   \n",
       "STD FV TRAIN PRED LAMBDA             0.042  0.052  0.088  0.193  0.447  0.871   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.042  0.052  0.088  0.193  0.447  0.871   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.154 11.154 11.154 11.154 11.154 11.154   \n",
       "STD FV VALID REAL LAMBDA            11.118 11.118 11.118 11.118 11.118 11.118   \n",
       "STD FV VALID PRED LAMBDA             0.042  0.051  0.088  0.191  0.443  0.865   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.042  0.051  0.088  0.191  0.443  0.865   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.118 11.118 11.118 11.118 11.118 11.118   \n",
       "STD FV TEST REAL LAMBDA             11.146 11.146 11.146 11.146 11.146 11.146   \n",
       "STD FV TEST PRED LAMBDA              0.042  0.052  0.088  0.192  0.444  0.866   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.042  0.052  0.088  0.192  0.444  0.866   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.146 11.146 11.146 11.146 11.146 11.146   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.154 11.154 11.154 11.154  ... 11.154   \n",
       "STD FV TRAIN PRED LAMBDA             1.419  2.059  2.751  3.439  ...  4.717   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  1.418  2.057  2.749  3.436  ...  4.711   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.154 11.154 11.154 11.154  ... 11.154   \n",
       "STD FV VALID REAL LAMBDA            11.118 11.118 11.118 11.118  ... 11.118   \n",
       "STD FV VALID PRED LAMBDA             1.409  2.045  2.732  3.416  ...  4.686   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  1.409  2.043  2.730  3.412  ...  4.680   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.118 11.118 11.118 11.118  ... 11.118   \n",
       "STD FV TEST REAL LAMBDA             11.146 11.146 11.146 11.146  ... 11.146   \n",
       "STD FV TEST PRED LAMBDA              1.411  2.047  2.736  3.421  ...  4.693   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   1.410  2.046  2.734  3.417  ...  4.687   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.146 11.146 11.146 11.146  ... 11.146   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.154 11.154 11.154 11.154 11.154 11.154   \n",
       "STD FV TRAIN PRED LAMBDA             5.278  5.763  6.162  6.480  6.729  6.925   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  5.271  5.754  6.153  6.471  6.719  6.915   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.154 11.154 11.154 11.154 11.154 11.154   \n",
       "STD FV VALID REAL LAMBDA            11.118 11.118 11.118 11.118 11.118 11.118   \n",
       "STD FV VALID PRED LAMBDA             5.245  5.727  6.125  6.442  6.690  6.885   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  5.238  5.719  6.116  6.433  6.680  6.875   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.118 11.118 11.118 11.118 11.118 11.118   \n",
       "STD FV TEST REAL LAMBDA             11.146 11.146 11.146 11.146 11.146 11.146   \n",
       "STD FV TEST PRED LAMBDA              5.253  5.736  6.135  6.452  6.701  6.897   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   5.246  5.728  6.126  6.443  6.692  6.887   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.146 11.146 11.146 11.146 11.146 11.146   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.154 11.154 11.154  \n",
       "STD FV TRAIN PRED LAMBDA             7.082  7.210  7.317  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.071  7.199  7.306  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.154 11.154 11.154  \n",
       "STD FV VALID REAL LAMBDA            11.118 11.118 11.118  \n",
       "STD FV VALID PRED LAMBDA             7.041  7.168  7.275  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.031  7.158  7.264  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.118 11.118 11.118  \n",
       "STD FV TEST REAL LAMBDA             11.146 11.146 11.146  \n",
       "STD FV TEST PRED LAMBDA              7.053  7.181  7.288  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.043  7.170  7.277  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.146 11.146 11.146  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:19:36.211765Z",
     "start_time": "2020-12-10T08:19:36.189301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.256</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.284</td>\n",
       "      <td>1.256</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.282</td>\n",
       "      <td>1.255</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.282</td>\n",
       "      <td>1.254</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.281</td>\n",
       "      <td>1.254</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.997</td>\n",
       "      <td>1.281</td>\n",
       "      <td>1.254</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.075  0.095  0.210  0.534  0.999   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.075  0.095  0.210  0.534  0.999   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV VALID REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV VALID PRED LAMBDA             0.075  0.095  0.210  0.533  0.997   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.075  0.095  0.210  0.533  0.997   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TEST PRED LAMBDA              0.075  0.095  0.210  0.533  0.997   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.075  0.095  0.210  0.533  0.997   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA             1.284  1.256  1.026  0.758  0.529  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  1.284  1.256  1.026  0.758  0.529  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV VALID PRED LAMBDA             1.282  1.255  1.026  0.759  0.531  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  1.282  1.254  1.026  0.759  0.532  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "MEAN FV TEST PRED LAMBDA              1.281  1.254  1.026  0.759  0.532  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   1.281  1.254  1.025  0.759  0.532  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.225  0.137  0.076  0.037  0.010   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.225  0.137  0.076  0.037  0.010   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079   \n",
       "MEAN FV VALID REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV VALID PRED LAMBDA             0.228  0.139  0.079  0.039  0.012   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.228  0.140  0.079  0.039  0.012   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "MEAN FV TEST PRED LAMBDA              0.230  0.143  0.083  0.043  0.016   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.230  0.143  0.083  0.044  0.017   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.079 -0.079 -0.079 -0.079 -0.079  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.009 -0.023 -0.035 -0.044 -0.052  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.009 -0.023 -0.035 -0.044 -0.052  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.079 -0.079 -0.079 -0.079 -0.079  \n",
       "MEAN FV VALID REAL LAMBDA            -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV VALID PRED LAMBDA            -0.008 -0.022 -0.034 -0.043 -0.051  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.008 -0.022 -0.034 -0.043 -0.051  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV TEST REAL LAMBDA             -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "MEAN FV TEST PRED LAMBDA             -0.004 -0.018 -0.030 -0.039 -0.047  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.003 -0.017 -0.029 -0.038 -0.047  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.078 -0.078 -0.078 -0.078 -0.078  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:19:36.314178Z",
     "start_time": "2020-12-10T08:19:36.213243Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_lambda_preds(i, \n",
    "                      lambda_indices,\n",
    "                      y_train_real_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_train_lambda_by_epoch, \n",
    "                      y_valid_real_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_valid_lambda_by_epoch, \n",
    "                      y_test_real_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_test_lambda_by_epoch):\n",
    "    \n",
    "    \n",
    "    index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "\n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n",
    "\n",
    "    return y_train_real_lambda_df, y_valid_real_lambda_df, y_test_real_lambda_df, y_train_pred_lambda_df, y_valid_pred_lambda_df, y_test_pred_lambda_df, y_train_pred_lambda_poly_lstsq_df, y_valid_pred_lambda_poly_lstsq_df, y_test_pred_lambda_poly_lstsq_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:27:42.099153Z",
     "start_time": "2020-12-10T08:19:36.316861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f24006bbc204c7e99f1c9fb388173fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of  21 | elapsed:  6.9min remaining: 66.0min\n",
      "[Parallel(n_jobs=-3)]: Done  10 out of  21 | elapsed:  7.4min remaining:  8.2min\n",
      "[Parallel(n_jobs=-3)]: Done  18 out of  21 | elapsed:  7.8min remaining:  1.3min\n",
      "[Parallel(n_jobs=-3)]: Done  21 out of  21 | elapsed:  7.9min finished\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    lambda_index_list = [clf[0][0] for clf in clf_list]\n",
    "    lambda_seed_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_real_list = [clf[0][2] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][3] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][4] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    lambda_indices_list = np.zeros((len(clf_list), 1))\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][3])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][4].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][7])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][8].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][11])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][12].shape)]][0])\n",
    "\n",
    "    for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        lambda_indices_list[index] = lambda_indices\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    lambda_indices_list = [np.zeros((len(clf_list), 1)) for i in epochs_save_range]\n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][3]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][4].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][7]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][8].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][12]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][12].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            lambda_indices_list[index][i] = lambda_indices\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    y_data_lambda_list = parallel(delayed(save_lambda_preds)(i, \n",
    "                                                           lambda_indices,\n",
    "                                                           y_train_real_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_train_lambda_by_epoch, \n",
    "                                                           y_valid_real_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_valid_lambda_by_epoch, \n",
    "                                                           y_test_real_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_test_lambda_by_epoch) for i, \n",
    "                                                                                        (lambda_indices,\n",
    "                                                                                         y_train_real_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_train_lambda_by_epoch, \n",
    "                                                                                         y_valid_real_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_valid_lambda_by_epoch, \n",
    "                                                                                         y_test_real_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_test_lambda_by_epoch) in enumerate(zip(lambda_indices_list,\n",
    "                                                                                                                                  y_train_real_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_train_lambda_list, \n",
    "                                                                                                                                  y_valid_real_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_valid_lambda_list, \n",
    "                                                                                                                                  y_test_real_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_test_lambda_list)))  \n",
    "    y_test_real_lambda_df = y_data_lambda_list[-1][2]\n",
    "    y_test_pred_lambda_df = y_data_lambda_list[-1][5]\n",
    "    y_test_pred_lambda_poly_lstsq_df = y_data_lambda_list[-1][8]\n",
    "    del parallel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:27:42.124786Z",
     "start_time": "2020-12-10T08:27:42.102742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-7.580</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>10.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-11.955</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-19.621</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-11.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-2.020</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>20.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>25.836</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-7.972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -7.580 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -11.955  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -19.621  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940  -2.020  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920  25.836 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0  10.155  \n",
       "1  -0.622  \n",
       "2 -11.502  \n",
       "3  20.981  \n",
       "4  -7.972  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:27:42.280989Z",
     "start_time": "2020-12-10T08:27:42.126250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-4.474</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>4.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-18.906</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-3.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.702</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-8.459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>8.264</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>2.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>5.376</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-5.237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -4.474 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -18.906  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.702  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   8.264  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   5.376 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   4.183  \n",
       "1  -3.629  \n",
       "2  -8.459  \n",
       "3   2.717  \n",
       "4  -5.237  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:27:42.300620Z",
     "start_time": "2020-12-10T08:27:42.282446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0000</th>\n",
       "      <th>0001</th>\n",
       "      <th>0002</th>\n",
       "      <th>0003</th>\n",
       "      <th>0010</th>\n",
       "      <th>0011</th>\n",
       "      <th>0012</th>\n",
       "      <th>0020</th>\n",
       "      <th>0021</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.300</td>\n",
       "      <td>-7.200</td>\n",
       "      <td>-9.400</td>\n",
       "      <td>8.900</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-3.800</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-4.197</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.110</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>4.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-9.900</td>\n",
       "      <td>9.400</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.300</td>\n",
       "      <td>-2.900</td>\n",
       "      <td>-6.100</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-18.917</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-3.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>-8.900</td>\n",
       "      <td>6.900</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>9.700</td>\n",
       "      <td>-2.600</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>-4.100</td>\n",
       "      <td>-7.500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.680</td>\n",
       "      <td>-24.961</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-8.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>4.200</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>7.500</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>9.600</td>\n",
       "      <td>9.800</td>\n",
       "      <td>-8.600</td>\n",
       "      <td>-4.200</td>\n",
       "      <td>-9.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.940</td>\n",
       "      <td>8.192</td>\n",
       "      <td>0.940</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.900</td>\n",
       "      <td>2.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-4.400</td>\n",
       "      <td>-6.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-7.700</td>\n",
       "      <td>9.300</td>\n",
       "      <td>-8.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.960</td>\n",
       "      <td>-0.920</td>\n",
       "      <td>5.042</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-5.318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index   0000   0001   0002   0003   0010   0011   0012   0020  \\\n",
       "0         0.000  6.300 -7.200 -9.400  8.900 -3.000 -3.800 -4.300 -6.500   \n",
       "1         1.000 -9.900  9.400 -6.000  7.800  0.800 -1.300 -2.900 -6.100   \n",
       "2         2.000 -8.900  6.900 -4.200  9.700 -2.600 -8.000 -4.100 -7.500   \n",
       "3         3.000  4.200 -4.400  7.500 -1.700  9.600  9.800 -8.600 -4.200   \n",
       "4         4.000  0.200 -0.800 -4.400 -6.500  3.000  2.600 -7.700  9.300   \n",
       "\n",
       "    0021  ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  \\\n",
       "0  8.800  ... -0.060  0.560  0.770  0.060  -4.197 -0.710  0.550  0.110 -0.450   \n",
       "1 -4.500  ...  0.450  0.510 -0.530 -0.220 -18.917  0.430 -0.500  0.260  0.430   \n",
       "2 -0.300  ... -0.660  0.140  0.150 -0.680 -24.961  0.110  0.200 -0.850 -0.150   \n",
       "3 -9.200  ... -0.940 -0.850  0.080 -0.940   8.192  0.940 -0.850  0.280  0.900   \n",
       "4 -8.800  ...  0.570  0.600  0.960 -0.920   5.042 -0.600  0.350  0.940  0.810   \n",
       "\n",
       "   FV_250  \n",
       "0   4.358  \n",
       "1  -3.759  \n",
       "2  -8.373  \n",
       "3   2.462  \n",
       "4  -5.318  \n",
       "\n",
       "[5 rows x 1286 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:27:45.020120Z",
     "start_time": "2020-12-10T08:27:42.302201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16ecf004ff04cb8aebbaa4f1ccb7528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:27:59.511006Z",
     "start_time": "2020-12-10T08:27:45.022012Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:28:00.132539Z",
     "start_time": "2020-12-10T08:27:59.513540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.219</td>\n",
       "      <td>10.193</td>\n",
       "      <td>10.168</td>\n",
       "      <td>10.143</td>\n",
       "      <td>10.119</td>\n",
       "      <td>10.094</td>\n",
       "      <td>10.069</td>\n",
       "      <td>10.044</td>\n",
       "      <td>10.019</td>\n",
       "      <td>9.993</td>\n",
       "      <td>...</td>\n",
       "      <td>5.227</td>\n",
       "      <td>5.221</td>\n",
       "      <td>5.215</td>\n",
       "      <td>5.209</td>\n",
       "      <td>5.204</td>\n",
       "      <td>5.198</td>\n",
       "      <td>5.192</td>\n",
       "      <td>5.186</td>\n",
       "      <td>5.181</td>\n",
       "      <td>5.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.339</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.311</td>\n",
       "      <td>2.298</td>\n",
       "      <td>2.284</td>\n",
       "      <td>2.271</td>\n",
       "      <td>2.258</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.231</td>\n",
       "      <td>2.217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.870</td>\n",
       "      <td>4.869</td>\n",
       "      <td>4.868</td>\n",
       "      <td>4.867</td>\n",
       "      <td>4.867</td>\n",
       "      <td>4.866</td>\n",
       "      <td>4.865</td>\n",
       "      <td>4.864</td>\n",
       "      <td>4.864</td>\n",
       "      <td>4.863</td>\n",
       "      <td>...</td>\n",
       "      <td>2.862</td>\n",
       "      <td>2.851</td>\n",
       "      <td>2.835</td>\n",
       "      <td>2.825</td>\n",
       "      <td>2.811</td>\n",
       "      <td>2.798</td>\n",
       "      <td>2.787</td>\n",
       "      <td>2.775</td>\n",
       "      <td>2.762</td>\n",
       "      <td>2.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.501</td>\n",
       "      <td>8.488</td>\n",
       "      <td>8.473</td>\n",
       "      <td>8.460</td>\n",
       "      <td>8.447</td>\n",
       "      <td>8.434</td>\n",
       "      <td>8.417</td>\n",
       "      <td>8.403</td>\n",
       "      <td>8.388</td>\n",
       "      <td>8.370</td>\n",
       "      <td>...</td>\n",
       "      <td>4.795</td>\n",
       "      <td>4.788</td>\n",
       "      <td>4.782</td>\n",
       "      <td>4.774</td>\n",
       "      <td>4.768</td>\n",
       "      <td>4.763</td>\n",
       "      <td>4.757</td>\n",
       "      <td>4.751</td>\n",
       "      <td>4.746</td>\n",
       "      <td>4.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.946</td>\n",
       "      <td>9.926</td>\n",
       "      <td>9.903</td>\n",
       "      <td>9.883</td>\n",
       "      <td>9.867</td>\n",
       "      <td>9.844</td>\n",
       "      <td>9.821</td>\n",
       "      <td>9.806</td>\n",
       "      <td>9.788</td>\n",
       "      <td>9.771</td>\n",
       "      <td>...</td>\n",
       "      <td>5.220</td>\n",
       "      <td>5.214</td>\n",
       "      <td>5.209</td>\n",
       "      <td>5.202</td>\n",
       "      <td>5.196</td>\n",
       "      <td>5.191</td>\n",
       "      <td>5.185</td>\n",
       "      <td>5.178</td>\n",
       "      <td>5.172</td>\n",
       "      <td>5.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.660</td>\n",
       "      <td>11.629</td>\n",
       "      <td>11.599</td>\n",
       "      <td>11.572</td>\n",
       "      <td>11.543</td>\n",
       "      <td>11.510</td>\n",
       "      <td>11.474</td>\n",
       "      <td>11.440</td>\n",
       "      <td>11.406</td>\n",
       "      <td>11.376</td>\n",
       "      <td>...</td>\n",
       "      <td>5.647</td>\n",
       "      <td>5.641</td>\n",
       "      <td>5.636</td>\n",
       "      <td>5.628</td>\n",
       "      <td>5.623</td>\n",
       "      <td>5.616</td>\n",
       "      <td>5.611</td>\n",
       "      <td>5.605</td>\n",
       "      <td>5.600</td>\n",
       "      <td>5.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.793</td>\n",
       "      <td>20.699</td>\n",
       "      <td>20.606</td>\n",
       "      <td>20.515</td>\n",
       "      <td>20.424</td>\n",
       "      <td>20.334</td>\n",
       "      <td>20.245</td>\n",
       "      <td>20.156</td>\n",
       "      <td>20.068</td>\n",
       "      <td>19.980</td>\n",
       "      <td>...</td>\n",
       "      <td>8.234</td>\n",
       "      <td>8.234</td>\n",
       "      <td>8.228</td>\n",
       "      <td>8.225</td>\n",
       "      <td>8.221</td>\n",
       "      <td>8.217</td>\n",
       "      <td>8.215</td>\n",
       "      <td>8.211</td>\n",
       "      <td>8.207</td>\n",
       "      <td>8.203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000     10000.000   \n",
       "mean         10.219        10.193        10.168        10.143        10.119   \n",
       "std           2.339         2.325         2.311         2.298         2.284   \n",
       "min           4.870         4.869         4.868         4.867         4.867   \n",
       "25%           8.501         8.488         8.473         8.460         8.447   \n",
       "50%           9.946         9.926         9.903         9.883         9.867   \n",
       "75%          11.660        11.629        11.599        11.572        11.543   \n",
       "max          20.793        20.699        20.606        20.515        20.424   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count     10000.000     10000.000     10000.000     10000.000      10000.000   \n",
       "mean         10.094        10.069        10.044        10.019          9.993   \n",
       "std           2.271         2.258         2.244         2.231          2.217   \n",
       "min           4.866         4.865         4.864         4.864          4.863   \n",
       "25%           8.434         8.417         8.403         8.388          8.370   \n",
       "50%           9.844         9.821         9.806         9.788          9.771   \n",
       "75%          11.510        11.474        11.440        11.406         11.376   \n",
       "max          20.334        20.245        20.156        20.068         19.980   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...       10000.000       10000.000       10000.000       10000.000   \n",
       "mean   ...           5.227           5.221           5.215           5.209   \n",
       "std    ...           0.646           0.646           0.645           0.645   \n",
       "min    ...           2.862           2.851           2.835           2.825   \n",
       "25%    ...           4.795           4.788           4.782           4.774   \n",
       "50%    ...           5.220           5.214           5.209           5.202   \n",
       "75%    ...           5.647           5.641           5.636           5.628   \n",
       "max    ...           8.234           8.234           8.228           8.225   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            5.204           5.198           5.192           5.186   \n",
       "std             0.645           0.644           0.644           0.643   \n",
       "min             2.811           2.798           2.787           2.775   \n",
       "25%             4.768           4.763           4.757           4.751   \n",
       "50%             5.196           5.191           5.185           5.178   \n",
       "75%             5.623           5.616           5.611           5.605   \n",
       "max             8.221           8.217           8.215           8.211   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count       10000.000       10000.000  \n",
       "mean            5.181           5.175  \n",
       "std             0.643           0.643  \n",
       "min             2.762           2.752  \n",
       "25%             4.746           4.740  \n",
       "50%             5.172           5.166  \n",
       "75%             5.600           5.592  \n",
       "max             8.207           8.203  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:28:00.736206Z",
     "start_time": "2020-12-10T08:28:00.134137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.210</td>\n",
       "      <td>10.185</td>\n",
       "      <td>10.160</td>\n",
       "      <td>10.135</td>\n",
       "      <td>10.111</td>\n",
       "      <td>10.086</td>\n",
       "      <td>10.061</td>\n",
       "      <td>10.036</td>\n",
       "      <td>10.011</td>\n",
       "      <td>9.986</td>\n",
       "      <td>...</td>\n",
       "      <td>5.320</td>\n",
       "      <td>5.314</td>\n",
       "      <td>5.309</td>\n",
       "      <td>5.303</td>\n",
       "      <td>5.298</td>\n",
       "      <td>5.292</td>\n",
       "      <td>5.287</td>\n",
       "      <td>5.281</td>\n",
       "      <td>5.276</td>\n",
       "      <td>5.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.396</td>\n",
       "      <td>2.382</td>\n",
       "      <td>2.369</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.342</td>\n",
       "      <td>2.329</td>\n",
       "      <td>2.315</td>\n",
       "      <td>2.302</td>\n",
       "      <td>2.288</td>\n",
       "      <td>2.275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.351</td>\n",
       "      <td>4.350</td>\n",
       "      <td>4.350</td>\n",
       "      <td>4.349</td>\n",
       "      <td>4.349</td>\n",
       "      <td>4.348</td>\n",
       "      <td>4.347</td>\n",
       "      <td>4.347</td>\n",
       "      <td>4.346</td>\n",
       "      <td>4.346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.923</td>\n",
       "      <td>2.921</td>\n",
       "      <td>2.920</td>\n",
       "      <td>2.917</td>\n",
       "      <td>2.915</td>\n",
       "      <td>2.913</td>\n",
       "      <td>2.910</td>\n",
       "      <td>2.908</td>\n",
       "      <td>2.906</td>\n",
       "      <td>2.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.473</td>\n",
       "      <td>8.457</td>\n",
       "      <td>8.441</td>\n",
       "      <td>8.426</td>\n",
       "      <td>8.409</td>\n",
       "      <td>8.391</td>\n",
       "      <td>8.372</td>\n",
       "      <td>8.359</td>\n",
       "      <td>8.342</td>\n",
       "      <td>8.330</td>\n",
       "      <td>...</td>\n",
       "      <td>4.816</td>\n",
       "      <td>4.811</td>\n",
       "      <td>4.806</td>\n",
       "      <td>4.802</td>\n",
       "      <td>4.796</td>\n",
       "      <td>4.791</td>\n",
       "      <td>4.787</td>\n",
       "      <td>4.781</td>\n",
       "      <td>4.774</td>\n",
       "      <td>4.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.936</td>\n",
       "      <td>9.917</td>\n",
       "      <td>9.897</td>\n",
       "      <td>9.878</td>\n",
       "      <td>9.860</td>\n",
       "      <td>9.838</td>\n",
       "      <td>9.822</td>\n",
       "      <td>9.798</td>\n",
       "      <td>9.777</td>\n",
       "      <td>9.756</td>\n",
       "      <td>...</td>\n",
       "      <td>5.298</td>\n",
       "      <td>5.293</td>\n",
       "      <td>5.288</td>\n",
       "      <td>5.283</td>\n",
       "      <td>5.278</td>\n",
       "      <td>5.272</td>\n",
       "      <td>5.266</td>\n",
       "      <td>5.261</td>\n",
       "      <td>5.255</td>\n",
       "      <td>5.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.647</td>\n",
       "      <td>11.619</td>\n",
       "      <td>11.592</td>\n",
       "      <td>11.561</td>\n",
       "      <td>11.529</td>\n",
       "      <td>11.498</td>\n",
       "      <td>11.469</td>\n",
       "      <td>11.439</td>\n",
       "      <td>11.405</td>\n",
       "      <td>11.373</td>\n",
       "      <td>...</td>\n",
       "      <td>5.803</td>\n",
       "      <td>5.798</td>\n",
       "      <td>5.793</td>\n",
       "      <td>5.789</td>\n",
       "      <td>5.783</td>\n",
       "      <td>5.778</td>\n",
       "      <td>5.771</td>\n",
       "      <td>5.765</td>\n",
       "      <td>5.759</td>\n",
       "      <td>5.755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.961</td>\n",
       "      <td>22.879</td>\n",
       "      <td>22.797</td>\n",
       "      <td>22.712</td>\n",
       "      <td>22.625</td>\n",
       "      <td>22.537</td>\n",
       "      <td>22.447</td>\n",
       "      <td>22.353</td>\n",
       "      <td>22.255</td>\n",
       "      <td>22.151</td>\n",
       "      <td>...</td>\n",
       "      <td>8.955</td>\n",
       "      <td>8.952</td>\n",
       "      <td>8.949</td>\n",
       "      <td>8.946</td>\n",
       "      <td>8.943</td>\n",
       "      <td>8.940</td>\n",
       "      <td>8.936</td>\n",
       "      <td>8.932</td>\n",
       "      <td>8.929</td>\n",
       "      <td>8.926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.210            10.185            10.160            10.135   \n",
       "std               2.396             2.382             2.369             2.355   \n",
       "min               4.351             4.350             4.350             4.349   \n",
       "25%               8.473             8.457             8.441             8.426   \n",
       "50%               9.936             9.917             9.897             9.878   \n",
       "75%              11.647            11.619            11.592            11.561   \n",
       "max              22.961            22.879            22.797            22.712   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean             10.111            10.086            10.061            10.036   \n",
       "std               2.342             2.329             2.315             2.302   \n",
       "min               4.349             4.348             4.347             4.347   \n",
       "25%               8.409             8.391             8.372             8.359   \n",
       "50%               9.860             9.838             9.822             9.798   \n",
       "75%              11.529            11.498            11.469            11.439   \n",
       "max              22.625            22.537            22.447            22.353   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count         10000.000          10000.000  ...           10000.000   \n",
       "mean             10.011              9.986  ...               5.320   \n",
       "std               2.288              2.275  ...               0.731   \n",
       "min               4.346              4.346  ...               2.923   \n",
       "25%               8.342              8.330  ...               4.816   \n",
       "50%               9.777              9.756  ...               5.298   \n",
       "75%              11.405             11.373  ...               5.803   \n",
       "max              22.255             22.151  ...               8.955   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                5.314               5.309               5.303   \n",
       "std                 0.731               0.730               0.730   \n",
       "min                 2.921               2.920               2.917   \n",
       "25%                 4.811               4.806               4.802   \n",
       "50%                 5.293               5.288               5.283   \n",
       "75%                 5.798               5.793               5.789   \n",
       "max                 8.952               8.949               8.946   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                5.298               5.292               5.287   \n",
       "std                 0.729               0.729               0.728   \n",
       "min                 2.915               2.913               2.910   \n",
       "25%                 4.796               4.791               4.787   \n",
       "50%                 5.278               5.272               5.266   \n",
       "75%                 5.783               5.778               5.771   \n",
       "max                 8.943               8.940               8.936   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count           10000.000           10000.000           10000.000  \n",
       "mean                5.281               5.276               5.270  \n",
       "std                 0.728               0.727               0.727  \n",
       "min                 2.908               2.906               2.904  \n",
       "25%                 4.781               4.774               4.769  \n",
       "50%                 5.261               5.255               5.249  \n",
       "75%                 5.765               5.759               5.755  \n",
       "max                 8.932               8.929               8.926  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:28:01.340416Z",
     "start_time": "2020-12-10T08:28:00.737898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.081</td>\n",
       "      <td>1.081</td>\n",
       "      <td>1.112</td>\n",
       "      <td>1.136</td>\n",
       "      <td>1.179</td>\n",
       "      <td>1.213</td>\n",
       "      <td>1.263</td>\n",
       "      <td>1.300</td>\n",
       "      <td>1.341</td>\n",
       "      <td>1.401</td>\n",
       "      <td>...</td>\n",
       "      <td>4.296</td>\n",
       "      <td>4.269</td>\n",
       "      <td>4.282</td>\n",
       "      <td>4.301</td>\n",
       "      <td>4.327</td>\n",
       "      <td>4.333</td>\n",
       "      <td>4.399</td>\n",
       "      <td>4.322</td>\n",
       "      <td>4.354</td>\n",
       "      <td>4.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.206</td>\n",
       "      <td>2.579</td>\n",
       "      <td>3.696</td>\n",
       "      <td>4.417</td>\n",
       "      <td>5.711</td>\n",
       "      <td>6.991</td>\n",
       "      <td>8.909</td>\n",
       "      <td>10.181</td>\n",
       "      <td>11.109</td>\n",
       "      <td>13.612</td>\n",
       "      <td>...</td>\n",
       "      <td>79.272</td>\n",
       "      <td>77.754</td>\n",
       "      <td>77.259</td>\n",
       "      <td>78.643</td>\n",
       "      <td>79.988</td>\n",
       "      <td>79.568</td>\n",
       "      <td>80.891</td>\n",
       "      <td>78.992</td>\n",
       "      <td>81.401</td>\n",
       "      <td>100.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.973</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.002</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.001</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.004</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.385</td>\n",
       "      <td>1.382</td>\n",
       "      <td>1.381</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.377</td>\n",
       "      <td>1.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.009</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.019</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.029</td>\n",
       "      <td>1.035</td>\n",
       "      <td>1.041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.848</td>\n",
       "      <td>1.850</td>\n",
       "      <td>1.848</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.845</td>\n",
       "      <td>1.846</td>\n",
       "      <td>1.848</td>\n",
       "      <td>1.842</td>\n",
       "      <td>1.838</td>\n",
       "      <td>1.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.027</td>\n",
       "      <td>1.027</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.077</td>\n",
       "      <td>1.089</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.117</td>\n",
       "      <td>...</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.633</td>\n",
       "      <td>2.643</td>\n",
       "      <td>2.636</td>\n",
       "      <td>2.633</td>\n",
       "      <td>2.634</td>\n",
       "      <td>2.641</td>\n",
       "      <td>2.638</td>\n",
       "      <td>2.630</td>\n",
       "      <td>2.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>185.283</td>\n",
       "      <td>249.112</td>\n",
       "      <td>349.883</td>\n",
       "      <td>418.940</td>\n",
       "      <td>509.291</td>\n",
       "      <td>635.860</td>\n",
       "      <td>804.889</td>\n",
       "      <td>914.432</td>\n",
       "      <td>954.953</td>\n",
       "      <td>1081.856</td>\n",
       "      <td>...</td>\n",
       "      <td>7587.768</td>\n",
       "      <td>7429.158</td>\n",
       "      <td>7358.479</td>\n",
       "      <td>7493.417</td>\n",
       "      <td>7608.217</td>\n",
       "      <td>7549.419</td>\n",
       "      <td>7529.932</td>\n",
       "      <td>7453.739</td>\n",
       "      <td>7682.202</td>\n",
       "      <td>9681.891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            1.081           1.081           1.112           1.136   \n",
       "std             2.206           2.579           3.696           4.417   \n",
       "min             0.973           0.964           0.954           0.945   \n",
       "25%             1.002           1.000           0.999           0.999   \n",
       "50%             1.009           1.007           1.008           1.010   \n",
       "75%             1.027           1.027           1.032           1.040   \n",
       "max           185.283         249.112         349.883         418.940   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count       10000.000       10000.000       10000.000       10000.000   \n",
       "mean            1.179           1.213           1.263           1.300   \n",
       "std             5.711           6.991           8.909          10.181   \n",
       "min             0.938           0.928           0.918           0.907   \n",
       "25%             1.000           1.001           1.003           1.004   \n",
       "50%             1.014           1.019           1.024           1.029   \n",
       "75%             1.051           1.064           1.077           1.089   \n",
       "max           509.291         635.860         804.889         914.432   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count       10000.000        10000.000  ...         10000.000   \n",
       "mean            1.341            1.401  ...             4.296   \n",
       "std            11.109           13.612  ...            79.272   \n",
       "min             0.896            0.880  ...             0.264   \n",
       "25%             1.006            1.008  ...             1.383   \n",
       "50%             1.035            1.041  ...             1.848   \n",
       "75%             1.103            1.117  ...             2.644   \n",
       "max           954.953         1081.856  ...          7587.768   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              4.269             4.282             4.301             4.327   \n",
       "std              77.754            77.259            78.643            79.988   \n",
       "min               0.266             0.266             0.262             0.264   \n",
       "25%               1.385             1.382             1.381             1.380   \n",
       "50%               1.850             1.848             1.845             1.845   \n",
       "75%               2.633             2.643             2.636             2.633   \n",
       "max            7429.158          7358.479          7493.417          7608.217   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count         10000.000         10000.000         10000.000         10000.000   \n",
       "mean              4.333             4.399             4.322             4.354   \n",
       "std              79.568            80.891            78.992            81.401   \n",
       "min               0.262             0.263             0.262             0.262   \n",
       "25%               1.380             1.378             1.378             1.377   \n",
       "50%               1.846             1.848             1.842             1.838   \n",
       "75%               2.634             2.641             2.638             2.630   \n",
       "max            7549.419          7529.932          7453.739          7682.202   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count         10000.000  \n",
       "mean              4.558  \n",
       "std             100.513  \n",
       "min               0.261  \n",
       "25%               1.380  \n",
       "50%               1.841  \n",
       "75%               2.625  \n",
       "max            9681.891  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:28:01.943023Z",
     "start_time": "2020-12-10T08:28:01.342015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.054</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.084</td>\n",
       "      <td>1.105</td>\n",
       "      <td>1.129</td>\n",
       "      <td>1.154</td>\n",
       "      <td>1.179</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.234</td>\n",
       "      <td>1.261</td>\n",
       "      <td>...</td>\n",
       "      <td>3.756</td>\n",
       "      <td>3.756</td>\n",
       "      <td>3.757</td>\n",
       "      <td>3.754</td>\n",
       "      <td>3.752</td>\n",
       "      <td>3.751</td>\n",
       "      <td>3.751</td>\n",
       "      <td>3.749</td>\n",
       "      <td>3.748</td>\n",
       "      <td>3.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.149</td>\n",
       "      <td>1.552</td>\n",
       "      <td>1.976</td>\n",
       "      <td>2.415</td>\n",
       "      <td>2.860</td>\n",
       "      <td>3.312</td>\n",
       "      <td>3.775</td>\n",
       "      <td>4.248</td>\n",
       "      <td>4.732</td>\n",
       "      <td>5.223</td>\n",
       "      <td>...</td>\n",
       "      <td>31.403</td>\n",
       "      <td>31.459</td>\n",
       "      <td>31.491</td>\n",
       "      <td>31.408</td>\n",
       "      <td>31.319</td>\n",
       "      <td>31.287</td>\n",
       "      <td>31.265</td>\n",
       "      <td>31.123</td>\n",
       "      <td>31.091</td>\n",
       "      <td>31.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.961</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.997</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.190</td>\n",
       "      <td>1.188</td>\n",
       "      <td>1.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.003</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.002</td>\n",
       "      <td>1.003</td>\n",
       "      <td>1.005</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.014</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.021</td>\n",
       "      <td>...</td>\n",
       "      <td>1.633</td>\n",
       "      <td>1.632</td>\n",
       "      <td>1.631</td>\n",
       "      <td>1.629</td>\n",
       "      <td>1.629</td>\n",
       "      <td>1.629</td>\n",
       "      <td>1.628</td>\n",
       "      <td>1.628</td>\n",
       "      <td>1.628</td>\n",
       "      <td>1.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.018</td>\n",
       "      <td>1.019</td>\n",
       "      <td>1.024</td>\n",
       "      <td>1.030</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.059</td>\n",
       "      <td>1.070</td>\n",
       "      <td>1.081</td>\n",
       "      <td>1.093</td>\n",
       "      <td>...</td>\n",
       "      <td>2.422</td>\n",
       "      <td>2.422</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.421</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.420</td>\n",
       "      <td>2.416</td>\n",
       "      <td>2.416</td>\n",
       "      <td>2.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.554</td>\n",
       "      <td>139.244</td>\n",
       "      <td>179.263</td>\n",
       "      <td>220.032</td>\n",
       "      <td>261.080</td>\n",
       "      <td>302.608</td>\n",
       "      <td>345.095</td>\n",
       "      <td>388.587</td>\n",
       "      <td>433.195</td>\n",
       "      <td>478.435</td>\n",
       "      <td>...</td>\n",
       "      <td>1916.444</td>\n",
       "      <td>1929.336</td>\n",
       "      <td>1934.528</td>\n",
       "      <td>1923.660</td>\n",
       "      <td>1911.381</td>\n",
       "      <td>1914.091</td>\n",
       "      <td>1904.443</td>\n",
       "      <td>1884.449</td>\n",
       "      <td>1881.267</td>\n",
       "      <td>1872.414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.054               1.065               1.084   \n",
       "std                 1.149               1.552               1.976   \n",
       "min                 0.961               0.944               0.926   \n",
       "25%                 0.997               0.995               0.994   \n",
       "50%                 1.003               1.002               1.002   \n",
       "75%                 1.018               1.019               1.024   \n",
       "max                99.554             139.244             179.263   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.105               1.129               1.154   \n",
       "std                 2.415               2.860               3.312   \n",
       "min                 0.909               0.895               0.882   \n",
       "25%                 0.993               0.993               0.993   \n",
       "50%                 1.003               1.005               1.007   \n",
       "75%                 1.030               1.039               1.049   \n",
       "max               220.032             261.080             302.608   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count           10000.000           10000.000           10000.000   \n",
       "mean                1.179               1.206               1.234   \n",
       "std                 3.775               4.248               4.732   \n",
       "min                 0.872               0.863               0.855   \n",
       "25%                 0.994               0.994               0.995   \n",
       "50%                 1.010               1.014               1.017   \n",
       "75%                 1.059               1.070               1.081   \n",
       "max               345.095             388.587             433.195   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count            10000.000  ...             10000.000             10000.000   \n",
       "mean                 1.261  ...                 3.756                 3.756   \n",
       "std                  5.223  ...                31.403                31.459   \n",
       "min                  0.849  ...                 0.259                 0.258   \n",
       "25%                  0.996  ...                 1.192                 1.192   \n",
       "50%                  1.021  ...                 1.633                 1.632   \n",
       "75%                  1.093  ...                 2.422                 2.422   \n",
       "max                478.435  ...              1916.444              1929.336   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.757                 3.754                 3.752   \n",
       "std                  31.491                31.408                31.319   \n",
       "min                   0.257                 0.257                 0.257   \n",
       "25%                   1.191                 1.191                 1.191   \n",
       "50%                   1.631                 1.629                 1.629   \n",
       "75%                   2.424                 2.420                 2.421   \n",
       "max                1934.528              1923.660              1911.381   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count             10000.000             10000.000             10000.000   \n",
       "mean                  3.751                 3.751                 3.749   \n",
       "std                  31.287                31.265                31.123   \n",
       "min                   0.256                 0.256                 0.256   \n",
       "25%                   1.190                 1.191                 1.190   \n",
       "50%                   1.629                 1.628                 1.628   \n",
       "75%                   2.420                 2.420                 2.416   \n",
       "max                1914.091              1904.443              1884.449   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count             10000.000             10000.000  \n",
       "mean                  3.748                 3.747  \n",
       "std                  31.091                31.027  \n",
       "min                   0.255                 0.254  \n",
       "25%                   1.188                 1.188  \n",
       "50%                   1.628                 1.627  \n",
       "75%                   2.416                 2.414  \n",
       "max                1881.267              1872.414  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:28:03.486213Z",
     "start_time": "2020-12-10T08:28:01.944742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:28:05.166588Z",
     "start_time": "2020-12-10T08:28:03.488488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['trainover', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
