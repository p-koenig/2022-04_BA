{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.723554Z",
     "start_time": "2020-12-19T14:17:01.720277Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return f(n) // f(r) // f(n-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.731386Z",
     "start_time": "2020-12-19T14:17:01.725230Z"
    }
   },
   "outputs": [],
   "source": [
    "d = 3\n",
    "n = 4\n",
    "sparsity = nCr(n+d, d)\n",
    "\n",
    "\n",
    "x_max = 1#10 #this number excluded\n",
    "x_min = -1#-10\n",
    "x_step = 0.01#0.1\n",
    "a_max = 10 #this number excluded\n",
    "a_min = -10\n",
    "a_step = 0.1\n",
    "\n",
    "lambda_dataset_size = 1000 #specify the number of data points to train the lambda net on for loading (must be same as in previous notebook)\n",
    "\n",
    "number_formulas_load = 1000 #manually specify number of formulas for loading file (must be the same as used in the previous notebook)\n",
    "number_of_lambda_nets = number_formulas_load #use this parameter to select how many lambda nets should be generated (must be smaller or equal to number_formulas_load)\n",
    "\n",
    "\n",
    "#lambda net specifications \n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "lambda_network_layers = [5*sparsity]\n",
    "dropout = 0.0\n",
    "optimizer='SGD'\n",
    "\n",
    "each_epochs_save = 10  #specifies the number of epochs to checkpoint lambda network weights, if None no checkpointing\n",
    "\n",
    "same_training_all_lambda_nets = False\n",
    "\n",
    "fixed_seed_lambda_training = False\n",
    "fixed_initialization_lambda_training = True\n",
    "number_different_lambda_trainings = 100\n",
    "\n",
    "n_jobs = -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.747362Z",
     "start_time": "2020-12-19T14:17:01.732955Z"
    }
   },
   "outputs": [],
   "source": [
    "##############DO NOT CHANGE###################\n",
    "variables = 'abcdefghijklmnopqrstuvwxyz'[:n]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if each_epochs_save != None:\n",
    "    epochs_save_range = range(1, epochs//each_epochs_save+1) if each_epochs_save == 1 else range(epochs//each_epochs_save+1)\n",
    "else:\n",
    "    epochs_save_range = None\n",
    "    \n",
    "layers_str = ''.join([str(neurons) + '-' for neurons in lambda_network_layers])\n",
    "\n",
    "advanced_metric_dataset_size = 10#200\n",
    "\n",
    "if same_training_all_lambda_nets:\n",
    "    training_string = '_same'\n",
    "else:\n",
    "    training_string = '_diverse'\n",
    "\n",
    "generate_subset = True if number_of_lambda_nets < number_formulas_load else False\n",
    "\n",
    "    \n",
    "if generate_subset:\n",
    "    data_size = number_of_lambda_nets\n",
    "else:\n",
    "    data_size = number_formulas_load\n",
    "\n",
    "if fixed_seed_lambda_training:\n",
    "    seed_shuffle_string = '_' + str(number_different_lambda_trainings) + '-FixedSeed'\n",
    "else:\n",
    "    seed_shuffle_string = '_NoFixedSeed'\n",
    "    \n",
    "if fixed_initialization_lambda_training:\n",
    "    seed_shuffle_string += '_' + str(number_different_lambda_trainings) + '-FixedEvaluation'\n",
    "else:\n",
    "    seed_shuffle_string += '_NoFixedEvaluation'\n",
    "    \n",
    "    \n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_numbers = '0'\n",
    "else:\n",
    "    gpu_numbers = ''\n",
    "    \n",
    "structure = '_' + layers_str + str(epochs) + 'e' + str(batch_size) + 'b_' + optimizer \n",
    "    \n",
    "filename = seed_shuffle_string + '_' + str(RANDOM_SEED) + structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.771146Z",
     "start_time": "2020-12-19T14:17:01.748957Z"
    }
   },
   "outputs": [],
   "source": [
    "import ttg\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import random \n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "#keras.backend.set_epsilon(10e-3)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.777186Z",
     "start_time": "2020-12-19T14:17:01.772922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Num XLA-GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.800219Z",
     "start_time": "2020-12-19T14:17:01.779222Z"
    }
   },
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "def chunks(lst, chunksize):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunksize):\n",
    "        yield lst[i:i + chunksize]\n",
    "\n",
    "#test for exact equality\n",
    "def arreq_in_list(myarr, list_arrays):\n",
    "    return next((True for elem in list_arrays if np.array_equal(elem, myarr)), False)\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "\n",
    "ALPHABET = \\\n",
    "  \"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def encode (n):\n",
    "    try:\n",
    "        return ALPHABET [n]\n",
    "    except IndexError:\n",
    "        raise Exception (\"cannot encode: %s\" % n)\n",
    "        \n",
    "def dec_to_base (dec = 0, base = 16):\n",
    "    if dec < base:\n",
    "        return encode (dec)\n",
    "    else:\n",
    "        return dec_to_base (dec // base, base) + encode (dec % base)\n",
    "    \n",
    "    \n",
    "def mergeDict(dict1, dict2):\n",
    "    #Merge dictionaries and keep values of common keys in list\n",
    "    newDict = {**dict1, **dict2}\n",
    "    for key, value in newDict.items():\n",
    "        if key in dict1 and key in dict2:\n",
    "            if isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend(value)\n",
    "            elif isinstance(dict1[key], list) and not isinstance(value, list):\n",
    "                newDict[key] = dict1[key]\n",
    "                newDict[key].extend([value])\n",
    "            elif not isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                newDict[key] = [dict1[key]]\n",
    "                newDict[key].extend(value)\n",
    "            else:\n",
    "                newDict[key] = [dict1[key], value]\n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/Error functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.811264Z",
     "start_time": "2020-12-19T14:17:01.802077Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calcualate_function_value_with_X_data_entry(coefficient_list, X_data_entry):\n",
    "    \n",
    "    global list_of_monomial_identifiers\n",
    "     \n",
    "    result = 0    \n",
    "    for coefficient_value, coefficient_multipliers in zip(coefficient_list, list_of_monomial_identifiers):\n",
    "        partial_results = [X_data_value**int(coefficient_multiplier) for coefficient_multiplier, X_data_value in zip(coefficient_multipliers, X_data_entry)]\n",
    "        \n",
    "        result += coefficient_value * reduce(lambda x, y: x*y, partial_results)\n",
    "        \n",
    "    return result, np.append(X_data_entry, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:01.846712Z",
     "start_time": "2020-12-19T14:17:01.813103Z"
    },
    "code_folding": [
     0,
     20,
     43,
     66,
     88,
     91,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "            \n",
    "            \n",
    "    return tf.math.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def accuracy_multilabel(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(tf.reduce_all(K.equal(y_true, y_pred), axis=1), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def accuracy_single(y_true, y_pred):\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    if 'float' in str(y_true[0].dtype):        \n",
    "        if tf.is_tensor(y_true):\n",
    "            y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "        else:\n",
    "            y_true = y_true.astype('float32')\n",
    "        if tf.is_tensor(y_pred):\n",
    "            y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        else:\n",
    "            y_pred = y_pred.astype('float32')\n",
    "            \n",
    "        n_digits = int(-np.log10(a_step))\n",
    "        \n",
    "        y_true = tf.math.round(y_true * 10**n_digits) / (10**n_digits) \n",
    "        y_pred = tf.math.round(y_pred * 10**n_digits) / (10**n_digits) \n",
    "        \n",
    "    return K.mean(tf.dtypes.cast(tf.dtypes.cast(K.equal(y_true, y_pred), tf.int32), tf.float32))#tf.reduce_all(K.equal(K.equal(y_true, y_pred), True), axis=1)#K.all(K.equal(y_true, y_pred)) #K.equal(y_true, y_pred)                        \n",
    "\n",
    "def mean_absolute_percentage_error_keras(y_true, y_pred, epsilon=10e-3): \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values    \n",
    "        \n",
    "    if tf.is_tensor(y_true):\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    else:\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_true = tf.dtypes.cast(y_true, tf.float32) \n",
    "    if tf.is_tensor(y_pred):\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    else:\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "        \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    epsilon = tf.dtypes.cast(epsilon, tf.float32)\n",
    "        \n",
    "    return tf.reduce_mean(tf.abs(tf.divide(tf.subtract(y_pred, y_true),(y_true + epsilon))))\n",
    "\n",
    "def huber_loss_delta_set(y_true, y_pred):\n",
    "    return keras.losses.huber_loss(y_true, y_pred, delta=0.3)\n",
    "\n",
    "def relative_absolute_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "       \n",
    "    #error value calculation    \n",
    "    result = np.sum(np.abs(y_true-y_pred))/(y_true.shape[0]*np.std(y_true)) #correct STD?\n",
    "    \n",
    "    return result\n",
    "\n",
    "def relative_maximum_average_error(y_true, y_pred):\n",
    "    \n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.values\n",
    "    \n",
    "    #error value calculation    \n",
    "    result = np.max(y_true-y_pred)/np.std(y_true) #correct STD?\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate List of Monomial Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:02.111111Z",
     "start_time": "2020-12-19T14:17:01.848459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87602632925b42b888ad482e7e0cce20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 256\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0013', '0020', '0021', '0022', '0023', '0030', '0031', '0032', '0033', '0100', '0101', '0102', '0103', '0110', '0111', '0112', '0113', '0120', '0121', '0122', '0123', '0130', '0131', '0132', '0133', '0200', '0201', '0202', '0203', '0210', '0211', '0212', '0213', '0220', '0221', '0222', '0223', '0230', '0231', '0232', '0233', '0300', '0301', '0302', '0303', '0310', '0311', '0312', '0313', '0320', '0321', '0322', '0323', '0330', '0331', '0332', '0333', '1000', '1001', '1002', '1003', '1010', '1011', '1012', '1013', '1020', '1021', '1022', '1023', '1030', '1031', '1032', '1033', '1100', '1101', '1102', '1103', '1110', '1111', '1112', '1113', '1120', '1121', '1122', '1123', '1130', '1131', '1132', '1133', '1200', '1201', '1202', '1203', '1210', '1211', '1212', '1213', '1220', '1221', '1222', '1223', '1230', '1231', '1232', '1233', '1300', '1301', '1302', '1303', '1310', '1311', '1312', '1313', '1320', '1321', '1322', '1323', '1330', '1331', '1332', '1333', '2000', '2001', '2002', '2003', '2010', '2011', '2012', '2013', '2020', '2021', '2022', '2023', '2030', '2031', '2032', '2033', '2100', '2101', '2102', '2103', '2110', '2111', '2112', '2113', '2120', '2121', '2122', '2123', '2130', '2131', '2132', '2133', '2200', '2201', '2202', '2203', '2210', '2211', '2212', '2213', '2220', '2221', '2222', '2223', '2230', '2231', '2232', '2233', '2300', '2301', '2302', '2303', '2310', '2311', '2312', '2313', '2320', '2321', '2322', '2323', '2330', '2331', '2332', '2333', '3000', '3001', '3002', '3003', '3010', '3011', '3012', '3013', '3020', '3021', '3022', '3023', '3030', '3031', '3032', '3033', '3100', '3101', '3102', '3103', '3110', '3111', '3112', '3113', '3120', '3121', '3122', '3123', '3130', '3131', '3132', '3133', '3200', '3201', '3202', '3203', '3210', '3211', '3212', '3213', '3220', '3221', '3222', '3223', '3230', '3231', '3232', '3233', '3300', '3301', '3302', '3303', '3310', '3311', '3312', '3313', '3320', '3321', '3322', '3323', '3330', '3331', '3332', '3333']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91bc06fbb10452cbd815d93b457c530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=256), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List length: 35\n",
      "Number of monomials in a polynomial with 4 variables and degree 3: 35\n",
      "Sparsity: 35\n",
      "['0000', '0001', '0002', '0003', '0010', '0011', '0012', '0020', '0021', '0030', '0100', '0101', '0102', '0110', '0111', '0120', '0200', '0201', '0210', '0300', '1000', '1001', '1002', '1010', '1011', '1020', '1100', '1101', '1110', '1200', '2000', '2001', '2010', '2100', '3000']\n"
     ]
    }
   ],
   "source": [
    "list_of_monomial_identifiers_extended = []\n",
    "for i in tqdm(range((d+1)**n)):    \n",
    "    monomial_identifier = dec_to_base(i, base = (d+1)).zfill(n) \n",
    "    list_of_monomial_identifiers_extended.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers_extended)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers_extended)\n",
    "\n",
    "list_of_monomial_identifiers = []\n",
    "for monomial_identifier in tqdm(list_of_monomial_identifiers_extended):\n",
    "    monomial_identifier_values = list(map(int, list(monomial_identifier)))\n",
    "    if sum(monomial_identifier_values) <= d:\n",
    "        list_of_monomial_identifiers.append(monomial_identifier)\n",
    "\n",
    "print('List length: ' + str(len(list_of_monomial_identifiers)))\n",
    "print('Number of monomials in a polynomial with ' + str(n) + ' variables and degree ' + str(d) + ': ' + str(nCr(n+d, d)))\n",
    "print('Sparsity: ' + str(sparsity))\n",
    "print(list_of_monomial_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.222606Z",
     "start_time": "2020-12-19T14:17:02.112723Z"
    }
   },
   "outputs": [],
   "source": [
    "path_polynomials = './data/saved_polynomial_lists/polynomials_sample' + str(number_formulas_load) + '_variables_' + str(n) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step) + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '.csv'\n",
    "polynomials_list_df = pd.read_csv(path_polynomials)\n",
    "\n",
    "path_X_data = './data/saved_polynomial_lists/X_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_polynomial_lists/y_sample' + str(number_formulas_load) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) +  training_string + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "if generate_subset:\n",
    "    polynomials_list_df = polynomials_list_df.sample(n=number_of_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.362826Z",
     "start_time": "2020-12-19T14:17:03.224610Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#clear files\n",
    "if each_epochs_save != None:\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        \n",
    "        for i in epochs_save_range:\n",
    "            index = i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "            path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size)  + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "\n",
    "            with open(path, 'wt') as text_file:\n",
    "                text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "\n",
    "        path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "        with open(path, 'wt') as text_file:\n",
    "            text_file.truncate()   \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.375386Z",
     "start_time": "2020-12-19T14:17:03.365304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.810</td>\n",
       "      <td>-0.660</td>\n",
       "      <td>-0.560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.970</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.560</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.620</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.860</td>\n",
       "      <td>-0.520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0  0.370  0.810 -0.660 -0.560\n",
       "1  0.970 -1.000 -0.520  0.530\n",
       "2 -0.560 -0.950 -0.190  0.250\n",
       "3  0.750 -0.030  0.540  0.930\n",
       "4  0.620  0.880  0.860 -0.520"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.385092Z",
     "start_time": "2020-12-19T14:17:03.377095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.920</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.650</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.630</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.240</td>\n",
       "      <td>0.300</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-0.240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0 -0.920 -0.440 -0.410 -1.000\n",
       "1  0.650 -0.280  0.100  0.280\n",
       "2  0.630  0.880  0.060  0.310\n",
       "3  0.640 -0.170 -0.750  0.510\n",
       "4  0.240  0.300 -0.700 -0.240"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.391295Z",
     "start_time": "2020-12-19T14:17:03.386712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000    6.300\n",
       "0001   -7.200\n",
       "0002   -9.400\n",
       "0003    8.900\n",
       "0010   -3.000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.397704Z",
     "start_time": "2020-12-19T14:17:03.392903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0000   -9.900\n",
       "0001    9.400\n",
       "0002   -6.000\n",
       "0003    7.800\n",
       "0010    0.800\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_list[1][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.424294Z",
     "start_time": "2020-12-19T14:17:03.399524Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_lambda_net(identifier, \n",
    "                        X_data_real_lambda, \n",
    "                        y_data_real_lambda, \n",
    "                        y_data_pred_lambda, \n",
    "                        y_data_pred_lambda_poly_lstsq, \n",
    "                        y_data_real_lambda_poly_lstsq):\n",
    "    \n",
    "    mae_real_VS_predLambda = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    mae_predLambda_VS_predPolyLstsq = np.round(mean_absolute_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    mae_real_VS_realPolyLstsq = np.round(mean_absolute_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    rmse_real_VS_predLambda = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    rmse_predLambda_VS_predPolyLstsq = np.round(root_mean_squared_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    rmse_real_VS_realPolyLstsq = np.round(root_mean_squared_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)    \n",
    "\n",
    "    mape_real_VS_predLambda = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)    \n",
    "    mape_predLambda_VS_predPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)    \n",
    "    mape_real_VS_realPolyLstsq = np.round(mean_absolute_percentage_error_keras(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)            \n",
    "\n",
    "    r2_real_VS_predLambda = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_predPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    r2_predLambda_VS_predPolyLstsq = np.round(r2_score(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    r2_real_VS_realPolyLstsq = np.round(r2_score(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    raae_real_VS_predLambda = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    raae_predLambda_VS_predPolyLstsq = np.round(relative_absolute_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    raae_real_VS_realPolyLstsq = np.round(relative_absolute_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "\n",
    "    rmae_real_VS_predLambda = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_pred_lambda_poly_lstsq), 4)\n",
    "    rmae_predLambda_VS_predPolyLstsq = np.round(relative_maximum_average_error(y_data_pred_lambda_poly_lstsq, y_data_pred_lambda), 4)\n",
    "    rmae_real_VS_realPolyLstsq = np.round(relative_maximum_average_error(y_data_real_lambda, y_data_real_lambda_poly_lstsq), 4)\n",
    "        \n",
    "    std_data_real_lambda = np.round(np.std(y_data_real_lambda), 4) \n",
    "    std_data_pred_lambda = np.round(np.std(y_data_pred_lambda), 4) \n",
    "    std_data_pred_lambda_poly_lstsq = np.round(np.std(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    std_data_real_lambda_poly_lstsq = np.round(np.std(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    mean_data_real_lambda = np.round(np.mean(y_data_real_lambda), 4) \n",
    "    mean_data_pred_lambda = np.round(np.mean(y_data_pred_lambda), 4) \n",
    "    mean_data_pred_lambda_poly_lstsq = np.round(np.mean(y_data_pred_lambda_poly_lstsq), 4) \n",
    "    mean_data_real_lambda_poly_lstsq = np.round(np.mean(y_data_real_lambda_poly_lstsq), 4) \n",
    "\n",
    "    return [{\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mae_real_VS_predLambda,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_real_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mae_predLambda_VS_predPolyLstsq,\n",
    "             'MAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mae_real_VS_realPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmse_real_VS_predLambda,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_real_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmse_predLambda_VS_predPolyLstsq,\n",
    "             'RMSE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmse_real_VS_realPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': mape_real_VS_predLambda,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_real_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': mape_predLambda_VS_predPolyLstsq,\n",
    "             'MAPE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': mape_real_VS_realPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': r2_real_VS_predLambda,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_real_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': r2_predLambda_VS_predPolyLstsq,\n",
    "             'R2 FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': r2_real_VS_realPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': raae_real_VS_predLambda,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_real_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': raae_predLambda_VS_predPolyLstsq,\n",
    "             'RAAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': raae_real_VS_realPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA': rmae_real_VS_predLambda,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_real_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' PRED LAMBDA VS PRED LAMBDA POLY LSTSQ': rmae_predLambda_VS_predPolyLstsq,\n",
    "             'RMAE FV ' + identifier + ' REAL LAMBDA VS REAL LAMBDA POLY LSTSQ': rmae_real_VS_realPolyLstsq,\n",
    "            },\n",
    "            {\n",
    "             'STD FV ' + identifier + ' REAL LAMBDA': std_data_real_lambda,\n",
    "             'STD FV ' + identifier + ' PRED LAMBDA': std_data_pred_lambda, \n",
    "             'STD FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': std_data_pred_lambda_poly_lstsq, \n",
    "             'STD FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': std_data_real_lambda_poly_lstsq, \n",
    "            },\n",
    "            {\n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA': mean_data_real_lambda,\n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA': mean_data_pred_lambda, \n",
    "             'MEAN FV ' + identifier + ' PRED LAMBDA POLY LSTSQ': mean_data_pred_lambda_poly_lstsq, \n",
    "             'MEAN FV ' + identifier + ' REAL LAMBDA POLY LSTSQ': mean_data_real_lambda_poly_lstsq, \n",
    "            }]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.433760Z",
     "start_time": "2020-12-19T14:17:03.426021Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_function_values_from_polynomial(X_data, polynomial):\n",
    "    function_value_list = []\n",
    "    for entry in X_data:\n",
    "        function_value, _ = calcualate_function_value_with_X_data_entry(polynomial, entry)\n",
    "        function_value_list.append(function_value)\n",
    "    function_value_array = np.array(function_value_list).reshape(len(function_value_list), 1)     \n",
    "\n",
    "    return function_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.441458Z",
     "start_time": "2020-12-19T14:17:03.435426Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_term_matric_for_lstsq(X_data, polynomial_indices):\n",
    "    term_list_all = []\n",
    "    y = 0\n",
    "    for term in list(polynomial_indices):\n",
    "        term_list = [int(value_mult) for value_mult in term]\n",
    "        term_list_all.append(term_list)\n",
    "    terms_matrix = []\n",
    "    for unknowns in X_data:\n",
    "        terms = []\n",
    "        for term_multipliers in term_list_all:\n",
    "            term_value = prod([unknown**multiplier for unknown, multiplier in zip(unknowns, term_multipliers)])\n",
    "            terms.append(term_value)\n",
    "        terms_matrix.append(np.array(terms))\n",
    "    terms_matrix = np.array(terms_matrix)\n",
    "    \n",
    "    return terms_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:17:03.505482Z",
     "start_time": "2020-12-19T14:17:03.443239Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_nn(lambda_index, X_data_lambda, y_data_real_lambda, polynomial, seed_list, callbacks=None, return_history=False, each_epochs_save=None, printing=False, return_model=False):\n",
    "    \n",
    "    current_seed = seed_list[lambda_index%number_different_lambda_trainings]\n",
    "    \n",
    "    if fixed_seed_lambda_training:\n",
    "        random.seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        if int(tf.__version__[0]) >= 2:\n",
    "            tf.random.set_seed(current_seed)\n",
    "        else:\n",
    "            tf.set_random_seed(current_seed) \n",
    "        \n",
    "    if isinstance(X_data_lambda, pd.DataFrame):\n",
    "        X_data_lambda = X_data_lambda.values\n",
    "    if isinstance(y_data_real_lambda, pd.DataFrame):\n",
    "        y_data_real_lambda = y_data_real_lambda.values\n",
    "                \n",
    "    X_train_lambda_with_valid, X_test_lambda, y_train_real_lambda_with_valid, y_test_real_lambda = train_test_split(X_data_lambda, y_data_real_lambda, test_size=0.25, random_state=current_seed)           \n",
    "    X_train_lambda, X_valid_lambda, y_train_real_lambda, y_valid_real_lambda = train_test_split(X_train_lambda_with_valid, y_train_real_lambda_with_valid, test_size=0.25, random_state=current_seed)           \n",
    "     \n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    #kerase defaults: kernel_initializer='glorot_uniform', bias_initializer='zeros'               \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1], kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros')) #1024\n",
    "    else:\n",
    "        model.add(Dense(lambda_network_layers[0], activation='relu', input_dim=X_data_lambda.shape[1])) #1024\n",
    "        \n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    for neurons in lambda_network_layers[1:]:\n",
    "        if fixed_initialization_lambda_training:\n",
    "            model.add(Dense(neurons, activation='relu', kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation='relu'))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))   \n",
    "    \n",
    "    if fixed_initialization_lambda_training:\n",
    "        model.add(Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=current_seed), bias_initializer='zeros'))\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mae',\n",
    "                  metrics=[mean_absolute_percentage_error_keras, root_mean_squared_error]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    weights = []\n",
    "    polynomial_lstsq_pred_list = []\n",
    "    polynomial_lstsq_true_list = []\n",
    "\n",
    "     \n",
    "    if each_epochs_save == None:   \n",
    "        model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=0)\n",
    "        weights.append(model.get_weights())\n",
    "        \n",
    "        history = model_history.history\n",
    "        \n",
    "        y_train_pred_lambda = model.predict(X_train_lambda) \n",
    "        y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "        y_test_pred_lambda = model.predict(X_test_lambda)\n",
    "    \n",
    "        terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                \n",
    "        polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1]\n",
    "        polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "        polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "        polynomial_lstsq_true_list.append(polynomial_lstsq_true)\n",
    "        \n",
    "        y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "        y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "        y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "        y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)    \n",
    "        y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)\n",
    "        y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)  \n",
    "        \n",
    "        pred_list = (lambda_index,\n",
    "                     y_train_real_lambda, \n",
    "                     y_train_pred_lambda, \n",
    "                     y_train_pred_lambda_poly_lstsq,\n",
    "                     #y_train_real_lambda_poly_lstsq,\n",
    "                     X_train_lambda, \n",
    "                     y_valid_real_lambda,\n",
    "                     y_valid_pred_lambda, \n",
    "                     y_valid_pred_lambda_poly_lstsq,\n",
    "                     #y_valid_real_lambda_poly_lstsq,\n",
    "                     X_valid_lambda, \n",
    "                     y_test_real_lambda, \n",
    "                     y_test_pred_lambda, \n",
    "                     y_test_pred_lambda_poly_lstsq, \n",
    "                     #y_test_real_lambda_poly_lstsq,\n",
    "                     X_test_lambda)\n",
    "\n",
    "        scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "        scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "        scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "        scores_std = {}\n",
    "        for aDict in (std_train, std_valid, std_test):\n",
    "            scores_std.update(aDict)      \n",
    "        scores_mean = {}\n",
    "        for aDict in (mean_train, mean_valid, mean_test):\n",
    "            scores_mean.update(aDict)\n",
    "        \n",
    "        scores_list = [lambda_index,\n",
    "                     scores_train,\n",
    "                     scores_valid,\n",
    "                     scores_test,\n",
    "                     scores_std,\n",
    "                     scores_mean]            \n",
    "                            \n",
    "    else:\n",
    "        scores_list = []\n",
    "        pred_list = []\n",
    "        for i in epochs_save_range:\n",
    "            train_epochs_step = each_epochs_save if i > 1 else max(each_epochs_save-1, 1) if i==1 else 1\n",
    "            \n",
    "            model_history = model.fit(X_train_lambda, \n",
    "                      y_train_real_lambda, \n",
    "                      epochs=train_epochs_step, \n",
    "                      batch_size=batch_size, \n",
    "                      callbacks=callbacks,\n",
    "                      validation_data=(X_valid_lambda, y_valid_real_lambda),\n",
    "                      verbose=0,\n",
    "                      workers=1,\n",
    "                      use_multiprocessing=False)\n",
    "            \n",
    "            #history adjustment for continuing training\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                history = model_history.history\n",
    "            else:\n",
    "                history = mergeDict(history, model_history.history)\n",
    "\n",
    "            weights.append(model.get_weights())\n",
    "            \n",
    "            y_train_pred_lambda = model.predict(X_train_lambda)                \n",
    "            y_valid_pred_lambda = model.predict(X_valid_lambda)                \n",
    "            y_test_pred_lambda = model.predict(X_test_lambda)        \n",
    "\n",
    "            terms_matrix = generate_term_matric_for_lstsq(X_train_lambda, list(polynomial.index))\n",
    "                        \n",
    "            polynomial_lstsq_pred, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_pred_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            #does not change over time\n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                polynomial_lstsq_true, _, _, _ = np.linalg.lstsq(terms_matrix, y_train_real_lambda.ravel(), rcond=-1)#[::-1] \n",
    "            polynomial_lstsq_pred_list.append(polynomial_lstsq_pred)\n",
    "            polynomial_lstsq_true_list.append(polynomial_lstsq_true)            \n",
    "\n",
    "            \n",
    "            y_train_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_pred)\n",
    "            y_valid_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_pred)\n",
    "            y_test_pred_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_pred)           \n",
    "            if i == 0 and each_epochs_save != 1 or i == 1 and each_epochs_save == 1:\n",
    "                y_train_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_train_lambda, polynomial_lstsq_true)\n",
    "                y_valid_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_valid_lambda, polynomial_lstsq_true)  \n",
    "                y_test_real_lambda_poly_lstsq = calculate_function_values_from_polynomial(X_test_lambda, polynomial_lstsq_true)                    \n",
    "                \n",
    "            pred_list.append((lambda_index,\n",
    "                              y_train_real_lambda, \n",
    "                              y_train_pred_lambda, \n",
    "                              y_train_pred_lambda_poly_lstsq,\n",
    "                              #y_train_real_lambda_poly_lstsq,\n",
    "                              X_train_lambda, \n",
    "                              y_valid_real_lambda,\n",
    "                              y_valid_pred_lambda, \n",
    "                              y_valid_pred_lambda_poly_lstsq,\n",
    "                              #y_valid_real_lambda_poly_lstsq,\n",
    "                              X_valid_lambda, \n",
    "                              y_test_real_lambda, \n",
    "                              y_test_pred_lambda, \n",
    "                              y_test_pred_lambda_poly_lstsq, \n",
    "                              #y_test_real_lambda_poly_lstsq,\n",
    "                              X_test_lambda))\n",
    "    \n",
    "            scores_train, std_train, mean_train = evaluate_lambda_net('TRAIN', X_train_lambda, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, y_train_real_lambda_poly_lstsq)\n",
    "            scores_valid, std_valid, mean_valid = evaluate_lambda_net('VALID', X_valid_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, y_valid_real_lambda_poly_lstsq)\n",
    "            scores_test, std_test, mean_test = evaluate_lambda_net('TEST', X_test_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, y_test_real_lambda_poly_lstsq)\n",
    "\n",
    "            scores_std = {}\n",
    "            for aDict in (std_train, std_valid, std_test):\n",
    "                scores_std.update(aDict)\n",
    "            scores_mean = {}\n",
    "            for aDict in (mean_train, mean_valid, mean_test):\n",
    "                scores_mean.update(aDict)\n",
    "\n",
    "            scores_list_single_epoch =  [lambda_index,\n",
    "                                         scores_train,\n",
    "                                          scores_valid,\n",
    "                                          scores_test,\n",
    "                                          scores_std,\n",
    "                                          scores_mean]        \n",
    "                  \n",
    "            scores_list.append(scores_list_single_epoch)\n",
    "       \n",
    "\n",
    "        \n",
    "    if printing:        \n",
    "        for i, (weights_for_epoch, polynomial_lstsq_pred_for_epoch, polynomial_lstsq_true_for_epoch) in enumerate(zip(weights, polynomial_lstsq_pred_list, polynomial_lstsq_true_list)):        \n",
    "            if each_epochs_save == None:\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "            else:\n",
    "                index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "                path = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(index).zfill(3)  + filename + '.txt'\n",
    "            with open(path, 'a') as text_file: \n",
    "                text_file.write(str(lambda_index))\n",
    "                text_file.write(', ' + str(current_seed))\n",
    "                for i, value in enumerate(polynomial.values): \n",
    "                    text_file.write(', ' + str(value))   \n",
    "                for value in polynomial_lstsq_pred_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for value in polynomial_lstsq_true_for_epoch:\n",
    "                    text_file.write(', ' + str(value))\n",
    "                for layer_weights, biases in pairwise(weights_for_epoch):    #clf.get_weights()\n",
    "                    for neuron in layer_weights:\n",
    "                        for weight in neuron:\n",
    "                            text_file.write(', ' + str(weight))\n",
    "                    for bias in biases:\n",
    "                        text_file.write(', ' + str(bias))\n",
    "                text_file.write('\\n')\n",
    "\n",
    "        text_file.close() \n",
    "\n",
    "        directory = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/'\n",
    "        path = directory + 'lambda_' + str(lambda_index) + '_test_data'\n",
    "        np.save(path, X_test_lambda)\n",
    "            \n",
    "    if return_model:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, model\n",
    "    elif return_history:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list, history, #polynomial_lstsq_pred_list, polynomial_lstsq_true_list#, weights, history\n",
    "    else:\n",
    "        return (lambda_index, current_seed, polynomial, polynomial_lstsq_pred_list, polynomial_lstsq_true_list), scores_list, pred_list#, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:38.081071Z",
     "start_time": "2020-12-19T14:17:03.507168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\ttraining         \t (min:    3.632, max:   10.056, cur:    3.632)\n",
      "\tvalidation       \t (min:    3.283, max:    8.997, cur:    3.283)\n",
      "mean_absolute_percentage_error_keras\n",
      "\ttraining         \t (min:    0.789, max:    1.170, cur:    0.789)\n",
      "\tvalidation       \t (min:    0.554, max:    0.967, cur:    0.554)\n",
      "root_mean_squared_error\n",
      "\ttraining         \t (min:    5.283, max:   12.643, cur:    5.298)\n",
      "\tvalidation       \t (min:    4.307, max:   10.874, cur:    4.307)\n"
     ]
    }
   ],
   "source": [
    "clf_list = []\n",
    "chunksize = 1000 if data_size > 10000 else max(data_size//10, min(50, data_size))\n",
    "X_data_list_splits = list(chunks(X_data_list, chunksize))\n",
    "y_data_list_splits = list(chunks(y_data_list, chunksize))\n",
    "\n",
    "max_seed = 2**32 - 1\n",
    "seed_list = random.sample(range(0, max_seed), number_different_lambda_trainings)\n",
    "chunk_multiplier = 0\n",
    "\n",
    "for X_data_list_split, y_data_list_split in tqdm(zip(X_data_list_splits, y_data_list_splits), total=max(len(X_data_list_splits), len(y_data_list_splits))):\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    clf_sublist = parallel(delayed(train_nn)(chunksize*chunk_multiplier+index, X_data[1].values, y_data[1].values, X_data[0], seed_list, return_history=True, each_epochs_save=each_epochs_save, printing=True) for index, (X_data, y_data) in enumerate(zip(X_data_list_split, y_data_list_split)))  \n",
    "    clf_list.extend(clf_sublist)\n",
    "    del parallel\n",
    "    chunk_multiplier +=1\n",
    "rand_index = np.random.randint(data_size)\n",
    "random_network = train_nn(rand_index, X_data_list[rand_index][1], y_data_list[rand_index][1], X_data_list[rand_index][0], seed_list, callbacks=[PlotLossesKerasTF()], return_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:39.804586Z",
     "start_time": "2020-12-19T14:46:38.083127Z"
    }
   },
   "outputs": [],
   "source": [
    "if each_epochs_save == None:\n",
    "    scores_list_train = [clf[1][1] for clf in clf_list]\n",
    "    scores_list_valid = [clf[1][2] for clf in clf_list]\n",
    "    scores_list_test = [clf[1][3] for clf in clf_list]\n",
    "    scores_list_stds = [clf[1][4] for clf in clf_list]\n",
    "    scores_list_means = [clf[1][5] for clf in clf_list]\n",
    "\n",
    "    scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()\n",
    "    df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED', 'TRAIN POLY', 'TRAIN POLY PRED', 'TRAIN LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()\n",
    "    df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED', 'VALID POLY', 'VALID POLY PRED', 'VALID LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "    df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED', 'TEST POLY', 'TEST POLY PRED', 'TEST LSTSQ'], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "\n",
    "    stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "    df_stds = pd.DataFrame(stds_mean, columns=['0'], index=stds_mean.index)\n",
    "    \n",
    "    means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "    df_means = pd.DataFrame(means_mean, columns=['0'], index=means_mean.index)\n",
    "    \n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "else:\n",
    "    scores_list = [clf[1] for clf in clf_list]\n",
    "    \n",
    "    scores_list_by_epochs = [[] for i in epochs_save_range]\n",
    "    for scores_list in scores_list:   \n",
    "        for index, scores in enumerate(scores_list):\n",
    "            scores_list_by_epochs[index].append(scores)\n",
    "            \n",
    "        \n",
    "    for i, scores_list_single_epoch in enumerate(scores_list_by_epochs):\n",
    "        index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "           \n",
    "        scores_list_train = [scores_list[1] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_valid = [scores_list[2] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_test = [scores_list[3] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_stds = [scores_list[4] for scores_list in scores_list_single_epoch]\n",
    "        scores_list_means = [scores_list[5] for scores_list in scores_list_single_epoch]\n",
    "        \n",
    "        scores_list_train_mean = pd.DataFrame(scores_list_train, columns=scores_list_train[0].keys()).mean()  \n",
    "        scores_list_valid_mean = pd.DataFrame(scores_list_valid, columns=scores_list_valid[0].keys()).mean()  \n",
    "        scores_list_test_mean = pd.DataFrame(scores_list_test, columns=scores_list_test[0].keys()).mean()\n",
    "        stds_mean = pd.DataFrame(scores_list_stds, columns=scores_list_stds[0].keys()).mean()\n",
    "        means_mean = pd.DataFrame(scores_list_means, columns=scores_list_means[0].keys()).mean()\n",
    "\n",
    "        if index == 1:\n",
    "            df_mean_scores_train = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_valid = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_test = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_stds = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)   \n",
    "            df_means = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)   \n",
    "        else:\n",
    "            df_mean_scores_train_new = pd.DataFrame(np.column_stack((scores_list_train_mean.values[0::4], scores_list_train_mean.values[1::4], scores_list_train_mean.values[2::4], scores_list_train_mean.values[3::4])), columns=['TRAIN PRED E' + str(index), 'TRAIN POLY E' + str(index), 'TRAIN POLY PRED E' + str(index), 'TRAIN LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_train = pd.concat([df_mean_scores_train, df_mean_scores_train_new],axis=1)  \n",
    "            \n",
    "            df_mean_scores_valid_new = pd.DataFrame(np.column_stack((scores_list_valid_mean.values[0::4], scores_list_valid_mean.values[1::4], scores_list_valid_mean.values[2::4], scores_list_valid_mean.values[3::4])), columns=['VALID PRED E' + str(index), 'VALID POLY E' + str(index), 'VALID POLY PRED E' + str(index), 'VALID LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_valid = pd.concat([df_mean_scores_valid, df_mean_scores_valid_new],axis=1)  \n",
    "\n",
    "            df_mean_scores_test_new = pd.DataFrame(np.column_stack((scores_list_test_mean.values[0::4], scores_list_test_mean.values[1::4], scores_list_test_mean.values[2::4], scores_list_test_mean.values[3::4])), columns=['TEST PRED E' + str(index), 'TEST POLY E' + str(index), 'TEST POLY PRED E' + str(index), 'TEST LSTSQ E' + str(index)], index=['MAE FV', 'RMSE FV', 'MAPE FV', 'R2 FV', 'RAAE FV', 'RMAE FV'])\n",
    "            df_mean_scores_test = pd.concat([df_mean_scores_test, df_mean_scores_test_new],axis=1)  \n",
    "\n",
    "            df_stds_new = pd.DataFrame(stds_mean, columns=['E' + str(index)], index=stds_mean.index)\n",
    "            df_stds = pd.concat([df_stds, df_stds_new],axis=1)  \n",
    "            \n",
    "            df_means_new = pd.DataFrame(means_mean, columns=['E' + str(index)], index=means_mean.index)\n",
    "            df_means = pd.concat([df_means, df_means_new],axis=1)     \n",
    "\n",
    "    path_scores_train = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_train_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_valid = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_valid_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_scores_test = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/scores_test_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_stds = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/stds_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_means = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/means_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    df_mean_scores_train.to_csv(path_scores_train, sep=',')\n",
    "    df_mean_scores_valid.to_csv(path_scores_valid, sep=',')\n",
    "    df_mean_scores_test.to_csv(path_scores_test, sep=',')\n",
    "    df_stds.to_csv(path_stds, sep=',')\n",
    "    df_means.to_csv(path_means, sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:39.826068Z",
     "start_time": "2020-12-19T14:46:39.806514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN PRED E1</th>\n",
       "      <th>TRAIN POLY E1</th>\n",
       "      <th>TRAIN POLY PRED E1</th>\n",
       "      <th>TRAIN LSTSQ E1</th>\n",
       "      <th>TRAIN PRED E10</th>\n",
       "      <th>TRAIN POLY E10</th>\n",
       "      <th>TRAIN POLY PRED E10</th>\n",
       "      <th>TRAIN LSTSQ E10</th>\n",
       "      <th>TRAIN PRED E20</th>\n",
       "      <th>TRAIN POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TRAIN POLY PRED E180</th>\n",
       "      <th>TRAIN LSTSQ E180</th>\n",
       "      <th>TRAIN PRED E190</th>\n",
       "      <th>TRAIN POLY E190</th>\n",
       "      <th>TRAIN POLY PRED E190</th>\n",
       "      <th>TRAIN LSTSQ E190</th>\n",
       "      <th>TRAIN PRED E200</th>\n",
       "      <th>TRAIN POLY E200</th>\n",
       "      <th>TRAIN POLY PRED E200</th>\n",
       "      <th>TRAIN LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.176</td>\n",
       "      <td>10.176</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.701</td>\n",
       "      <td>9.702</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.971</td>\n",
       "      <td>8.973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.428</td>\n",
       "      <td>4.464</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.340</td>\n",
       "      <td>4.377</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>12.978</td>\n",
       "      <td>12.978</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.487</td>\n",
       "      <td>12.487</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.729</td>\n",
       "      <td>11.729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.234</td>\n",
       "      <td>6.211</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.122</td>\n",
       "      <td>6.096</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.319</td>\n",
       "      <td>1.318</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.434</td>\n",
       "      <td>1.433</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.816</td>\n",
       "      <td>1.819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.057</td>\n",
       "      <td>3.164</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.010</td>\n",
       "      <td>3.128</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.916</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.461</td>\n",
       "      <td>3.461</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.435</td>\n",
       "      <td>3.435</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.388</td>\n",
       "      <td>3.387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.375</td>\n",
       "      <td>2.328</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.347</td>\n",
       "      <td>2.296</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TRAIN PRED E1  TRAIN POLY E1  TRAIN POLY PRED E1  TRAIN LSTSQ E1  \\\n",
       "MAE FV          10.176         10.176               0.013           0.000   \n",
       "RMSE FV         12.978         12.978               0.016           0.000   \n",
       "MAPE FV          1.319          1.318               0.402           0.000   \n",
       "R2 FV           -0.400         -0.400               0.999           1.000   \n",
       "RAAE FV          0.916          0.916               0.022           0.000   \n",
       "RMAE FV          3.461          3.461               0.096           0.000   \n",
       "\n",
       "         TRAIN PRED E10  TRAIN POLY E10  TRAIN POLY PRED E10  TRAIN LSTSQ E10  \\\n",
       "MAE FV            9.701           9.702                0.015            0.000   \n",
       "RMSE FV          12.487          12.487                0.020            0.000   \n",
       "MAPE FV           1.434           1.433                0.197            0.000   \n",
       "R2 FV            -0.283          -0.283                0.999            1.000   \n",
       "RAAE FV           0.871           0.871                0.024            0.000   \n",
       "RMAE FV           3.435           3.435                0.116            0.000   \n",
       "\n",
       "         TRAIN PRED E20  TRAIN POLY E20  ...  TRAIN POLY PRED E180  \\\n",
       "MAE FV            8.971           8.973  ...                 0.363   \n",
       "RMSE FV          11.729          11.729  ...                 0.467   \n",
       "MAPE FV           1.816           1.819  ...                 0.505   \n",
       "R2 FV            -0.114          -0.114  ...                 0.996   \n",
       "RAAE FV           0.803           0.803  ...                 0.048   \n",
       "RMAE FV           3.388           3.387  ...                 0.241   \n",
       "\n",
       "         TRAIN LSTSQ E180  TRAIN PRED E190  TRAIN POLY E190  \\\n",
       "MAE FV              0.000            4.428            4.464   \n",
       "RMSE FV             0.000            6.234            6.211   \n",
       "MAPE FV             0.000            3.057            3.164   \n",
       "R2 FV               1.000            0.655            0.657   \n",
       "RAAE FV             0.000            0.408            0.412   \n",
       "RMAE FV             0.000            2.375            2.328   \n",
       "\n",
       "         TRAIN POLY PRED E190  TRAIN LSTSQ E190  TRAIN PRED E200  \\\n",
       "MAE FV                  0.387             0.000            4.340   \n",
       "RMSE FV                 0.495             0.000            6.122   \n",
       "MAPE FV                 0.408             0.000            3.010   \n",
       "R2 FV                   0.995             1.000            0.666   \n",
       "RAAE FV                 0.051             0.000            0.400   \n",
       "RMAE FV                 0.249             0.000            2.347   \n",
       "\n",
       "         TRAIN POLY E200  TRAIN POLY PRED E200  TRAIN LSTSQ E200  \n",
       "MAE FV             4.377                 0.410             0.000  \n",
       "RMSE FV            6.096                 0.524             0.000  \n",
       "MAPE FV            3.128                 0.465             0.000  \n",
       "R2 FV              0.669                 0.995             1.000  \n",
       "RAAE FV            0.404                 0.054             0.000  \n",
       "RMAE FV            2.296                 0.258             0.000  \n",
       "\n",
       "[6 rows x 84 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:39.870609Z",
     "start_time": "2020-12-19T14:46:39.827599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID PRED E1</th>\n",
       "      <th>VALID POLY E1</th>\n",
       "      <th>VALID POLY PRED E1</th>\n",
       "      <th>VALID LSTSQ E1</th>\n",
       "      <th>VALID PRED E10</th>\n",
       "      <th>VALID POLY E10</th>\n",
       "      <th>VALID POLY PRED E10</th>\n",
       "      <th>VALID LSTSQ E10</th>\n",
       "      <th>VALID PRED E20</th>\n",
       "      <th>VALID POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>VALID POLY PRED E180</th>\n",
       "      <th>VALID LSTSQ E180</th>\n",
       "      <th>VALID PRED E190</th>\n",
       "      <th>VALID POLY E190</th>\n",
       "      <th>VALID POLY PRED E190</th>\n",
       "      <th>VALID LSTSQ E190</th>\n",
       "      <th>VALID PRED E200</th>\n",
       "      <th>VALID POLY E200</th>\n",
       "      <th>VALID POLY PRED E200</th>\n",
       "      <th>VALID LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.195</td>\n",
       "      <td>10.195</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.724</td>\n",
       "      <td>9.724</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>9.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.553</td>\n",
       "      <td>4.568</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.469</td>\n",
       "      <td>4.483</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>13.004</td>\n",
       "      <td>13.004</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.516</td>\n",
       "      <td>12.516</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.765</td>\n",
       "      <td>11.764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.342</td>\n",
       "      <td>6.301</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.236</td>\n",
       "      <td>6.189</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>65.298</td>\n",
       "      <td>67.076</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>113.807</td>\n",
       "      <td>115.506</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.000</td>\n",
       "      <td>193.826</td>\n",
       "      <td>196.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.000</td>\n",
       "      <td>256.837</td>\n",
       "      <td>252.637</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>264.414</td>\n",
       "      <td>258.485</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.019</td>\n",
       "      <td>3.019</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.993</td>\n",
       "      <td>2.993</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.948</td>\n",
       "      <td>2.947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.010</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.985</td>\n",
       "      <td>1.939</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VALID PRED E1  VALID POLY E1  VALID POLY PRED E1  VALID LSTSQ E1  \\\n",
       "MAE FV          10.195         10.195               0.014           0.000   \n",
       "RMSE FV         13.004         13.004               0.018           0.000   \n",
       "MAPE FV         65.298         67.076               0.206           0.000   \n",
       "R2 FV           -0.413         -0.413               0.999           1.000   \n",
       "RAAE FV          0.920          0.920               0.024           0.000   \n",
       "RMAE FV          3.019          3.019               0.097           0.000   \n",
       "\n",
       "         VALID PRED E10  VALID POLY E10  VALID POLY PRED E10  VALID LSTSQ E10  \\\n",
       "MAE FV            9.724           9.724                0.017            0.000   \n",
       "RMSE FV          12.516          12.516                0.022            0.000   \n",
       "MAPE FV         113.807         115.506                0.218            0.000   \n",
       "R2 FV            -0.294          -0.294                0.999            1.000   \n",
       "RAAE FV           0.875           0.875                0.026            0.000   \n",
       "RMAE FV           2.993           2.993                0.112            0.000   \n",
       "\n",
       "         VALID PRED E20  VALID POLY E20  ...  VALID POLY PRED E180  \\\n",
       "MAE FV            9.000           9.002  ...                 0.388   \n",
       "RMSE FV          11.765          11.764  ...                 0.507   \n",
       "MAPE FV         193.826         196.015  ...                 0.477   \n",
       "R2 FV            -0.125          -0.124  ...                 0.995   \n",
       "RAAE FV           0.807           0.807  ...                 0.052   \n",
       "RMAE FV           2.948           2.947  ...                 0.228   \n",
       "\n",
       "         VALID LSTSQ E180  VALID PRED E190  VALID POLY E190  \\\n",
       "MAE FV              0.000            4.553            4.568   \n",
       "RMSE FV             0.000            6.342            6.301   \n",
       "MAPE FV             0.000          256.837          252.637   \n",
       "R2 FV               1.000            0.642            0.646   \n",
       "RAAE FV             0.000            0.420            0.422   \n",
       "RMAE FV             0.000            2.010            1.968   \n",
       "\n",
       "         VALID POLY PRED E190  VALID LSTSQ E190  VALID PRED E200  \\\n",
       "MAE FV                  0.413             0.000            4.469   \n",
       "RMSE FV                 0.537             0.000            6.236   \n",
       "MAPE FV                 0.467             0.000          264.414   \n",
       "R2 FV                   0.994             1.000            0.653   \n",
       "RAAE FV                 0.055             0.000            0.413   \n",
       "RMAE FV                 0.237             0.000            1.985   \n",
       "\n",
       "         VALID POLY E200  VALID POLY PRED E200  VALID LSTSQ E200  \n",
       "MAE FV             4.483                 0.438             0.000  \n",
       "RMSE FV            6.189                 0.568             0.000  \n",
       "MAPE FV          258.485                 0.503             0.000  \n",
       "R2 FV              0.658                 0.994             1.000  \n",
       "RAAE FV            0.414                 0.058             0.000  \n",
       "RMAE FV            1.939                 0.247             0.000  \n",
       "\n",
       "[6 rows x 84 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:39.916647Z",
     "start_time": "2020-12-19T14:46:39.872092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEST PRED E1</th>\n",
       "      <th>TEST POLY E1</th>\n",
       "      <th>TEST POLY PRED E1</th>\n",
       "      <th>TEST LSTSQ E1</th>\n",
       "      <th>TEST PRED E10</th>\n",
       "      <th>TEST POLY E10</th>\n",
       "      <th>TEST POLY PRED E10</th>\n",
       "      <th>TEST LSTSQ E10</th>\n",
       "      <th>TEST PRED E20</th>\n",
       "      <th>TEST POLY E20</th>\n",
       "      <th>...</th>\n",
       "      <th>TEST POLY PRED E180</th>\n",
       "      <th>TEST LSTSQ E180</th>\n",
       "      <th>TEST PRED E190</th>\n",
       "      <th>TEST POLY E190</th>\n",
       "      <th>TEST POLY PRED E190</th>\n",
       "      <th>TEST LSTSQ E190</th>\n",
       "      <th>TEST PRED E200</th>\n",
       "      <th>TEST POLY E200</th>\n",
       "      <th>TEST POLY PRED E200</th>\n",
       "      <th>TEST LSTSQ E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAE FV</th>\n",
       "      <td>10.198</td>\n",
       "      <td>10.198</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.726</td>\n",
       "      <td>9.727</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.004</td>\n",
       "      <td>9.005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.548</td>\n",
       "      <td>4.562</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.464</td>\n",
       "      <td>4.477</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE FV</th>\n",
       "      <td>13.006</td>\n",
       "      <td>13.006</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.518</td>\n",
       "      <td>12.517</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.765</td>\n",
       "      <td>11.765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.343</td>\n",
       "      <td>6.300</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.237</td>\n",
       "      <td>6.189</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE FV</th>\n",
       "      <td>1.205</td>\n",
       "      <td>1.205</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.287</td>\n",
       "      <td>1.287</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.539</td>\n",
       "      <td>1.541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.317</td>\n",
       "      <td>2.335</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.282</td>\n",
       "      <td>2.304</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2 FV</th>\n",
       "      <td>-0.413</td>\n",
       "      <td>-0.413</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.994</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAAE FV</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMAE FV</th>\n",
       "      <td>3.142</td>\n",
       "      <td>3.142</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.116</td>\n",
       "      <td>3.116</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.070</td>\n",
       "      <td>3.070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.124</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.098</td>\n",
       "      <td>2.048</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEST PRED E1  TEST POLY E1  TEST POLY PRED E1  TEST LSTSQ E1  \\\n",
       "MAE FV         10.198        10.198              0.014          0.000   \n",
       "RMSE FV        13.006        13.006              0.018          0.000   \n",
       "MAPE FV         1.205         1.205              0.242          0.000   \n",
       "R2 FV          -0.413        -0.413              0.999          1.000   \n",
       "RAAE FV         0.920         0.920              0.024          0.000   \n",
       "RMAE FV         3.142         3.142              0.101          0.000   \n",
       "\n",
       "         TEST PRED E10  TEST POLY E10  TEST POLY PRED E10  TEST LSTSQ E10  \\\n",
       "MAE FV           9.726          9.727               0.017           0.000   \n",
       "RMSE FV         12.518         12.517               0.022           0.000   \n",
       "MAPE FV          1.287          1.287               0.175           0.000   \n",
       "R2 FV           -0.294         -0.294               0.999           1.000   \n",
       "RAAE FV          0.875          0.875               0.026           0.000   \n",
       "RMAE FV          3.116          3.116               0.119           0.000   \n",
       "\n",
       "         TEST PRED E20  TEST POLY E20  ...  TEST POLY PRED E180  \\\n",
       "MAE FV           9.004          9.005  ...                0.388   \n",
       "RMSE FV         11.765         11.765  ...                0.508   \n",
       "MAPE FV          1.539          1.541  ...                0.463   \n",
       "R2 FV           -0.125         -0.125  ...                0.995   \n",
       "RAAE FV          0.807          0.807  ...                0.052   \n",
       "RMAE FV          3.070          3.070  ...                0.245   \n",
       "\n",
       "         TEST LSTSQ E180  TEST PRED E190  TEST POLY E190  TEST POLY PRED E190  \\\n",
       "MAE FV             0.000           4.548           4.562                0.413   \n",
       "RMSE FV            0.000           6.343           6.300                0.539   \n",
       "MAPE FV            0.000           2.317           2.335                0.360   \n",
       "R2 FV              1.000           0.642           0.646                0.994   \n",
       "RAAE FV            0.000           0.420           0.421                0.055   \n",
       "RMAE FV            0.000           2.124           2.078                0.255   \n",
       "\n",
       "         TEST LSTSQ E190  TEST PRED E200  TEST POLY E200  TEST POLY PRED E200  \\\n",
       "MAE FV             0.000           4.464           4.477                0.438   \n",
       "RMSE FV            0.000           6.237           6.189                0.570   \n",
       "MAPE FV            0.000           2.282           2.304                0.619   \n",
       "R2 FV              1.000           0.653           0.658                0.994   \n",
       "RAAE FV            0.000           0.412           0.413                0.058   \n",
       "RMAE FV            0.000           2.098           2.048                0.265   \n",
       "\n",
       "         TEST LSTSQ E200  \n",
       "MAE FV             0.000  \n",
       "RMSE FV            0.000  \n",
       "MAPE FV            0.000  \n",
       "R2 FV              1.000  \n",
       "RAAE FV            0.000  \n",
       "RMAE FV            0.000  \n",
       "\n",
       "[6 rows x 84 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:39.968883Z",
     "start_time": "2020-12-19T14:46:39.918125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA</th>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>...</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.666</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.655</td>\n",
       "      <td>2.531</td>\n",
       "      <td>3.481</td>\n",
       "      <td>4.386</td>\n",
       "      <td>5.165</td>\n",
       "      <td>5.787</td>\n",
       "      <td>6.271</td>\n",
       "      <td>...</td>\n",
       "      <td>6.931</td>\n",
       "      <td>7.156</td>\n",
       "      <td>7.336</td>\n",
       "      <td>7.484</td>\n",
       "      <td>7.608</td>\n",
       "      <td>7.716</td>\n",
       "      <td>7.814</td>\n",
       "      <td>7.904</td>\n",
       "      <td>7.991</td>\n",
       "      <td>8.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.666</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.655</td>\n",
       "      <td>2.530</td>\n",
       "      <td>3.479</td>\n",
       "      <td>4.384</td>\n",
       "      <td>5.162</td>\n",
       "      <td>5.784</td>\n",
       "      <td>6.266</td>\n",
       "      <td>...</td>\n",
       "      <td>6.924</td>\n",
       "      <td>7.148</td>\n",
       "      <td>7.327</td>\n",
       "      <td>7.474</td>\n",
       "      <td>7.597</td>\n",
       "      <td>7.704</td>\n",
       "      <td>7.800</td>\n",
       "      <td>7.889</td>\n",
       "      <td>7.974</td>\n",
       "      <td>8.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>...</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "      <td>11.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA</th>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>...</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA</th>\n",
       "      <td>0.573</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.646</td>\n",
       "      <td>2.515</td>\n",
       "      <td>3.458</td>\n",
       "      <td>4.358</td>\n",
       "      <td>5.133</td>\n",
       "      <td>5.752</td>\n",
       "      <td>6.234</td>\n",
       "      <td>...</td>\n",
       "      <td>6.893</td>\n",
       "      <td>7.118</td>\n",
       "      <td>7.297</td>\n",
       "      <td>7.444</td>\n",
       "      <td>7.567</td>\n",
       "      <td>7.674</td>\n",
       "      <td>7.770</td>\n",
       "      <td>7.859</td>\n",
       "      <td>7.945</td>\n",
       "      <td>8.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.573</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.646</td>\n",
       "      <td>2.514</td>\n",
       "      <td>3.457</td>\n",
       "      <td>4.356</td>\n",
       "      <td>5.131</td>\n",
       "      <td>5.750</td>\n",
       "      <td>6.231</td>\n",
       "      <td>...</td>\n",
       "      <td>6.888</td>\n",
       "      <td>7.112</td>\n",
       "      <td>7.290</td>\n",
       "      <td>7.437</td>\n",
       "      <td>7.559</td>\n",
       "      <td>7.665</td>\n",
       "      <td>7.760</td>\n",
       "      <td>7.848</td>\n",
       "      <td>7.933</td>\n",
       "      <td>8.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>...</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "      <td>11.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA</th>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>...</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA</th>\n",
       "      <td>0.575</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.649</td>\n",
       "      <td>2.521</td>\n",
       "      <td>3.466</td>\n",
       "      <td>4.369</td>\n",
       "      <td>5.146</td>\n",
       "      <td>5.767</td>\n",
       "      <td>6.250</td>\n",
       "      <td>...</td>\n",
       "      <td>6.909</td>\n",
       "      <td>7.133</td>\n",
       "      <td>7.312</td>\n",
       "      <td>7.458</td>\n",
       "      <td>7.581</td>\n",
       "      <td>7.687</td>\n",
       "      <td>7.782</td>\n",
       "      <td>7.870</td>\n",
       "      <td>7.955</td>\n",
       "      <td>8.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.665</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.648</td>\n",
       "      <td>2.519</td>\n",
       "      <td>3.465</td>\n",
       "      <td>4.366</td>\n",
       "      <td>5.143</td>\n",
       "      <td>5.763</td>\n",
       "      <td>6.245</td>\n",
       "      <td>...</td>\n",
       "      <td>6.903</td>\n",
       "      <td>7.127</td>\n",
       "      <td>7.305</td>\n",
       "      <td>7.451</td>\n",
       "      <td>7.572</td>\n",
       "      <td>7.677</td>\n",
       "      <td>7.772</td>\n",
       "      <td>7.859</td>\n",
       "      <td>7.943</td>\n",
       "      <td>8.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STD FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>...</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "      <td>11.208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        E1    E10    E20    E30    E40    E50  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.220 11.220 11.220 11.220 11.220 11.220   \n",
       "STD FV TRAIN PRED LAMBDA             0.574  0.666  1.010  1.655  2.531  3.481   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  0.574  0.666  1.010  1.655  2.530  3.479   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.220 11.220 11.220 11.220 11.220 11.220   \n",
       "STD FV VALID REAL LAMBDA            11.210 11.210 11.210 11.210 11.210 11.210   \n",
       "STD FV VALID PRED LAMBDA             0.573  0.664  1.007  1.646  2.515  3.458   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  0.573  0.664  1.006  1.646  2.514  3.457   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.210 11.210 11.210 11.210 11.210 11.210   \n",
       "STD FV TEST REAL LAMBDA             11.208 11.208 11.208 11.208 11.208 11.208   \n",
       "STD FV TEST PRED LAMBDA              0.575  0.665  1.008  1.649  2.521  3.466   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   0.574  0.665  1.007  1.648  2.519  3.465   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.208 11.208 11.208 11.208 11.208 11.208   \n",
       "\n",
       "                                       E60    E70    E80    E90  ...   E110  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.220 11.220 11.220 11.220  ... 11.220   \n",
       "STD FV TRAIN PRED LAMBDA             4.386  5.165  5.787  6.271  ...  6.931   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  4.384  5.162  5.784  6.266  ...  6.924   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.220 11.220 11.220 11.220  ... 11.220   \n",
       "STD FV VALID REAL LAMBDA            11.210 11.210 11.210 11.210  ... 11.210   \n",
       "STD FV VALID PRED LAMBDA             4.358  5.133  5.752  6.234  ...  6.893   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  4.356  5.131  5.750  6.231  ...  6.888   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.210 11.210 11.210 11.210  ... 11.210   \n",
       "STD FV TEST REAL LAMBDA             11.208 11.208 11.208 11.208  ... 11.208   \n",
       "STD FV TEST PRED LAMBDA              4.369  5.146  5.767  6.250  ...  6.909   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   4.366  5.143  5.763  6.245  ...  6.903   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.208 11.208 11.208 11.208  ... 11.208   \n",
       "\n",
       "                                      E120   E130   E140   E150   E160   E170  \\\n",
       "STD FV TRAIN REAL LAMBDA            11.220 11.220 11.220 11.220 11.220 11.220   \n",
       "STD FV TRAIN PRED LAMBDA             7.156  7.336  7.484  7.608  7.716  7.814   \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.148  7.327  7.474  7.597  7.704  7.800   \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.220 11.220 11.220 11.220 11.220 11.220   \n",
       "STD FV VALID REAL LAMBDA            11.210 11.210 11.210 11.210 11.210 11.210   \n",
       "STD FV VALID PRED LAMBDA             7.118  7.297  7.444  7.567  7.674  7.770   \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.112  7.290  7.437  7.559  7.665  7.760   \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.210 11.210 11.210 11.210 11.210 11.210   \n",
       "STD FV TEST REAL LAMBDA             11.208 11.208 11.208 11.208 11.208 11.208   \n",
       "STD FV TEST PRED LAMBDA              7.133  7.312  7.458  7.581  7.687  7.782   \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.127  7.305  7.451  7.572  7.677  7.772   \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.208 11.208 11.208 11.208 11.208 11.208   \n",
       "\n",
       "                                      E180   E190   E200  \n",
       "STD FV TRAIN REAL LAMBDA            11.220 11.220 11.220  \n",
       "STD FV TRAIN PRED LAMBDA             7.904  7.991  8.073  \n",
       "STD FV TRAIN PRED LAMBDA POLY LSTSQ  7.889  7.974  8.054  \n",
       "STD FV TRAIN REAL LAMBDA POLY LSTSQ 11.220 11.220 11.220  \n",
       "STD FV VALID REAL LAMBDA            11.210 11.210 11.210  \n",
       "STD FV VALID PRED LAMBDA             7.859  7.945  8.025  \n",
       "STD FV VALID PRED LAMBDA POLY LSTSQ  7.848  7.933  8.012  \n",
       "STD FV VALID REAL LAMBDA POLY LSTSQ 11.210 11.210 11.210  \n",
       "STD FV TEST REAL LAMBDA             11.208 11.208 11.208  \n",
       "STD FV TEST PRED LAMBDA              7.870  7.955  8.035  \n",
       "STD FV TEST PRED LAMBDA POLY LSTSQ   7.859  7.943  8.021  \n",
       "STD FV TEST REAL LAMBDA POLY LSTSQ  11.208 11.208 11.208  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:40.022846Z",
     "start_time": "2020-12-19T14:46:39.970380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>E10</th>\n",
       "      <th>E20</th>\n",
       "      <th>E30</th>\n",
       "      <th>E40</th>\n",
       "      <th>E50</th>\n",
       "      <th>E60</th>\n",
       "      <th>E70</th>\n",
       "      <th>E80</th>\n",
       "      <th>E90</th>\n",
       "      <th>...</th>\n",
       "      <th>E110</th>\n",
       "      <th>E120</th>\n",
       "      <th>E130</th>\n",
       "      <th>E140</th>\n",
       "      <th>E150</th>\n",
       "      <th>E160</th>\n",
       "      <th>E170</th>\n",
       "      <th>E180</th>\n",
       "      <th>E190</th>\n",
       "      <th>E200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA</th>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-0.193</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>-0.181</td>\n",
       "      <td>-0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TRAIN REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>-0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA</th>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>-0.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV VALID REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA</th>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.218</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST PRED LAMBDA POLY LSTSQ</th>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN FV TEST REAL LAMBDA POLY LSTSQ</th>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         E1    E10    E20    E30    E40  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.183 -0.183 -0.183 -0.183 -0.183   \n",
       "MEAN FV TRAIN PRED LAMBDA             0.005 -0.017 -0.038 -0.066 -0.136   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ  0.005 -0.017 -0.038 -0.066 -0.136   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.183 -0.183 -0.183 -0.183 -0.183   \n",
       "MEAN FV VALID REAL LAMBDA            -0.239 -0.239 -0.239 -0.239 -0.239   \n",
       "MEAN FV VALID PRED LAMBDA             0.005 -0.017 -0.041 -0.075 -0.154   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ  0.005 -0.017 -0.041 -0.075 -0.154   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.239 -0.239 -0.239 -0.239 -0.239   \n",
       "MEAN FV TEST REAL LAMBDA             -0.225 -0.225 -0.225 -0.225 -0.225   \n",
       "MEAN FV TEST PRED LAMBDA              0.006 -0.017 -0.039 -0.069 -0.141   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ   0.006 -0.017 -0.039 -0.069 -0.141   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.225 -0.225 -0.225 -0.225 -0.225   \n",
       "\n",
       "                                        E50    E60    E70    E80    E90  ...  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.183 -0.183 -0.183 -0.183 -0.183  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.184 -0.193 -0.195 -0.197 -0.201  ...   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.184 -0.193 -0.195 -0.197 -0.201  ...   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.183 -0.183 -0.183 -0.183 -0.183  ...   \n",
       "MEAN FV VALID REAL LAMBDA            -0.239 -0.239 -0.239 -0.239 -0.239  ...   \n",
       "MEAN FV VALID PRED LAMBDA            -0.212 -0.230 -0.238 -0.244 -0.252  ...   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.212 -0.230 -0.237 -0.244 -0.251  ...   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.239 -0.239 -0.239 -0.239 -0.239  ...   \n",
       "MEAN FV TEST REAL LAMBDA             -0.225 -0.225 -0.225 -0.225 -0.225  ...   \n",
       "MEAN FV TEST PRED LAMBDA             -0.192 -0.204 -0.208 -0.212 -0.220  ...   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.192 -0.204 -0.208 -0.212 -0.220  ...   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.225 -0.225 -0.225 -0.225 -0.225  ...   \n",
       "\n",
       "                                       E110   E120   E130   E140   E150  \\\n",
       "MEAN FV TRAIN REAL LAMBDA            -0.183 -0.183 -0.183 -0.183 -0.183   \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.196 -0.191 -0.186 -0.183 -0.179   \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.196 -0.191 -0.186 -0.183 -0.179   \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.183 -0.183 -0.183 -0.183 -0.183   \n",
       "MEAN FV VALID REAL LAMBDA            -0.239 -0.239 -0.239 -0.239 -0.239   \n",
       "MEAN FV VALID PRED LAMBDA            -0.250 -0.245 -0.240 -0.236 -0.233   \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.249 -0.244 -0.239 -0.235 -0.232   \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.239 -0.239 -0.239 -0.239 -0.239   \n",
       "MEAN FV TEST REAL LAMBDA             -0.225 -0.225 -0.225 -0.225 -0.225   \n",
       "MEAN FV TEST PRED LAMBDA             -0.221 -0.218 -0.214 -0.211 -0.208   \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.221 -0.217 -0.213 -0.211 -0.208   \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.225 -0.225 -0.225 -0.225 -0.225   \n",
       "\n",
       "                                       E160   E170   E180   E190   E200  \n",
       "MEAN FV TRAIN REAL LAMBDA            -0.183 -0.183 -0.183 -0.183 -0.183  \n",
       "MEAN FV TRAIN PRED LAMBDA            -0.178 -0.178 -0.178 -0.181 -0.180  \n",
       "MEAN FV TRAIN PRED LAMBDA POLY LSTSQ -0.178 -0.178 -0.178 -0.181 -0.180  \n",
       "MEAN FV TRAIN REAL LAMBDA POLY LSTSQ -0.183 -0.183 -0.183 -0.183 -0.183  \n",
       "MEAN FV VALID REAL LAMBDA            -0.239 -0.239 -0.239 -0.239 -0.239  \n",
       "MEAN FV VALID PRED LAMBDA            -0.233 -0.233 -0.234 -0.237 -0.236  \n",
       "MEAN FV VALID PRED LAMBDA POLY LSTSQ -0.232 -0.232 -0.232 -0.235 -0.233  \n",
       "MEAN FV VALID REAL LAMBDA POLY LSTSQ -0.239 -0.239 -0.239 -0.239 -0.239  \n",
       "MEAN FV TEST REAL LAMBDA             -0.225 -0.225 -0.225 -0.225 -0.225  \n",
       "MEAN FV TEST PRED LAMBDA             -0.208 -0.209 -0.209 -0.212 -0.212  \n",
       "MEAN FV TEST PRED LAMBDA POLY LSTSQ  -0.208 -0.209 -0.209 -0.213 -0.212  \n",
       "MEAN FV TEST REAL LAMBDA POLY LSTSQ  -0.225 -0.225 -0.225 -0.225 -0.225  \n",
       "\n",
       "[12 rows x 21 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Lambda-Net Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:46:40.102563Z",
     "start_time": "2020-12-19T14:46:40.024396Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_lambda_preds(i, \n",
    "                      lambda_indices,\n",
    "                      y_train_real_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_by_epoch, \n",
    "                      y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_train_lambda_by_epoch, \n",
    "                      y_valid_real_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_by_epoch, \n",
    "                      y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_valid_lambda_by_epoch, \n",
    "                      y_test_real_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_by_epoch, \n",
    "                      y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                      X_test_lambda_by_epoch):\n",
    "    \n",
    "    \n",
    "    index = (i+1)*each_epochs_save if each_epochs_save==1 else i*each_epochs_save if i > 1 else each_epochs_save if i==1 else 1\n",
    "        \n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_by_epoch, y_train_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_by_epoch, y_valid_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_by_epoch, y_test_real_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_by_epoch, y_train_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_by_epoch, y_test_pred_lambda_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_by_epoch, y_train_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_by_epoch, y_valid_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_by_epoch[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_by_epoch, y_test_pred_lambda_poly_lstsq_by_epoch)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_by_epoch[0].shape[0])]).ravel())    \n",
    "\n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "\n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + 'epoch_' + str(index).zfill(3) + '_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)         \n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)    \n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False) \n",
    "\n",
    "    return y_train_real_lambda_df, y_valid_real_lambda_df, y_test_real_lambda_df, y_train_pred_lambda_df, y_valid_pred_lambda_df, y_test_pred_lambda_df, y_train_pred_lambda_poly_lstsq_df, y_valid_pred_lambda_poly_lstsq_df, y_test_pred_lambda_poly_lstsq_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:47:58.229558Z",
     "start_time": "2020-12-19T14:46:40.104366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98345ebb29b145efbbbe2176b9f3c0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend MultiprocessingBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   2 out of  21 | elapsed:  1.0min remaining:  9.6min\n",
      "[Parallel(n_jobs=-3)]: Done  10 out of  21 | elapsed:  1.1min remaining:  1.3min\n",
      "[Parallel(n_jobs=-3)]: Done  18 out of  21 | elapsed:  1.2min remaining:   11.9s\n",
      "[Parallel(n_jobs=-3)]: Done  21 out of  21 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "if each_epochs_save == None:\n",
    "    \n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    lambda_index_list = [clf[0][0] for clf in clf_list]\n",
    "    lambda_seed_list = [clf[0][1] for clf in clf_list] \n",
    "    polynomial_real_list = [clf[0][2] for clf in clf_list] \n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][3] for clf in clf_list] \n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][4] for clf in clf_list] \n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "\n",
    "    lambda_indices_list = np.zeros((len(clf_list), 1))\n",
    "    y_train_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][1])))\n",
    "    y_train_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][2])))\n",
    "    y_train_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][3])))\n",
    "    X_train_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][4].shape)]][0])\n",
    "    y_valid_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][5])))\n",
    "    y_valid_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][6])))\n",
    "    y_valid_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][7])))\n",
    "    X_valid_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][8].shape)]][0])\n",
    "    y_test_real_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][9])))\n",
    "    y_test_pred_lambda_list = np.zeros((len(clf_list), len(clf_list[0][2][10])))\n",
    "    y_test_pred_lambda_poly_lstsq_list = np.zeros((len(clf_list), len(clf_list[0][2][11])))\n",
    "    X_test_lambda_list = np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][12].shape)]][0])\n",
    "\n",
    "    for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate([clf[2] for clf in clf_list]):\n",
    "        lambda_indices_list[index] = lambda_indices\n",
    "        y_train_real_lambda_list[index] = y_train_real_lambda.ravel()\n",
    "        y_train_pred_lambda_list[index] = y_train_pred_lambda.ravel()\n",
    "        y_train_pred_lambda_poly_lstsq_list[index] = y_train_pred_lambda_poly_lstsq.ravel()\n",
    "        X_train_lambda_list[index] = X_train_lambda#.ravel()\n",
    "\n",
    "        y_valid_real_lambda_list[index] = y_valid_real_lambda.ravel()\n",
    "        y_valid_pred_lambda_list[index] = y_valid_pred_lambda.ravel()\n",
    "        y_valid_pred_lambda_poly_lstsq_list[index] = y_valid_pred_lambda_poly_lstsq.ravel()\n",
    "        X_valid_lambda_list[index] = X_valid_lambda#.ravel()\n",
    "\n",
    "        y_test_real_lambda_list[index] = y_test_real_lambda.ravel()\n",
    "        y_test_pred_lambda_list[index] = y_test_pred_lambda.ravel()\n",
    "        y_test_pred_lambda_poly_lstsq_list[index] = y_test_pred_lambda_poly_lstsq.ravel()\n",
    "        X_test_lambda_list[index] = X_test_lambda#.ravel()\n",
    "    \n",
    "    #add x_data before each pred\n",
    "    y_train_real_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_real_lambda.reshape(len(y_train_real_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_real_lambda in zip(X_train_lambda_list, y_train_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_real_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_real_lambda.reshape(len(y_valid_real_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_real_lambda in zip(X_valid_lambda_list, y_valid_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_real_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_real_lambda.reshape(len(y_test_real_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_real_lambda in zip(X_test_lambda_list, y_test_real_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda.reshape(len(y_train_pred_lambda),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda in zip(X_train_lambda_list, y_train_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda.reshape(len(y_valid_pred_lambda),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda in zip(X_valid_lambda_list, y_valid_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda.reshape(len(y_test_pred_lambda),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda in zip(X_test_lambda_list, y_test_pred_lambda_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_train_lambda, y_train_pred_lambda_poly_lstsq.reshape(len(y_train_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_train_lambda, y_train_pred_lambda_poly_lstsq in zip(X_train_lambda_list, y_train_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_train_lambda_list[0].shape[0])]).ravel())\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_valid_lambda, y_valid_pred_lambda_poly_lstsq.reshape(len(y_valid_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_valid_lambda, y_valid_pred_lambda_poly_lstsq in zip(X_valid_lambda_list, y_valid_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_valid_lambda_list[0].shape[0])]).ravel())\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.DataFrame([np.concatenate([X_test_lambda, y_test_pred_lambda_poly_lstsq.reshape(len(y_test_pred_lambda_poly_lstsq),1)], axis=1).ravel() for X_test_lambda, y_test_pred_lambda_poly_lstsq in zip(X_test_lambda_list, y_test_pred_lambda_poly_lstsq_list)], columns=np.array([[variable_name + str(i+1) for variable_name in variable_names_list] for i in range(X_test_lambda_list[0].shape[0])]).ravel())    \n",
    "    \n",
    "    y_train_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_real_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_real_lambda_df], axis=1)], axis=1)\n",
    "    y_test_real_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_real_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_df], axis=1)], axis=1)\n",
    "    y_train_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_train_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_valid_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_valid_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "    y_test_pred_lambda_poly_lstsq_df = pd.concat([pd.DataFrame(lambda_indices_list, columns=['lambda_index']), pd.concat([polynomial_real_df, y_test_pred_lambda_poly_lstsq_df], axis=1)], axis=1)\n",
    "       \n",
    "    path_y_train_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_real_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_real_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_train_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_train_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_valid_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_valid_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "    path_y_test_pred_lambda_poly_lstsq = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/y_test_pred_lambda_poly_lstsq_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "   \n",
    "    y_train_real_lambda_df.to_csv(path_y_train_real_lambda, sep=',', index=False)\n",
    "    y_valid_real_lambda_df.to_csv(path_y_valid_real_lambda, sep=',', index=False)\n",
    "    y_test_real_lambda_df.to_csv(path_y_test_real_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_df.to_csv(path_y_train_pred_lambda, sep=',', index=False)\n",
    "    y_valid_pred_lambda_df.to_csv(path_y_valid_pred_lambda, sep=',', index=False)\n",
    "    y_test_pred_lambda_df.to_csv(path_y_test_pred_lambda, sep=',', index=False)\n",
    "    y_train_pred_lambda_poly_lstsq_df.to_csv(path_y_train_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_valid_pred_lambda_poly_lstsq_df.to_csv(path_y_valid_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    y_test_pred_lambda_poly_lstsq_df.to_csv(path_y_test_pred_lambda_poly_lstsq, sep=',', index=False)\n",
    "    \n",
    "else:\n",
    "    variable_names = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    variable_names_list = [variable + '_' for variable in variable_names[:n]]\n",
    "    variable_names_list.append('FV_')\n",
    "    \n",
    "    polynomial_real_list = [clf[0][0] for clf in clf_list]\n",
    "    polynomial_lstsq_pred_lambda_list = [clf[0][1] for clf in clf_list]\n",
    "    polynomial_lstsq_real_lambda_list = [clf[0][2] for clf in clf_list]\n",
    "    \n",
    "    polynomial_real_df = pd.DataFrame(polynomial_real_list)\n",
    "    \n",
    "    lambda_indices_list = [np.zeros((len(clf_list), 1)) for i in epochs_save_range]\n",
    "    y_train_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][1]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][2]), 1)) for i in epochs_save_range]\n",
    "    y_train_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][3]), 1)) for i in epochs_save_range]\n",
    "    X_train_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][4].shape)]][0]) for i in epochs_save_range]\n",
    "    y_valid_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][5]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][6]), 1)) for i in epochs_save_range]\n",
    "    y_valid_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][7]), 1)) for i in epochs_save_range]\n",
    "    X_valid_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][8].shape)]][0]) for i in epochs_save_range]\n",
    "    y_test_real_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][9]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][10]), 1)) for i in epochs_save_range]\n",
    "    y_test_pred_lambda_poly_lstsq_list = [np.zeros((len(clf_list), len(clf_list[0][2][0][12]), 1)) for i in epochs_save_range]\n",
    "    X_test_lambda_list = [np.zeros([(a, *rest) for a, rest in [(len(clf_list), clf_list[0][2][0][12].shape)]][0]) for i in epochs_save_range]\n",
    "    \n",
    "    for i, y_data_list_per_epoch in tqdm(enumerate([clf[2] for clf in clf_list]), total=len(clf_list)):\n",
    "        \n",
    "        for index, (lambda_indices, y_train_real_lambda, y_train_pred_lambda, y_train_pred_lambda_poly_lstsq, X_train_lambda, y_valid_real_lambda, y_valid_pred_lambda, y_valid_pred_lambda_poly_lstsq, X_valid_lambda, y_test_real_lambda, y_test_pred_lambda, y_test_pred_lambda_poly_lstsq, X_test_lambda) in enumerate(y_data_list_per_epoch):\n",
    "            lambda_indices_list[index][i] = lambda_indices\n",
    "            y_train_real_lambda_list[index][i] = y_train_real_lambda#.ravel()\n",
    "            y_train_pred_lambda_list[index][i] = y_train_pred_lambda#.ravel()\n",
    "            y_train_pred_lambda_poly_lstsq_list[index][i] = y_train_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_train_lambda_list[index][i] = X_train_lambda#.ravel()\n",
    "            \n",
    "            y_valid_real_lambda_list[index][i] = y_valid_real_lambda#.ravel()\n",
    "            y_valid_pred_lambda_list[index][i] = y_valid_pred_lambda#.ravel()\n",
    "            y_valid_pred_lambda_poly_lstsq_list[index][i] = y_valid_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_valid_lambda_list[index][i] = X_valid_lambda#.ravel()\n",
    "            \n",
    "            y_test_real_lambda_list[index][i] = y_test_real_lambda#.ravel()\n",
    "            y_test_pred_lambda_list[index][i] = y_test_pred_lambda#.ravel()\n",
    "            y_test_pred_lambda_poly_lstsq_list[index][i] = y_test_pred_lambda_poly_lstsq#.ravel()\n",
    "            X_test_lambda_list[index][i] = X_test_lambda#.ravel()\n",
    "    \n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='multiprocessing')\n",
    "    y_data_lambda_list = parallel(delayed(save_lambda_preds)(i, \n",
    "                                                           lambda_indices,\n",
    "                                                           y_train_real_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_by_epoch, \n",
    "                                                           y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_train_lambda_by_epoch, \n",
    "                                                           y_valid_real_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_by_epoch, \n",
    "                                                           y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_valid_lambda_by_epoch, \n",
    "                                                           y_test_real_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_by_epoch, \n",
    "                                                           y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                           X_test_lambda_by_epoch) for i, \n",
    "                                                                                        (lambda_indices,\n",
    "                                                                                         y_train_real_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_by_epoch, \n",
    "                                                                                         y_train_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_train_lambda_by_epoch, \n",
    "                                                                                         y_valid_real_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_by_epoch, \n",
    "                                                                                         y_valid_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_valid_lambda_by_epoch, \n",
    "                                                                                         y_test_real_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_by_epoch, \n",
    "                                                                                         y_test_pred_lambda_poly_lstsq_by_epoch, \n",
    "                                                                                         X_test_lambda_by_epoch) in enumerate(zip(lambda_indices_list,\n",
    "                                                                                                                                  y_train_real_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_list, \n",
    "                                                                                                                                  y_train_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_train_lambda_list, \n",
    "                                                                                                                                  y_valid_real_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_list, \n",
    "                                                                                                                                  y_valid_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_valid_lambda_list, \n",
    "                                                                                                                                  y_test_real_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_list, \n",
    "                                                                                                                                  y_test_pred_lambda_poly_lstsq_list, \n",
    "                                                                                                                                  X_test_lambda_list)))  \n",
    "    y_test_real_lambda_df = y_data_lambda_list[-1][2]\n",
    "    y_test_pred_lambda_df = y_data_lambda_list[-1][5]\n",
    "    y_test_pred_lambda_poly_lstsq_df = y_data_lambda_list[-1][8]\n",
    "    del parallel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:47:58.254226Z",
     "start_time": "2020-12-19T14:47:58.231874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>d_1</th>\n",
       "      <th>FV_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>c_2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-29.297</td>\n",
       "      <td>0.590</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>0.760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>1.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-35.619</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-2.749</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>-26.317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-9.850</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-11.731</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-12.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.680</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>14.664</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.150</td>\n",
       "      <td>5.818</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>0.490</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-22.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-7.303</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.920</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.802</td>\n",
       "      <td>-0.790</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>19.133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index  0    a_1    b_1    c_1    d_1    FV_1    a_2    b_2    c_2  \\\n",
       "0         0.000  0  0.430  0.220  0.880 -0.930 -29.297  0.590 -0.960  0.760   \n",
       "1         1.000  1 -0.750 -0.060 -0.440 -0.880 -35.619  0.930  0.370 -0.170   \n",
       "2         2.000  2  0.970 -0.720 -0.560  0.900  -9.850 -0.520 -0.890  0.340   \n",
       "3         3.000  3  0.680 -0.810 -0.190 -0.780  14.664  0.380 -0.550 -0.180   \n",
       "4         4.000  4 -0.170 -0.730  0.610  0.430  -7.303  0.060  0.230  0.320   \n",
       "\n",
       "   ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  ...  0.470 -0.760 -0.550 -0.700  -0.225 -0.190 -0.540  0.170 -0.010   1.967  \n",
       "1  ... -0.890 -0.450  0.710  0.240  -2.749 -0.330 -0.410 -0.400 -0.710 -26.317  \n",
       "2  ... -0.490  0.330 -0.100  0.000 -11.731 -0.010 -0.010  0.270 -0.290 -12.293  \n",
       "3  ...  0.430 -0.750  0.220  0.150   5.818 -0.810  0.490 -0.810  0.660 -22.838  \n",
       "4  ...  0.610  0.920 -0.870  0.560   1.802 -0.790 -0.760 -0.040 -0.770  19.133  \n",
       "\n",
       "[5 rows x 1252 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:47:58.281545Z",
     "start_time": "2020-12-19T14:47:58.261667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>d_1</th>\n",
       "      <th>FV_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>c_2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-5.386</td>\n",
       "      <td>0.590</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>0.760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-3.223</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-5.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-34.910</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-7.374</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>-27.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-10.695</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-17.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.680</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>6.580</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.150</td>\n",
       "      <td>6.145</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>0.490</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-18.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-6.196</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.920</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-1.196</td>\n",
       "      <td>-0.790</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>8.785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index  0    a_1    b_1    c_1    d_1    FV_1    a_2    b_2    c_2  \\\n",
       "0         0.000  0  0.430  0.220  0.880 -0.930  -5.386  0.590 -0.960  0.760   \n",
       "1         1.000  1 -0.750 -0.060 -0.440 -0.880 -34.910  0.930  0.370 -0.170   \n",
       "2         2.000  2  0.970 -0.720 -0.560  0.900  -0.736 -0.520 -0.890  0.340   \n",
       "3         3.000  3  0.680 -0.810 -0.190 -0.780   6.580  0.380 -0.550 -0.180   \n",
       "4         4.000  4 -0.170 -0.730  0.610  0.430  -6.196  0.060  0.230  0.320   \n",
       "\n",
       "   ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  ...  0.470 -0.760 -0.550 -0.700  -3.223 -0.190 -0.540  0.170 -0.010  -5.478  \n",
       "1  ... -0.890 -0.450  0.710  0.240  -7.374 -0.330 -0.410 -0.400 -0.710 -27.513  \n",
       "2  ... -0.490  0.330 -0.100  0.000 -10.695 -0.010 -0.010  0.270 -0.290 -17.138  \n",
       "3  ...  0.430 -0.750  0.220  0.150   6.145 -0.810  0.490 -0.810  0.660 -18.126  \n",
       "4  ...  0.610  0.920 -0.870  0.560  -1.196 -0.790 -0.760 -0.040 -0.770   8.785  \n",
       "\n",
       "[5 rows x 1252 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:47:58.303108Z",
     "start_time": "2020-12-19T14:47:58.283762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda_index</th>\n",
       "      <th>0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>c_1</th>\n",
       "      <th>d_1</th>\n",
       "      <th>FV_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>c_2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_249</th>\n",
       "      <th>b_249</th>\n",
       "      <th>c_249</th>\n",
       "      <th>d_249</th>\n",
       "      <th>FV_249</th>\n",
       "      <th>a_250</th>\n",
       "      <th>b_250</th>\n",
       "      <th>c_250</th>\n",
       "      <th>d_250</th>\n",
       "      <th>FV_250</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>-5.642</td>\n",
       "      <td>0.590</td>\n",
       "      <td>-0.960</td>\n",
       "      <td>0.760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-2.526</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.170</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-5.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>-0.880</td>\n",
       "      <td>-35.010</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.240</td>\n",
       "      <td>-7.311</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.410</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.710</td>\n",
       "      <td>-27.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.970</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-0.890</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>0.330</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-11.017</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.270</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-17.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.680</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>6.839</td>\n",
       "      <td>0.380</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.150</td>\n",
       "      <td>5.710</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>0.490</td>\n",
       "      <td>-0.810</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-18.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.000</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.430</td>\n",
       "      <td>-5.794</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.920</td>\n",
       "      <td>-0.870</td>\n",
       "      <td>0.560</td>\n",
       "      <td>-1.029</td>\n",
       "      <td>-0.790</td>\n",
       "      <td>-0.760</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>8.421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lambda_index  0    a_1    b_1    c_1    d_1    FV_1    a_2    b_2    c_2  \\\n",
       "0         0.000  0  0.430  0.220  0.880 -0.930  -5.642  0.590 -0.960  0.760   \n",
       "1         1.000  1 -0.750 -0.060 -0.440 -0.880 -35.010  0.930  0.370 -0.170   \n",
       "2         2.000  2  0.970 -0.720 -0.560  0.900  -0.305 -0.520 -0.890  0.340   \n",
       "3         3.000  3  0.680 -0.810 -0.190 -0.780   6.839  0.380 -0.550 -0.180   \n",
       "4         4.000  4 -0.170 -0.730  0.610  0.430  -5.794  0.060  0.230  0.320   \n",
       "\n",
       "   ...  a_249  b_249  c_249  d_249  FV_249  a_250  b_250  c_250  d_250  FV_250  \n",
       "0  ...  0.470 -0.760 -0.550 -0.700  -2.526 -0.190 -0.540  0.170 -0.010  -5.922  \n",
       "1  ... -0.890 -0.450  0.710  0.240  -7.311 -0.330 -0.410 -0.400 -0.710 -27.553  \n",
       "2  ... -0.490  0.330 -0.100  0.000 -11.017 -0.010 -0.010  0.270 -0.290 -17.782  \n",
       "3  ...  0.430 -0.750  0.220  0.150   5.710 -0.810  0.490 -0.810  0.660 -18.898  \n",
       "4  ...  0.610  0.920 -0.870  0.560  -1.029 -0.790 -0.760 -0.040 -0.770   8.421  \n",
       "\n",
       "[5 rows x 1252 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_lambda_poly_lstsq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:47:58.706556Z",
     "start_time": "2020-12-19T14:47:58.304676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1378bbdcbe4f29b657a734fc5bb49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    entry = entry[3]\n",
    "    loss_list = []\n",
    "    metric_list = []\n",
    "    val_loss_list = []\n",
    "    val_metric_list = []\n",
    "    for i in range(epochs):  \n",
    "        loss_list.append(entry['loss'][i])\n",
    "        metric_list.append(entry['mean_absolute_percentage_error_keras'][i])\n",
    "        val_loss_list.append(entry['val_loss'][i])\n",
    "        val_metric_list.append(entry['val_mean_absolute_percentage_error_keras'][i])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=loss_list_total, columns=['loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "metric_df = pd.DataFrame(data=metric_list_total, columns=['metric_epoch_' + str(i+1) for i in range(epochs)]) \n",
    "val_loss_df = pd.DataFrame(data=val_loss_list_total, columns=['val_loss_epoch_' + str(i+1) for i in range(epochs)])\n",
    "val_metric_df = pd.DataFrame(data=val_metric_list_total, columns=['val_metric_epoch_' + str(i+1) for i in range(epochs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:48:00.757604Z",
     "start_time": "2020-12-19T14:47:58.708421Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_loss_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/history_val_metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:48:01.312043Z",
     "start_time": "2020-12-19T14:48:00.759600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_epoch_1</th>\n",
       "      <th>loss_epoch_2</th>\n",
       "      <th>loss_epoch_3</th>\n",
       "      <th>loss_epoch_4</th>\n",
       "      <th>loss_epoch_5</th>\n",
       "      <th>loss_epoch_6</th>\n",
       "      <th>loss_epoch_7</th>\n",
       "      <th>loss_epoch_8</th>\n",
       "      <th>loss_epoch_9</th>\n",
       "      <th>loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_epoch_191</th>\n",
       "      <th>loss_epoch_192</th>\n",
       "      <th>loss_epoch_193</th>\n",
       "      <th>loss_epoch_194</th>\n",
       "      <th>loss_epoch_195</th>\n",
       "      <th>loss_epoch_196</th>\n",
       "      <th>loss_epoch_197</th>\n",
       "      <th>loss_epoch_198</th>\n",
       "      <th>loss_epoch_199</th>\n",
       "      <th>loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.206</td>\n",
       "      <td>10.154</td>\n",
       "      <td>10.103</td>\n",
       "      <td>10.053</td>\n",
       "      <td>10.002</td>\n",
       "      <td>9.950</td>\n",
       "      <td>9.898</td>\n",
       "      <td>9.845</td>\n",
       "      <td>9.790</td>\n",
       "      <td>9.734</td>\n",
       "      <td>...</td>\n",
       "      <td>4.428</td>\n",
       "      <td>4.419</td>\n",
       "      <td>4.410</td>\n",
       "      <td>4.401</td>\n",
       "      <td>4.392</td>\n",
       "      <td>4.383</td>\n",
       "      <td>4.375</td>\n",
       "      <td>4.366</td>\n",
       "      <td>4.357</td>\n",
       "      <td>4.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.437</td>\n",
       "      <td>2.414</td>\n",
       "      <td>2.391</td>\n",
       "      <td>2.368</td>\n",
       "      <td>2.344</td>\n",
       "      <td>2.321</td>\n",
       "      <td>2.297</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.247</td>\n",
       "      <td>2.220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.286</td>\n",
       "      <td>5.268</td>\n",
       "      <td>5.251</td>\n",
       "      <td>5.233</td>\n",
       "      <td>5.215</td>\n",
       "      <td>5.198</td>\n",
       "      <td>5.180</td>\n",
       "      <td>5.162</td>\n",
       "      <td>5.144</td>\n",
       "      <td>5.125</td>\n",
       "      <td>...</td>\n",
       "      <td>2.824</td>\n",
       "      <td>2.818</td>\n",
       "      <td>2.811</td>\n",
       "      <td>2.803</td>\n",
       "      <td>2.797</td>\n",
       "      <td>2.790</td>\n",
       "      <td>2.783</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.771</td>\n",
       "      <td>2.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.362</td>\n",
       "      <td>8.321</td>\n",
       "      <td>8.275</td>\n",
       "      <td>8.254</td>\n",
       "      <td>8.222</td>\n",
       "      <td>8.182</td>\n",
       "      <td>8.152</td>\n",
       "      <td>8.107</td>\n",
       "      <td>8.066</td>\n",
       "      <td>8.030</td>\n",
       "      <td>...</td>\n",
       "      <td>4.040</td>\n",
       "      <td>4.034</td>\n",
       "      <td>4.025</td>\n",
       "      <td>4.017</td>\n",
       "      <td>4.010</td>\n",
       "      <td>3.998</td>\n",
       "      <td>3.989</td>\n",
       "      <td>3.978</td>\n",
       "      <td>3.972</td>\n",
       "      <td>3.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.874</td>\n",
       "      <td>9.819</td>\n",
       "      <td>9.775</td>\n",
       "      <td>9.719</td>\n",
       "      <td>9.679</td>\n",
       "      <td>9.636</td>\n",
       "      <td>9.596</td>\n",
       "      <td>9.554</td>\n",
       "      <td>9.511</td>\n",
       "      <td>9.477</td>\n",
       "      <td>...</td>\n",
       "      <td>4.381</td>\n",
       "      <td>4.367</td>\n",
       "      <td>4.362</td>\n",
       "      <td>4.353</td>\n",
       "      <td>4.342</td>\n",
       "      <td>4.332</td>\n",
       "      <td>4.328</td>\n",
       "      <td>4.323</td>\n",
       "      <td>4.313</td>\n",
       "      <td>4.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.638</td>\n",
       "      <td>11.579</td>\n",
       "      <td>11.535</td>\n",
       "      <td>11.470</td>\n",
       "      <td>11.422</td>\n",
       "      <td>11.345</td>\n",
       "      <td>11.265</td>\n",
       "      <td>11.186</td>\n",
       "      <td>11.126</td>\n",
       "      <td>11.078</td>\n",
       "      <td>...</td>\n",
       "      <td>4.763</td>\n",
       "      <td>4.756</td>\n",
       "      <td>4.747</td>\n",
       "      <td>4.736</td>\n",
       "      <td>4.724</td>\n",
       "      <td>4.716</td>\n",
       "      <td>4.707</td>\n",
       "      <td>4.695</td>\n",
       "      <td>4.683</td>\n",
       "      <td>4.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.239</td>\n",
       "      <td>20.107</td>\n",
       "      <td>19.975</td>\n",
       "      <td>19.842</td>\n",
       "      <td>19.710</td>\n",
       "      <td>19.576</td>\n",
       "      <td>19.439</td>\n",
       "      <td>19.297</td>\n",
       "      <td>19.149</td>\n",
       "      <td>18.995</td>\n",
       "      <td>...</td>\n",
       "      <td>7.004</td>\n",
       "      <td>6.992</td>\n",
       "      <td>6.983</td>\n",
       "      <td>6.971</td>\n",
       "      <td>6.959</td>\n",
       "      <td>6.948</td>\n",
       "      <td>6.936</td>\n",
       "      <td>6.924</td>\n",
       "      <td>6.914</td>\n",
       "      <td>6.901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_epoch_1  loss_epoch_2  loss_epoch_3  loss_epoch_4  loss_epoch_5  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000      1000.000   \n",
       "mean         10.206        10.154        10.103        10.053        10.002   \n",
       "std           2.437         2.414         2.391         2.368         2.344   \n",
       "min           5.286         5.268         5.251         5.233         5.215   \n",
       "25%           8.362         8.321         8.275         8.254         8.222   \n",
       "50%           9.874         9.819         9.775         9.719         9.679   \n",
       "75%          11.638        11.579        11.535        11.470        11.422   \n",
       "max          20.239        20.107        19.975        19.842        19.710   \n",
       "\n",
       "       loss_epoch_6  loss_epoch_7  loss_epoch_8  loss_epoch_9  loss_epoch_10  \\\n",
       "count      1000.000      1000.000      1000.000      1000.000       1000.000   \n",
       "mean          9.950         9.898         9.845         9.790          9.734   \n",
       "std           2.321         2.297         2.272         2.247          2.220   \n",
       "min           5.198         5.180         5.162         5.144          5.125   \n",
       "25%           8.182         8.152         8.107         8.066          8.030   \n",
       "50%           9.636         9.596         9.554         9.511          9.477   \n",
       "75%          11.345        11.265        11.186        11.126         11.078   \n",
       "max          19.576        19.439        19.297        19.149         18.995   \n",
       "\n",
       "       ...  loss_epoch_191  loss_epoch_192  loss_epoch_193  loss_epoch_194  \\\n",
       "count  ...        1000.000        1000.000        1000.000        1000.000   \n",
       "mean   ...           4.428           4.419           4.410           4.401   \n",
       "std    ...           0.573           0.573           0.572           0.571   \n",
       "min    ...           2.824           2.818           2.811           2.803   \n",
       "25%    ...           4.040           4.034           4.025           4.017   \n",
       "50%    ...           4.381           4.367           4.362           4.353   \n",
       "75%    ...           4.763           4.756           4.747           4.736   \n",
       "max    ...           7.004           6.992           6.983           6.971   \n",
       "\n",
       "       loss_epoch_195  loss_epoch_196  loss_epoch_197  loss_epoch_198  \\\n",
       "count        1000.000        1000.000        1000.000        1000.000   \n",
       "mean            4.392           4.383           4.375           4.366   \n",
       "std             0.570           0.570           0.569           0.568   \n",
       "min             2.797           2.790           2.783           2.777   \n",
       "25%             4.010           3.998           3.989           3.978   \n",
       "50%             4.342           4.332           4.328           4.323   \n",
       "75%             4.724           4.716           4.707           4.695   \n",
       "max             6.959           6.948           6.936           6.924   \n",
       "\n",
       "       loss_epoch_199  loss_epoch_200  \n",
       "count        1000.000        1000.000  \n",
       "mean            4.357           4.348  \n",
       "std             0.567           0.566  \n",
       "min             2.771           2.767  \n",
       "25%             3.972           3.965  \n",
       "50%             4.313           4.304  \n",
       "75%             4.683           4.671  \n",
       "max             6.914           6.901  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:48:01.847545Z",
     "start_time": "2020-12-19T14:48:01.313761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss_epoch_1</th>\n",
       "      <th>val_loss_epoch_2</th>\n",
       "      <th>val_loss_epoch_3</th>\n",
       "      <th>val_loss_epoch_4</th>\n",
       "      <th>val_loss_epoch_5</th>\n",
       "      <th>val_loss_epoch_6</th>\n",
       "      <th>val_loss_epoch_7</th>\n",
       "      <th>val_loss_epoch_8</th>\n",
       "      <th>val_loss_epoch_9</th>\n",
       "      <th>val_loss_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_loss_epoch_191</th>\n",
       "      <th>val_loss_epoch_192</th>\n",
       "      <th>val_loss_epoch_193</th>\n",
       "      <th>val_loss_epoch_194</th>\n",
       "      <th>val_loss_epoch_195</th>\n",
       "      <th>val_loss_epoch_196</th>\n",
       "      <th>val_loss_epoch_197</th>\n",
       "      <th>val_loss_epoch_198</th>\n",
       "      <th>val_loss_epoch_199</th>\n",
       "      <th>val_loss_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.195</td>\n",
       "      <td>10.144</td>\n",
       "      <td>10.094</td>\n",
       "      <td>10.043</td>\n",
       "      <td>9.993</td>\n",
       "      <td>9.941</td>\n",
       "      <td>9.889</td>\n",
       "      <td>9.836</td>\n",
       "      <td>9.781</td>\n",
       "      <td>9.724</td>\n",
       "      <td>...</td>\n",
       "      <td>4.544</td>\n",
       "      <td>4.536</td>\n",
       "      <td>4.527</td>\n",
       "      <td>4.519</td>\n",
       "      <td>4.510</td>\n",
       "      <td>4.502</td>\n",
       "      <td>4.494</td>\n",
       "      <td>4.485</td>\n",
       "      <td>4.477</td>\n",
       "      <td>4.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.480</td>\n",
       "      <td>2.456</td>\n",
       "      <td>2.433</td>\n",
       "      <td>2.410</td>\n",
       "      <td>2.387</td>\n",
       "      <td>2.363</td>\n",
       "      <td>2.338</td>\n",
       "      <td>2.313</td>\n",
       "      <td>2.287</td>\n",
       "      <td>2.260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.098</td>\n",
       "      <td>5.081</td>\n",
       "      <td>5.065</td>\n",
       "      <td>5.049</td>\n",
       "      <td>5.035</td>\n",
       "      <td>5.020</td>\n",
       "      <td>5.006</td>\n",
       "      <td>4.993</td>\n",
       "      <td>4.980</td>\n",
       "      <td>4.967</td>\n",
       "      <td>...</td>\n",
       "      <td>2.874</td>\n",
       "      <td>2.873</td>\n",
       "      <td>2.871</td>\n",
       "      <td>2.868</td>\n",
       "      <td>2.866</td>\n",
       "      <td>2.865</td>\n",
       "      <td>2.863</td>\n",
       "      <td>2.863</td>\n",
       "      <td>2.862</td>\n",
       "      <td>2.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.407</td>\n",
       "      <td>8.370</td>\n",
       "      <td>8.335</td>\n",
       "      <td>8.300</td>\n",
       "      <td>8.272</td>\n",
       "      <td>8.226</td>\n",
       "      <td>8.167</td>\n",
       "      <td>8.116</td>\n",
       "      <td>8.064</td>\n",
       "      <td>8.030</td>\n",
       "      <td>...</td>\n",
       "      <td>4.071</td>\n",
       "      <td>4.064</td>\n",
       "      <td>4.058</td>\n",
       "      <td>4.049</td>\n",
       "      <td>4.043</td>\n",
       "      <td>4.036</td>\n",
       "      <td>4.029</td>\n",
       "      <td>4.022</td>\n",
       "      <td>4.015</td>\n",
       "      <td>4.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.828</td>\n",
       "      <td>9.787</td>\n",
       "      <td>9.750</td>\n",
       "      <td>9.701</td>\n",
       "      <td>9.664</td>\n",
       "      <td>9.604</td>\n",
       "      <td>9.559</td>\n",
       "      <td>9.510</td>\n",
       "      <td>9.483</td>\n",
       "      <td>9.435</td>\n",
       "      <td>...</td>\n",
       "      <td>4.501</td>\n",
       "      <td>4.490</td>\n",
       "      <td>4.482</td>\n",
       "      <td>4.475</td>\n",
       "      <td>4.467</td>\n",
       "      <td>4.459</td>\n",
       "      <td>4.452</td>\n",
       "      <td>4.442</td>\n",
       "      <td>4.435</td>\n",
       "      <td>4.426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.620</td>\n",
       "      <td>11.585</td>\n",
       "      <td>11.509</td>\n",
       "      <td>11.441</td>\n",
       "      <td>11.390</td>\n",
       "      <td>11.336</td>\n",
       "      <td>11.280</td>\n",
       "      <td>11.222</td>\n",
       "      <td>11.172</td>\n",
       "      <td>11.122</td>\n",
       "      <td>...</td>\n",
       "      <td>4.945</td>\n",
       "      <td>4.935</td>\n",
       "      <td>4.927</td>\n",
       "      <td>4.922</td>\n",
       "      <td>4.914</td>\n",
       "      <td>4.904</td>\n",
       "      <td>4.897</td>\n",
       "      <td>4.888</td>\n",
       "      <td>4.877</td>\n",
       "      <td>4.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.535</td>\n",
       "      <td>19.417</td>\n",
       "      <td>19.298</td>\n",
       "      <td>19.178</td>\n",
       "      <td>19.058</td>\n",
       "      <td>18.934</td>\n",
       "      <td>18.806</td>\n",
       "      <td>18.674</td>\n",
       "      <td>18.539</td>\n",
       "      <td>18.399</td>\n",
       "      <td>...</td>\n",
       "      <td>7.048</td>\n",
       "      <td>7.044</td>\n",
       "      <td>7.039</td>\n",
       "      <td>7.034</td>\n",
       "      <td>7.029</td>\n",
       "      <td>7.025</td>\n",
       "      <td>7.020</td>\n",
       "      <td>7.015</td>\n",
       "      <td>7.010</td>\n",
       "      <td>7.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_loss_epoch_1  val_loss_epoch_2  val_loss_epoch_3  val_loss_epoch_4  \\\n",
       "count          1000.000          1000.000          1000.000          1000.000   \n",
       "mean             10.195            10.144            10.094            10.043   \n",
       "std               2.480             2.456             2.433             2.410   \n",
       "min               5.098             5.081             5.065             5.049   \n",
       "25%               8.407             8.370             8.335             8.300   \n",
       "50%               9.828             9.787             9.750             9.701   \n",
       "75%              11.620            11.585            11.509            11.441   \n",
       "max              19.535            19.417            19.298            19.178   \n",
       "\n",
       "       val_loss_epoch_5  val_loss_epoch_6  val_loss_epoch_7  val_loss_epoch_8  \\\n",
       "count          1000.000          1000.000          1000.000          1000.000   \n",
       "mean              9.993             9.941             9.889             9.836   \n",
       "std               2.387             2.363             2.338             2.313   \n",
       "min               5.035             5.020             5.006             4.993   \n",
       "25%               8.272             8.226             8.167             8.116   \n",
       "50%               9.664             9.604             9.559             9.510   \n",
       "75%              11.390            11.336            11.280            11.222   \n",
       "max              19.058            18.934            18.806            18.674   \n",
       "\n",
       "       val_loss_epoch_9  val_loss_epoch_10  ...  val_loss_epoch_191  \\\n",
       "count          1000.000           1000.000  ...            1000.000   \n",
       "mean              9.781              9.724  ...               4.544   \n",
       "std               2.287              2.260  ...               0.651   \n",
       "min               4.980              4.967  ...               2.874   \n",
       "25%               8.064              8.030  ...               4.071   \n",
       "50%               9.483              9.435  ...               4.501   \n",
       "75%              11.172             11.122  ...               4.945   \n",
       "max              18.539             18.399  ...               7.048   \n",
       "\n",
       "       val_loss_epoch_192  val_loss_epoch_193  val_loss_epoch_194  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                4.536               4.527               4.519   \n",
       "std                 0.651               0.650               0.649   \n",
       "min                 2.873               2.871               2.868   \n",
       "25%                 4.064               4.058               4.049   \n",
       "50%                 4.490               4.482               4.475   \n",
       "75%                 4.935               4.927               4.922   \n",
       "max                 7.044               7.039               7.034   \n",
       "\n",
       "       val_loss_epoch_195  val_loss_epoch_196  val_loss_epoch_197  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean                4.510               4.502               4.494   \n",
       "std                 0.649               0.648               0.647   \n",
       "min                 2.866               2.865               2.863   \n",
       "25%                 4.043               4.036               4.029   \n",
       "50%                 4.467               4.459               4.452   \n",
       "75%                 4.914               4.904               4.897   \n",
       "max                 7.029               7.025               7.020   \n",
       "\n",
       "       val_loss_epoch_198  val_loss_epoch_199  val_loss_epoch_200  \n",
       "count            1000.000            1000.000            1000.000  \n",
       "mean                4.485               4.477               4.469  \n",
       "std                 0.646               0.646               0.645  \n",
       "min                 2.863               2.862               2.860  \n",
       "25%                 4.022               4.015               4.005  \n",
       "50%                 4.442               4.435               4.426  \n",
       "75%                 4.888               4.877               4.866  \n",
       "max                 7.015               7.010               7.006  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:48:02.381987Z",
     "start_time": "2020-12-19T14:48:01.849146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_epoch_1</th>\n",
       "      <th>metric_epoch_2</th>\n",
       "      <th>metric_epoch_3</th>\n",
       "      <th>metric_epoch_4</th>\n",
       "      <th>metric_epoch_5</th>\n",
       "      <th>metric_epoch_6</th>\n",
       "      <th>metric_epoch_7</th>\n",
       "      <th>metric_epoch_8</th>\n",
       "      <th>metric_epoch_9</th>\n",
       "      <th>metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>metric_epoch_191</th>\n",
       "      <th>metric_epoch_192</th>\n",
       "      <th>metric_epoch_193</th>\n",
       "      <th>metric_epoch_194</th>\n",
       "      <th>metric_epoch_195</th>\n",
       "      <th>metric_epoch_196</th>\n",
       "      <th>metric_epoch_197</th>\n",
       "      <th>metric_epoch_198</th>\n",
       "      <th>metric_epoch_199</th>\n",
       "      <th>metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.324</td>\n",
       "      <td>1.315</td>\n",
       "      <td>1.322</td>\n",
       "      <td>1.324</td>\n",
       "      <td>1.333</td>\n",
       "      <td>1.347</td>\n",
       "      <td>1.356</td>\n",
       "      <td>1.373</td>\n",
       "      <td>1.399</td>\n",
       "      <td>1.423</td>\n",
       "      <td>...</td>\n",
       "      <td>3.083</td>\n",
       "      <td>3.041</td>\n",
       "      <td>3.026</td>\n",
       "      <td>3.031</td>\n",
       "      <td>3.040</td>\n",
       "      <td>3.014</td>\n",
       "      <td>3.043</td>\n",
       "      <td>3.026</td>\n",
       "      <td>3.007</td>\n",
       "      <td>2.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.942</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.231</td>\n",
       "      <td>2.330</td>\n",
       "      <td>2.546</td>\n",
       "      <td>2.650</td>\n",
       "      <td>2.824</td>\n",
       "      <td>3.035</td>\n",
       "      <td>3.276</td>\n",
       "      <td>3.522</td>\n",
       "      <td>...</td>\n",
       "      <td>13.896</td>\n",
       "      <td>13.556</td>\n",
       "      <td>13.526</td>\n",
       "      <td>13.391</td>\n",
       "      <td>13.485</td>\n",
       "      <td>13.349</td>\n",
       "      <td>13.586</td>\n",
       "      <td>13.513</td>\n",
       "      <td>13.351</td>\n",
       "      <td>13.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.983</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.053</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.040</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.038</td>\n",
       "      <td>1.037</td>\n",
       "      <td>1.041</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.047</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.210</td>\n",
       "      <td>1.213</td>\n",
       "      <td>1.210</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.208</td>\n",
       "      <td>1.207</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.104</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.097</td>\n",
       "      <td>1.095</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.101</td>\n",
       "      <td>1.103</td>\n",
       "      <td>1.106</td>\n",
       "      <td>1.114</td>\n",
       "      <td>1.125</td>\n",
       "      <td>...</td>\n",
       "      <td>1.626</td>\n",
       "      <td>1.620</td>\n",
       "      <td>1.616</td>\n",
       "      <td>1.612</td>\n",
       "      <td>1.610</td>\n",
       "      <td>1.602</td>\n",
       "      <td>1.614</td>\n",
       "      <td>1.605</td>\n",
       "      <td>1.594</td>\n",
       "      <td>1.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.209</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.205</td>\n",
       "      <td>1.210</td>\n",
       "      <td>1.209</td>\n",
       "      <td>1.218</td>\n",
       "      <td>1.228</td>\n",
       "      <td>1.238</td>\n",
       "      <td>1.252</td>\n",
       "      <td>1.260</td>\n",
       "      <td>...</td>\n",
       "      <td>2.338</td>\n",
       "      <td>2.348</td>\n",
       "      <td>2.345</td>\n",
       "      <td>2.352</td>\n",
       "      <td>2.337</td>\n",
       "      <td>2.339</td>\n",
       "      <td>2.342</td>\n",
       "      <td>2.329</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>54.361</td>\n",
       "      <td>57.582</td>\n",
       "      <td>65.905</td>\n",
       "      <td>69.555</td>\n",
       "      <td>76.949</td>\n",
       "      <td>79.622</td>\n",
       "      <td>85.662</td>\n",
       "      <td>92.663</td>\n",
       "      <td>99.598</td>\n",
       "      <td>107.301</td>\n",
       "      <td>...</td>\n",
       "      <td>381.367</td>\n",
       "      <td>382.486</td>\n",
       "      <td>383.110</td>\n",
       "      <td>377.320</td>\n",
       "      <td>378.292</td>\n",
       "      <td>377.646</td>\n",
       "      <td>380.673</td>\n",
       "      <td>379.608</td>\n",
       "      <td>377.172</td>\n",
       "      <td>377.619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       metric_epoch_1  metric_epoch_2  metric_epoch_3  metric_epoch_4  \\\n",
       "count        1000.000        1000.000        1000.000        1000.000   \n",
       "mean            1.324           1.315           1.322           1.324   \n",
       "std             1.942           2.000           2.231           2.330   \n",
       "min             0.983           0.954           0.941           0.928   \n",
       "25%             1.053           1.047           1.040           1.040   \n",
       "50%             1.104           1.101           1.097           1.095   \n",
       "75%             1.209           1.204           1.205           1.210   \n",
       "max            54.361          57.582          65.905          69.555   \n",
       "\n",
       "       metric_epoch_5  metric_epoch_6  metric_epoch_7  metric_epoch_8  \\\n",
       "count        1000.000        1000.000        1000.000        1000.000   \n",
       "mean            1.333           1.347           1.356           1.373   \n",
       "std             2.546           2.650           2.824           3.035   \n",
       "min             0.920           0.904           0.884           0.861   \n",
       "25%             1.037           1.038           1.037           1.041   \n",
       "50%             1.096           1.101           1.103           1.106   \n",
       "75%             1.209           1.218           1.228           1.238   \n",
       "max            76.949          79.622          85.662          92.663   \n",
       "\n",
       "       metric_epoch_9  metric_epoch_10  ...  metric_epoch_191  \\\n",
       "count        1000.000         1000.000  ...          1000.000   \n",
       "mean            1.399            1.423  ...             3.083   \n",
       "std             3.276            3.522  ...            13.896   \n",
       "min             0.837            0.818  ...             0.411   \n",
       "25%             1.043            1.047  ...             1.214   \n",
       "50%             1.114            1.125  ...             1.626   \n",
       "75%             1.252            1.260  ...             2.338   \n",
       "max            99.598          107.301  ...           381.367   \n",
       "\n",
       "       metric_epoch_192  metric_epoch_193  metric_epoch_194  metric_epoch_195  \\\n",
       "count          1000.000          1000.000          1000.000          1000.000   \n",
       "mean              3.041             3.026             3.031             3.040   \n",
       "std              13.556            13.526            13.391            13.485   \n",
       "min               0.412             0.431             0.417             0.431   \n",
       "25%               1.212             1.210             1.213             1.210   \n",
       "50%               1.620             1.616             1.612             1.610   \n",
       "75%               2.348             2.345             2.352             2.337   \n",
       "max             382.486           383.110           377.320           378.292   \n",
       "\n",
       "       metric_epoch_196  metric_epoch_197  metric_epoch_198  metric_epoch_199  \\\n",
       "count          1000.000          1000.000          1000.000          1000.000   \n",
       "mean              3.014             3.043             3.026             3.007   \n",
       "std              13.349            13.586            13.513            13.351   \n",
       "min               0.414             0.413             0.408             0.426   \n",
       "25%               1.204             1.208             1.207             1.206   \n",
       "50%               1.602             1.614             1.605             1.594   \n",
       "75%               2.339             2.342             2.329             2.325   \n",
       "max             377.646           380.673           379.608           377.172   \n",
       "\n",
       "       metric_epoch_200  \n",
       "count          1000.000  \n",
       "mean              2.990  \n",
       "std              13.318  \n",
       "min               0.412  \n",
       "25%               1.204  \n",
       "50%               1.598  \n",
       "75%               2.326  \n",
       "max             377.619  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:48:02.915099Z",
     "start_time": "2020-12-19T14:48:02.383511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_metric_epoch_1</th>\n",
       "      <th>val_metric_epoch_2</th>\n",
       "      <th>val_metric_epoch_3</th>\n",
       "      <th>val_metric_epoch_4</th>\n",
       "      <th>val_metric_epoch_5</th>\n",
       "      <th>val_metric_epoch_6</th>\n",
       "      <th>val_metric_epoch_7</th>\n",
       "      <th>val_metric_epoch_8</th>\n",
       "      <th>val_metric_epoch_9</th>\n",
       "      <th>val_metric_epoch_10</th>\n",
       "      <th>...</th>\n",
       "      <th>val_metric_epoch_191</th>\n",
       "      <th>val_metric_epoch_192</th>\n",
       "      <th>val_metric_epoch_193</th>\n",
       "      <th>val_metric_epoch_194</th>\n",
       "      <th>val_metric_epoch_195</th>\n",
       "      <th>val_metric_epoch_196</th>\n",
       "      <th>val_metric_epoch_197</th>\n",
       "      <th>val_metric_epoch_198</th>\n",
       "      <th>val_metric_epoch_199</th>\n",
       "      <th>val_metric_epoch_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>1000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.964</td>\n",
       "      <td>68.459</td>\n",
       "      <td>73.213</td>\n",
       "      <td>78.026</td>\n",
       "      <td>82.917</td>\n",
       "      <td>88.110</td>\n",
       "      <td>93.559</td>\n",
       "      <td>99.280</td>\n",
       "      <td>105.212</td>\n",
       "      <td>111.464</td>\n",
       "      <td>...</td>\n",
       "      <td>250.857</td>\n",
       "      <td>251.280</td>\n",
       "      <td>252.050</td>\n",
       "      <td>253.440</td>\n",
       "      <td>254.938</td>\n",
       "      <td>255.439</td>\n",
       "      <td>258.129</td>\n",
       "      <td>259.817</td>\n",
       "      <td>259.189</td>\n",
       "      <td>258.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1982.619</td>\n",
       "      <td>2124.954</td>\n",
       "      <td>2275.411</td>\n",
       "      <td>2427.616</td>\n",
       "      <td>2582.087</td>\n",
       "      <td>2745.950</td>\n",
       "      <td>2917.839</td>\n",
       "      <td>3098.265</td>\n",
       "      <td>3285.308</td>\n",
       "      <td>3482.344</td>\n",
       "      <td>...</td>\n",
       "      <td>7839.615</td>\n",
       "      <td>7853.164</td>\n",
       "      <td>7877.567</td>\n",
       "      <td>7921.834</td>\n",
       "      <td>7969.436</td>\n",
       "      <td>7985.318</td>\n",
       "      <td>8070.694</td>\n",
       "      <td>8124.235</td>\n",
       "      <td>8104.607</td>\n",
       "      <td>8097.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.932</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.022</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.008</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.007</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1.008</td>\n",
       "      <td>...</td>\n",
       "      <td>1.061</td>\n",
       "      <td>1.058</td>\n",
       "      <td>1.056</td>\n",
       "      <td>1.054</td>\n",
       "      <td>1.055</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.048</td>\n",
       "      <td>1.045</td>\n",
       "      <td>1.043</td>\n",
       "      <td>1.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.067</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.059</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.069</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.079</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511</td>\n",
       "      <td>1.510</td>\n",
       "      <td>1.508</td>\n",
       "      <td>1.507</td>\n",
       "      <td>1.506</td>\n",
       "      <td>1.503</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.499</td>\n",
       "      <td>1.496</td>\n",
       "      <td>1.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.159</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.162</td>\n",
       "      <td>1.169</td>\n",
       "      <td>1.173</td>\n",
       "      <td>1.182</td>\n",
       "      <td>1.188</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.219</td>\n",
       "      <td>...</td>\n",
       "      <td>2.200</td>\n",
       "      <td>2.197</td>\n",
       "      <td>2.195</td>\n",
       "      <td>2.192</td>\n",
       "      <td>2.188</td>\n",
       "      <td>2.180</td>\n",
       "      <td>2.176</td>\n",
       "      <td>2.178</td>\n",
       "      <td>2.174</td>\n",
       "      <td>2.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62697.152</td>\n",
       "      <td>67198.164</td>\n",
       "      <td>71956.031</td>\n",
       "      <td>76769.188</td>\n",
       "      <td>81654.008</td>\n",
       "      <td>86835.805</td>\n",
       "      <td>92271.430</td>\n",
       "      <td>97977.023</td>\n",
       "      <td>103891.852</td>\n",
       "      <td>110122.719</td>\n",
       "      <td>...</td>\n",
       "      <td>247913.047</td>\n",
       "      <td>248341.500</td>\n",
       "      <td>249113.172</td>\n",
       "      <td>250513.016</td>\n",
       "      <td>252018.328</td>\n",
       "      <td>252520.562</td>\n",
       "      <td>255220.391</td>\n",
       "      <td>256913.484</td>\n",
       "      <td>256292.812</td>\n",
       "      <td>256062.672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       val_metric_epoch_1  val_metric_epoch_2  val_metric_epoch_3  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               63.964              68.459              73.213   \n",
       "std              1982.619            2124.954            2275.411   \n",
       "min                 0.932               0.929               0.912   \n",
       "25%                 1.022               1.016               1.013   \n",
       "50%                 1.067               1.063               1.059   \n",
       "75%                 1.159               1.162               1.162   \n",
       "max             62697.152           67198.164           71956.031   \n",
       "\n",
       "       val_metric_epoch_4  val_metric_epoch_5  val_metric_epoch_6  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               78.026              82.917              88.110   \n",
       "std              2427.616            2582.087            2745.950   \n",
       "min                 0.889               0.869               0.850   \n",
       "25%                 1.008               1.007               1.007   \n",
       "50%                 1.060               1.063               1.065   \n",
       "75%                 1.169               1.173               1.182   \n",
       "max             76769.188           81654.008           86835.805   \n",
       "\n",
       "       val_metric_epoch_7  val_metric_epoch_8  val_metric_epoch_9  \\\n",
       "count            1000.000            1000.000            1000.000   \n",
       "mean               93.559              99.280             105.212   \n",
       "std              2917.839            3098.265            3285.308   \n",
       "min                 0.831               0.811               0.789   \n",
       "25%                 1.007               1.007               1.006   \n",
       "50%                 1.064               1.069               1.074   \n",
       "75%                 1.188               1.200               1.212   \n",
       "max             92271.430           97977.023          103891.852   \n",
       "\n",
       "       val_metric_epoch_10  ...  val_metric_epoch_191  val_metric_epoch_192  \\\n",
       "count             1000.000  ...              1000.000              1000.000   \n",
       "mean               111.464  ...               250.857               251.280   \n",
       "std               3482.344  ...              7839.615              7853.164   \n",
       "min                  0.766  ...                 0.258                 0.257   \n",
       "25%                  1.008  ...                 1.061                 1.058   \n",
       "50%                  1.079  ...                 1.511                 1.510   \n",
       "75%                  1.219  ...                 2.200                 2.197   \n",
       "max             110122.719  ...            247913.047            248341.500   \n",
       "\n",
       "       val_metric_epoch_193  val_metric_epoch_194  val_metric_epoch_195  \\\n",
       "count              1000.000              1000.000              1000.000   \n",
       "mean                252.050               253.440               254.938   \n",
       "std                7877.567              7921.834              7969.436   \n",
       "min                   0.257                 0.256                 0.256   \n",
       "25%                   1.056                 1.054                 1.055   \n",
       "50%                   1.508                 1.507                 1.506   \n",
       "75%                   2.195                 2.192                 2.188   \n",
       "max              249113.172            250513.016            252018.328   \n",
       "\n",
       "       val_metric_epoch_196  val_metric_epoch_197  val_metric_epoch_198  \\\n",
       "count              1000.000              1000.000              1000.000   \n",
       "mean                255.439               258.129               259.817   \n",
       "std                7985.318              8070.694              8124.235   \n",
       "min                   0.257                 0.256                 0.257   \n",
       "25%                   1.051                 1.048                 1.045   \n",
       "50%                   1.503                 1.500                 1.499   \n",
       "75%                   2.180                 2.176                 2.178   \n",
       "max              252520.562            255220.391            256913.484   \n",
       "\n",
       "       val_metric_epoch_199  val_metric_epoch_200  \n",
       "count              1000.000              1000.000  \n",
       "mean                259.189               258.955  \n",
       "std                8104.607              8097.330  \n",
       "min                   0.255                 0.256  \n",
       "25%                   1.043                 1.040  \n",
       "50%                   1.496                 1.492  \n",
       "75%                   2.174                 2.182  \n",
       "max              256292.812            256062.672  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:48:04.300949Z",
     "start_time": "2020-12-19T14:48:02.916694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/metric_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 0#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True)\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model metric')\n",
    "plt.ylabel('metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:48:05.647938Z",
     "start_time": "2020-12-19T14:48:04.303346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + str(data_size) + '_train_' + str(lambda_dataset_size) + '_variables_' + str(n) + '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_astep_' + str(a_step)  + '_amin_' + str(a_min) + '_amax_' + str(a_max) + '_xstep_' + str(x_step) + '_xmin_' + str(x_min) + '_xmax_' + str(x_max) + training_string + filename + '/loss_' + str(data_size) +  '_degree_' + str(d) + '_sparsity_' + str(sparsity) + '_amax_' + str(a_max) + '_xmax_' + str(x_max) + training_string + '_epoch_' + str(epochs).zfill(3)  + filename + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True)\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True)\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['trainover', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
